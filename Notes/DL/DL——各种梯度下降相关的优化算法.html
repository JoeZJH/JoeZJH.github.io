<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="En/中">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="DL,">










<meta name="description" content="本文从梯度下降(Gradient Descent, GD)开始,讲述深度学习中的各种优化算法（优化器，Optimizer）     参考文章:【干货】深度学习必备：随机梯度下降（SGD）优化算法及可视化   三种梯度下降框架随机梯度下降核心思想 每次从随机从训练集中选择一个训练样本来计算误差,进而更新模型参数 单次迭代时参数移动方向可能不太精确甚至相反,但是最终会收敛 单次迭代的波动也带来了一个好">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="DL——各种梯度下降相关的优化算法">
<meta property="og:url" content="https://JoeZJH.github.io/Notes/DL/DL——各种梯度下降相关的优化算法.html">
<meta property="og:site_name" content="Jiahong的个人博客">
<meta property="og:description" content="本文从梯度下降(Gradient Descent, GD)开始,讲述深度学习中的各种优化算法（优化器，Optimizer）     参考文章:【干货】深度学习必备：随机梯度下降（SGD）优化算法及可视化   三种梯度下降框架随机梯度下降核心思想 每次从随机从训练集中选择一个训练样本来计算误差,进而更新模型参数 单次迭代时参数移动方向可能不太精确甚至相反,但是最终会收敛 单次迭代的波动也带来了一个好">
<meta property="og:locale" content="En/中">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_sgd.png">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_description.png">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/nag.png">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/nag_description.png">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/SGDW-AdamW.png">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/gd_comparations_a.gif">
<meta property="og:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/gd_comparations_b.gif">
<meta property="og:updated_time" content="2024-11-13T16:32:40.405Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DL——各种梯度下降相关的优化算法">
<meta name="twitter:description" content="本文从梯度下降(Gradient Descent, GD)开始,讲述深度学习中的各种优化算法（优化器，Optimizer）     参考文章:【干货】深度学习必备：随机梯度下降（SGD）优化算法及可视化   三种梯度下降框架随机梯度下降核心思想 每次从随机从训练集中选择一个训练样本来计算误差,进而更新模型参数 单次迭代时参数移动方向可能不太精确甚至相反,但是最终会收敛 单次迭代的波动也带来了一个好">
<meta name="twitter:image" content="https://joezjh.github.io/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_sgd.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://JoeZJH.github.io/Notes/DL/DL——各种梯度下降相关的优化算法.html">





  <title>DL——各种梯度下降相关的优化算法 | Jiahong的个人博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="En/中">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiahong的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">凡事预则立，不预则废</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/DL/DL——各种梯度下降相关的优化算法.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DL——各种梯度下降相关的优化算法</h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><em>本文从梯度下降(Gradient Descent, GD)开始,讲述深度学习中的各种优化算法（优化器，Optimizer）</em></p>
<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<p>参考文章:<a href="https://m.sohu.com/a/131923387_473283/?pvid=000115_3w_a" title="https://m.sohu.com/a/131923387_473283/?pvid=000115_3w_a" target="_blank" rel="noopener">【干货】深度学习必备：随机梯度下降（SGD）优化算法及可视化</a> </p>
<hr>
<h3 id="三种梯度下降框架"><a href="#三种梯度下降框架" class="headerlink" title="三种梯度下降框架"></a>三种梯度下降框架</h3><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><h5 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>每次从随机从训练集中选择一个训练样本来计算误差,进而更新模型参数</li>
<li>单次迭代时参数移动方向可能不太精确甚至相反,但是最终会收敛</li>
<li>单次迭代的波动也带来了一个好处,可以到达一个更好的局部最优点,甚至到达全局最优点</li>
</ul>
<h5 id="参数更新公式"><a href="#参数更新公式" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><p><em>Stochastic Gradient Descent, SGD</em></p>
<ul>
<li>公式: \(\theta=\theta-\lambda\frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta}\)</li>
<li>其中:\(L(\theta;x_{i};y_{i})=L(f(\theta;x_{i}),y_{i})\)</li>
</ul>
<h4 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h4><p><em>Batch Gradient Descent, BGD</em></p>
<h5 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>每次使用全量的训练集样本(假设共m个)来计算误差,进而更新模型参数</li>
<li>每次参数能够朝着正确的方向移动</li>
<li>每次遍历所有数据,耗费时间较长</li>
</ul>
<h5 id="参数更新公式-1"><a href="#参数更新公式-1" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>公式: \(\theta=\theta-\lambda\frac{\partial L(\theta;x_{1:m};y_{1:m})}{\partial \theta}\)</li>
<li>一般来说: \(L(\theta;x_{1:m};y_{1:m}) = \frac{1}{m}\sum_{i=1}^{m} L(\theta;x_{i};y_{i})\)</li>
</ul>
<h4 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h4><h5 id="核心思想-2"><a href="#核心思想-2" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>每次从随机从训练集中选择k(k &lt; m)个训练样本来计算误差,进而更新模型参数</li>
<li>介于SGD和BGD之间<ul>
<li>波动小</li>
<li>内存占用也相对较小</li>
</ul>
</li>
</ul>
<h5 id="参数更新公式-2"><a href="#参数更新公式-2" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><p><em>Mini-Batch Gradient Descent, MBGD</em></p>
<ul>
<li>公式: \(\theta=\theta-\lambda\frac{\partial L(\theta;x_{i:i+k};y_{i:i+k})}{\partial \theta}\)</li>
<li>一般来说: \(L(\theta;x_{1:k};y_{1:k}) = \frac{1}{k}\sum_{i=1}^{k} L(\theta;x_{i};y_{i})\)</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>梯度下降算法应用广泛,算法效果很好</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><h6 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h6><ul>
<li>大小很难确定,太大容易震荡,太小则收敛太慢</li>
<li>学习速率一般为定值,有时候会实现为逐步衰减</li>
<li>但是无论如何,都需要事前固定一个值,因此无法自适应不同的数据集特点</li>
</ul>
<h6 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h6><ul>
<li>对于非凸的目标函数,容易陷入局部极值点中</li>
<li>比局部极值点更严重的问题:有时候会嵌入鞍点?</li>
</ul>
<hr>
<h3 id="SD算法的优化"><a href="#SD算法的优化" class="headerlink" title="SD算法的优化"></a>SD算法的优化</h3><h4 id="Momentum法-动量法"><a href="#Momentum法-动量法" class="headerlink" title="Momentum法(动量法)"></a>Momentum法(动量法)</h4><h5 id="核心思想-3"><a href="#核心思想-3" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>考虑一种情况,在峡谷地区(某些方向比另一些方向陡峭很多)<ul>
<li>SGD(或者MBGD,实际上,SGD是特殊的MBGD,平时可以认为这两者是相同的东西)会在这些放附近振荡,从而导致收敛速度变慢</li>
<li>这里最好的例子是鞍点,鞍点出的形状像一个马鞍,一个方向两头上翘,一个方向两头下垂,当上翘的方向比下垂的方向陡峭很多时,SDG和MDG等方法容易在上翘方向上震荡</li>
</ul>
</li>
<li>此时动量可以使得<ul>
<li>当前梯度方向与上一次梯度方向相同的地方进行加强,从而加快收敛速度</li>
<li>当前梯度方向与上一次梯度方向不同的地方进行削减,从而减少振荡</li>
</ul>
</li>
<li>动量可以理解为一个从山顶滚下的小球,遇到新的力(当前梯度)时,会结合之前的梯度方向决定接下来的运动方向</li>
</ul>
<h5 id="参数更新公式-3"><a href="#参数更新公式-3" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>公式: \(\theta=\theta-m_{t}\)<ul>
<li>\(m_{t}\)表示当前下降方向, \(m_{t-1}\)表示上一次的下降方向</li>
<li>\(m_{t}=\gamma m_{t-1}+\lambda\frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta}\)</li>
<li>\(\gamma&lt;1\),值一般取0.9</li>
<li>\(\gamma m_{t-1}\)是动量项</li>
<li>\(\gamma\)是衰减量</li>
<li>\(\lambda\)是学习率</li>
</ul>
</li>
</ul>
<h5 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h5><img src="/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_sgd.png" title="momentum_sgd.png">
<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_description.png" title="momentum_description.png">
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><ul>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新v和参数\(\theta\)</li>
</ul>
</li>
</ul>
<h4 id="NAG-涅斯捷罗夫梯度加速法"><a href="#NAG-涅斯捷罗夫梯度加速法" class="headerlink" title="NAG,涅斯捷罗夫梯度加速法"></a>NAG,涅斯捷罗夫梯度加速法</h4><p><em>Nesterov Accelerated Gradient,NAG</em></p>
<h5 id="核心思想-4"><a href="#核心思想-4" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>继续考虑普通的SDG算法,添加了Momentum,此时从山顶滚下的球会盲目的选择斜坡</li>
<li>更好的方式是在遇到向上的斜坡时减慢速度</li>
<li>NAG在计算梯度时首先获取(近似获得)未来的参数而不是当前参数,然后计算未来参数对应的损失函数的梯度</li>
<li>NAG在预测了未来的梯度后,根据<strong>未来</strong>(\(\theta - \gamma m_{t-1}\))梯度方向和之前梯度的方向决定当前的方向, 这样可以保证在遇到下一点为上升斜坡时适当减慢当前点的速度(否则可能由于惯性走上斜坡, 提前知道\(\theta - \gamma m_{t-1}\)处的梯度, 从而保证不要走上去), 从而找到了比Momentum超前的更新方向</li>
<li>对比: Momentum是根据<strong>当前</strong>梯度方向和之前梯度方向决定当前的方向</li>
</ul>
<h5 id="参数更新公式-4"><a href="#参数更新公式-4" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>公式: \(\theta=\theta-m_{t}\)<ul>
<li>\(m_{t}=\gamma m_{t-1}+\lambda\frac{\partial L(\theta - \gamma v_{t-1};x_{i};y_{i})}{\partial \theta}\)</li>
<li><strong>NAG</strong>使用的是<strong>未来</strong>的梯度方向(<strong>Momentum</strong>使用的是<strong>当前</strong>梯度方向)和之前的梯度方向</li>
</ul>
</li>
</ul>
<h5 id="图示-1"><a href="#图示-1" class="headerlink" title="图示"></a>图示</h5><ul>
<li>Momentum(动量)法首先计算当前的梯度值(小蓝色向量)，然后在更新的积累向量（大蓝色向量）方向前进一大步</li>
<li>NAG 法则首先(试探性地)在之前积累的梯度方向(棕色向量)前进一大步，再根据当前地情况修正，以得到最终的前进方向(绿色向量)</li>
<li>这种基于预测的更新方法，使我们避免过快地前进，并提高了算法地响应能力(responsiveness)，大大改进了 RNN 在一些任务上的表现<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/nag.png" title="nag.png">
<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/nag_description.png" title="nag_description.png">
<ul>
<li>公式中\(-\gamma m_{t-1}\)对应BC向量</li>
<li>\(\theta-\gamma m_{t-1}\)就对应C点(参数)</li>
</ul>
</li>
</ul>
<h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><ul>
<li>Momentum和NAG法可以使得参数更新过程中根据随时函数的斜率自适应的学习,从而加速SGD的收敛</li>
<li>实际应用中,NAG将比Momentum收敛快很多</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新v和参数\(\theta\)</li>
</ul>
</li>
</ul>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><h5 id="核心思想-5"><a href="#核心思想-5" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>对于较少出现的特征,使用较大的学习率更新,即对低频的参数给予更大的更新</li>
<li>对于较多出现的特征,使用较小的学习率更新,即对高频的参数给予更小的更新</li>
<li>很适合处理稀疏数据</li>
</ul>
<h5 id="参数更新公式-5"><a href="#参数更新公式-5" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>计算梯度<ul>
<li>分量形式: \(g_{t,k} = \frac{\partial L(\theta;x_{i};y_{i})}{\theta}|_{\theta = \theta_{t-1,k}}\)<ul>
<li>\(g_{t,k}\)是指第t次迭代时第k个参数\(\theta_{t-1, k}\)的梯度</li>
<li>有些地方会这样表达: \(g_{t,k} = \frac{\partial L(\theta_{t-1,k};x_{i};y_{i})}{\theta_{t-1,k}}\)<ul>
<li>式子中使用\(\theta_{t-1, k}\)在梯度中,事实上不够严谨, 容易让人误解分子分母都不是函数,而是一个确定的值, 事实上我们是先求了导数然后再带入 \(\theta = \theta_{t-1}\) 的</li>
</ul>
</li>
</ul>
</li>
<li>向量形式: \(g_{t} = \frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta}|_{\theta=\theta_{t-1}}\)</li>
</ul>
</li>
<li>此时普通的SGD如下更新参数<ul>
<li>分量形式:\(\theta_{t,k} = \theta_{t-1,k} - \lambda g_{t,k}\)</li>
<li>向量形式:\(\theta_{t} = \theta_{t-1} - \lambda g_{t}\)</li>
</ul>
</li>
<li>而Adagrad对学习率\(\lambda\)根据不同参数进行了修正<ul>
<li>分量形式:\(\theta_{t,k} = \theta_{t-1,k} - \frac{\lambda}{\sqrt{G_{t,kk}+\epsilon}} g_{t,k}\)<ul>
<li>\(G_{t,kk}=\sum_{r=1}^{t}(g_{r,k})^{2}\)</li>
</ul>
</li>
<li>向量形式:\(\theta_{t} = \theta_{t-1} - \frac{\lambda}{\sqrt{G_{t}+\epsilon}}\bigodot g_{t}\)<ul>
<li>\(G_{t}=\sum_{r=1}^{t}g_{r}\bigodot g_{r}\)</li>
<li>\(\bigodot\)表示按照对角线上的值与对应梯度相乘</li>
<li>进一步可以简化写为: \(G_t = G_{t-1} + g_t^2\)<ul>
<li>注意: 这里\(g_t^2\)是指向量按照维度分别相乘, 计算后还是原始向量维度</li>
</ul>
</li>
</ul>
</li>
<li>G是一个对角矩阵,对角线上的元素(\(G_{k,k}\))是从一开始到k次迭代目标函数对于参数(\(\theta_{k}\))的梯度的平方和<ul>
<li>G的累计效果保证了出现次数多的参数(\(\theta_{k}\))对应的对角线上的元素(\(G_{k,k}\))大,从而得到更小的更新</li>
</ul>
</li>
<li>\(\epsilon\)是一个平滑项,用于防止分母为0</li>
</ul>
</li>
<li>总结参数更新公式:<ul>
<li>\(\theta_{t} = \theta_{t-1} - \frac{\lambda}{\sqrt{G_{t}+\epsilon}} g_{t}\)</li>
<li>\(g_{t} = \frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta }|_{\theta = \theta_{t-1}}\)</li>
<li>\(G_t = G_{t-1} + g_t^2\)</li>
</ul>
</li>
</ul>
<h5 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h5><ul>
<li>在分母上<strong>累计了平方梯度和</strong>,造成训练过程中<strong>G的对角线元素越来越大</strong>,最终导致<strong>学习率非常小</strong>,甚至是无限小的值,从而<strong>学不到东西</strong></li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新G的每个元素,再根据G以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><h5 id="核心思想-6"><a href="#核心思想-6" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>是Adagrad的一个扩展,目标是解决Adagrad学习率单调下降的问题</li>
<li>解决方案:只累计一段时间内的平方梯度值?</li>
<li>实际上实现是累加时给前面的平方梯度和一个衰减值</li>
<li>方法名delta的来源是选取部分</li>
</ul>
<h5 id="参数更新公式-6"><a href="#参数更新公式-6" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>将矩阵G的每一项变成当前梯度平方加上过去梯度平方的衰减值(指数衰减)即可<ul>
<li>指数衰减:前n-1项的系数是衰减率的n-1次方</li>
<li>实现指数衰减</li>
<li>在Adagrad的基础上修改为: \(G_t = \gamma G_{t-1} + (1-\gamma)g_t^2\)<ul>
<li>注意: 这里\(g_t^2\)是指向量按照维度分别相乘, 计算后还是原始向量维度</li>
</ul>
</li>
<li>我们通常也把 \(G_t\) 表达为 \(E[g^2]_t\)<ul>
<li>因为修改后的 \(G_t\)可以视为于对 \(g_t^2\) 求期望(不同的\(t\)概率权重不一样的分布的期望)</li>
<li>进一步表达为: \(E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h5><ul>
<li>经过衰减后,G的每一项(忽略掉平滑项\(\epsilon\))相当于有权重的梯度均方差(Root Mean Square, RMS),后面RMSprop算法就用了这个RMS来命名<ul>
<li>均方根的定义是:对所有数求平方和,取平均值(每一项的权重根据概率分布可以不同),再开方</li>
</ul>
</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新G的每个元素,再根据G以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p><em>Root Mean Square prop</em></p>
<h5 id="核心思想-7"><a href="#核心思想-7" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>一种适应性学习率方法,至今未公开发表</li>
<li>是Adagrad的一个扩展,目标也是解决Adagrad学习率单调下降的问题</li>
<li>RMS的来源是由于分母相当于(忽略掉平滑项\(\epsilon\))是梯度的均方根(Root Mean Squared, RMS)</li>
</ul>
<h5 id="参数更新公式-7"><a href="#参数更新公式-7" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>参见Adadelta</li>
<li>RMSprop的本质是对Adadelta简单的取之前值和当前值的权重为0.9和0.1实现指数加权平均, 即 \(\gamma = 0.9\)</li>
<li>有些地方也说RMSprop权重取的是0.5和0.5实现指数加权平均即 \(\gamma = 0.5\)</li>
<li>学习率\(\lambda\)一般取值为0.001</li>
</ul>
<h5 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h5><ul>
<li><strong>RMSprop是Adadelta的一种特殊形式</strong></li>
<li>Adagrad的分母不能算是均方差(即使忽略平滑项\(\epsilon\)),因为这里没有取平均值的操作</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新G的每个元素,再根据G以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p><em>Adaptive Moment Estimation</em></p>
<h5 id="核心思想-8"><a href="#核心思想-8" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>一种适应性学习率方法,相当于 <strong>RMSprop + Momentum + Bias Correction</strong></li>
<li>像Adadelta和RMSprop一样存储了梯度的平方的指数衰减平均值</li>
<li>像Momentum一样保持了过去梯度的指数衰减平均值</li>
<li>Bias Correction是为了得到期望的<strong>无偏估计</strong></li>
</ul>
<h5 id="参数更新公式-8"><a href="#参数更新公式-8" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>\(\theta_{t} = \theta_{t-1} - \frac{\lambda}{\sqrt{\tilde{v}_t+\epsilon}} \tilde{m}_t\)</li>
<li>\(\tilde{v}_t=\frac{v_{t}}{1-\beta_{1}^{t}}\)</li>
<li>\(\tilde{m}_t=\frac{m_{t}}{1-\beta_{2}^{t}}\)</li>
<li>\(\lambda\)是外层学习率，实际使用中，常常可以通过指数衰减、固定步长衰减、余弦退火衰减等学习率衰减策略更新</li>
<li>梯度的指数衰减:\(m_{t} = \beta_{2}m_{t-1}+(1-\beta_{2})g_{t}\)</li>
<li>梯度平方的指数衰减:\(v_{t} = \beta_{1}v_{t-1}+(1-\beta_{1})g_{t}^{2}\)<ul>
<li>\(m_t\) 和 \(v_t\) 也叫作一阶动量和二阶动量，是对梯度一阶矩估计和二阶矩估计<ul>
<li>数学定义：随机变量的一阶矩是随机变量的期望\(E[X]\)，二阶矩是随机变量的方差\(E[X-E[X]]\)</li>
<li>其实梯度平方的期望不是梯度的方差，这只是一种近似，数学上，随机变量\(X\)二阶矩等价于方差，是\(E[(X-E[X])^2] = E[X^2]-E[X]^2\)，当\(E[X]=0\)时，\(E[X^2]\)就是方差</li>
<li>这种滑动平均之所以能代表期望，是因为滑动平均的思想是一种折扣平均，确实可以用来作为期望和方差的估计</li>
</ul>
</li>
<li>\(m_t\) 和 \(v_t\) 可以看做是对 \(E[g]_t\) 和 \(E[g^2]_t\) 的估计</li>
<li>\(\tilde{m}_t\) 和 \(\tilde{v}_t\) 是对 \(m_t\) 和 \(v_t\) 的 <strong>Bias Correction</strong>, 这样可以近似为对对期望 \(E[g]_t\) 和 \(E[g^2]_t\) 的<strong>无偏估计</strong><ul>
<li>注意：修正项\(\tilde{v}_t=\frac{v_{t}}{1-\beta_{1}^{t}}\)中的\(\beta_{1}^{t}\)是\(\beta_{1}\)的\(t\)次方的意思，基本思路可以理解为在每一步都尽量将梯度修正到\(t=0\)大小</li>
<li>进行修正的原因是当\(t\)较小时，\(v_t\)也较小，而\(\beta\)一般较大（0.9或者0.999），此时加权平均的结果也会很小，当\(t\)很大时，实际上可以不用修正了，个人理解：应该可以不用修正，只是前期训练时更新速度比较慢而已</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h5><ul>
<li>超参数设定推荐<ul>
<li>梯度平方衰减率:\(\beta_{1}=0.999\)</li>
<li>梯度动量衰减率:\(\beta_{2}=0.9\)</li>
<li>平滑项:\(\epsilon=10e^-8=1*10^{-8}\)</li>
<li>一阶动量\(v\),初始化为0</li>
<li>二阶动量\(m\),初始化为0</li>
</ul>
</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新\(v\)和\(m\),再根据\(v\)和\(m\)以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h4><p><em>Adam with Weight decay是Adam的一种优化</em></p>
<h5 id="Adam中的L2正则"><a href="#Adam中的L2正则" class="headerlink" title="Adam中的L2正则"></a>Adam中的L2正则</h5><ul>
<li><p>一般的L2正则<br>$$<br>Loss(w) = f(w) + \frac{1}{2}\eta||w||^2<br>$$</p>
</li>
<li><p>权重衰减后的参数更新如下<br>$$<br>\begin{align}<br>w &amp;= w - \alpha\nabla Loss(w) \\<br>&amp;= w - \alpha (\nabla f(w) + \eta w) \\<br>&amp;= w - \alpha \nabla f(w) - \alpha \eta w \\<br>\end{align}<br>$$</p>
</li>
<li><p>由于L2正则化项的存在，每次权重更新时都会减去一定比例的权重，即 \(\alpha \eta w \) ，这种现象叫做权重衰减（L2正则的目标就是让权重往小的方向更新，所以L2正则也叫作权重衰减）</p>
</li>
<li><p>L2正则也称为权重衰减，所以Adam优化的损失函数中添加L2正则的目标本应该也是为了权重衰减</p>
</li>
<li><p>Adam中的L2正则</p>
<ul>
<li>在每次求损失函数梯度前都计算\(\nabla Loss(w) = \nabla f(w) + \eta w\)</li>
<li>由于L2正则项的梯度\(\eta w\)也会被累加到一阶动量和二阶动量中，带有L2的Adam不再是简单的权重衰减，L2正则项还会影响到其他值的更新</li>
<li>Adam中的L2正则会产生我们不期望的结果，因为此时L2正则项影响了Adam参数的正常更新（我们想要L2做的仅仅是权重衰减，但在Adam中，L2产生了别的影响，这个不是我们想要的）</li>
</ul>
</li>
</ul>
<h5 id="AdamW——Adam-权重衰减"><a href="#AdamW——Adam-权重衰减" class="headerlink" title="AdamW——Adam+权重衰减"></a>AdamW——Adam+权重衰减</h5><ul>
<li>AdamW则不直接将L2添加到损失函数中，而是显示的把权重衰减提出来，主要修改是下面两步<ul>
<li>在计算梯度时，将L2正则从损失函数中去除</li>
<li>在更新参数时，显示增加权重衰减项</li>
</ul>
</li>
<li>相当于在更新参数时增加了L2正则，但是计算梯度时没有L2正则</li>
<li>原始论文：<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="noopener">Decoupled Weight Decay Regularization</a><img src="/Notes/DL/DL——各种梯度下降相关的优化算法/SGDW-AdamW.png">
<ul>
<li>图中紫色是原始Adam+L2实现部分，在AdamW中会被去除；</li>
<li>绿色是AdamW中新增的权重衰减部分（相当于更新参数时增加了L2正则项）</li>
</ul>
</li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/643452086" target="_blank" rel="noopener">Adam和AdamW</a>，<a href="https://zhuanlan.zhihu.com/p/653605711" target="_blank" rel="noopener">从梯度下降到AdamW一文读懂机器学习优化算法</a></li>
<li>目前大模型常用的就是AdamW</li>
</ul>
<hr>
<h3 id="优化器与内存-显存"><a href="#优化器与内存-显存" class="headerlink" title="优化器与内存/显存"></a>优化器与内存/显存</h3><ul>
<li>训练的过程中，需要的内存/显存大小与优化器（Optimizer）有关<ul>
<li>需要存储到内存的变量包括以下几个方面<ul>
<li>梯度</li>
<li>参数</li>
<li>优化器状态（Optimizer States)，普通SGD没有这一项，而Adam和AdamW则需要存储一阶动量和二阶动量</li>
</ul>
</li>
</ul>
</li>
<li>优化器、参数量、内存/显存消耗、混合精度训练相关概念可参考<a href="https://arxiv.org/pdf/1910.02054.pdf" target="_blank" rel="noopener">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><ul>
<li>有些论文中也会直接将二阶动量叫做方差(Variance)或者二阶矩，因为二阶动量可以近似方差（当期望为0时）</li>
</ul>
</li>
<li>ZeRO论文中指出，在混合精度训练 + Adam/AdamW时，需要存储的变量包括<ul>
<li>FP16的参数</li>
<li>FP16的梯度</li>
<li>FP32的参数</li>
<li>FP32的一阶动量</li>
<li>FP32的二阶动量</li>
<li>注意：动量不能使用FP16吗？是的，不能，因为为了精度考虑使用时还是要被转换到FP32</li>
</ul>
</li>
</ul>
<hr>
<h3 id="各种优化方法的比较"><a href="#各种优化方法的比较" class="headerlink" title="各种优化方法的比较"></a>各种优化方法的比较</h3><h4 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h4><ul>
<li>SGD optimization on saddle point<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/gd_comparations_a.gif" title="gd_comparations_a.gif">

</li>
</ul>
<h4 id="等高线表面"><a href="#等高线表面" class="headerlink" title="等高线表面"></a>等高线表面</h4><ul>
<li><p>SGD optimization on loss surface contours</p>
<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/gd_comparations_b.gif" title="gd_comparations_b.gif">
</li>
<li><p>上面两种情况都可以看出，Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到</p>
</li>
<li><p>由图可知自适应学习率方法即 Adagrad, Adadelta, RMSprop, Adam 在这种情景下会更合适而且收敛性更好</p>
</li>
</ul>
<h4 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h4><ul>
<li>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam<ul>
<li>因为他们能够为出现更新次数少(确切的说是梯度累计结果小)的特征分配更高的权重</li>
</ul>
</li>
<li>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的</li>
<li>Adam 可解释为 <strong>RMSprop + Momentum + Bias Correction</strong></li>
<li>随着梯度变的稀疏，Adam 比 RMSprop 效果会好</li>
<li><strong>整体来讲，Adam 是最好的选择</strong></li>
<li>很多论文里都会用 SGD，没有 momentum 等, SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点, 在不正确的方向上来回震荡</li>
<li>如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Notes/DL/DL——特殊函数的反向传播.html" rel="next" title="DL——特殊函数的反向传播">
                <i class="fa fa-chevron-left"></i> DL——特殊函数的反向传播
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Notes/DL/DL——关于参数的初始化.html" rel="prev" title="DL——关于参数的初始化">
                DL——关于参数的初始化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/jiahong-head.png" alt="Joe Zhou">
            
              <p class="site-author-name" itemprop="name">Joe Zhou</p>
              <p class="site-description motion-element" itemprop="description">本博客主要用于记录个人学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">285</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JoeZJH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="JoeZJiahong@Foxmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#三种梯度下降框架"><span class="nav-number">1.</span> <span class="nav-text">三种梯度下降框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度下降"><span class="nav-number">1.1.</span> <span class="nav-text">随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想"><span class="nav-number">1.1.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式"><span class="nav-number">1.1.2.</span> <span class="nav-text">参数更新公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#批量梯度下降"><span class="nav-number">1.2.</span> <span class="nav-text">批量梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-1"><span class="nav-number">1.2.2.</span> <span class="nav-text">参数更新公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#小批量梯度下降"><span class="nav-number">1.3.</span> <span class="nav-text">小批量梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-2"><span class="nav-number">1.3.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-2"><span class="nav-number">1.3.2.</span> <span class="nav-text">参数更新公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">1.4.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#优点"><span class="nav-number">1.4.1.</span> <span class="nav-text">优点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#缺点"><span class="nav-number">1.4.2.</span> <span class="nav-text">缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#学习速率"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">学习速率</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#局部最优"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">局部最优</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SD算法的优化"><span class="nav-number">2.</span> <span class="nav-text">SD算法的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Momentum法-动量法"><span class="nav-number">2.1.</span> <span class="nav-text">Momentum法(动量法)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-3"><span class="nav-number">2.1.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-3"><span class="nav-number">2.1.2.</span> <span class="nav-text">参数更新公式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#图示"><span class="nav-number">2.1.3.</span> <span class="nav-text">图示</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小结"><span class="nav-number">2.1.4.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAG-涅斯捷罗夫梯度加速法"><span class="nav-number">2.2.</span> <span class="nav-text">NAG,涅斯捷罗夫梯度加速法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-4"><span class="nav-number">2.2.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-4"><span class="nav-number">2.2.2.</span> <span class="nav-text">参数更新公式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#图示-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">图示</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小结-1"><span class="nav-number">2.2.4.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adagrad"><span class="nav-number">2.3.</span> <span class="nav-text">Adagrad</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-5"><span class="nav-number">2.3.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-5"><span class="nav-number">2.3.2.</span> <span class="nav-text">参数更新公式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小结-2"><span class="nav-number">2.3.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adadelta"><span class="nav-number">2.4.</span> <span class="nav-text">Adadelta</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-6"><span class="nav-number">2.4.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-6"><span class="nav-number">2.4.2.</span> <span class="nav-text">参数更新公式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小结-3"><span class="nav-number">2.4.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSprop"><span class="nav-number">2.5.</span> <span class="nav-text">RMSprop</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-7"><span class="nav-number">2.5.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-7"><span class="nav-number">2.5.2.</span> <span class="nav-text">参数更新公式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小结-4"><span class="nav-number">2.5.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-number">2.6.</span> <span class="nav-text">Adam</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#核心思想-8"><span class="nav-number">2.6.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数更新公式-8"><span class="nav-number">2.6.2.</span> <span class="nav-text">参数更新公式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#小结-5"><span class="nav-number">2.6.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdamW"><span class="nav-number">2.7.</span> <span class="nav-text">AdamW</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Adam中的L2正则"><span class="nav-number">2.7.1.</span> <span class="nav-text">Adam中的L2正则</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AdamW——Adam-权重衰减"><span class="nav-number">2.7.2.</span> <span class="nav-text">AdamW——Adam+权重衰减</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化器与内存-显存"><span class="nav-number">3.</span> <span class="nav-text">优化器与内存/显存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#各种优化方法的比较"><span class="nav-number">4.</span> <span class="nav-text">各种优化方法的比较</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#鞍点"><span class="nav-number">4.1.</span> <span class="nav-text">鞍点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#等高线表面"><span class="nav-number">4.2.</span> <span class="nav-text">等高线表面</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何选择"><span class="nav-number">4.3.</span> <span class="nav-text">如何选择</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joe Zhou</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
