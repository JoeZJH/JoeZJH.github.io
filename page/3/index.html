<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="En/中">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="本博客主要用于记录个人学习笔记">
<meta name="keywords" content="Python, Computer, ML, Linux, Ubuntu, NLP, Git, DL,">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiahong的个人博客">
<meta property="og:url" content="https://JoeZJH.github.io/page/3/index.html">
<meta property="og:site_name" content="Jiahong的个人博客">
<meta property="og:description" content="本博客主要用于记录个人学习笔记">
<meta property="og:locale" content="En/中">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jiahong的个人博客">
<meta name="twitter:description" content="本博客主要用于记录个人学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://JoeZJH.github.io/page/3/">





  <title>Jiahong的个人博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="En/中">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiahong的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">凡事预则立，不预则废</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/DL/DL——Transformer.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/DL/DL——Transformer.html" itemprop="url">DL——Transformer</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文主要介绍Transformer和Attention相关内容</em></p>
<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<ul>
<li>由于LaTex中矩阵的黑体表示过于复杂，在不会引起混淆的情况下，本文中有些地方会被简写为非黑体</li>
</ul>
<hr>
<h3 id="相关论文介绍"><a href="#相关论文介绍" class="headerlink" title="相关论文介绍"></a>相关论文介绍</h3><ul>
<li>Transformer原始文章: <ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a></li>
<li>文章中介绍了一种应用Attention机制的新型特征提取器,命名为Transformer, 实验证明Transformer优于RNN(LSTM),CNN等常规的特征提取器</li>
</ul>
</li>
<li>Transformer的使用: <ul>
<li>GPT: <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li>
<li>BERT: <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li>以上两个工作都使用了Transformer作为特征提取器, 使用两阶段训练的方式实现迁移学习(Pre-Training and Fine-Training)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="相关博客介绍"><a href="#相关博客介绍" class="headerlink" title="相关博客介绍"></a>相关博客介绍</h3><ul>
<li>强烈推荐看看jalammar的博客: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">illustrated-transformer</a></li>
<li>另一篇不错的Attention和Transformer讲解<a href="https://www.sohu.com/a/226596189_500659" target="_blank" rel="noopener">自然语言处理中的自注意力机制(Self-Attention Mechanism)</a></li>
<li>一篇很多个人理解的博客 <a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读</a></li>
</ul>
<hr>
<h3 id="Transformer讲解"><a href="#Transformer讲解" class="headerlink" title="Transformer讲解"></a>Transformer讲解</h3><ul>
<li>最直观的动态图理解<img src="/Notes/DL/DL——Transformer/transform_dynamic.gif"></li>
<li>本文讲解主要按照<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a>的思路走,该论文的亮点在于:<ul>
<li>不同于以往主流机器翻译使用基于 RNN 的 Seq2Seq 模型框架，该论文用 <strong>Attention 机制代替了 RNN</strong> 搭建了整个模型框架, 这是一个从换自行车零件到把自行车换成汽车的突破</li>
<li>提出了<strong>多头注意力</strong>(Multi-Head Attention)机制方法，在编码器和解码器中大量的使用了多头自注意力机制(Multi-Head self-attention)</li>
<li>在WMT2014语料库的英德和英法语言翻译任务上取得了先进结果</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Transformer是什么"><a href="#Transformer是什么" class="headerlink" title="Transformer是什么?"></a>Transformer是什么?</h3><ul>
<li>本质上是个序列转换器 <img src="/Notes/DL/DL——Transformer/the_transformer_high_level.png"></li>
<li>进一步讲,是个 Encoder-Decoder 模型的序列转换器<img src="/Notes/DL/DL——Transformer/The_transformer_encoders_decoders.png"></li>
<li>更进一步的讲,是个 6层Encoder + 6层Decoder 结构的序列转换器<img src="/Notes/DL/DL——Transformer/The_transformer_encoder_decoder_stack.png"></li>
<li>上面的图中,每个 Encoder 是<img src="/Notes/DL/DL——Transformer/Transformer_encoder.png"></li>
<li>详细的讲, 每个Encoder是<img src="/Notes/DL/DL——Transformer/encoder_with_tensors.png"></li>
<li>展开看里面 Encoder 中的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm.png"></li>
<li>更进一步的展开看 Encoder 中的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm_2.png"></li>
<li>两层 Encoder + 两层Decoder (其中一个Decoder没有完全画出来) 的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm_3.png"></li>
<li>带细节动图查看数据流向<img src="/Notes/DL/DL——Transformer/transformer_decoding_1.gif">
</li>
<li>最后,我们给出Transformer的结构图(来自原文中)<img src="/Notes/DL/DL——Transformer/Transfomer_Architecture.png">


</li>
</ul>
<hr>
<h3 id="Transformer中的Attention"><a href="#Transformer中的Attention" class="headerlink" title="Transformer中的Attention"></a>Transformer中的Attention</h3><p><em>Transformer中使用了 Multi-Head Attention, 同时也是一种 Self Attention</em></p>
<ul>
<li>由于Transformer的Multi_Head Attention中 <strong>Query == Key == Query</strong>, 所以也是一种 <strong>Self Attention</strong><ul>
<li>即$$\boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
</li>
<li>更多关于广义Attention的理解请参考: <a href="/Notes/DL/DL%E2%80%94%E2%80%94Attention.html">DL——Attention</a></li>
</ul>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul>
<li>Muti-Head Attention，也称为多头Attention，由 \(h\) 个 Scaled Dot-Product Attention和其他线性层和Concat操作等组成<img src="/Notes/DL/DL——Transformer/Scaled_dot_product_attention_and_Multi_head_attention.png">
<ul>
<li>Scaled Dot Product Attention中Mask操作是可选的</li>
<li>Scaled Dot Product Attention数学定义为(没有Mask操作)<br>$$<br>\begin{align}<br>Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}<br>\end{align}<br>$$<ul>
<li>Softmax前除以\(\sqrt{d_k}\)的原因是防止梯度消失问题，基本思想是（原始论文脚注中有提到）：假设\(\boldsymbol{Q},\boldsymbol{K}\)中每个元素是服从均值为0，方差为1的正太分布（\(\sim N(0,1)\)），那么他们任意取两个列向量\(\boldsymbol{q}_i,\boldsymbol{k}_i\)的内积服从均值为0，方差为\(d_k\)的正太分布（\(\sim N(0,d_k)\)），具体证明可参考<a href="https://zhuanlan.zhihu.com/p/584569220" target="_blank" rel="noopener">没有比这更详细的推导 attention为什么除以根号dk——深入理解Bert系列文章</a>，过大的方差会导致softmax后梯度消失</li>
</ul>
</li>
<li>Multi-Head Attention的某个输出的数学定义为<br>$$<br>\begin{align}<br>MultiHead(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) &amp;= Concat(head_1,\dots,head_h)\boldsymbol{W}^{O} \\<br>where \quad head_i &amp;= Attention(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)<br>\end{align}<br>$$</li>
<li>注意，在一般的Attention中，没有\(\boldsymbol{W}^{O}\)这个参数，这个是用于多头Attention中，将多头的输出Concat后映射一下再输出</li>
<li>一般来说，\(head_i\)的维度是\(\frac{d_{model}}{N_{head}}=\frac{d_{model}}{h}=d_v = d_k\)，所以Multi-Head Attention的参数数量与head的数量无关，且无论多少个头，其的输出结果还是\(d_{model} = d_v * h\)维</li>
<li>原始论文中常用\(d_{model} = h * d_k = h * d_v\)，且base模型的参数设置为\(512 = 8 * 64\)</li>
</ul>
</li>
</ul>
<h5 id="有关Multi-Head-Attention的理解"><a href="#有关Multi-Head-Attention的理解" class="headerlink" title="有关Multi-Head Attention的理解"></a>有关Multi-Head Attention的理解</h5><ul>
<li>原论文的描述:</li>
</ul>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, </p>
</blockquote>
<ul>
<li>理解:<ul>
<li>所谓多头,就是多做几次(\(h\)次)同样的事情(参数\((W_i^Q, W_i^K, W_i^V)\)不共享, 即当 \(i \neq j \) 时, \((W_i^Q, W_i^K, W_i^V) \neq (W_j^Q, W_j^K, W_j^V)\)),然后把结果拼接</li>
<li>Multi-Head Attention中, 每个头(Scaled Dot-Product Attention)负责不同的子空间(subspaces at differect positions)</li>
<li>每个头权重不同, 所以他们的关注点也会不同,注意, 初始化时他们的参数不能相同, 否则会造成他们的参数永远相同, 因为他们是同构的</li>
<li>个人理解: 多头的作用可以类比于CNN中的卷积层, 负责从不同的角度提取原始数据的特征</li>
</ul>
</li>
</ul>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul>
<li>Self Attention是只 Key和Query相同的 Attention, 这里因为 Key 和 Value 也相同,所以有 <strong>Query == Key == Query</strong></li>
<li>即$$ \boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
<h4 id="Transformer中的Attention-1"><a href="#Transformer中的Attention-1" class="headerlink" title="Transformer中的Attention"></a>Transformer中的Attention</h4><ul>
<li>既是Multi-Head Attention, 也是 Self Attention</li>
<li>所以有$$\boldsymbol{Y_{AttentionOutput}} = MultiHead(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
<h4 id="Masked-Multi-Head-Attetion"><a href="#Masked-Multi-Head-Attetion" class="headerlink" title="Masked Multi-Head Attetion"></a>Masked Multi-Head Attetion</h4><ul>
<li>MaskedMHA，掩码多头Attention，用于Decoder中防止前面的token看到后面的token，Encoder中不需要MaskedMHA</li>
<li>一般性的，Masked Self-Attention是更一般的实现，不一定非要和Multi-Head绑定</li>
<li>代码实现时，主要是在计算Softmax前，按照掩码将看不到的token对应的q,k内积替换为一个大负数，比如\(-1e9\)</li>
</ul>
<h4 id="Cross-Multi-Head-Attention"><a href="#Cross-Multi-Head-Attention" class="headerlink" title="Cross Multi-Head Attention"></a>Cross Multi-Head Attention</h4><ul>
<li>CrossMHA不是Self-Attention，CrossMHA的Q,K是Encoder的输出，V来自Decoder</li>
</ul>
<hr>
<h3 id="Transformer-输入层"><a href="#Transformer-输入层" class="headerlink" title="Transformer 输入层"></a>Transformer 输入层</h3><ul>
<li>Transformer的输入层使用了 Word Embedding + Position Embedding</li>
<li>由于Transformer去除RNN的Attention机制完全不考虑词的顺序, 也就是说, 随机打乱句子中词的顺序 (也就是将键值对\((\boldsymbol{K}, \boldsymbol{V})\)对随机打乱), Transformer中Attention的结果不变</li>
<li>实际上, <strong>目前为止, Transformer中的Attention模型顶多是个非常精妙的”词袋模型”</strong> (这句话来自博客:<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">https://kexue.fm/archives/4765</a>)</li>
</ul>
<h4 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h4><ul>
<li>和之前的词嵌入一样, 将One-Hot值映射成词向量嵌入模型中</li>
</ul>
<h4 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h4><p><em>FaceBook的《Convolutional Sequence to Sequence Learning》中曾经用过Position Embedding</em></p>
<ul>
<li>在不使用RNN的情况下建模词的顺序, 弥补”词袋模型”的不足</li>
<li>用 Position Embedding来为每个位置一个向量化表示<ul>
<li>将每个位置编号，然后每个编号对应一个向量</li>
<li>通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了</li>
</ul>
</li>
<li>原始论文中, 作者提出了一种周期性位置编码的表示, 数学公式如下:<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin(pos/10000^{2i/d_{\text{model}}}) \\<br>  PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{\text{model}}})<br>  \end{align}<br>  $$</li>
<li>我觉得上述公式太丑了,转换一下写法可能更容易理解<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\<br>  PE(pos, 2i+1) &amp;= cos\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)<br>  \end{align}<br>  $$<ul>
<li>\(pos\) 是位置编号</li>
<li>\(i\) 表示位置向量的第 \(i\) 维</li>
<li>从公式来看，为什么选择\(10000^{\frac{2i}{d_{\text{model}}}}\)? <ul>
<li>\(i\)表示频率随模型embedding维度变动（模型embedding不同维度频率不同，低维度高频，高维度低频）</li>
<li>\(pos\) 表示周期，随着位置变化，每个维度的值呈现周期变化，但是不同维度的变化周期（频率）不同</li>
<li>10000是一个放缩因子，理论上可以换，在transformer原始论文实现中用了这个，且效果不错</li>
</ul>
</li>
<li>选择正弦函数的原因是假设这将允许模型学到相对位置信息<ul>
<li>因为对于固定的 \(k\), \(PE_{pos+k} = LinearFuction(PE_{pos})\), 所以这给模型提供了表达相对位置的可能性</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="与之前的Position-Embedding的区别"><a href="#与之前的Position-Embedding的区别" class="headerlink" title="与之前的Position Embedding的区别"></a>与之前的Position Embedding的区别</h5><ul>
<li>Position Embedding对模型的意义不同:<ul>
<li>以前在RNN、CNN模型中Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，因为RNN、CNN本身就能捕捉到位置信息</li>
<li>在Transformer这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段</li>
</ul>
</li>
<li>Position Embedding的向量构造方式不同<ul>
<li>在以往的Position Embedding中，基本都是根据任务训练出来的向量</li>
<li>而Google直接给出了一个构造Position Embedding的公式:<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\<br>  PE(pos, 2i+1) &amp;= cos\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)<br>  \end{align}<br>  $$</li>
<li>Google经过实验, 学到的位置嵌入和这种计算得到的位置嵌入结果很相近</li>
<li>Google选用这种嵌入方式的原因是这种方式允许模型以后可以<strong>扩展到比训练时遇到的序列长度更长的句子</strong></li>
</ul>
</li>
</ul>
<h4 id="输入层的输出-Attention的输入"><a href="#输入层的输出-Attention的输入" class="headerlink" title="输入层的输出(Attention的输入)"></a>输入层的输出(Attention的输入)</h4><ul>
<li>综合词嵌入和位置嵌入信息,我们可以得到下面的公式<br>$$<br>\begin{align}<br>\boldsymbol{x} = \boldsymbol{x}_{WE} + \boldsymbol{x}_{PE}<br>\end{align}<br>$$<ul>
<li>\(\boldsymbol{x}\) 为输入层经过词嵌入和位置嵌入后的 输出, 也就是Attention的输入</li>
<li>\(\boldsymbol{x}_{WE}\) 指词嵌入的结果</li>
<li>\(\boldsymbol{x}_{PE}\) 指位置嵌入的结果</li>
</ul>
</li>
</ul>
<h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><ul>
<li>FFN，Feed Forward Network，前馈网络层<br>$$<br>FFN(\mathbf{X}) = ReLU(\mathbf{X}\mathbf{W}^U + \mathbf{b}_1)\mathbf{W}^D + \mathbf{b}_2<br>$$</li>
<li>原始Transformer使用的是ReLU作为激活函数，现在很多时候也会选用sigmoid</li>
<li>可以看到前馈神经网络包含了两层 </li>
</ul>
<h3 id="Layer-Normaliztion"><a href="#Layer-Normaliztion" class="headerlink" title="Layer Normaliztion"></a>Layer Normaliztion</h3><ul>
<li>层归一化，是Transformer特有的一种归一化方法</li>
<li>Batch Normalization(BN)不适用与Transformer中，至少有以下原因：<ul>
<li>Transformer训练样本通常（特别是模型很大时）可能会比较小，在Batch较小时BN不再适用</li>
<li>BN是按照token维度（特征维度）来归一化的，不利于处理变长输入序列</li>
</ul>
</li>
</ul>
<p>$$<br>LayerNorm(\mathbf{x}) = \frac{\mathbf{x}-\mathbf{\mu}}{\mathbf{\sigma}}\cdot \mathbf{\gamma} + \mathbf{\beta} \\<br>\mathbf{\mu} = \frac{1}{H}\sum_{i=1}^H x_i, \quad \mathbf{\sigma} = \sqrt{\frac{1}{H}\sum_{i=1}^H(x_i-\mathbf{\mu})^2} \\<br>$$</p>
<ul>
<li>代码实现是会在分母的更号内增加一个极小量 \(\epsilon\)，防止出现除0的情况</li>
</ul>
<h4 id="LN是token维度的"><a href="#LN是token维度的" class="headerlink" title="LN是token维度的"></a>LN是token维度的</h4><ul>
<li><p>按照Transformer源码实现来看，LayerNorm是Token维度的，不是Seq维度，也就是说，token向量LayerNorm的结果只与token向量自身相关，与所在序列的其他token无关</p>
<ul>
<li>这一点是Decoder可以增量解码的关键，这一点保证了Decoder的前序词不会受到后续词的影响</li>
<li>增量解码是指：Decoder中输出下一个词时，可以使用前序词的缓存结果，由于前面的词看不到后面的词，所以增加词前后Transformer-Decoder中前序每个词的输出在每一层都不会受到影响</li>
</ul>
</li>
<li><p>一个LayerNorm的示例如下，Transformer源码中实现与这个类似</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 假设d_model=4，d_model在有些实现中为hidden_size</span><br><span class="line">layer_norm = nn.LayerNorm(4) # 这里使用LayerNorm，也可以使用RMSNorm，两者作用维度相同，只是公式不同</span><br><span class="line"># test case 1:</span><br><span class="line">input_tensor = torch.Tensor([[[1, 2, 3, 4],</span><br><span class="line">                              [2, 3, 4, 5]]])</span><br><span class="line">output_tensor = layer_norm(input_tensor)</span><br><span class="line">print(output_tensor)</span><br><span class="line"># output:</span><br><span class="line"># tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],</span><br><span class="line">#          [-1.3416, -0.4472,  0.4472,  1.3416]]],</span><br><span class="line">#        grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class="line"></span><br><span class="line"># test case 2:</span><br><span class="line">input_tensor = torch.Tensor([[[1, 2, 3, 4],</span><br><span class="line">                              [200, 3, 4, 5]]])</span><br><span class="line">output_tensor = layer_norm(input_tensor)</span><br><span class="line">print(output_tensor)</span><br><span class="line"></span><br><span class="line"># tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],</span><br><span class="line">#          [ 1.7320, -0.5891, -0.5773, -0.5655]]],</span><br><span class="line">#        grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>从示例中可以看出：</p>
<ul>
<li>修改第二个token的某个元素值，只影响第二个token的LN输出，不影响第一个token</li>
</ul>
</li>
</ul>
<h3 id="Transformer改进-LN"><a href="#Transformer改进-LN" class="headerlink" title="Transformer改进-LN"></a>Transformer改进-LN</h3><p><em>原始LN参见本文之前的内容</em></p>
<h4 id="LN的改进——RMSNorm"><a href="#LN的改进——RMSNorm" class="headerlink" title="LN的改进——RMSNorm"></a>LN的改进——RMSNorm</h4><p>$$<br>\begin{align}<br>RMSNorm(\mathbf{x}) &amp;= \frac{\mathbf{x}-\mathbf{\mu}}{RMS(\mathbf{x})}\cdot \mathbf{\gamma} \\<br>  RMS(\mathbf{x}) &amp;= \sqrt{\frac{1}{H}\sum_{i=1}^H x_i^2} \\<br>\end{align}<br>$$</p>
<ul>
<li>代码实现是会在分母的更号内增加一个极小量 \(\epsilon\)，防止出现除0的情况</li>
</ul>
<h4 id="LN的改进——DeepNorm"><a href="#LN的改进——DeepNorm" class="headerlink" title="LN的改进——DeepNorm"></a>LN的改进——DeepNorm</h4><p>$$<br>DeepNorm(\mathbf{x}) = LayerNorm(\alpha\cdot \mathbf{x} + Sublayer(\mathbf{x})) \\<br>$$</p>
<ul>
<li>这里的\(Sublayer(\mathbf{x})\)是指Transformer中的前馈神经网络层或自注意力模块（两者都会作为LN的输入）</li>
<li>实际上，原始的Transformer中，每次LN的内容都是加上残差的，这里根据归一化位置的不同还有会有不同的实现</li>
<li>原始的Transformer中，相当于\(\alpha=1\)的DeepNorm</li>
<li>这里叫做<strong>DeepNorm</strong>的原因是因为缩放残差\(\mathbf{x}\)可以扩展Transformer的深度，有论文提到利用该方法可将深度提升到1000层（<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231" target="_blank" rel="noopener">DeepNet: Scaling Transformers to 1,000 Layers</a>）</li>
</ul>
<h4 id="归一化的位置"><a href="#归一化的位置" class="headerlink" title="归一化的位置"></a>归一化的位置</h4><ul>
<li>归一化的位置包括Post-Norm、Pre-Norm和Sandwich-Norm等</li>
<li>Post-Norm<ul>
<li>原始Transformer使用的方法</li>
<li>将归一化模块使用到加法（需要把残差加到FFN/MHA的输出上）之后，详细公式是：<br>$$<br>\text{Post-Norm}(\mathbf{x}) = Norm(\mathbf{x} + Sublayer(\mathbf{x}))<br>$$</li>
</ul>
</li>
<li>Pre-Norm<ul>
<li>归一化模块放到FFN/MHA之前，详细公式是：<br>$$<br>\text{Pre-Norm}(\mathbf{x}) = \mathbf{x} + Sublayer(Norm(\mathbf{x}))<br>$$</li>
</ul>
</li>
<li>Sandwich_Norm<ul>
<li>三明治归一化，从字面意思可以知道，是两个Norm将某个层夹起来，实际上，该层是前馈神经网络层或自注意力模块<br>$$<br>\text{Sandwish-Norm}(\mathbf{x}) = \mathbf{x} + Norm(Sublayer(Norm(\mathbf{x})))<br>$$</li>
</ul>
</li>
</ul>
<h4 id="归一化位置的比较"><a href="#归一化位置的比较" class="headerlink" title="归一化位置的比较"></a>归一化位置的比较</h4><p><em><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231" target="_blank" rel="noopener">DeepNet: Scaling Transformers to 1,000 Layers</a>中也有有关Pre-Norm和Post-Norm的探讨</em></p>
<ul>
<li>一般来说，使用Post-Norm比较多，效果也更好</li>
<li>Pre-Norm在深层Transformer中容易训练（容易训练不代表效果好，Pre-Norm的拟合能力一般不如Post-Norm）<ul>
<li>所以有些模型还是会使用Pre-Norm，因为它更稳定</li>
</ul>
</li>
<li>浅层中建议使用Post-Norm</li>
<li>详情参考苏神的回答<a href="https://zhuanlan.zhihu.com/p/494661681" target="_blank" rel="noopener">为什么Pre Norm的效果不如Post Norm？</a></li>
</ul>
<h3 id="Transformer改进-激活函数"><a href="#Transformer改进-激活函数" class="headerlink" title="Transformer改进-激活函数"></a>Transformer改进-激活函数</h3><p><em>原始激活函数是ReLU(Rectified Linear Unit)</em></p>
<h4 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h4><ul>
<li>Swish, Sigmoid-weighted Linear Unit<ul>
<li><em>Swish 的全称为 “Scaled Exponential Linear Unit with Squishing Hyperbolic Tangent”,直译为“带有压缩的缩放指数线性单元”</em><br>$$<br>\text{Swish}_\beta(x) = x \cdot sigmoid(\beta x)<br>$$</li>
</ul>
</li>
<li>许多实现中常常设置\(\beta=1\)</li>
</ul>
<h4 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h4><ul>
<li>GELU, Gaussion Error Linear Unit，有时候也写作GeLU<br>$$<br>\text{GELU}(x) = 0.5x \cdot [1+erf(\frac{x}{\sqrt{2}})], \quad erf(x) = \frac{2}{\sqrt{\pi}}\int_1^x e^{-t^2} dt<br>$$</li>
<li>从公式可以看出GELU的本质是对一个正太分布的概率密度函数进行积分，实际上就是累积分布函数</li>
<li>GELU和ReLU的比较如下（图片来自<a href="https://zhuanlan.zhihu.com/p/662042707" target="_blank" rel="noopener">简单理解GELU 激活函数</a>）：<img src="/Notes/DL/DL——Transformer/GELU-ReLU.webp">

</li>
</ul>
<h4 id="补充：GLU及其变换"><a href="#补充：GLU及其变换" class="headerlink" title="补充：GLU及其变换"></a>补充：GLU及其变换</h4><ul>
<li>GLU，Gated Linear Units，是一种利用门的思想实现的激活函数，该激活函数可以理解为对输入进行门控选择，一些维度的值可以通过门，一些则不可以，门一般是一个基础的非线性激活函数</li>
<li>原始GLU形式如下：<br>$$<br>GLU = \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \odot (\mathbf{W}_2\mathbf{x} + \mathbf{b}_2)<br>$$</li>
<li>\(\sigma\)可以替换成其他非线性激活函数<ul>
<li>注意整个公式中始终只有一个非线性激活函数，其他部分都是线性映射（线性激活函数）</li>
</ul>
</li>
<li>\(\odot\)表示矩阵按照元素相乘，\(W_1,W_2,b_1,b_2\)是可学习的参数</li>
<li>该激活函数非常特殊，首先使用两个权重矩阵对输入数据进行线性变换，然后通过sigmoid激活函数进行非线性变换。这种设计使得GLU在前馈传播过程中能够更好地捕捉输入数据的非线性特征，从而提高模型的表达能力和泛化能力</li>
<li>原始论文<a href="https://arxiv.org/pdf/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer</a>中也写作下面的形式(其中\(W,V,b,c\)是可学习的参数)：<br>$$<br>GLU(\mathbf{x,W,V,b,c}) = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b}) \odot (\mathbf{V}\mathbf{x} + \mathbf{c})<br>$$</li>
<li>去掉激活函数的版本也叫作Bilinear，写作<br>$$<br>Bilinear(\mathbf{x,W,V,b,c}) = (\mathbf{W}\mathbf{x} + \mathbf{b}) \odot (\mathbf{V}\mathbf{x} + \mathbf{c})<br>$$</li>
<li>其他相关形式<br>$$<br>ReGLU(x, W, V, b, c) = max(0, xW + b) \odot (xV + c) \\<br>GEGLU(x, W, V, b, c) = GELU(xW + b) \odot (xV + c) \\<br>SwiGLU(x, W, V, b, c, \beta) = Swish_\beta(xW + b) \odot (xV + c) \\<br>$$</li>
</ul>
<h4 id="补充：FFN激活函数形式"><a href="#补充：FFN激活函数形式" class="headerlink" title="补充：FFN激活函数形式"></a>补充：FFN激活函数形式</h4><ul>
<li>FFN的ReLU激活函数形式<br>$$<br>FFN(x, W_1, W_2, b_1, b_2) = max(0, xW_1 + b_1)W_2 + b_2<br>$$</li>
<li>为了表示方便，也因为在一些文章中使用了简化，后续该形式会被简化成没有偏置项(bias)的形式：<br>$$<br>FFNReLU(x, W_1, W_2) = max(xW_1, 0)W_2<br>$$</li>
</ul>
<h4 id="FFN各种激活函数形式"><a href="#FFN各种激活函数形式" class="headerlink" title="FFN各种激活函数形式"></a>FFN各种激活函数形式</h4><ul>
<li>常用FFN的激活函数改进有，GLU,Bilinear,ReGLU,GEGLU(GeGLU),SwiGLU等<br>$$<br>\begin{align}<br>FFN_{GLU}(x, W, V, W_2) &amp;= (\sigma(xW) \odot xV )W_2 \\<br>FFN_{Bilinear}(x, W, V, W_2) &amp;= (xW \odot xV )W_2 \\<br>FFN_{ReGLU}(x, W, V, W_2) &amp;= (max(0, xW) \odot xV )W_2 \\<br>FFN_{GEGLU}(x, W, V, W_2) &amp;= (GELU(xW) \odot xV )W_2 \\<br>FFN_{SwiGLU}(x, W, V, W_2) &amp;= (Swish_1(xW) \odot xV )W_2 \\<br>\end{align}<br>$$</li>
<li>可以理解为\(\mathbf{W}^G,\mathbf{W}^U\)中包含了偏置项\(\mathbf{b}\)，有些文章/模型中则会将偏置项\(\mathbf{b}\)去掉</li>
<li>最常用的是SwiGLU</li>
<li>从形式上看，可以知道相对原始FFN激活函数形式，SwiGLU等改进增加了一个参数矩阵，为了保证原始参数数量不变，原始论文<a href="https://arxiv.org/pdf/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer</a>中提出了一种方法，通过将矩阵设置为如下的大小来保证参数数量相等<ul>
<li>原始FFN层参数为(下面\(d = d_{model}\)是模型的隐藏层大小，注意，同一层的不同token是共享FFN的):<br>$$<br>W_1^{d\times d} + W_2^{d\times d}<br>$$</li>
<li>使用SwiGLU且对齐参数数量后<br>$$<br>W^{r\times d} + V^{d\times r} + W_{2}^{r\times d}<br>$$</li>
<li>显然，当 \(r=\frac{2}{3}d\) 时，使用 SwiGLU 前后FFN层参数数量相同，都等于 \(2d^2\)</li>
</ul>
</li>
<li>一个疑问：原始的SwiGLU函数会引入两个参数矩阵 \(W,V\)，原始的FFN包含两个参数矩阵 \(W_1, W_2\)，为什么两者结合以后只剩下 \(FFN_{SwiGLU}\) 只剩三个参数\(W,V,W_2\)呢？<ul>
<li>回答：因为两个线性矩阵相乘，可以合并为 \(W = WV\)，虽然还叫做\(W\)，但实际上是多了一个矩阵乘进去的，线上训练时也只需要训练这一个矩阵即可</li>
</ul>
</li>
</ul>
<h3 id="Transformer总结"><a href="#Transformer总结" class="headerlink" title="Transformer总结"></a>Transformer总结</h3><ul>
<li>Transformer是一个特征提取能力非常强(超越LSTM)的特征提取器</li>
<li>一些讨论<ul>
<li>Transformer与CNN没关系,但是Transformer中使用多个 Scaled Dot-Product Attention 来最后拼接的方法(Multi-Head Attention), 就是CNN的多个卷积核的思想</li>
<li>Transformer论文原文中提到的残差结构也来源于CNN</li>
<li>无法对位置信息进行很好地建模，这是硬伤。尽管可以引入Position Embedding，但我认为这只是一个缓解方案，并没有根本解决问题。举个例子，用这种纯Attention机制训练一个文本分类模型或者是机器翻译模型，效果应该都还不错，但是用来训练一个序列标注模型（分词、实体识别等），效果就不怎么好了。那为什么在机器翻译任务上好？我觉得原因是机器翻译这个任务并不特别强调语序，因此Position Embedding 所带来的位置信息已经足够了，此外翻译任务的评测指标BLEU也并不特别强调语序</li>
<li>Attention如果作为一个和CNN,RNN平级的组件来使用,可能会集成到各自的优点, 而不是”口气”很大的 “Attention is All You Need”</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/ComputationalAdvertising/CA——BCB出价推导.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/ComputationalAdvertising/CA——BCB出价推导.html" itemprop="url">CA——BCB出价推导</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<p><em>本文主要记录BCB出价的推导</em></p>
<ul>
<li>其他参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/532453764" target="_blank" rel="noopener">智能出价——BCB求解</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/588970232" target="_blank" rel="noopener">互联网广告算法漫谈——浅谈广告中的出价技术</a>：包含许多公式的详细推导</li>
</ul>
</li>
</ul>
<hr>
<h3 id="BCB介绍"><a href="#BCB介绍" class="headerlink" title="BCB介绍"></a>BCB介绍</h3><ul>
<li>预算约束的出价，Budget Constrained Bidding（简称BCB），广告主的出价目标是设置一定预算，拿到最多的流量（点击或者订单）</li>
</ul>
<hr>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><ul>
<li>一般BCB优化问题的定义:</li>
</ul>
<p>$$<br>\begin{align}<br>\max_{x_{ij}} \sum^N_{i=1} x_{ij}  v_{ij} \\<br>s.t. \sum^N_{i=1} x_{ij}  c_{ij} \le B \\<br>      \sum^M_{j=1} x_{ij} = 1 \\<br>      x_{ij} \in {0, 1}<br>\end{align}<br>$$</p>
<hr>
<h3 id="解法一"><a href="#解法一" class="headerlink" title="解法一"></a>解法一</h3><p>可以解得，最终结果为(求解过程参见RL-MPCA论文):<br>$$ j^* = \arg\max_{j} (v_{ij} - \lambda c_{ij}) $$</p>
<hr>
<h3 id="解法二"><a href="#解法二" class="headerlink" title="解法二"></a>解法二</h3><ul>
<li>如果<strong>只有一个广告位置，且使用二价计费</strong>，原始问题可化简为</li>
</ul>
<p>$$<br>\begin{align}<br>\max_{x_{i}} \sum^N_{i=1} x_{i} v_{i} \\<br>s.t. \sum^N_{i=1} x_{i} c_{i} \le B \\<br>      x_{i} \in {0, 1}<br>\end{align}<br>$$</p>
<ul>
<li>最终可解得结果为(求解参考链接：<a href="https://zhuanlan.zhihu.com/p/532453764" target="_blank" rel="noopener">智能出价——BCB求解</a>)：<br>$$ bid = v_i / \lambda $$</li>
</ul>
<hr>
<h3 id="关于两种解法的结果分析"><a href="#关于两种解法的结果分析" class="headerlink" title="关于两种解法的结果分析"></a>关于两种解法的结果分析</h3><ul>
<li>本质上，两种解法结果应该是完全相同的，第一种解法中，如果只有一个广告位置，且使用二价计费，求解到的结果本质于第二种解法结果一致。<ul>
<li>当 \(v_{i} &gt; \lambda \cdot Price_{win} \)时(\(Price_{win} = c_{i}\),表示净胜价格), 此时选择\(bid &gt; Price_{win}\)的出价即可获得本次竞拍，此时不管是解法一(取\(v_{ij} - \lambda * c_{ij}\) 最大的动作)还是解法二(\(bid=v_i/\lambda\))都会选择执行竞拍动作。</li>
<li>反之，当当\(v_{i} &lt; \lambda \cdot Price_{win}\)时, 此时选择\(bid &lt; Price_{win}\)的出价即可获得本次竞拍，此时不管是解法一还是解法二都会选择执行不竞拍动作。</li>
</ul>
</li>
<li>解法一适用于任何场景，解法二则仅适用于<strong>只有一个广告位置，且使用二价计费</strong>的场景</li>
<li>解法二的结果使用起来会更简单：<ul>
<li>解法一需要预估不同出价下的收益，实际上，在<strong>只有一个广告位置，且使用二价计费</strong>的场景，不竞争当前广告位置的价值为0，计费为0，若竞争，则价值为\(v_i\)，计费为\(c_i\)，所以仅需要预估价值\(v_i\)，计费\(c_i\)。</li>
<li>解法二则可直接预估一个值\(v_i\)即可，不需要预估\(c_i\)，即在线不需要预估净胜价格<ul>
<li>但是离线流量回放求解\(\lambda\)时，理论上也需要预估一个净胜价格，或者在给出一个价格时，需要知道是否会竞争成功。\(c_i\)已经隐含在求解到的\(\lambda\)中</li>
</ul>
</li>
<li>若对于任意竞拍，都给定净胜价格，那么解法一和解法二等价，都只需要预估\(v_i\)一个值即可</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/ComputationalAdvertising/CA——WideAndDeep.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/ComputationalAdvertising/CA——WideAndDeep.html" itemprop="url">CA——WideAndDeep</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>一般来说，模型最重要的是 Memorization 和 Generalization <ul>
<li>Memorization: 可以不精确地定义为学习 items 或者特征的频繁共现，并且从历史数据里面开发(exploiting)</li>
<li>Generalization: 基于相关性的传递性(transitivity of correlation), 探索发现一些新的特征组合(从我出现或很少出现的组合)</li>
</ul>
</li>
</ul>
<h3 id="Wide"><a href="#Wide" class="headerlink" title="Wide"></a>Wide</h3><ul>
<li>注重记忆能力</li>
<li>手动特征交叉组合(实现低阶的特征组合)</li>
</ul>
<h3 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h3><ul>
<li>泛化能力</li>
<li>高阶特征</li>
<li>低维稠密embedding</li>
</ul>
<h3 id="Wide-and-Deep"><a href="#Wide-and-Deep" class="headerlink" title="Wide and Deep"></a>Wide and Deep</h3><ul>
<li>结合了Wide和Deep两个模型</li>
<li>不是模型的集成(Ensemble)，这里称为联合训练(Joint Training)<ul>
<li>集成的两个模型是分开独立训练的，然后预测的时候合并了两个模型的预测</li>
<li>联合训练是两个模型同时训练，训练时是加权相加的，每次训练时损失是一起传递</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/ComputationalAdvertising/CA——多任务学习总结.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/ComputationalAdvertising/CA——多任务学习总结.html" itemprop="url">CA——多任务学习总结</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>多任务学习，multi-task learning</em></p>
<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<h3 id="多任务学习的结构"><a href="#多任务学习的结构" class="headerlink" title="多任务学习的结构"></a>多任务学习的结构</h3><h4 id="ESMM及扩展"><a href="#ESMM及扩展" class="headerlink" title="ESMM及扩展"></a>ESMM及扩展</h4><h4 id="MMoE及相关变体"><a href="#MMoE及相关变体" class="headerlink" title="MMoE及相关变体"></a>MMoE及相关变体</h4><h3 id="多任务学习的损失函数权重设置"><a href="#多任务学习的损失函数权重设置" class="headerlink" title="多任务学习的损失函数权重设置"></a>多任务学习的损失函数权重设置</h3><h4 id="基于人工经验的人工优化"><a href="#基于人工经验的人工优化" class="headerlink" title="基于人工经验的人工优化"></a>基于人工经验的人工优化</h4><ul>
<li>一般思想是对齐损失函数均值，并按照业务偏好有所倾斜，如果愿意花时间尝试，往往能拿到不错的结果</li>
</ul>
<h4 id="基于贝叶斯推论的权重优化"><a href="#基于贝叶斯推论的权重优化" class="headerlink" title="基于贝叶斯推论的权重优化"></a>基于贝叶斯推论的权重优化</h4><ul>
<li>基于不确定性的权重设置方法（Uncertainty Weighting）</li>
<li>基本公式<br>$$<br>L(\mathbf{W}, \sigma_1, \sigma_2,…,\sigma_K) = \sum_{k=1}^K \frac{1}{2\sigma^2}L(\mathbf{W}) + \log \sigma^2<br>$$<ul>
<li>上述公式可通过推导得出<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</a><ul>
<li>可以推导，无论是回归问题还是分类问题（也可以是分类和回归问题的混合），都可以按照上面的方法设置损失函数（分类问题证明中会使用到一个近似值，不是严格推导）</li>
<li>推导是在假设</li>
</ul>
</li>
<li>\(\sigma\)是可学习的参数，初始设置固定值，然后使用梯度更新学习即可</li>
<li>使用简单，实际使用时效果也确实不错，建议人工调参也可以在先使用该方案拿到权重量级后继续</li>
</ul>
</li>
</ul>
<h4 id="帕累托最优权重优化"><a href="#帕累托最优权重优化" class="headerlink" title="帕累托最优权重优化"></a>帕累托最优权重优化</h4><ul>
<li><p>原始论文：<a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf" target="_blank" rel="noopener">Multi-Task Learning as Multi-Objective Optimization</a></p>
<img src="/Notes/ComputationalAdvertising/CA——多任务学习总结/pareto-optimazation-for-MTL.png"></li>
<li><p>内积的含义：向量A到向量B的投影长度与向量B长度的乘积</p>
</li>
<li><p>Algorithm1展示的是，对于两个任务的情况，图示展示了二维向量的情况，可以通过判断向量之间的关系确定求解\(\alpha\)的方式</p>
<ul>
<li>Algorithm2中的FrankWolfeSlover算法则是对Algorithm1的多任务扩展</li>
</ul>
</li>
<li><p>其他参考，阿里巴巴多任务学习帕累托最优论文：<a href="https://yongfeng.me/attach/lin-recsys2019.pdf" target="_blank" rel="noopener">A Pareto-Efficient Algorithm for Multiple Objective Optimization in E-Commerce Recommendation</a></p>
</li>
</ul>
<h3 id="损失函数优化"><a href="#损失函数优化" class="headerlink" title="损失函数优化"></a>损失函数优化</h3><h4 id="损失函数归一化"><a href="#损失函数归一化" class="headerlink" title="损失函数归一化"></a>损失函数归一化</h4><p><em>可用于解决由于不同任务损失函数量级差异带来的问题</em></p>
<h5 id="普通版本"><a href="#普通版本" class="headerlink" title="普通版本"></a>普通版本</h5><p>$$<br>L_{norm} = \frac{L_k(\mathbf{W})}{L_0(\mathbf{W_0})}<br>$$</p>
<ul>
<li>使用各个任务自己的第一次输出的损失函数作为基础损失函数，其中\(L_0(\mathbf{W_0}\)为第一次计算loss的到的损失函数</li>
</ul>
<h5 id="滑动平均版本"><a href="#滑动平均版本" class="headerlink" title="滑动平均版本"></a>滑动平均版本</h5><p>$$<br>L_{base} = \alpha L_{base} + (1-\alpha) L_k \\<br>L_{norm} = \frac{L_k(\mathbf{W})}{L_{base}} \\<br>$$</p>
<ul>
<li>使用滑动平均来记录基础损失函数，该方案可进一步减少由于初始化损失误差过大带来的问题</li>
</ul>
<h4 id="梯度归一化（GradNorm）"><a href="#梯度归一化（GradNorm）" class="headerlink" title="梯度归一化（GradNorm）"></a>梯度归一化（GradNorm）</h4><ul>
<li>原始论文：<a href="https://proceedings.mlr.press/v80/chen18a/chen18a.pdf" target="_blank" rel="noopener">GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</a><img src="/Notes/ComputationalAdvertising/CA——多任务学习总结/grad-norm-definition-for-MTL.png">
<img src="/Notes/ComputationalAdvertising/CA——多任务学习总结/grad-norm-for-MTL.png"></li>
<li>核心思想是对各任务的损失函数进行加权(\(w_i\))求和得到更新共享参数的损失函数</li>
<li>“GradNorm”这个名字的由来是因为权重\(w_i\)是与梯度2范数的期望等有关的？</li>
<li>公式中\(w_i\)是各个任务损失函数对共享参数损失函数的权重，该权重初始值为1，在训练过程中逐步更新，每一步最后都保持该权重加和为\(T\)（\(T\)为任务数量，即保证权重均值为1）</li>
<li>问题：文中没有明确各个任务各自的参数如何更新，猜测各自更新即可</li>
</ul>
<h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><ul>
<li>一般情况下，根据业务特点，尽量使用类似于ESMM结构</li>
<li>权重设置尝试次序：<ul>
<li>对损失函数进行归一化（梯度归一化好像效果一般？）</li>
<li>权重设置时，先使用不确定性权重（Uncertainty Weighting）跑一版，得到基线</li>
<li>在Uncertainty Weighting的基础上，人工可以根据业务需要微调，可以偏向于需要的任务</li>
<li>帕累托最优实现复杂，且不一定有收益<ul>
<li>复杂体现在：需要在每个batch上重新求解最优化问题，得到当前的loss权重（用上一个batch的梯度求解这一个batch的最优权重）</li>
</ul>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/ComputationalAdvertising/CA——Google-Ad-Click-Prediction.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/ComputationalAdvertising/CA——Google-Ad-Click-Prediction.html" itemprop="url">CA——Google-Ad-Click-Prediction</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<ul>
<li>参考文献：<a href="https://dl.acm.org/doi/pdf/10.1145/2487575.2488200?download=true" target="_blank" rel="noopener">Google KDD 2013，Ad Click Prediction: a View from the Trenches</a>(引用量500+)</li>
<li>介绍了截止到2013年之前的点击率预估常用算法，FTRL是Google三年的结晶（2010-2013）</li>
<li><strong>在线学习</strong>优化算法的发展历程：SGD-&gt;TG-&gt;FOBOS-&gt;RDA-&gt;FTRL-Proximal</li>
</ul>
<h3 id="核心贡献"><a href="#核心贡献" class="headerlink" title="核心贡献"></a>核心贡献</h3><ul>
<li>基于传统的逻辑回归算法(Regularized Logistic Regression，正则化的逻辑回归)在点击率预估时的不足，提出一种<strong>在线逻辑回归</strong>算法，FTRL(Follow The Regularized Leader)</li>
<li>per-coordinate learning rate</li>
</ul>
<h3 id="一些模型的比较和介绍"><a href="#一些模型的比较和介绍" class="headerlink" title="一些模型的比较和介绍"></a>一些模型的比较和介绍</h3><ul>
<li>传统逻辑回归算法中使用OGD(Online Gradient Descent)是非常高效的，使用很小的计算资源就能得到较好的精确度.</li>
<li>但是OGD在生成<strong>稀疏</strong>模型方面表现不好(OGD + L1)</li>
<li>其他在稀疏性方面表现良好的方法有FOBOS, 截断梯度（Truncated Gradient）和 RDA（Regularized Dual Averaging)<ul>
<li>RDA在精确度和稀疏性方面做tradeoff， 效果好于FOBOS</li>
</ul>
</li>
<li>FTRL-Proximal号称可以同时获得RDA的稀疏性和OGD的精确度</li>
<li>RDA模型是微软提出的一种在线优化算法，与OGD完全不同，能得到更加稀疏的模型，但是精确度不如OGD</li>
</ul>
<h3 id="FTRL-Proximal-Learning-Online-Learning-And-Sparsity"><a href="#FTRL-Proximal-Learning-Online-Learning-And-Sparsity" class="headerlink" title="FTRL-Proximal Learning (Online Learning And Sparsity)"></a>FTRL-Proximal Learning (Online Learning And Sparsity)</h3><ul>
<li>也是通过L1正则化控制模型的稀疏度</li>
</ul>
<h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><h5 id="FTL-Follow-The-Leader-的介绍"><a href="#FTL-Follow-The-Leader-的介绍" class="headerlink" title="FTL(Follow The Leader)的介绍"></a>FTL(Follow The Leader)的介绍</h5><h5 id="OGD的更新方式"><a href="#OGD的更新方式" class="headerlink" title="OGD的更新方式"></a>OGD的更新方式</h5><ul>
<li>更新规则：<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;= \mathbf{w}_t-\eta_t\mathbf{g}_t<br>\end{align}<br>$$</li>
</ul>
<h5 id="FTLR-Follow-The-Regularized-Leader"><a href="#FTLR-Follow-The-Regularized-Leader" class="headerlink" title="FTLR(Follow The Regularized Leader)"></a>FTLR(Follow The Regularized Leader)</h5><p><em>加上正则项的FTL</em></p>
<ul>
<li>更新规则：<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;= \arg\min_{\mathbf{w}}\left ( \mathbf{g}_{1:t}\cdot \mathbf{w} + \frac{1}{2}\sum_{s=1}^t \sigma_s || \mathbf{w} - \mathbf{w}_s||_2^2 + \lambda_1||\mathbf{w}||_1 \right ) \\<br>\mathbf{g}_{1:t} &amp;= \sum_{i=1}^t\mathbf{g}_i<br>\end{align}<br>$$<ul>
<li>第一项是对损失函数梯度的贡献的一个估计</li>
<li>第二项是控制参数\(\mathbf{w}\)在每次迭代中变化不要太大</li>
<li>第三项是L1正则化，用于使模型变得稀疏（除了L1正则化项以外，也可以再加上L2正则化）</li>
<li>去掉正则化项就是FTL（Follow The Leader）</li>
<li>\(\sigma_s\)是学习速率</li>
<li>这个学习速率可以用Per-Coordinate Learning Rate:<br>$$<br>\begin{align}<br>\eta_{t, i} &amp;= \frac{\alpha}{\beta + \sqrt{\sum_{s=1}^t g_{s,i}^2}} \\<br>\mathbf{g}_s &amp;= \nabla l_s(\mathbf{w})<br>\end{align}<br>$$</li>
</ul>
</li>
</ul>
<h3 id="Per-Coordinate-Learning-Rates"><a href="#Per-Coordinate-Learning-Rates" class="headerlink" title="Per-Coordinate Learning Rates"></a>Per-Coordinate Learning Rates</h3><ul>
<li>对参数的每一维度分开训练，每个维度有自己的学习率</li>
<li>某个特征出现的次数越多，说明当前该特征对应的参数值越可信，学习率就应该越小</li>
<li>考虑了数据在每个特征上的分布不均匀性<ul>
<li>参数某个维度上的样本数越少，这些样本就会得到越大的利用(具体表现就是该特征的学习率会比较大)</li>
</ul>
</li>
</ul>
<h3 id="一个思考"><a href="#一个思考" class="headerlink" title="一个思考"></a>一个思考</h3><ul>
<li>问题：为什么机器学习中的学习率都是越来越小？</li>
<li>答案：因为刚开始训练时，参数的值不太可信（也就是说最终参数与当前参数的置信度比较低），所以更新时应该更新的步骤大一些，让当前的参数变化大一些，训练到后来，随着参数的值越来越可信（当前参数的置信度比较高），更新的步骤就应该小一些，让当前的变化小一些</li>
</ul>
<h3 id="一些工程上的Trick"><a href="#一些工程上的Trick" class="headerlink" title="一些工程上的Trick"></a>一些工程上的Trick</h3><h4 id="Saving-Memory-at-Massive-Scale"><a href="#Saving-Memory-at-Massive-Scale" class="headerlink" title="Saving Memory at Massive Scale"></a>Saving Memory at Massive Scale</h4><h5 id="Probabilistic-Feature-Inclusion"><a href="#Probabilistic-Feature-Inclusion" class="headerlink" title="Probabilistic Feature Inclusion"></a>Probabilistic Feature Inclusion</h5><ul>
<li>在高维数据中，大量的特征是出现频率非常低的(rare)，半数的唯一特征甚至只出现一次</li>
<li>统计这些特征的代价是非常昂贵的，有些特征可能永远不会被实际使用(这里如何理解昂贵？也就是说训练了也没用？)</li>
<li>额外的读写数据是昂贵的，如果能丢弃一部分出现评论特别低的特征(比如出现频率低于k次)</li>
<li>实现稀疏可以使用L1正则化，也可以使用Probabilistic Feature Inclusion</li>
<li>关于Probabilistic Feature Inclusion的做法<ul>
<li>当一个特征第一次出现时，以一定的概率接受这个新特征</li>
<li>效果作用于数据预处理阶段，但是可以在在线执行中设置</li>
</ul>
</li>
</ul>
<h6 id="Possion-Inclusion"><a href="#Possion-Inclusion" class="headerlink" title="Possion Inclusion"></a>Possion Inclusion</h6><ul>
<li>对于新的特征，以概率p添加入模型</li>
<li>对于已经存在模型中的特征，正常更新其系数</li>
</ul>
<h6 id="Bloom-Filter-Inclusion"><a href="#Bloom-Filter-Inclusion" class="headerlink" title="Bloom Filter Inclusion"></a>Bloom Filter Inclusion</h6><ul>
<li>用布隆过滤器仅仅保留出现次数在n次以上的特征</li>
</ul>
<img src="/Notes/ComputationalAdvertising/CA——Google-Ad-Click-Prediction/inclusion_methods.png">

<h5 id="Encoding-Values-with-Fewer-Bits"><a href="#Encoding-Values-with-Fewer-Bits" class="headerlink" title="Encoding Values with Fewer Bits"></a>Encoding Values with Fewer Bits</h5><ul>
<li>常用的OGD使用32或者64位浮点数编码来存储模型的系数</li>
<li>Google提出可以使用16位浮点数来存储系数，同时加上一些策略</li>
<li>实验结果：将64位的浮点值换为为系数存储节省了75%的RAM内存空间(这还用实验？直接计算就得到了啊)</li>
</ul>
<h5 id="Training-Many-Similar-Models"><a href="#Training-Many-Similar-Models" class="headerlink" title="Training Many Similar Models"></a>Training Many Similar Models</h5><ul>
<li>同时训练多个模型是超参数选择常用的方法</li>
<li>将可以共享的东西共享</li>
<li>在节省内存的同时，还可以节约网络带宽(存储一份Per-coordinate学习率，节省内存和带宽等)，CPU(用同一个hash表检索特征，节省CPU)和硬盘空间</li>
</ul>
<h5 id="A-Single-Value-Structure"><a href="#A-Single-Value-Structure" class="headerlink" title="A Single Value Structure"></a>A Single Value Structure</h5><ul>
<li>有时候我们希望评估大量的模型在只有少量的特征组(groups of features)添加或者删除时的变化</li>
<li>对于每一维度(coordinate),仅仅存储一个系数值而不是多个(正常应该为每个模型存储一个)</li>
<li>存储该维度对应特征组的模型共享该系数</li>
<li>对每个特征，训练时每个模型都会计算自己的值，然后所有模型的取平均作为所有包含该维度特征模型的共享</li>
</ul>
<h5 id="Computing-Learning-Rates-with-Counts"><a href="#Computing-Learning-Rates-with-Counts" class="headerlink" title="Computing Learning Rates with Counts"></a>Computing Learning Rates with Counts</h5><ul>
<li>对于每个特征来说，<strong>梯度和</strong>以及<strong>梯度平方和</strong>是必须计算的</li>
<li>梯度的计算必须准确，但是计算学习率却是可以粗糙计算的</li>
<li>仅仅统计样本出现次数(Counts)就能大概计算学习率</li>
</ul>
<h6 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h6><ul>
<li>假设包含一个给定特征的所有事件(events)具有有相同的概率(一般来说，这个近似是不可能的，但是在这个目标里面是可行的)<ul>
<li>如何理解这个假设的意义呢？<strong>对于具有某个特征的所有样本，其取值为(正负例)是的概率是相等的，正例(click)概率都为\(p\)</strong></li>
</ul>
</li>
<li>进一步假设模型已经精确地学习到了概率</li>
<li>如果有分别有\(P，N\)个正负样本(events),则有\(p=\frac{P}{N+P}\)</li>
<li>则有对于逻辑回归(\(p(1-p)\))来说，正例的梯度为\(p-1\),负例的梯度为\(p\)<br>$$<br>\begin{align}<br>\sum g_{t,i}^2 &amp;= \sum_{positive \ events}(1-p_t)^2 + \sum_{negative \ events}p_t^2 \\<br>&amp;\approx P(1-\frac{P}{N+P})^2 + N(\frac{P}{N+P})^2 \\<br>&amp;= \frac{PN}{N+P}<br>\end{align}<br>$$</li>
<li>也就是说，为了近似计算\(\sum g_{t,i}^2\)，我们仅需要记录\(P，N\)即可</li>
</ul>
<h5 id="Subsampling-Traning-Data"><a href="#Subsampling-Traning-Data" class="headerlink" title="Subsampling Traning Data"></a>Subsampling Traning Data</h5><ul>
<li>典型的CTRs是远远低于50%，数据偏差很大，正例(点击)的样本很稀疏</li>
<li>在模型训练中正例样本相对而言更有价值</li>
<li>使用下采样：很大程度上降低数据量，同时保证对精度最小程度的影响</li>
</ul>
<h6 id="采样方法："><a href="#采样方法：" class="headerlink" title="采样方法："></a>采样方法：</h6><ul>
<li>保留所有至少被点击过一次的请求(Query，也就是样本)</li>
<li>以一定比例\(r\in(0, 1]\)采样没有被点击过的请求</li>
<li>因为包含通用的特征(Feature Phase)计算，所以这种方法是合理的，但是需要纠偏(直接用采样后的样本训练会造成预测偏差)</li>
<li>加入一个重要性权重\(w_t\)<ul>
<li>\(w_t = 1\) for clicked queries</li>
<li>\(w_t = \frac{1}{r}\) for queries with no clicks</li>
<li>本质上可以理解为这里是将采样时造成的负例比例偏差补齐</li>
</ul>
</li>
</ul>
<h3 id="模型性能评估"><a href="#模型性能评估" class="headerlink" title="模型性能评估"></a>模型性能评估</h3><h4 id="Progressive-Validation"><a href="#Progressive-Validation" class="headerlink" title="Progressive Validation"></a>Progressive Validation</h4><ul>
<li>Progressive验证又称为在线损失(online loss)</li>
<li>与交叉验证(cross-validation)或留出法(hold out)验证是不同的</li>
<li>在服务查询中，在线损失能很好的代表我们的精度，因为在训练模型前，它仅仅在最新数据上评估其性能(因为这准确的模拟了当模型进行服务查询时发生了什么)</li>
<li>由于用了100%的数据作为训练和测试，在线损失比留出法验证有更好的统计数据</li>
<li>绝对指标往往会带来误导<ul>
<li>即使预测是完美的，对数损失和其他指标的差异也依赖着问题的困难程度</li>
<li>不同的国家，不同请求的点击率不同(同为对数损失的最佳实践，50%的点击率会好于2%的点击率)</li>
</ul>
</li>
<li>所以使用相对变化，看指标相对于base line改变了多少</li>
<li>从经验来看，相对指标在整个时间段内都很稳定</li>
<li>相同的指标计算应该对应在完全相同的数据(比如不同时段的损失比较是没有意义的)</li>
</ul>
<h3 id="置信度评估-Confidence-Estimates"><a href="#置信度评估-Confidence-Estimates" class="headerlink" title="置信度评估(Confidence Estimates)"></a>置信度评估(Confidence Estimates)</h3><ul>
<li>对很多应用来说，除了评估广告的CTR，还要量化预测的期望精确度(the expected accuracy of the prediction)</li>
</ul>
<h3 id="校正预测-Calibrating-Predictions"><a href="#校正预测-Calibrating-Predictions" class="headerlink" title="校正预测(Calibrating Predictions)"></a>校正预测(Calibrating Predictions)</h3><ul>
<li>系统偏差(Systematic Bias)指平均预测CTR(Average Predicted CTR)和观测CTR(Observed CTR)的差异</li>
<li>造成系统偏差的原因包括：<ul>
<li>不精确的模型假设</li>
<li>学习算法的缺陷</li>
<li>在训练或者服务(预测)时不可用的隐藏特征</li>
</ul>
</li>
<li>解决方案：<ul>
<li>添加一个校正层将预测CTRs与观测CTR做匹配(match predicted CTRSs to observed click-through rates)</li>
<li>暂时不能提供有效的保证，保证校正影响的有效</li>
<li>校正的本质是找到(拟合)一个函数映射\(g\),使得模型输出值与真实概率值一一对应</li>
<li>逻辑回归中的sigmoid函数可以看做是一个校正预测的函数吗？</li>
</ul>
</li>
<li>更多参考<ul>
<li><a href="https://blog.csdn.net/fzcoolbaby/article/details/99174601?utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">风险模型 - 概率校准</a></li>
<li><a href="https://www.cnblogs.com/lc1217/p/7069000.html" target="_blank" rel="noopener">机器学习：概率校准</a>, 文中有代码示例</li>
<li><a href="https://blog.csdn.net/fjsd155/article/details/84382838?utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">概率校准 Probability Calibration</a></li>
</ul>
</li>
</ul>
<h4 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h4><p><em>参考博客：<a href="https://blog.csdn.net/fzcoolbaby/article/details/99174601?utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">https://blog.csdn.net/fzcoolbaby/article/details/99174601?utm_source=distribute.pc_relevant.none-task</a></em></p>
<ul>
<li>概率模型的搭建过程中，由于抽样与正则化等原因，导致模型的输出概率值明显偏离真实概率值<ul>
<li>[待更新：为什么抽样和正则化会影响模型的输出概率发生变化？]</li>
</ul>
</li>
<li>此时的模型输出概率值仅仅有排序的意义，其绝对值没有意义(定序值，而非定距数值)</li>
<li>校正预测的过程就是把模型输出概率值的校正成真实的概率的过程，使得校正后的概率有绝对值意义</li>
</ul>
<h3 id="自动特征管理-Automated-Feature-Management"><a href="#自动特征管理-Automated-Feature-Management" class="headerlink" title="自动特征管理(Automated Feature Management)"></a>自动特征管理(Automated Feature Management)</h3><ul>
<li>将特征空间描绘成上下文和语义信号，每个信号都可以被翻译成一个在学习时有真实值的特征集合</li>
<li>[待更新]</li>
</ul>
<h3 id="一些不成功的实验记录-Unsuccessful-Experiments"><a href="#一些不成功的实验记录-Unsuccessful-Experiments" class="headerlink" title="一些不成功的实验记录(Unsuccessful Experiments)"></a>一些不成功的实验记录(Unsuccessful Experiments)</h3><p><em>本节记录google的一些失败的尝试方向(有些可能会让人很惊讶)，这些方向模型没有明显收益</em></p>
<h4 id="Aggressive-Feature-Hashing"><a href="#Aggressive-Feature-Hashing" class="headerlink" title="Aggressive Feature Hashing"></a>Aggressive Feature Hashing</h4><p><em>关于特征哈希(Feature Hashing)的相关知识可参考<a href="/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94Feature-Hashing.html">Feature-Hashing</a></em></p>
<ul>
<li><a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" target="_blank" rel="noopener">Feature Hashing for Large Scale Multitask Learning</a>论文指出，Feature Hashing方法效果显著<ul>
<li>报告显示使用特征hashing技巧，可以能将学习一个垃圾邮件过滤模型的特征空间映射成包含\(2^24\)个特征的特征空间(疑问：这里的特征原来不止\(2^24\)个吗？)</li>
</ul>
</li>
<li>但是实验证明，使用 Feature Hashing 方法并不能提升我们的方法，所以建议保留 interpretable(non-hashed)的特征向量</li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><ul>
<li>Google用网格搜索的方法测试了从0.1到0.5的Dropout特征丢弃概率</li>
<li>所有情况均没有带来任何收益(包括精度指标和泛化能力)，还往往给模型带来损害(detriment)</li>
<li>Google给出的一个解释是：Dropout在图像领域取得较好收益的原因是因为图像领域的数据特征分布与计算广告领域不同<ul>
<li>图像领域：稠密特征，此时Dropout把结果(effect)从强相关的特征中分离开来，从而得到泛化效果更好的分类器</li>
<li>计算广告：稀疏特征，且有噪音</li>
</ul>
</li>
</ul>
<h4 id="Feature-Bagging"><a href="#Feature-Bagging" class="headerlink" title="Feature Bagging"></a>Feature Bagging</h4><ul>
<li>对特征进行overlapping采样(注意，样本Bagging和特征Bagging不同)，然后训练多个独立的模型，最后取平均值</li>
<li>实验证明模型的的AucLoss降低了0.1%-0.6%</li>
</ul>
<h4 id="Feature-Vector-Normalization"><a href="#Feature-Vector-Normalization" class="headerlink" title="Feature Vector Normalization"></a>Feature Vector Normalization</h4><ul>
<li>\(\mathbf{x} = \frac{\mathbf{x}}{||\mathbf{x}||}\)</li>
<li>开始有一点精度上的收益，但是后面也出现了一定程度的detriment</li>
<li>Google的解释是可能是由于per-coordinate learning rates和正则化的影响</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Hexo/Hexo——Next主题搜索窗口无法弹出.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Hexo/Hexo——Next主题搜索窗口无法弹出.html" itemprop="url">Hexo——Next主题搜索窗口无法弹出</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考博客： <a href="https://www.sqlsec.com/2017/12/hexosearch.html" target="_blank" rel="noopener">https://www.sqlsec.com/2017/12/hexosearch.html</a></li>
</ul>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>有时候会遇到在Mac上Next主题窗口无法弹出的问题</li>
<li>问题分为两类<ul>
<li>一类为找不到 <code>search.xml</code> 文件：加载 <code>search.xml</code> 时错误为404</li>
<li>另一类为文章中有特殊字符：加载 <code>search.xml</code> 检查时错误为304</li>
</ul>
</li>
</ul>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="404类"><a href="#404类" class="headerlink" title="404类"></a>404类</h4><ul>
<li><p>修改最外层配置文件<code>./_config.yml</code>,添加以下语句(实测这一步非必须)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装搜索插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="304类"><a href="#304类" class="headerlink" title="304类"></a>304类</h4><ul>
<li>304状态说明是加载文件存在，但是无法正常解析文件</li>
<li>直接用浏览器访问 <code>search.xml</code> 文件链接，然后查看文件解析异常出现在哪个地方，然后删除特殊字符即可<ul>
<li>个人理解，从哪个文件不能搜索，特殊字符就出现在哪个文件中</li>
<li>说明：目前还没遇到过这种情况，后面遇到会再补充</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/ComputationalAdvertising/CA——Feature-Hashing.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/ComputationalAdvertising/CA——Feature-Hashing.html" itemprop="url">CA——Feature-Hashing</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考文献: <a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" target="_blank" rel="noopener">Feature Hashing for Large Scale Multitask Learning</a></li>
<li>特征哈希(Feature Hashing)常用于数据特征降维，同时尽量保留原始特征的表达能力<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

</li>
</ul>
<h3 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h3><ul>
<li>给出了一种高维数据降维方法，特征哈希(Feature Hashing)</li>
<li>严格证明了特征哈希的可用性</li>
</ul>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>一种构造组合特征的方法是笛卡尔乘积</li>
<li>计算广告领域往往有数十亿的高维特征</li>
<li>一种表达方式是使用词表法，对每个特征从词表里面进行查询<ul>
<li>缺陷一：拓展问题，每次拓展词表时难度较大(难以进行模型升级，因为特征维度在变化)</li>
<li>缺陷二：无法处理词表中不存在的特征(Unknown特征)</li>
</ul>
</li>
<li>一般的降维方法容易带来特征表达能力的损失</li>
</ul>
<h3 id="特征哈希"><a href="#特征哈希" class="headerlink" title="特征哈希"></a>特征哈希</h3><ul>
<li><p>哈希函数定义(参考自博客：<a href="https://blog.csdn.net/qjf42/article/details/82387559" target="_blank" rel="noopener">https://blog.csdn.net/qjf42/article/details/82387559</a>)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def feature_hashing(features, m):</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	Args:</span><br><span class="line">		features: 输入特征序列，可以是数字，字符串(本质还是数字)</span><br><span class="line">		m: 输出的特征维度，通常是2**26(vw),2**20(scikit-learn)</span><br><span class="line">	Returns:</span><br><span class="line">		m维的（稀疏）向量</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	# 这里的x一般是稀疏表示的（如：scipy.sparse.csr_matrix），这里是为了方便说明</span><br><span class="line">    x = np.zeros(m)</span><br><span class="line">    for feature in features:</span><br><span class="line">    	# hash_func_1保证散列尽量平均且散列速度快</span><br><span class="line">        idx = hash_func_1(feature) % m </span><br><span class="line">        # 这里在原始论文中</span><br><span class="line">        sign = hash_func_2(feature) % 2</span><br><span class="line">        if sign == 1:</span><br><span class="line">            x[idx] += 1</span><br><span class="line">        else:</span><br><span class="line">            x[idx] -= 1</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<ul>
<li>输出特征维度一般为\(m = 2^24\)等，是一个认为给定的数字</li>
</ul>
</li>
<li><p>与词表法比较：</p>
<ul>
<li>解决了模型升级时的特征拓展问题(增加新特征时，特征的维度不会变化)</li>
<li>解决了Unknown特征问题(个人理解：因为hash函数不管是什么特征，都可以一视同仁)</li>
<li>无需查表，节省了查表的时间(个人理解：其实查表时一般实现方式也是用哈希表构建索引，所以这里不能算是优势)</li>
<li>完成了降维(这里是把字典法里面对邮件或者文档的id也算作一个特征，这个特征one hot表示一下将会造成数据维度变得非常大？但是id算做特征有什么意义吗？)</li>
</ul>
</li>
</ul>
<h3 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h3><ul>
<li>冲突总会发生，也就是说不同一个特征总有一定的概率被映射到同一个维度(即两个不同特征的<code>idx</code>值可能相等)上</li>
<li>Paper中的垃圾邮件过滤模型实验证明：冲突造成的损失漏召率在\(m=2^22\)时影响约为1%，接近不做hash时的效果(特征维度在不做hash约为\(2^{25}\))且在\(m=2^{18}\)时为1.12%，也只升高了一点点</li>
<li>另外：无论如何，总有特殊情况，比如重要的特征如用户的性别特征“男”和“女”二者可能被映射到同一个维度上<ul>
<li>这里编码时是男:<code>(1, 0)</code>, 女<code>(0, 1)</code>这样的，所以如果二者映射到同一个维度上，那么这两个特征丢失了原本的表达能力</li>
<li>真实环境中如果遇到这些问题将会很难调试，如果找到了相关的问题，可以通过修改映射函数的输入参数字符串等方式来错开两个特征</li>
</ul>
</li>
<li>特征哈希本身可以类比于机器学习中的核技巧,所以特征哈希也称为哈希技巧</li>
</ul>
<h3 id="一些理解"><a href="#一些理解" class="headerlink" title="一些理解"></a>一些理解</h3><h4 id="知乎用户"><a href="#知乎用户" class="headerlink" title="知乎用户"></a>知乎用户</h4><ul>
<li>参考<a href="https://www.zhihu.com/people/ainika-peng" target="_blank" rel="noopener">Ainika Peng</a>的博客：<a href="https://www.zhihu.com/question/264165760/answer/279676705" target="_blank" rel="noopener">https://www.zhihu.com/question/264165760/answer/279676705</a></li>
<li>一般而言这类技术是为了解决两个问题：<ul>
<li>一是<strong>将categorical的特征编码为模型可理解的格式</strong>, 这是个基础问题。One-Hot Serializing就可以达到这个效果，例如将训练样本中出现过的的每个deviceid按顺序递增编号（比如deviceid@xxx:1 -&gt; feature@10000:1）。<ul>
<li>缺点1：这个映射表需要传递给引擎端，在prediction的时候照着再查一遍，数据量大且数据结构设计不好的时候很费时间；</li>
<li>缺点2：有些频次很低的特征置信度不高，单独出现对模型无益（甚至over-fitting）。这时候可以使用按频次截断技术进行降维。比如微软在deep crossing中提到的特征工程方法：只保留曝光最多的10k个campaign id编码为0-9999，其余的id全部编码为10000，但辅以poCTR等连续特征作为辅助。事实上这是一种手工的Hashing方法。</li>
</ul>
</li>
<li>二是<strong>尽可能在保留有效信息的基础上降低训练和预测过程的时间复杂度</strong></li>
</ul>
</li>
<li>自动Hashing方法的好处是：<ul>
<li>只要训练和预测时使用的hashing方法一致，对同一个特征的编码结果即一致，因此引擎预测或提供数据dump的时候无需查找编码表。所以其最大优点在于数据一致性和速度提升，这在极大规模特征和在线学习中至关重要。</li>
</ul>
</li>
<li>我们说的Hashing算法一般而言均特意设计为低碰撞率。<ul>
<li>因此一般hashing算法本身不会大幅降低特征维度，自然也不会大幅损失特征信息。真正可能存在问题的是hashing之后的降维过程。</li>
<li>一个非常常见的陷阱是string哈希到int64后取模m，试图将特征压缩至m维one-hot空间。在这种情况下，对于不知情的随机hashing过程，不同特征的碰撞率为1/m。举个例子，对于“性别”特征，将male哈希为一个int64，female哈希为另一个int64，很难发生碰撞；但如果你试图使用mod2将其压缩，如果你的算法哈希出的这两个int64奇偶性相同，则会导致特征失效。在你有很多feature需要哈希而又不清楚hashing算法细节的情况下，这在概率意义上是很容易发生的。<ul>
<li>这里的mod2是极端举例，其实m的取值小于原始维度的情况下都有一定概率造成冲突</li>
</ul>
</li>
</ul>
</li>
<li>因此我们会更倾向于所谓的embedding算法<ul>
<li>例如将70万维的userid通过weight embedding到32维的连续值特征上（不同于传统hashing的低维离散值特征）。这意味着训练过程更加复杂（有更多的weight需要optimize）；但对于预测过程，其特征性能十分良好且时间复杂度得以降低。同时，由于连续值特征空间的表达能力大幅高于离散值特征空间，整个模型的表达能力并不会明显下降，也基本不会发生离散hashing的碰撞问题。</li>
<li>当然，如果是<strong>FM这类倾向于接受离散值的模型，手工serializing+精心设计的hashing是较好的选择</strong>。</li>
</ul>
</li>
<li>优点：<ul>
<li>训练和预测的时间复杂度大幅降低；</li>
<li>数据的一致性强，不存在同一个特征今天编码成这个、明天编码成那个的情况，便于跟踪单特征效果；</li>
<li>对new feature可以直接编码并加入训练，无需等待编码表统计并反馈；</li>
<li>降低feature space大小，（精心设计可以）降低over-fitting的几率。</li>
</ul>
</li>
<li>缺点<ul>
<li>在不清楚hashing function细节的情况下，容易导致特征碰撞失效，且难以排查；</li>
<li>难以通过hashing出的特征反推源特征；</li>
<li>embedding会降低模型的可解释性。</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Others/General——各种包的管理总结.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Others/General——各种包的管理总结.html" itemprop="url">General——各种包的管理总结</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h3 id="编程语言相关"><a href="#编程语言相关" class="headerlink" title="编程语言相关"></a>编程语言相关</h3><ul>
<li>参考链接: <a href="https://help.github.com/en/github/managing-packages-with-github-packages/about-github-packages#supported-clients-and-formats" target="_blank" rel="noopener">https://help.github.com/en/github/managing-packages-with-github-packages/about-github-packages#supported-clients-and-formats</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">Package client</th>
<th align="left">Language</th>
<th align="left">Package format</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">npm</td>
<td align="left">JavaScript</td>
<td align="left">package.json</td>
<td align="left">Node package manager</td>
</tr>
<tr>
<td align="left">gem</td>
<td align="left">Ruby</td>
<td align="left">Gemfile</td>
<td align="left">RubyGems package manager</td>
</tr>
<tr>
<td align="left">mvn</td>
<td align="left">Java</td>
<td align="left">pom.xml</td>
<td align="left">Apache Maven project management and comprehension tool</td>
</tr>
<tr>
<td align="left">gradle</td>
<td align="left">Java</td>
<td align="left">build.gradle or build.gradle.kts</td>
<td align="left">Gradle build automation tool for Java</td>
</tr>
<tr>
<td align="left">docker</td>
<td align="left">N/A</td>
<td align="left">Dockerfile</td>
<td align="left">Docker container management platform</td>
</tr>
<tr>
<td align="left">nuget</td>
<td align="left">.NET</td>
<td align="left">nupkg</td>
<td align="left">NuGet package management for .NET</td>
</tr>
<tr>
<td align="left">pip</td>
<td align="left">Python</td>
<td align="left">requirements.txt</td>
<td align="left">use <code>pip install -r requirements.txt</code></td>
</tr>
</tbody></table>
<hr>
<h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><ul>
<li>参考链接: <a href="https://www.iteye.com/blog/justcoding-1937171" target="_blank" rel="noopener">https://www.iteye.com/blog/justcoding-1937171</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">软件管理方式</th>
<th align="left">线下安装命令</th>
<th align="left">线上安装命令</th>
<th align="left">distribution 操作系统</th>
</tr>
</thead>
<tbody><tr>
<td align="left">RPM</td>
<td align="left">rpm, rpmbuild</td>
<td align="left">yum</td>
<td align="left">Red Hat/Fedora</td>
</tr>
<tr>
<td align="left">DPKG</td>
<td align="left">dpkg</td>
<td align="left">apt, apt-get</td>
<td align="left">Debian/Ubuntu</td>
</tr>
</tbody></table>
<hr>
<h3 id="rpm和dpkg常用命令总结"><a href="#rpm和dpkg常用命令总结" class="headerlink" title="rpm和dpkg常用命令总结"></a>rpm和dpkg常用命令总结</h3><ul>
<li>参考链接: <a href="http://cha.homeip.net/blog/archives/2005/08/rpm_vs_dpkg.html" target="_blank" rel="noopener">http://cha.homeip.net/blog/archives/2005/08/rpm_vs_dpkg.html</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">操作描述</th>
<th align="center">rpm</th>
<th align="center">dpkg</th>
</tr>
</thead>
<tbody><tr>
<td align="left">安装指定套件</td>
<td align="center">rpm -i pkgfile.rpm</td>
<td align="center">dpkg -i pkgfile.deb</td>
</tr>
<tr>
<td align="left">显示所有已安装的套件名称</td>
<td align="center">rpm -qa</td>
<td align="center">dpkg -l</td>
</tr>
<tr>
<td align="left">显示套件包含的所有档案</td>
<td align="center">rpm -ql [softwarename]</td>
<td align="center">dpkg -L [softwarename]</td>
</tr>
<tr>
<td align="left">显示特定档案所属套件名称</td>
<td align="center">rpm -qf [/path/to/file]</td>
<td align="center">dpkg -S [/path/to/file]</td>
</tr>
<tr>
<td align="left">显示制定套件是否安装</td>
<td align="center">rpm -q [softwarename]</td>
<td align="center">dpkg -l [softwarename], -s或-p显示详细咨询, -l只列出简洁咨询</td>
</tr>
<tr>
<td align="left">移除指定套件</td>
<td align="center">rpm -e [softwarename]</td>
<td align="center">dpkg -r softwarename, -r 留下套件设定, -P完全移除</td>
</tr>
</tbody></table>
<hr>
<h3 id="apt和yum常用命令总结"><a href="#apt和yum常用命令总结" class="headerlink" title="apt和yum常用命令总结"></a>apt和yum常用命令总结</h3><ul>
<li>参考博客: <a href="https://cnblogs.com/lanbosm/p/9130211.html" target="_blank" rel="noopener">https://cnblogs.com/lanbosm/p/9130211.html</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">操作描述</th>
<th align="center">yum</th>
<th align="center">apt</th>
</tr>
</thead>
<tbody><tr>
<td align="left">软件源配置文件路径</td>
<td align="center">/etc/yum.conf</td>
<td align="center">/etc/apt/sources.list</td>
</tr>
<tr>
<td align="left">安装软件包</td>
<td align="center">yum install [package]</td>
<td align="center">apt-get install [package]</td>
</tr>
<tr>
<td align="left">删除软件包</td>
<td align="center">yum uninstall [package]</td>
<td align="center">apt-get remove [package]</td>
</tr>
<tr>
<td align="left">删除有依赖关系的软件包和配置文件</td>
<td align="center">yum uninstall [package]</td>
<td align="center">apt-get autoremove [package] –purge</td>
</tr>
<tr>
<td align="left">查看安装包信息</td>
<td align="center">yum info [package]</td>
<td align="center">apt-cache show [package]</td>
</tr>
<tr>
<td align="left">更新软件包列表</td>
<td align="center">yum update</td>
<td align="center">apt-get update</td>
</tr>
<tr>
<td align="left">清空缓存</td>
<td align="center">yum clean</td>
<td align="center">apt-get clean</td>
</tr>
<tr>
<td align="left">搜索包名</td>
<td align="center">yum</td>
<td align="center">apt-cahce search</td>
</tr>
</tbody></table>
<hr>
<h3 id="一些特殊命令"><a href="#一些特殊命令" class="headerlink" title="一些特殊命令"></a>一些特殊命令</h3><h4 id="apt"><a href="#apt" class="headerlink" title="apt"></a>apt</h4><ul>
<li><p>列出所有可用包名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-cache pkgnames</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过描述列出包名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-cache search [keys]</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定包的版本号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install [package]=[version]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h4><ul>
<li><p>搜索包的可用版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --showduplicates list [package] | expand</span><br></pre></td></tr></table></figure>

<ul>
<li><code>expand</code>命令用于将文件的制表符<code>tab</code>转换成空格符<code>space</code><ul>
<li>默认一个<code>tab</code>对应8个<code>space</code></li>
<li>若不指定文件名(或者文件名为<code>-</code>), 则<code>expand</code>会从标准输入读取数据</li>
</ul>
</li>
<li><code>unexpand</code>命令与expand相反</li>
</ul>
</li>
<li><p>安装时指定包的版本号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install [package]-[version]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="yum和apt安装的常用参数"><a href="#yum和apt安装的常用参数" class="headerlink" title="yum和apt安装的常用参数"></a>yum和apt安装的常用参数</h4><ul>
<li><code>-y</code>: 指定在询问是否安装时均选择<code>yes</code></li>
<li><code>-q</code>: <code>quiet</code>,安装途中不打印log信息</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Others/General——各种压缩方式总结.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Others/General——各种压缩方式总结.html" itemprop="url">General——各种压缩方式总结</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h3 id="tar-gz"><a href="#tar-gz" class="headerlink" title=".tar.gz"></a>.tar.gz</h3><h3 id="tar-bz2"><a href="#tar-bz2" class="headerlink" title=".tar.bz2"></a>.tar.bz2</h3><h3 id="tar-xz"><a href="#tar-xz" class="headerlink" title=".tar.xz"></a>.tar.xz</h3><h3 id="tgz"><a href="#tgz" class="headerlink" title=".tgz"></a>.tgz</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Linux/Centos——clamav安装与杀毒.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Linux/Centos——clamav安装与杀毒.html" itemprop="url">Centos——clamav安装与杀毒</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>clam是一款Linux上免费的杀毒软件</em></p>
<hr>
<h3 id="安装与配置"><a href="#安装与配置" class="headerlink" title="安装与配置"></a>安装与配置</h3><h4 id="命令行安装"><a href="#命令行安装" class="headerlink" title="命令行安装"></a>命令行安装</h4><p><em>命令行安装clamav会自动创建clamav用户和clamav组</em></p>
<ul>
<li><p>Centos</p>
</li>
<li><p>在Centos7亲测*</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install –y clamav clamav-update</span><br></pre></td></tr></table></figure>
</li>
<li><p>ubuntu</p>
</li>
<li><p>在Ubuntu16.04上亲测*</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install clamav</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h4><p><em>如果因为某些原因无法从命令行安装,可以尝试用源码安装,此时需要首先手动创建相关组和用户</em></p>
<h5 id="安装前的配置"><a href="#安装前的配置" class="headerlink" title="安装前的配置"></a>安装前的配置</h5><ul>
<li><p>创建clamav组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupadd clamav</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户并添加到clamav组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -g clamav clamav</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="源码下载和安装"><a href="#源码下载和安装" class="headerlink" title="源码下载和安装"></a>源码下载和安装</h5><ul>
<li><p>下载</p>
<ul>
<li>下载链接: <a href="http://www.clamav.net/downloads" target="_blank" rel="noopener">http://www.clamav.net/downloads</a>, 因为版本可能会有更新,这里我们直接给出网站下载地址,可以随时查看版本信息</li>
<li>找到软件包下载链接后,使用wget下载即可,比如<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.clamav.net/downloads/production/clamav-0.102.0.tar.gz</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>安装</p>
<ul>
<li><p>解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xf clamav-0.102.0.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>切换目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd clamav-0.102.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum/apt install gcc openssl openssl-devel</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="升级病毒库"><a href="#升级病毒库" class="headerlink" title="升级病毒库"></a>升级病毒库</h4><ul>
<li><p>升级命令</p>
<ul>
<li><p>命令行安装后更新命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">freshclam</span><br></pre></td></tr></table></figure>
</li>
<li><p>源码安装后更新命令, 也可以建立软连接后直接使用上面的命令行启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/clamav/bin/freshclam</span><br></pre></td></tr></table></figure>
</li>
<li><p>建立链接指令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s [source path] [target path]</span><br></pre></td></tr></table></figure>


</li>
</ul>
</li>
</ul>
<h4 id="查找病毒文件"><a href="#查找病毒文件" class="headerlink" title="查找病毒文件"></a>查找病毒文件</h4><ul>
<li><p>常用命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup clamscan / -r --infected -l clamscan.log &gt; clamscan.out &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-r</code> 指明递归查杀</li>
<li><code>--infected</code> 表示仅仅输出被感染的部分文件, 否则没有被感染的文件会输出<code>文件名: OK</code>这样无用的信息</li>
<li><code>-l</code> 指明日志文件路径</li>
<li><code>/</code> 是查找的目标目录,如果是整个机器查找则使用<code>/</code></li>
<li>由于查杀病毒需要很长时间,所以建议使用后台进程进行, 如果是远程, 建议使用<code>nohup</code></li>
<li>由于输出非常多,所以一般我们使用<code>clamscan.out</code>和<code>clamscan.log</code>分别存储输出和日志</li>
</ul>
</li>
<li><p>列出被感染文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat clamscan.out | grep FOUND</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/jiahong-head.png" alt="Joe Zhou">
            
              <p class="site-author-name" itemprop="name">Joe Zhou</p>
              <p class="site-description motion-element" itemprop="description">本博客主要用于记录个人学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">279</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JoeZJH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="JoeZJiahong@Foxmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joe Zhou</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
