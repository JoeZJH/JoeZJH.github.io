<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="En/中">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="本博客主要用于记录个人学习笔记">
<meta name="keywords" content="Python, Computer, ML, Linux, Ubuntu, NLP, Git, DL,">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiahong的个人博客">
<meta property="og:url" content="https://JoeZJH.github.io/page/2/index.html">
<meta property="og:site_name" content="Jiahong的个人博客">
<meta property="og:description" content="本博客主要用于记录个人学习笔记">
<meta property="og:locale" content="En/中">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jiahong的个人博客">
<meta name="twitter:description" content="本博客主要用于记录个人学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://JoeZJH.github.io/page/2/">





  <title>Jiahong的个人博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="En/中">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiahong的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">凡事预则立，不预则废</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——Diffuser.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——Diffuser.html" itemprop="url">RL——Diffuser</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><em>Diffuser</em></p>
<ul>
<li>参考链接：<ul>
<li>原始论文：</li>
<li>源码：</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——DDPG和TD3.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——DDPG和TD3.html" itemprop="url">RL——DDPG和TD3</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/384497349" target="_blank" rel="noopener">强化学习之图解PPO算法和TD3算法</a>  <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


</li>
</ul>
</li>
</ul>
<hr>
<h3 id="DPG"><a href="#DPG" class="headerlink" title="DPG"></a>DPG</h3><ul>
<li>原始论文：<a href="https://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">Deterministic Policy Gradient Algorithms</a>（论文中详细描述了SGD，DPG两种算法的off-policy，on-policy版本的分析）  </li>
</ul>
<h4 id="Stochastic-Actor-Critic-Algorithms"><a href="#Stochastic-Actor-Critic-Algorithms" class="headerlink" title="Stochastic Actor-Critic Algorithms"></a>Stochastic Actor-Critic Algorithms</h4><ul>
<li>Critic网络损失函数<br>$$<br>Loss_{\text{critic}} = \sum_t (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ <ul>
<li>这里要求\(a_{t+1} \sim \pi_\theta\)，\(Q^w(s_t,a_t)\)值拟合的目标是策略\(\pi_\theta\)对应的Q值\(Q^{\pi_\theta}(s_{t}, a_t)\)</li>
<li>这里训练使用的\((s_t,a_t,s_{t+1})\)是当前策略采样到的数据（实际上，Q值的学习样本保证\(a_{t+1}\)的采样策略即可，样本可以是任意策略采样的，当然，用当前策略采样的会更好些）</li>
</ul>
</li>
<li>Actor网络优化梯度：<br>$$<br>\nabla_\theta J(\pi_\theta) = \mathbb{E}_{s\sim \rho^{\pi},a\sim\pi_\theta}\left[ \nabla_\theta \log \pi_\theta(a|s)Q^w(s,a) \right]<br>$$<ul>
<li>问题：\(s\sim \rho^{\pi}\)中的\(\pi\)必须是\(\pi_\theta\)吗？<ul>
<li>回答：是的，原始推导中，回合\(\tau\)必须是来自当前策略的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Off-Policy-Actor-Critic"><a href="#Off-Policy-Actor-Critic" class="headerlink" title="Off-Policy Actor Critic"></a>Off-Policy Actor Critic</h4><ul>
<li>Critic网络损失函数<br>$$<br>Loss_{\text{critic}} = \sum_t (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ <ul>
<li>这里要求\(a_{t+1} \sim \pi_\theta\)，\(Q^w(s_t,a_t)\)值拟合的目标是策略\(\pi_\theta\)对应的Q值\(Q^{\pi_\theta}(s_{t}, a_t)\)</li>
<li>这里训练使用的\((s_t,a_t,s_{t+1})\)是当前其他行为策略采样到的数据</li>
</ul>
</li>
<li>Actor网络优化梯度：<br>$$<br>\nabla_\theta J(\pi_\theta) = \mathbb{E}_{s\sim \rho^\beta,a\sim \beta}\left[ \frac{\pi_\theta(a|s)}{\beta_\theta(a|s)} \nabla_\theta \log \pi_\theta(a|s)Q^w(s,a) \right]<br>$$<ul>
<li>问题：<strong>动作的偏移通过重要性采样\(\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)} \)来解决，但是状态也可以吗？</strong> <ul>
<li>回答：不可以，这里状态采用行为策略采样是因为off-policy场景下，策略梯度的目标就是在行为策略采样的状态上最大化目标函数（这样得到的不是最优策略，线上serving时的状态分布肯定与当前行为策略采样的状态不一致，所以是一个妥协的次优解）</li>
</ul>
</li>
<li>思考：<strong>off-policy AC 与 DQN 的区别</strong><ul>
<li>对于DQN，我们通过在每一个状态上让Q值拟合到最优策略对应的Q值（与状态分布无关，任意的状态我们都可以找到最优策略对应的Q值），然后通过\(\mathop{\arg\max}_a Q(s,a)\)来找到最优策略。</li>
<li>对于off-policy AC，如果不考虑状态的分布，这里带来的偏差是从优化目标上出现的，即off-policy AC最大化的目标是，在行为策略采样的状态分布下，寻找一个策略，最大化累计策略收益期望。这里的目标显然与on-policy的原始目标不同了，状态分布线上线下不一致问题会导致天然的偏差。</li>
<li>问题：为什么不可以理解为与DQN一样？任意给定的状态我都做到策略最大化了，实际上就已经求到了最优策略了？（按照这个理解，除了off-policy都会遇到的训练评估数据分布不一致外，没有别的问题？）<ul>
<li>回答：不可以，因为<strong>DQN的目标是拟合\(Q^*(s,a)\)，与状态分布无关</strong>；而<strong>策略梯度法的目标找到一个最优策略\(\pi^*\)，最大化策略该策略下的累计收益，这里要求状态分布和动作分布均来自求解到的最优策略\(\pi^*\)</strong>，off-policy AC下的状态分布是行为策略的，存在偏差</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>off-policy AC方法不常用，因为从目标上天然旧带着偏差</li>
</ul>
<h5 id="Off-Policy-AC如何对混合策略采样的样本进行重要性采样？"><a href="#Off-Policy-AC如何对混合策略采样的样本进行重要性采样？" class="headerlink" title="Off-Policy AC如何对混合策略采样的样本进行重要性采样？"></a>Off-Policy AC如何对混合策略采样的样本进行重要性采样？</h5><ul>
<li>在Replay Buffer中记录下每个样本的采样策略（<a href="https://github.com/Kaixhin/ACER/blob/master/train.py#L194" target="_blank" rel="noopener">代码示例</a>），并在更新时逐个样本计算动作概率比值（<a href="https://github.com/Kaixhin/ACER/blob/master/train.py#L89-L92" target="_blank" rel="noopener">代码示例</a>），参见：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## 策略记录</span><br><span class="line">memory.append(state, action, reward, policy.detach()) </span><br><span class="line"></span><br><span class="line">## 动作概率比值计算</span><br><span class="line">if off_policy:</span><br><span class="line">    rho = policies[i].detach() / old_policies[i]</span><br><span class="line">else:</span><br><span class="line">    rho = torch.ones(1, action_size)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="On-Policy-Deterministic-Actor-Critic"><a href="#On-Policy-Deterministic-Actor-Critic" class="headerlink" title="On-Policy Deterministic Actor-Critic"></a>On-Policy Deterministic Actor-Critic</h4><ul>
<li>优化目标<br>$$ J(\theta) = \int_\mathcal{S} \rho^{\mu_\theta}(s) Q^{\mu_\theta}(s, \mu_\theta(s)) ds $$ <ul>
<li>其中\(\rho^{\mu_\theta}(s’) = \int_\mathcal{S} \sum_{k=1}^\infty \gamma^{k-1} \rho_0(s) \rho^\mu(s \to s’, k) ds\)</li>
</ul>
</li>
<li>确定性梯度定理：<br>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)<br>&amp;= \int_\mathcal{S} \rho^{\mu_\theta}(s) \nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ds \\<br>&amp;= \mathbb{E}_{s \sim \rho^{\mu_\theta}} [\nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}]<br>\end{aligned}<br>$$</li>
<li>确定性策略看作是随机策略的一种特殊形式，也就是策略的概率分布仅在某一个动作上有非零概率(该动作概率为1)。实际上，在DPG的论文中，作者指出：如果对随机策略，通过确定性策略和一个随机变量进行重参数化(re-parameterize)，那么随机策略最终会在方差\(\sigma=0\)时与确定性策略等价。由于随机策略需要对整个状态和动作空间进行积分，我们可以预计随机策略的学习需要比确定性策略更多的样本（这里只是猜测，没有证据证明）</li>
</ul>
<h4 id="Off-Policy-Deterministic-Actor-Critic"><a href="#Off-Policy-Deterministic-Actor-Critic" class="headerlink" title="Off-Policy Deterministic Actor-Critic"></a>Off-Policy Deterministic Actor-Critic</h4><ul>
<li><p>优化目标<br>$$ J_\beta(\theta) = \int_\mathcal{S} \rho^\beta(s) Q^{\mu_\theta}(s, \mu_\theta(s)) ds $$ </p>
<ul>
<li>其中\(\rho^\beta(s)\)是行为策略上采样得到的样本状态分布，这里直接导致了优化目标不是在最优策略下的回合（回合包含状态和动作）分布下的奖励期望最大，相对on-policy Deterministic AC算是次优解</li>
</ul>
</li>
<li><p>推导结果：<br>$$<br>\begin{aligned}<br>\nabla_\theta J_\beta(\theta) &amp;= \mathbb{E}_{s \sim \rho^\beta(s)} \left[\nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s)  \rvert_{a=\mu_\theta(s)} \right]<br>\end{aligned}<br>$$</p>
</li>
<li><p>Critic网络更新（TD-Error）<br>$$<br>Loss_{\text{critic}} = \sum_t (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ </p>
<ul>
<li>这里要求\(a_{t+1} = \mu_\theta(s_{t+1})\)，\(Q^w(s_t,a_t)\)值拟合的目标是策略\(\mu_\theta\)对应的Q值\(Q^{\mu_\theta}(s_{t}, a_t)\)，实际更新中常使用Target网络\(\bar{\theta}\)</li>
<li>这里训练使用的\((s_t,a_t,s_{t+1})\)是行为策略采样到的数据（Q值\(Q^{\mu_\theta}(s_{t}, a_t)\)的学习样本保证\(a_{t+1}\)的采样策略是\(\mu_\theta\)即可，样本可以是任意策略采样的，当然，用当前策略采样的会更好些）</li>
</ul>
</li>
<li><p>Actor网络更新<br>$$<br>\begin{aligned}<br>Loss_{\text{actor}} = - \mathbb{E}_{s_t \sim \rho^\beta(s)} [Q_w(s_t,\mu_\theta(s_t))]<br>\end{aligned}<br>$$</p>
<ul>
<li>上面的Loss求导就可以得到梯度是\(\mathbb{E}_{s \sim \rho^\beta(s)} \left[\nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s)  \rvert_{a=\mu_\theta(s)} \right]\)，与之前推导结论一致</li>
<li>直观上理解，这里的目标是对于任意给定的状态\(s_t\)下（这个状态样本是行为策略采样得到的），找到一个最大最大化当前\(Q_w(s_t,\mu_\theta(s_t)) \)的动作参数\(\mu_\theta\)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h3><ul>
<li>Deep Deterministic Policy Gradient Algorithms，是DPG的Deep网络版本，原始论文地址<a href="https://arxiv.org/pdf/1509.02971" target="_blank" rel="noopener">CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</a>  </li>
</ul>
<h4 id="DDPG训练流程"><a href="#DDPG训练流程" class="headerlink" title="DDPG训练流程"></a>DDPG训练流程</h4><ul>
<li><p>伪代码如下（其中\(\theta^{\mu’}\)和\(\theta^{Q’}\)分别表示策略\(\mu’\)和价值\(Q’\)的参数）：</p>
<img src="/Notes/RL/RL——DDPG和TD3/DDPG-Algorithm.png" title height="60%" width="60%">
<!-- <img src="/Notes/RL/RL——DDPG和TD3/DDPG-Algorithm.png"> -->
</li>
<li><p><strong>随机探索</strong>：做选择动作\(a_t\)时，添加一个随机噪声，可以增强探索能力，使得模型更加鲁棒，如果没有随机噪声，可能会很快收敛到局部最优</p>
</li>
<li><p><strong>软更新</strong>：Target网络的更新选择软更新，DQN中是硬更新</p>
</li>
</ul>
<hr>
<h3 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h3><ul>
<li>TD3是对DDPG的改进，全称为Twin Delayed Deep Deterministic Policy Gradient Algorithm，原始论文：<a href="https://arxiv.org/pdf/1802.09477" target="_blank" rel="noopener">Addressing Function Approximation Error in Actor-Critic Methods</a>  </li>
<li>有两个改进包含在名字中，Twin和Delayed</li>
<li>其他改进是在Actor 的target网络输出中，增加噪声</li>
</ul>
<h4 id="TD3训练流程"><a href="#TD3训练流程" class="headerlink" title="TD3训练流程"></a>TD3训练流程</h4><ul>
<li>伪代码如下（其中，\(t \ \text{mod} \ d\) 表示策略更新比Q值更新慢一些，\(d\) 次Q值更新对应一次策略更新）：<img src="/Notes/RL/RL——DDPG和TD3/TD3-Algorithm.png" title height="40%" width="40%">
<!-- <img src="/Notes/RL/RL——DDPG和TD3/TD3-Algorithm.png"> -->


</li>
</ul>
<h4 id="改进1：Twin"><a href="#改进1：Twin" class="headerlink" title="改进1：Twin"></a>改进1：Twin</h4><ul>
<li>采用双Critic网络（训练网络和target网络均为双网络），缓解Q值高估问题</li>
</ul>
<h4 id="改进2：Delayed"><a href="#改进2：Delayed" class="headerlink" title="改进2：Delayed"></a>改进2：Delayed</h4><ul>
<li>Actor的目标是在Q值更新时，寻找最优的策略，如果Q值更新太快，容易波动，可以让Q值比较稳定了再更新Actor网络</li>
<li>具体做法，Critic网络更新\(d\)次再更新一次Actor</li>
</ul>
<h4 id="改进3：Target策略网络增加噪声"><a href="#改进3：Target策略网络增加噪声" class="headerlink" title="改进3：Target策略网络增加噪声"></a>改进3：Target策略网络增加噪声</h4><ul>
<li>在Actor 的target策略网络输出的策略中，增加噪声，可以缓解Q值高估问题</li>
</ul>
<hr>
<h3 id="TD3-BC（for-Offline-RL）"><a href="#TD3-BC（for-Offline-RL）" class="headerlink" title="TD3+BC（for Offline RL）"></a>TD3+BC（for Offline RL）</h3><ul>
<li>TD3+BC，在TD3的基础上，增加策略模仿，即对策略进行迭代时，损失函数中增加\(loss_{\text{BC}} = (\pi_{\theta}(s) - a)^2\)</li>
<li>DDPG是Online RL的算法，TD3+BC是Offline RL的算法</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——Decision-Transformer.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——Decision-Transformer.html" itemprop="url">RL——Decision-Transformer</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><em>Decision-Transformer，简称DT，使用序列预估的思想去实现决策问题</em></p>
<ul>
<li>参考链接：<ul>
<li>原始论文：<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf" target="_blank" rel="noopener">Decision Transformer: Reinforcement Learning via Sequence Modeling</a>  </li>
<li>源码：<a href="https://github.com/kzl/decision-transformer" target="_blank" rel="noopener">/github.com/kzl/decision-transformer</a>  </li>
</ul>
</li>
</ul>
<hr>
<h3 id="HER技术"><a href="#HER技术" class="headerlink" title="HER技术"></a>HER技术</h3><ul>
<li>在Decision Transformer之前，HER（Hindsight Experience Replay）方法已经有这种事后的思想，HER通过将想要达到的目标状态添加到策略网络的输入端，实现在给定目标的情况下，进行决策</li>
</ul>
<hr>
<h3 id="Decision-Transformer"><a href="#Decision-Transformer" class="headerlink" title="Decision Transformer"></a>Decision Transformer</h3><h4 id="returns-to-go轨迹变换"><a href="#returns-to-go轨迹变换" class="headerlink" title="returns-to-go轨迹变换"></a>returns-to-go轨迹变换</h4><ul>
<li>原始的轨迹：\( \tau = (s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_T,a_T,r_T) \)</li>
<li>returns-to-go对应的轨迹：\( \tau = (\hat{R}_1,s_1,a_1,\hat{R}_2,s_2,a_2,\cdots,\hat{R}_T,s_T,a_T) \)<ul>
<li>\(\hat{R}_t = \sum_{t’=t}^T r_{t’}\)被称为return-to-go（与state、action等一样的粒度），表示复数或者泛指时，也是用returns-to-go</li>
<li>注意，\(\hat{R}_t\)没有使用discount ratio，是无折扣的奖励，方便后续实现中减去已获得的奖励实现目标值变换</li>
</ul>
</li>
</ul>
<h4 id="建模方式"><a href="#建模方式" class="headerlink" title="建模方式"></a>建模方式</h4><ul>
<li>整体架构  <img src="/Notes/RL/RL——Decision-Transformer/DT-Architecture.png" title height="70%" width="70%"></li>
<li>demo  <img src="/Notes/RL/RL——Decision-Transformer/DT-Demo.png" title height="70%" width="70%">

</li>
</ul>
<h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><ul>
<li>在原始Transformer的基础上，DT算法的实现非常简单，DT算法的整体伪代码如下：  <img src="/Notes/RL/RL——Decision-Transformer/DT-Algorithm.png" title height="70%" width="70%"></li>
<li>伪代码讲解：<ul>
<li>token：包含三种模态的token，分别为return-to-go、state和action</li>
<li>位置编码：虽然是三个token，但是同一个时间片的return-to-go、state和action，对应的位置编码相同</li>
<li>模型的输入：过去\(K-1\)个时间片的(return-to-go,state,action)完整信息和当前时间片的(return-to-go,state)，共\((K-1)*3+2\)个tokens</li>
<li>输出：仅在输入是state token对应的位置上，输出action token为决策目标</li>
<li>损失函数：伪代码中使用的是MSE损失函数（对应连续动作场景），实际上对于离散动作场景， 可以使用交叉熵损失函数（策略网络输出Softmax后的多个头）</li>
<li>训练时：<ul>
<li>每个样本仅保留最近的\(K\)个步骤，模型输入是\(K*\)个样本</li>
<li>时间步\(t\)是一直累计的，与\(K\)无关</li>
</ul>
</li>
<li>推断时的自回归：<ul>
<li>初始化时先指定初始状态\(s_0\)和最终目标target_return \(R_0\)</li>
<li>通过DT模型决策得到动作\(a_t\)</li>
<li>与环境交互执行动作\(a_t\)并得到reward \(r_{t}\)，并跳转到状态\(s_{t+1}\)</li>
<li>通过reward \(r_t\)计算下个时间步的return-to-go \(R_{t+1} = R_t - r_t\)</li>
<li>将\((a_t, s_{t+1}, R_{t+1})\)分别加入到各自token列表中</li>
<li>截断到\(K\)个时间步，注意动作是保留\(K-1\)个，不足\(K\)个时间步时，会自动paddding（直接调用Transformer）即可，此时也需要保证模型输入action比return-to-go和state少一个</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="预测时如何指定Reward目标？"><a href="#预测时如何指定Reward目标？" class="headerlink" title="预测时如何指定Reward目标？"></a>预测时如何指定Reward目标？</h4><ul>
<li><p>可以使用离线采样样本中Reward最大值作为目标，论文原始表述如下</p>
  <img src="/Notes/RL/RL——Decision-Transformer/DT-Setting-of-Return-to-go.png" title height="100%" width="100%">
</li>
<li><p>个人理解：这个目标不一定要是最优目标，也不需要与离线目标完全相等，但是比较难设置：</p>
<ul>
<li>如果太小，但是生成的不一定是最优的路径</li>
<li>如果太大，理论上，可以生成最优解，但是因为模型没有见过该目标值（模型做不到，因为训练时也收集不到这样的样本），可能会发生意想不到情况</li>
</ul>
</li>
</ul>
<hr>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul>
<li><p>Atari上收集的实验数据集训练的实验结果见如下图：</p>
  <img src="/Notes/RL/RL——Decision-Transformer/DT-Experiments-Atari.png" title height="100%" width="100%">

<ul>
<li>在部分场景上，CQL效果更好</li>
</ul>
</li>
<li><p>D4RL以及OpenAI-Gym上收集的数据集上的实验结果如下图（注意，补充了一些）：</p>
  <img src="/Notes/RL/RL——Decision-Transformer/DT-Experiments-OpenAI-Gym-and-D4RL.png" title height="100%" width="100%">

<ul>
<li>补充了一些D4RL中没有的数据集（Medium是指直接用Medium Agent与环境交互生成的样本；Medium-Replay是指训练一个Medium Agent时收集的Replay Buffer；Medium-Expert是Medium Agent和Expert Agent两种策略收集到的数据集的混合）<img src="/Notes/RL/RL——Decision-Transformer/DT-Experiments-OpenAI-Gym-Replay.png" title height="100%" width="100%">

</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——HER技术.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——HER技术.html" itemprop="url">RL——HER技术</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><em>Hindsight Experience Replay，简称HER，一种用于解决奖励稀疏问题的方法</em></p>
<ul>
<li>参考链接：<ul>
<li>原始论文：<a href="https://arxiv.org/pdf/1707.01495" target="_blank" rel="noopener">Hindsight Experience Replay，NIPS 2017，OpenAI</a>  </li>
<li>论文解读：<a href="https://stepneverstop.github.io/Hindsight-Experience-Replay.html" target="_blank" rel="noopener">Hindsight Experience Replay，Kenvnn’s Blog</a>，本文的许多笔记参考自该博客  </li>
</ul>
</li>
</ul>
<hr>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。</li>
</ul>
<hr>
<h3 id="HER技术思路"><a href="#HER技术思路" class="headerlink" title="HER技术思路"></a>HER技术思路</h3><ul>
<li>奖励函数的设计往往需要很强的先验知识，且往往比较难，特别在稀疏奖励和二分奖励场景中<ul>
<li>系数奖励：完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多</li>
<li>二分奖励：完成目标时，奖励为A值，完不成目标时，奖励为B值</li>
</ul>
</li>
<li>一句话说：通过在经验池中引入“Hindsight”（事后聪明）来解决稀疏奖励和二分奖励中的问题<ul>
<li>Hindsight的理解：hind表示“后面的”，sight则表示“看见；视力；视野等”，综合起来表示“事后聪明；事后的领悟”。与Foresight表示“先见之明”对比来看，翻译Hindsight为“后见之明”可能更为合适</li>
</ul>
</li>
<li>可以用于所有Off-policy方法中</li>
</ul>
<hr>
<h3 id="A-motivating-example"><a href="#A-motivating-example" class="headerlink" title="A motivating example"></a>A motivating example</h3><ul>
<li>一个来自HER论文中的例子：bit-flipping environment<ul>
<li>状态空间\(\mathcal{S}=\left \{ 0,1 \right \}^{n}\)，维度为n</li>
<li>动作空间\(\mathcal{A}=\left \{ 0,1,\cdots,n-1 \right \}\)，动作空间大小也为n</li>
<li>规则：对于每个episode，给定目标状态\(s_{g}\)，从任意初始状态\(s_{0}\)（如\(n=5，s_{0}=10101\)）每一步从动作空间中选取一个动作\(a\)，翻转\(s_{0}\)第\(a\)个位置的值，如\(a=1\Rightarrow s_{1}=11101\)，直到回合结束或者翻转后的状态与\(s_{g}\)相同</li>
<li>奖励函数：\(r_{g}(s,a)=-\left [ s \neq g \right ]\)，即达到目标状态则为0，未达到目标状态则为-1。（\(s \neq g \Rightarrow true \doteq 1，s = g \Rightarrow false \doteq 0\)）</li>
<li>注：后续将目标状态\(s_{g}\)简化为\(g\)</li>
</ul>
</li>
<li>数组长度越长，反馈越稀疏，当\(n&gt;40\)时，几乎没有除了-1以外的奖励，标准的强化学习算法很难求解该问题。即使使用一些探索技术也不行，因为这个问题完全不是缺乏探索，而是状态太多，探索不完，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。</li>
</ul>
<hr>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><h4 id="reward-shaping"><a href="#reward-shaping" class="headerlink" title="reward shaping"></a>reward shaping</h4><ul>
<li>将reward设计成某些变量的函数，如\(r_{g}(s,a)=-\left || s-g \right ||^{2}\)，将训练的算法逐步引导至奖励函数增大的决策空间。<ul>
<li>该方案在这种场景中可以使用，但是不通用，无法应用到其他更加复杂的现实问题中</li>
</ul>
</li>
</ul>
<h4 id="任务分解"><a href="#任务分解" class="headerlink" title="任务分解"></a>任务分解</h4><ul>
<li>将目标分解成多个粒度更小的，更容易解决的任务，使用类似层次强化学习（Hierarchical reinforcement learning）的方法去解决</li>
<li>这个文章中没有提到，一些书籍会提到</li>
</ul>
<h4 id="HER的做法"><a href="#HER的做法" class="headerlink" title="HER的做法"></a>HER的做法</h4><ul>
<li><p>HER的主要思想就是：</p>
<ul>
<li>通过修改目标简化问题，从而让奖励变得稠密。具体地，假定序列为\(s_{0},s_{1},s_{2}, \cdots ,s_{T}\)，目标为\(g\)，如果将目标状态\(g\)修改为\(s_{T}\)，即\(g=s_{T}\)，那么这样看来智能体就可以获得奖励了</li>
<li>利用这个思想对经验池进行扩充即可，可以将稀疏奖励问题给转化成非稀疏奖励，使得奖励变得稠密</li>
</ul>
</li>
<li><p>HER具体做法：</p>
<ul>
<li>将经验池中的状态\(s\)改为\(s||g\)，也就是tf.concat(s,g)</li>
<li>训练算法的输入也是\(s||g\)，也就是需要在当前状态后边连结上每个episode的目标状态，每个episode的目标状态可能不同</li>
<li>HER对经验池进行了扩充，不仅存入实际采样得到的transition/experience，\(\left ( s_{t}||g,a_{t},r_{t},s_{t+1}||g \right )\)，也要在回合结束时重新设置目标状态，得到相应的奖励值（在二分奖励问题中，只有在\(s=g\)时奖励才需要更改），存入“事后”（当初如果这样就好啦！）的经验\(\left ( s_{t}||g’,a_{t},r_{t}’,s_{t+1}||g’ \right )\)，详见伪代码，这个事后经验究竟存入多少份、多少种，由超参数\(k\)控制，下文讲解。</li>
<li>HER更适合解决多目标问题，多目标的意思为，目标点非固定，每个episode的目标状态可以不相同。详见实验部分</li>
</ul>
</li>
<li><p>HER的几种扩展方式：</p>
<ul>
<li>未来模式——future：在一个序列\(s_{0},s_{1},s_{2},\cdots,s_{T}\)中，如果遍历到状态\(s_{2}\)，则在\(s_{3},\cdots,s_{T}\)之间随机抽取\(k\)个状态作为目标状态\(g’\)，并依此向经验池中存入\(\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \right )\)，特点：一个episode的后续部分</li>
<li>回合模式——episode：在一个序列\(s_{0},s_{1},s_{2},…,s_{T}\)中，如果遍历到状态\(s_{2}\)，则在整个序列中随机抽取\(k\)个状态作为目标状态\(g’\)，并依此向经验池中存入\(\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \right )\)，特点：一个episode</li>
<li>随机模式——random：在一个序列\(s_{0},s_{1},s_{2},…,s_{T}\)中，如果遍历到状态\(s_{2}\)，则在多个序列\(\tau_{0},\tau_{1},\tau_{2},\cdots\)中随机抽取\(k\)个状态作为目标状态\(g’\)，并依此向经验池中存入\(\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \right )\)，特点：多个episode</li>
<li>最终模式——final：在一个序列\(s_{0},s_{1},s_{2},\cdots,s_{T}\)中，如果遍历到状态\(s_{2}\)，则之间令\(g’=s_{T}\)，并向经验池中存入\(\left ( s_{2}||g’,a_{2},r_{2}’,s_{3}||g’ \right )\)，特点：一个episode的最后一个状态，如果设置k，则存入k个相同的经验</li>
<li>注：上面的\(s_{T}\)不一定是当前episode的最终目标值，因为此时我们拟合的本意已经变成了“任意给定一个起始状态\(s_0\)和目标状态\(s_i\)，找到一个动作来实现将动作从\(s_0\)变化到\(s_i\)”</li>
</ul>
</li>
<li><p>伪代码及其解析</p>
  <img src="/Notes/RL/RL——HER技术/HER-Algorithm.png" title height="100%" width="100%">

<ul>
<li>\(r(s,a,g)\)表示奖励不仅仅与当前状态和动作有关，还与最终的目标状态有关</li>
<li>伪代码中没有提到超参数\(k\)，其实在循环条件\(\textbf{for} \ g’ \in G \ \textbf{do}\)中循环执行了\(k\)次</li>
<li>\(||\)操作为连结操作，简言之，将两个长度为5的向量合并成一个长度为10的向量</li>
<li>\(G:=\mathbb{S}(\textbf{current episode})\)即为上文提到的四种扩展模式：future、episode、random、final。</li>
<li>奖励函数\(r(s,a,g)=-\left [ f_{g}(s)=0 \right ]\)即为前文提到的\(r_{g}(s,a)=-\left [ s \neq g \right ]\)，即完成为0，未完成为-1，具体奖励函数可以根据我们的使用环境设计</li>
<li>\(a_{t} \leftarrow \pi_{b}(s_{t}||g)\)表示神经网络的输入为当前状态与目标状态的连结</li>
</ul>
</li>
<li><p>HER的优点</p>
<ul>
<li>可解决稀疏奖励、二分奖励问题</li>
<li>可适用于所有的Off-Policy算法</li>
<li>提升了数据采样效率</li>
</ul>
</li>
</ul>
<hr>
<h3 id="如何选择HER的模式？"><a href="#如何选择HER的模式？" class="headerlink" title="如何选择HER的模式？"></a>如何选择HER的模式？</h3><ul>
<li>实验结果：<ul>
<li>效果：future&gt;final&gt;episode&gt;random&gt;no HER</li>
<li>稳定性：final(好)=no-HER(差)&gt;future&gt;episode&gt;random </li>
</ul>
</li>
</ul>
<hr>
<h3 id="超参数的设置"><a href="#超参数的设置" class="headerlink" title="超参数的设置"></a>超参数的设置</h3><ul>
<li>\(k\)值不能太大，太大了会导致数据集中真实样本减少，出现问题</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——PPO.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——PPO.html" itemprop="url">RL——PPO</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<h3 id="PPO方法介绍"><a href="#PPO方法介绍" class="headerlink" title="PPO方法介绍"></a>PPO方法介绍</h3><h4 id="PPO的目标"><a href="#PPO的目标" class="headerlink" title="PPO的目标"></a>PPO的目标</h4><p>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</p>
<ul>
<li>PPO目标详细推导见<a href="/Notes/RL/RL%E2%80%94%E2%80%94TRPO-PPO-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%9F%BA%E7%A1%80%E6%8E%A8%E5%AF%BC.html">RL——TRPO-PPO-目标函数基础推导</a></li>
</ul>
<h4 id="PPO-Penalty"><a href="#PPO-Penalty" class="headerlink" title="PPO-Penalty"></a>PPO-Penalty</h4><ul>
<li>又名PPO-惩罚<br>\begin{aligned}<br>\max_{\theta}&amp;\ \  \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a) - \beta D_{KL}(\pi_{\theta_{\text{old}}}(\cdot|s), \pi_\theta(\cdot|s))\right]<br>\end{aligned}</li>
</ul>
<h4 id="PPO-Clip"><a href="#PPO-Clip" class="headerlink" title="PPO-Clip"></a>PPO-Clip</h4><ul>
<li>又名PPO截断<br>\begin{aligned}<br>\max_\theta&amp;\ \  \mathbb{E}_{s\sim \rho_{\theta_{\text{old}}},a\sim q(a|s)}\min\left(\frac{\pi_\theta(a|s)}{q(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{q(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a,s)\right)<br>\end{aligned}</li>
<li>理论上，以上采样分布可以是任意分布，实际上使用原始策略效果更好（样本利用率也更高）<br>\begin{aligned}<br>\max_\theta&amp;\ \  \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a,s)\right)<br>\end{aligned}</li>
<li>令\(r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \)，则有：<br>\begin{aligned}<br>\max_\theta&amp;\ \  \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\min\left(r(\theta)A_{\theta_{\text{old}}}(s,a), clip\left(r(\theta), 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a,s)\right)<br>\end{aligned}</li>
<li>对Clip的思考和讨论（以下讨论中，）<ul>
<li>Clip方法限制变化过大或者过小的策略，使得这些\((s,a)\)样本在梯度回传时不生效（被固定值限制，所以没有梯度回传，从而保证不会使用这些样本去更新）；</li>
<li>问题：实际上限定住\(r(\theta)\)就能保证更新后的策略不会偏离原始策略太远吗？<ul>
<li>只有\(\pi_\theta\)中包含策略参数，\(A_{\theta_{\text{old}}}(s,a)\)与策略参数无关，所以使得\(r(\theta)\)失效，就可以让偏离太大的样本不影响当前策略更新</li>
</ul>
</li>
</ul>
</li>
<li>关于Clip仅限制了单边的讨论：<ul>
<li>当\(A_{\theta_{\text{old}}}(a,s) &gt; 0\)时，\(r(\theta)\)在Clip后的生效范围在\(r(\theta) \in [0,1+\epsilon]\)，也就是\(1-\epsilon\)边界会失效</li>
<li>当\(A_{\theta_{\text{old}}}(a,s) &lt; 0\)时，\(r(\theta)\)在Clip后的生效范围在\(r(\theta) \in [1-\epsilon,+\infty]\)，也就是\(1+\epsilon\)边界会失效</li>
<li>在on-policy的设定下，我们认为策略新旧策略的比值\(r(\theta)\)不会太大，一般不会出现问题，但是off-policy设定下，可能会出现问题，所以需要再加一层Clip（参考自：<a href="https://huggingface.co/learn/deep-rl-course/unit8/visualize" target="_blank" rel="noopener">Visualize the Clipped Surrogate Objective Function</a>）<br>  \begin{aligned}<br>  \max_\theta&amp;\ \  \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\max( \eta A_{\theta_{\text{old}}}(a,s)), \min\left(r(\theta)A_{\theta_{\text{old}}}(s,a), clip\left(r(\theta), 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a,s)\right)<br>  \end{aligned}<ul>
<li>其中\(\eta\)是超参数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="PPO网络更新"><a href="#PPO网络更新" class="headerlink" title="PPO网络更新"></a>PPO网络更新</h4><ul>
<li>Critic网络更新<br>$$<br>Loss_{\text{critic}} = \sum (r_t + \gamma V^{\bar{w}}(s_{t+1}) - V^{w}(s_{t})) ^ 2<br>$$ <ul>
<li>这里V值拟合的目标是策略\(\pi_\theta\)对应的V值\(V^{\pi_\theta}\)</li>
<li>\(r_t = r(s_t, a_t)\vert_{a_t \sim \pi_\theta(\cdot|s_t)}\)，训练用的整个轨迹链路都是从策略\(\pi_\theta\)采样得到的</li>
</ul>
</li>
<li>Actor网络更新<br>$$<br>Loss_{\text{actor}} = - \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\min\left(r(\theta)A_{\theta_{\text{old}}}(s,a), clip\left(r(\theta), 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a,s)\right)<br>$$ <ul>
<li>其中：\(r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \)</li>
</ul>
</li>
</ul>
<h4 id="PPO实践说明"><a href="#PPO实践说明" class="headerlink" title="PPO实践说明"></a>PPO实践说明</h4><ul>
<li>常用的形式是PPO-Clip形式，实践中效果更好</li>
<li>PPO-Clip中一般设置\(\epsilon=0.2\)</li>
<li>PPO原始论文中，每次采样到的数据会作K次epochs，且不同游戏使用的次数不同，在Mujoco中使用\(epochs=10\)，Roboschool中使用\(epochs=15\)，Atari中使用\(epochs=3\)<img src="/Notes/RL/RL——PPO/PPO-Algorithm.png">

</li>
</ul>
<h4 id="PPO连续动作实现离散动作的实现主要区别"><a href="#PPO连续动作实现离散动作的实现主要区别" class="headerlink" title="PPO连续动作实现离散动作的实现主要区别"></a>PPO连续动作实现离散动作的实现主要区别</h4><h5 id="模型建模"><a href="#模型建模" class="headerlink" title="模型建模"></a>模型建模</h5><ul>
<li><p><strong>策略网络</strong>：连续动作需要使用\(\mu_\theta,\sigma_\theta\)表示均值和方差，连续分布下，每个动作的概率理论上都是0，但借助概率密度函数的含义，可以通过计算</p>
</li>
<li><p><strong>采样方式</strong>：采样时需要创建分布来采样，由于不需要梯度回传，所以不需要使用重参数法</p>
</li>
<li><p><strong>Critic网络</strong>：由于离散连续场景都用V网络，仅仅评估状态下的价值即可，与动作无关，连续动作处理不需要特殊修改</p>
<h5 id="新旧策略比值计算方式不同"><a href="#新旧策略比值计算方式不同" class="headerlink" title="新旧策略比值计算方式不同"></a>新旧策略比值计算方式不同</h5></li>
<li><p>离线动作按照推导中的实现即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def compute_surrogate_obj(self, states, actions, advantage, old_log_probs, actor):  # 计算策略目标</span><br><span class="line">    log_probs = torch.log(actor(states).gather(1, actions))</span><br><span class="line">    ratio = torch.exp(log_probs - old_log_probs)</span><br><span class="line">    return torch.mean(ratio * advantage)</span><br></pre></td></tr></table></figure>
</li>
<li><p>连续动作需要使用概率密度函数来实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def compute_surrogate_obj(self, states, actions, advantage, old_log_probs, actor):</span><br><span class="line">    mu, std = actor(states)</span><br><span class="line">    action_dists = torch.distributions.Normal(mu, std)</span><br><span class="line">    log_probs = action_dists.log_prob(actions) # 返回\log(f(actions)),f为概率密度函数</span><br><span class="line">    ratio = torch.exp(log_probs - old_log_probs) # 这里可以直接用于算概率之间的比值理论是概率密度函数的含义</span><br><span class="line">    return torch.mean(ratio * advantage) # 注意torch内部实现这里的梯度可以回传到actor网络上（基于参数mu,std可以运算得到log_prob，所以梯度可以回传）</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="PPO的训练技巧"><a href="#PPO的训练技巧" class="headerlink" title="PPO的训练技巧"></a>PPO的训练技巧</h3><ul>
<li>参考：<a href="https://zhuanlan.zhihu.com/p/512327050" target="_blank" rel="noopener">影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）</a>  </li>
<li>TODO：真实训练一下，测试各种优化技巧的收益</li>
</ul>
<h4 id="Advantage-Normalization"><a href="#Advantage-Normalization" class="headerlink" title="Advantage Normalization"></a>Advantage Normalization</h4><ul>
<li>最早出自<a href="https://proceedings.mlr.press/v80/tucker18a/tucker18a.pdf" target="_blank" rel="noopener">The Mirage of Action-Dependent Baselines in Reinforcement Learning</a>，对Advantage   Function进行归一化，用于提升PG方法的性能</li>
<li>具体方法：减去均值除以方差</li>
<li>实现方案：<ul>
<li>方案一：Batch Advantage Normalization（BAN），对当前Batch的所有Advantage求均值和方差</li>
<li>方案二：Mini-Batch Advantage Normalization（MBAN），仅对当前Mini-Batch Advantage Normalization</li>
</ul>
</li>
<li>实践中，BAN效果最好，MBAN效果次之，不使用任何AN效果最差；<ul>
<li>理解，方案一和方案二并未限定具体采样的轨迹是多少个，但是主要思路是尽量在更多的样本上统计均值和方差，减少波动，这样效果更好些</li>
</ul>
</li>
<li>关于MBAN，<a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/" target="_blank" rel="noopener">ppo-implementation-details</a>博客中有详细实现  </li>
<li>问题：GAE中还需要做Advantage Normalization吗？是否是在计算GAE之前做归一化？</li>
</ul>
<h4 id="State-Normalization"><a href="#State-Normalization" class="headerlink" title="State Normalization"></a>State Normalization</h4><ul>
<li>对状态做归一化</li>
<li>State Normalization的核心在于，与环境交互的过程中，维护一个动态的关于所有经历过的所有state的mean和std, 然后对当前的获得的state做normalization。经过normalization后的state符合mean=0,std=1的正态分布，用这样的状态作为神经网络的输入，更有利于神经网络的训练。</li>
<li>采用滑动增量更新的方式（详细证明见附录）：<ul>
<li>均值：\(\mu_{\text{new}} = \mu_{\text{old}} + \frac{1}{n}(x-\mu_{\text{old}})\)</li>
<li>方差中间变量：\(S_{\text{new}} = S_{\text{old}} + (x-\mu_{\text{old}})\cdot(x-\mu_{\text{new}})\)<ul>
<li>注意这个值除以n才是方差</li>
</ul>
</li>
</ul>
</li>
<li>代码：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class RunningMeanStd:</span><br><span class="line">    # Dynamically calculate mean and std</span><br><span class="line">    def __init__(self, shape):  # shape:the dimension of input data</span><br><span class="line">        self.n = 0</span><br><span class="line">        self.mean = np.zeros(shape)</span><br><span class="line">        self.S = np.zeros(shape)</span><br><span class="line">        self.std = np.sqrt(self.S)</span><br><span class="line"></span><br><span class="line">    def update(self, x):</span><br><span class="line">        x = np.array(x)</span><br><span class="line">        self.n += 1</span><br><span class="line">        if self.n == 1:</span><br><span class="line">            self.mean = x</span><br><span class="line">            self.std = x</span><br><span class="line">        else:</span><br><span class="line">            old_mean = self.mean.copy()</span><br><span class="line">            self.mean = old_mean + (x - old_mean) / self.n</span><br><span class="line">            self.S = self.S + (x - old_mean) * (x - self.mean)</span><br><span class="line">            self.std = np.sqrt(self.S / self.n )</span><br><span class="line"></span><br><span class="line">class Normalization:</span><br><span class="line">    def __init__(self, shape):</span><br><span class="line">        self.running_ms = RunningMeanStd(shape=shape)</span><br><span class="line"></span><br><span class="line">    def __call__(self, x, update=True):</span><br><span class="line">        # Whether to update the mean and std,during the evaluating,update=Flase</span><br><span class="line">        if update:  </span><br><span class="line">            self.running_ms.update(x)</span><br><span class="line">        x = (x - self.running_ms.mean) / (self.running_ms.std + 1e-8)</span><br><span class="line"></span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="Reward-Normalization"><a href="#Reward-Normalization" class="headerlink" title="Reward Normalization"></a>Reward Normalization</h4><ul>
<li>与State Normalization的方案，动态维护所有获得过的Reward的Mean和Std，然后再对当前的Reward做Normalization</li>
<li>问题：是对单次Reward做了Normalization吗？如果Reward已经做了Normalization，GAE中实际上就已经使用了标准化后的Reward了，Advantage是否还需要做呢？</li>
<li>常常被替换为Reward Scaling</li>
</ul>
<h4 id="Reward-Scaling"><a href="#Reward-Scaling" class="headerlink" title="Reward Scaling"></a>Reward Scaling</h4><ul>
<li>相关论文：《PPO-Implementation matters in deep policy gradients A case study on PPO and TRPO》</li>
<li>Reward Scaling与Reward Normalization的区别在于，Reward Scaling是动态计算一个standard deviation of a rolling discounted sum of the rewards，然后只对当前的reward除以这个std（不减去均值？）</li>
<li>Reward Normalization和Reward Scaling二选一即可，建议使用Reward Scaling而不是Reward Normalization即可</li>
<li>代码实现<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class RewardScaling:</span><br><span class="line">    def __init__(self, shape, gamma):</span><br><span class="line">        self.shape = shape  # reward shape=1</span><br><span class="line">        self.gamma = gamma  # discount factor</span><br><span class="line">        self.running_ms = RunningMeanStd(shape=self.shape)</span><br><span class="line">        self.R = np.zeros(self.shape)</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        self.R = self.gamma * self.R + x</span><br><span class="line">        self.running_ms.update(self.R)</span><br><span class="line">        x = x / (self.running_ms.std + 1e-8)  # Only divided std</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def reset(self):  # When an episode is done,we should reset &apos;self.R&apos;</span><br><span class="line">        self.R = np.zeros(self.shape)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="Policy-Entropy"><a href="#Policy-Entropy" class="headerlink" title="Policy Entropy"></a>Policy Entropy</h4><ul>
<li>在actor的loss中增加一项策略熵，在最大化收益的同时，最大化策略熵，增加探索性（理解：同时有正则的作用）</li>
</ul>
<h4 id="Learning-Rate-Decay"><a href="#Learning-Rate-Decay" class="headerlink" title="Learning Rate Decay"></a>Learning Rate Decay</h4><ul>
<li>学习率逐步衰减</li>
<li>代码<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def lr_decay(self, total_steps):</span><br><span class="line">    lr_a_now = self.lr_a * (1 - total_steps / self.max_train_steps)</span><br><span class="line">    lr_c_now = self.lr_c * (1 - total_steps / self.max_train_steps)</span><br><span class="line">    for p in self.optimizer_actor.param_groups:</span><br><span class="line">        p[&apos;lr&apos;] = lr_a_now</span><br><span class="line">    for p in self.optimizer_critic.param_groups:</span><br><span class="line">        p[&apos;lr&apos;] = lr_c_now</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="Gradient-Clip"><a href="#Gradient-Clip" class="headerlink" title="Gradient Clip"></a>Gradient Clip</h4><ul>
<li>梯度裁剪</li>
<li>代码<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Update actor</span><br><span class="line">self.optimizer_actor.zero_grad()</span><br><span class="line">actor_loss.mean().backward()</span><br><span class="line">if self.use_grad_clip: # Trick 7: Gradient clip</span><br><span class="line">    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5) </span><br><span class="line">self.optimizer_actor.step()</span><br><span class="line"></span><br><span class="line"># Update critic</span><br><span class="line">self.optimizer_critic.zero_grad()</span><br><span class="line">critic_loss.backward()</span><br><span class="line">if self.use_grad_clip: # Trick 7: Gradient clip</span><br><span class="line">    torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5) </span><br><span class="line">self.optimizer_critic.step()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="Orthogonal-Initialization"><a href="#Orthogonal-Initialization" class="headerlink" title="Orthogonal Initialization"></a>Orthogonal Initialization</h4><ul>
<li>正交初始化（Orthogonal Initialization）是为了防止在训练开始时出现梯度消失、梯度爆炸等问题所提出的一种神经网络初始化方式。具体的方法分为两步：<ul>
<li>用均值为0，标准差为1的高斯分布初始化权重矩阵</li>
<li>对这个权重矩阵进行奇异值分解，得到两个正交矩阵，取其中之一作为该层神经网络的权重矩阵。</li>
</ul>
</li>
<li>代码<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># orthogonal init</span><br><span class="line">def orthogonal_init(layer, gain=1.0):</span><br><span class="line">    nn.init.orthogonal_(layer.weight, gain=gain)</span><br><span class="line">    nn.init.constant_(layer.bias, 0)</span><br><span class="line"></span><br><span class="line">class Actor_Gaussian(nn.Module):</span><br><span class="line">    def __init__(self, args):</span><br><span class="line">        super(Actor_Gaussian, self).__init__()</span><br><span class="line">        self.max_action = args.max_action</span><br><span class="line">        self.fc1 = nn.Linear(args.state_dim, args.hidden_width)</span><br><span class="line">        self.fc2 = nn.Linear(args.hidden_width, args.hidden_width)</span><br><span class="line">        self.mean_layer = nn.Linear(args.hidden_width, args.action_dim)</span><br><span class="line">        self.log_std = nn.Parameter(torch.zeros(1, args.action_dim))  # We use &apos;nn.Paremeter&apos; to train log_std automatically</span><br><span class="line">        if args.use_orthogonal_init:</span><br><span class="line">            print(&quot;------use_orthogonal_init------&quot;)</span><br><span class="line">            orthogonal_init(self.fc1)</span><br><span class="line">            orthogonal_init(self.fc2)</span><br><span class="line">            orthogonal_init(self.mean_layer, gain=0.01)</span><br><span class="line"></span><br><span class="line">    def forward(self, s):</span><br><span class="line">        s = torch.tanh(self.fc1(s))</span><br><span class="line">        s = torch.tanh(self.fc2(s))</span><br><span class="line">        mean = self.max_action * torch.tanh(self.mean_layer(s))  # [-1,1]-&gt;[-max_action,max_action]</span><br><span class="line">        return mean</span><br><span class="line"></span><br><span class="line">    def get_dist(self, s):</span><br><span class="line">        mean = self.forward(s)</span><br><span class="line">        log_std = self.log_std.expand_as(mean)  # To make &apos;log_std&apos; have the same dimension as &apos;mean&apos;</span><br><span class="line">        std = torch.exp(log_std)  # The reason we train the &apos;log_std&apos; is to ensure std=exp(log_std)&gt;0</span><br><span class="line">        dist = Normal(mean, std)  # Get the Gaussian distribution</span><br><span class="line">        return dist</span><br><span class="line"></span><br><span class="line">class Critic(nn.Module):</span><br><span class="line">    def __init__(self, args):</span><br><span class="line">        super(Critic, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(args.state_dim, args.hidden_width)</span><br><span class="line">        self.fc2 = nn.Linear(args.hidden_width, args.hidden_width)</span><br><span class="line">        self.fc3 = nn.Linear(args.hidden_width, 1)</span><br><span class="line">        if args.use_orthogonal_init:</span><br><span class="line">            print(&quot;------use_orthogonal_init------&quot;)</span><br><span class="line">            orthogonal_init(self.fc1)</span><br><span class="line">            orthogonal_init(self.fc2)</span><br><span class="line">            orthogonal_init(self.fc3)</span><br><span class="line"></span><br><span class="line">    def forward(self, s):</span><br><span class="line">        s = torch.tanh(self.fc1(s))</span><br><span class="line">        s = torch.tanh(self.fc2(s))</span><br><span class="line">        v_s = self.fc3(s)</span><br><span class="line">        return v_s</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="Adam-Optimizer-Epsilon-Parameter"><a href="#Adam-Optimizer-Epsilon-Parameter" class="headerlink" title="Adam Optimizer Epsilon Parameter"></a>Adam Optimizer Epsilon Parameter</h4><ul>
<li>实践中，从官方默认值1e-8改成1e-5</li>
<li>原因？</li>
</ul>
<h4 id="Tanh-Activation-Function"><a href="#Tanh-Activation-Function" class="headerlink" title="Tanh Activation Function"></a>Tanh Activation Function</h4><ul>
<li>将ReLU换成tanh激活函数</li>
<li>建议PPO算法默认使用激活函数</li>
<li>原因？</li>
</ul>
<hr>
<h3 id="附录-均值方差滑动更新公式证明"><a href="#附录-均值方差滑动更新公式证明" class="headerlink" title="附录-均值方差滑动更新公式证明"></a>附录-均值方差滑动更新公式证明</h3><h4 id="均值更新推导"><a href="#均值更新推导" class="headerlink" title="均值更新推导"></a>均值更新推导</h4><ul>
<li>均值更新推导详情：<br>$$<br>\begin{align}<br>\mu_n &amp;= \frac{1}{n}\sum_{i=1}^n x_i \\<br>&amp;= \frac{1}{n}(\sum_{i=1}^{n-1} x_i + x_n) \\<br>&amp;= \frac{1}{n}(\sum_{i=1}^{n-1} x_i + x_n) \\<br>&amp;= \frac{1}{n}((n-1)\cdot\frac{1}{n-1}\sum_{i=1}^{n-1} x_i + x_n) \\<br>&amp;= \frac{1}{n}((n-1)\mu_{n-1} + x_n) \\<br>&amp;= \frac{1}{n}(n\mu_{n-1} + x_n - \mu_{n-1}) \\<br>&amp;= \mu_{n-1} + \frac{1}{n}(x_n - \mu_{n-1})<br>\end{align}<br>$$</li>
</ul>
<h4 id="方差更新推导"><a href="#方差更新推导" class="headerlink" title="方差更新推导"></a>方差更新推导</h4><ul>
<li>将方差中间变量\(S_n\)展开（这里\(S_n\)除以n才是方差）有：<br>$$<br>\begin{align}<br>S_n &amp;= \sum_{i=1}^{n} (x_i - \mu_n)^2 \\<br>&amp;= \sum_{i=1}^{n-1} (x_i - \mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= \sum_{i=1}^{n-1} (x_i - \mu_{n-1} + \mu_{n-1} - \mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= \sum_{i=1}^{n-1} (x_i - \mu_{n-1})^2 + 2(\mu_{n-1} - \mu_{n})\sum_{i=1}^{n-1}(x_i - \mu_{n-1}) + (n-1)(\mu_{n-1} -\mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + 2(\mu_{n-1} - \mu_{n})(\sum_{i=1}^{n-1} x_i - (n-1)\mu_{n-1}) + (n-1)(\mu_{n-1} -\mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + 2(\mu_{n-1} - \mu_{n})((n-1)\mu_{n-1} - (n-1)\mu_{n-1}) + (n-1)(\mu_{n-1} -\mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + 2(\mu_{n-1} - \mu_{n})\cdot 0 + (n-1)(\mu_{n-1} -\mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + (n-1)(\mu_{n-1} -\mu_n)^2 + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + (n-1)(\mu_{n-1} -\mu_n)(\mu_{n-1} -\mu_n) + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + ((n-1)\mu_{n-1} -(n-1)\mu_n)(\mu_{n-1} -\mu_n) + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + (n\mu_{n} - x_n -(n-1)\mu_n)(\mu_{n-1} -\mu_n) + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + (\mu_{n} - x_n)(\mu_{n-1} -\mu_n) + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + (x_n - \mu_{n})(\mu_n - \mu_{n-1}) + (x_n - \mu_n)^2 \\<br>&amp;= S_{n-1} + (x_n - \mu_{n})(\mu_n - \mu_{n-1} + x_n - \mu_n)\\<br>&amp;= S_{n-1} + (x_n - \mu_{n})(x_n - \mu_{n-1})\\<br>\end{align}<br>$$</li>
<li>最终有：\(S_n = S_{n+1} + (x_n - \mu_{n})(x_n - \mu_{n-1})\)</li>
</ul>
<hr>
<h3 id="DPPO-Distributed-PPO"><a href="#DPPO-Distributed-PPO" class="headerlink" title="DPPO(Distributed PPO)"></a>DPPO(Distributed PPO)</h3><ul>
<li>DPPO是PPO的分布式版本，引入了分布式计算的概念，允许多个计算节点（或智能体）并行地与环境交互，收集数据，并将这些数据用于更新全局模型。</li>
<li>分布式架构不仅加快了数据收集的速度，还提高了算法处理大规模并行任务的能力，使得学习过程更加高效</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——TD误差和优势函数的区别.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——TD误差和优势函数的区别.html" itemprop="url">RL——TD误差和优势函数的区别</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/264806566" target="_blank" rel="noopener">TD误差 vs 优势函数 vs贝尔曼误差</a></li>
</ul>
</li>
</ul>
<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>




<hr>
<h3 id="TD误差"><a href="#TD误差" class="headerlink" title="TD误差"></a>TD误差</h3><ul>
<li>时间差分误差，TD error</li>
<li>定义如下：<br>$$<br>\delta_{\theta}(s, a, s’) = R(s, a, s’) + \gamma v_{\theta}(s’) - v_{\theta}(s)<br>$$</li>
<li>\(R(s,a,s’)=r(s,a,s’)\)，表示从状态\(s\)执行\(a\)之后转移到\(s’\)获得的立即回报</li>
<li>TD error是针对确定的\(s’\)来说的</li>
</ul>
<hr>
<h3 id="优势函数"><a href="#优势函数" class="headerlink" title="优势函数"></a>优势函数</h3><ul>
<li>优势函数，Advantage Function<br>$$<br>A_{\theta}(s,a) = \mathbb{E}_{s’\sim P}[\delta_{\theta}(s, a, s’)] = \mathbb{E}_{s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)] - v_{\theta}(s) = Q_{\theta}(s,a) - v_{\theta}(s)<br>$$</li>
<li>优势函数是TD误差关于状态\(s’\)的期望，即从状态\(s\)执行\(a\)之后关于状态\(s’\)的期望</li>
</ul>
<hr>
<h3 id="贝尔曼误差"><a href="#贝尔曼误差" class="headerlink" title="贝尔曼误差"></a>贝尔曼误差</h3><ul>
<li>贝尔曼误差<br>$$<br>\epsilon_{\theta}(s) = \mathbb{E}_{a\sim \pi} [A_{\theta}(s,a)] = \mathbb{E}_{a\sim \pi,s’\sim P}[\delta_{\theta}(s, a, s’)] = \mathbb{E}_{a \sim \pi, s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)] - v_{\theta}(s)<br>$$</li>
<li>贝尔曼误差是优势函数关于动作\(a\)的期望</li>
</ul>
<hr>
<h3 id="期望贝尔曼误差"><a href="#期望贝尔曼误差" class="headerlink" title="期望贝尔曼误差"></a>期望贝尔曼误差</h3><ul>
<li>期望贝尔曼误差<br>$$<br>\mathbb{E}_{s\sim \mu} [\epsilon_{\theta}(s)] = \mathbb{E}_{s\sim \mu}[\mathbb{E}_{a \sim \pi, s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)]] -  \mathbb{E}_{s\sim \mu}[ v_{\theta}(s)]<br>$$</li>
<li>贝尔曼误差是优势函数关于动作\(a\)的期望</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——SAC.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——SAC.html" itemprop="url">RL——SAC</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/385658411" target="_blank" rel="noopener">强化学习之图解SAC算法</a>      </li>
<li><a href="https://blog.csdn.net/baishuiniyaonulia/article/details/121538413" target="_blank" rel="noopener">Soft Actor Critic算法论文公式详解</a>   </li>
</ul>
</li>
</ul>
<hr>
<h3 id="SAC"><a href="#SAC" class="headerlink" title="SAC"></a>SAC</h3><ul>
<li>原始论文：<ul>
<li>论文1（2V+2Q+1策略, 2018，UC Berkeley）：<a href="https://arxiv.org/pdf/1801.01290" target="_blank" rel="noopener">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>      </li>
<li>论文2（4Q+1策略，自适应温度系数，2019，UC Berkeley）：<a href="https://arxiv.org/pdf/1812.05905" target="_blank" rel="noopener">Soft Actor-Critic Algorithms and Applications</a>      </li>
</ul>
</li>
</ul>
<h4 id="相关推导"><a href="#相关推导" class="headerlink" title="相关推导"></a>相关推导</h4><ul>
<li>目标定义<br>$$ J(\phi) = \sum_{t=0}^T \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi_\phi}} [r(s_t, a_t) + \alpha \mathcal{H}(\pi_\phi(\cdot \vert s_t))] $$<ul>
<li>这里目标中增加的熵就是Soft名字的来源</li>
</ul>
</li>
<li>Q值V值定义<br>$$<br>\begin{aligned}<br>Q(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p(\cdot|s_t,a_t)} [V(s_{t+1})] \\<br>V(s_t) &amp;= \mathbb{E}_{a_t \sim \pi} [Q(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)] \\<br>\end{aligned}<br>$$</li>
</ul>
<h4 id="SAC-2V-2Q-1策略"><a href="#SAC-2V-2Q-1策略" class="headerlink" title="SAC(2V+2Q+1策略)"></a>SAC(2V+2Q+1策略)</h4><ul>
<li>原始论文：<a href="https://arxiv.org/pdf/1801.01290" target="_blank" rel="noopener">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>       <ul>
<li>目标定义见上面部分，由于温度系数\(\alpha\)是一个固定的超参数，于是，当我们可以通过将Reward乘以\(\frac{1}{\alpha}\)来Rescaling Reward，使得熵的系数为1，所以本论文后面的推导都可以将\(\alpha=1\)来推导</li>
</ul>
</li>
<li>网络结构<ul>
<li>一个V网络及其对应的一个Target V网络，两个Q网络（Twin Q），一个策略网络</li>
<li>由于存在V网络和Target V网络，不需要Q网络的Target Q网络了</li>
</ul>
</li>
<li>训练流程<img src="/Notes/RL/RL——SAC/SAC-2V-2Q-Algorithm.png">
<ul>
<li>原始论文的伪代码中没有强调Twin Q的使用，但源码实现更新V网络时有使用Twin Q的思想，更新V网络时使用的是两个Q网络中各个动作上的较小者，用于防止V的估计过高</li>
<li>引申问题：策略更新时是否需要使用Twin Q中的较小者计算损失函数呢？<ul>
<li>SAC作者开源的源码中没有使用Twin Q（<a href="https://github.com/haarnoja/sac/blob/master/sac/algos/sac.py#L312-L325" target="_blank" rel="noopener">SAC(V)-原论文开源实现</a>），包括<a href="https://zhuanlan.zhihu.com/p/385658411" target="_blank" rel="noopener">强化学习之图解SAC算法</a>中也没有使用Twin Q，都仅使用了一个Q网络计算策略损失函数，但其他开源实现中许多都取Twin Q的最小值作为损失函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Soft-Policy-Evaluation"><a href="#Soft-Policy-Evaluation" class="headerlink" title="Soft Policy Evaluation"></a>Soft Policy Evaluation</h5><ul>
<li>V值更新：<br>  $$<br>  \begin{aligned}<br>  J_V(\psi) &amp;= \mathbb{E}_{s_t \sim \mathcal{D}} [\frac{1}{2} \big(V_\psi(s_t) - \mathbb{E}_{a_t \sim \pi_\phi}[Q_\theta(s_t, a_t) - \log \pi_\phi(a_t \vert s_t)] \big)^2] \\<br>  \nabla_\psi J_V(\psi) &amp;= \nabla_\psi V_\psi(s_t)\big( V_\psi(s_t) - Q_\theta(s_t, a_t) + \log \pi_\phi (a_t \vert s_t) \big)<br>  \end{aligned}<br>  $$</li>
<li>Q值更新：<br>  $$<br>  \begin{aligned}<br>  J_Q(\theta) &amp;= \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}} [\frac{1}{2}\big( Q_\theta(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p(\cdot|s_t,a_t)}[V_{\bar{\psi}}(s_{t+1})]) \big)^2] \\<br>  \nabla_\theta J_Q(\theta) &amp;= \nabla_\theta Q_\theta(s_t, a_t) \big( Q_\theta(s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1})\big)<br>  \end{aligned}<br>  $$<ul>
<li><strong>策略评估（Q值和V值的更新过程统称为策略评估过程）的收敛性证明</strong>：<ul>
<li>定义回报为\(r_\pi(s_t, a_t) \triangleq r(s_t, a_t) + \alpha \mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t)}[\mathcal{H}(\pi(\cdot\vert s_{t+1}))]\)（注意：其中\(p(s_{t+1}|s_t,a_t)\)在论文中常简写为\(p\)或\(p(\cdot|s_t,a_t)\)，在不影响理解的情况下，可以混用）</li>
<li>则有：\(Q(s_t, a_t) = r_\pi(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t), a_{t+1}\sim\pi}[Q(s_{t+1},a_{t+1})]\)</li>
<li>此时有贝尔曼算子（Bellman backup operator） \(\mathcal{T}^\pi\)为：\(\mathcal{T}^\pi Q(s_t, a_t) \triangleq r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t)}[V(s_{t+1})]\)<ul>
<li>其中\(V(s_{t}) = \mathbb{E}_{a_t \sim \pi}[Q(s_t, a_t) - log\pi(a_t|s_t)]\)</li>
<li>贝尔曼算子\(\mathcal{T}^\pi\)是一种操作符，它表示对当前的价值函数集Q利用贝尔曼方程进行更新</li>
</ul>
</li>
<li><strong>Lemma 1 (Soft Policy Evaluation)</strong>: 按照上式定义的\(\mathcal{T}^\pi\)，当动作维度\(|\mathcal{A}|&lt;\infty\)时，\(Q^{k+1} = \mathcal{T}^\pi Q^{k}\)会收敛到策略\(\pi\)对应的soft Q-value<ul>
<li>问题这里的动作维度有限，是否说明连续动作是不可以的？证明中哪里用到了动作有限？<ul>
<li>Lemma 1 (Soft Policy Evaluation)中引用的论文证明需要保证动作空间有限，从而保证熵是有界的，此时可以保证加入熵以后的reward还是可以收敛的<blockquote>
<p>apply the standard convergence results for policy evaluation (Sutton &amp; Barto, 1998). The assumption |A| &lt; ∞ is required to guarantee that the entropy augmented reward is bounded.</p>
</blockquote>
</li>
<li>连续动作的熵如果是有界的理论上也可以?</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Soft-Policy-Improvement"><a href="#Soft-Policy-Improvement" class="headerlink" title="Soft Policy Improvement"></a>Soft Policy Improvement</h5><ul>
<li><p>策略更新目标推导（新策略本质是在拟合函数Q的玻尔兹曼分布，后续有相关证明这种方式得到的新策略一定优于旧策略）：<br>  $$<br>  \begin{aligned}<br>  \pi_\text{new}<br>  &amp;= \mathop{\arg\min}_{\pi’ \in \Pi} \mathbb{E}_{s_t \sim \mathcal{D}} \Big[ D_\text{KL} \Big( \pi’(\cdot\vert s_t) | \frac{\exp(Q^{\pi_\text{old}}(s_t, \cdot))}{Z^{\pi_\text{old}}(s_t)} \Big) \Big] \\[6pt]<br>  &amp;= \mathop{\arg\min}_{\pi’ \in \Pi} \mathbb{E}_{s_t \sim \mathcal{D}} \Big[ D_\text{KL} \big( \pi’(\cdot\vert s_t) | \exp(Q^{\pi_\text{old}}(s_t, \cdot) - \log Z^{\pi_\text{old}}(s_t)) \big) \Big] \\[6pt]<br>  \text{目标函数: } J_\pi(\phi) &amp;= \mathbb{E}_{s_t \sim \mathcal{D}} \Big[ D_\text{KL} \big( \pi_\phi(\cdot\vert s_t) | \exp(Q_\theta(s_t, \cdot) - \log Z_w(s_t)) \big) \Big] \\[6pt]<br>  &amp;= \mathbb{E}_{s_t \sim \mathcal{D}, a_t\sim\pi_\phi} \Big[ \log \big( \frac{\pi_\phi(a_t \vert s_t)}{\exp(Q_\theta(s_t, a_t) - \log Z_w(s_t))} \big) \Big] \\[6pt]<br>  &amp;= \mathbb{E}_{s_t \sim \mathcal{D}, a_t\sim\pi_\phi} [ \log \pi_\phi(a_t \vert s_t) - Q_\theta(s_t, a_t) + \log Z_w(s_t) ]<br>  \end{aligned}<br>  $$</p>
<ul>
<li>Lemma 2 (Soft Policy Improvement)，详细证明见附录：<ul>
<li>若新策略\(\pi_\text{new}\)定义为\(<br>\pi_\text{new} = \mathop{\arg\min}_{\pi’ \in \Pi} D_\text{KL} \Big( \pi’(\cdot\vert s_t) | \frac{\exp(Q^{\pi_\text{old}}(s_t, \cdot))}{Z^{\pi_\text{old}}(s_t)} \Big)\)</li>
<li>则有\(Q^{\pi_\text{new}}(s_t, a_t) \ge Q^{\pi_\text{old}}(s_t, a_t), \quad \forall (s_t, a_t) \in \mathcal{S} \times \mathcal{A}\)</li>
</ul>
</li>
</ul>
</li>
<li><p>由于\(\log Z_w(s_t)\)与参数\(\phi\)无关，梯度为0，故而可以消掉\(\log Z_w(s_t)\)，最终得到<strong>SAC离散策略的的更新目标</strong>（实现时，离散版本是可以直接按照动作概率加权求和得到期望的）：<br>  $$<br>  J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, a_t\sim\pi_\phi} [ \log \pi_\phi(a_t \vert s_t) - Q_\theta(s_t, a_t) ]<br>  $$</p>
</li>
<li><p><strong>SAC连续动作的策略更新目标</strong>，首先需要采用重参数法建模连续动作\(a_t = f_\phi(\epsilon_t; s_t)\)，然后有：<br>  $$<br>  J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}[\log \pi_\phi(f_\phi(\epsilon_t;s_t)\vert s_t) - Q_\theta(s_t, f_\phi(\epsilon_t; s_t))]<br>  $$</p>
<ul>
<li>\(f_\phi(\epsilon_t;s_t)\)是中按照策略\(\pi_\phi\)输出的均值方差通过采样（重参数化技术）得到的，可以回传梯度</li>
<li>\(\log \pi_\phi(f_\phi(\epsilon_t;s_t)\vert s_t)\)是概率密度函数\(\pi_\phi(\cdot|s_t)\)的对数，也可以回传梯度，注意这里的梯度回传包含了\(\pi_\phi\)和\(f_\phi\)都需要回传回去，由于只需要定义一个\(f_\phi\)后，\(\pi_\phi\)自然也就被定义出来了，无需重新定义参数，所以\(\pi_\phi\)和\(f_\phi\)共用了参数</li>
<li>上述损失函数的近似梯度（移除期望）可以是：<br>$$ \hat{\nabla}_\phi J_\pi(\phi) = \nabla_\phi \log \pi_\phi(\mathbf{a_t} \vert s_t) + (\nabla_{\mathbf{a_t}} \log \pi_\phi(\mathbf{a_t} \vert s_t) - \nabla_{\mathbf{a_t}} Q(s_t, \mathbf{a_t})) \nabla_\phi f_\phi(\epsilon_t; s_t) $$<ul>
<li>论文中伪代码没有显示指定\(\nabla_{\mathbf{a_t}} Q(s_t, \mathbf{a_t})\)中的Q是什么值，实际上实现时，使用的是Twin Q的最小值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Soft-Policy-Iteration"><a href="#Soft-Policy-Iteration" class="headerlink" title="Soft Policy Iteration"></a>Soft Policy Iteration</h5><ul>
<li>Theorem 1 (Soft Policy Iteration). 重复应用上面的Soft Policy Evaluation 和 Soft Policy Improvement，最终策略\(\pi\)会收敛到最优策略\(\pi^*\)，使得\(Q^{\pi^*}(s_t, a_t) \ge Q^{\pi}(s_t, a_t)\)对所有的\(\pi \in \Pi\)和\((s_t, a_t) \in \mathcal{S} \times \mathcal{A}\)成立<ul>
<li>原始论文中有证明，实际上可以表述为“单调有界，所以收敛：Soft Q单调有界，所以会收敛到一个最优的Soft Q，对应的就是最优策略”</li>
</ul>
</li>
</ul>
<h4 id="SAC-4Q-1策略"><a href="#SAC-4Q-1策略" class="headerlink" title="SAC(4Q+1策略)"></a>SAC(4Q+1策略)</h4><ul>
<li><p>原始论文：<a href="https://arxiv.org/pdf/1812.05905" target="_blank" rel="noopener">Soft Actor-Critic Algorithms and Applications</a>      </p>
<ul>
<li>本论文是第一篇论文的改进版本，主要改进是允许温度系数\(\alpha\)变得可学习</li>
<li>由于\(\alpha\)是一个是可学习的变量，所以不再可以像之前的论文一样，通过将Reward乘以\(\frac{1}{\alpha}\)来Rescaling Reward，使得熵的系数为1。所以本论文后面的推导都可以将带着\(\alpha\)</li>
</ul>
</li>
<li><p>网络结构  </p>
<ul>
<li>两个Q网络（Twin Q）及其分别对应的Target Q网络，一个策略网络</li>
<li>两个Q网络分别迭代，有各自的Target Q网络</li>
</ul>
</li>
<li><p>训练流程</p>
<img src="/Notes/RL/RL——SAC/SAC-4Q-Algorithm.png"></li>
<li><p>Q值更新：<br>  $$<br>  \begin{aligned}<br>  J_Q(\theta) &amp;= \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}} [\frac{1}{2}\big( Q_\theta(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p, a_{t+1} \sim \pi_\phi}(Q_\bar{\theta}(s_{t+1}, a_{t+1}) - \alpha \log \pi_\phi(a_{t+1} \vert s_{t+1}))) \big)^2] \\<br>  \end{aligned}<br>  $$</p>
<ul>
<li>论文中没有明确，但实际实现时，\(Q_\bar{\theta}(s_{t+1}, a_{t+1})\)使用的是Twin Q中的较小值</li>
</ul>
</li>
<li><p>策略更新（在前一篇文章的基础上，增加\(\alpha\)即可）：</p>
<ul>
<li><strong>SAC离散策略的的更新目标</strong>：<br>  $$<br>  J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, a_t\sim\pi_\phi} [ \alpha \log \pi_\phi(a_t \vert s_t) - Q_\theta(s_t, a_t) ]<br>  $$</li>
<li><strong>SAC连续动作的策略更新目标</strong>，首先需要采用重参数法建模连续动作\(a_t = f_\phi(\epsilon_t; s_t)\)，然后有：<br>  $$<br>  J_\pi(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}[\alpha \log \pi_\phi(f_\phi(\epsilon_t;s_t)\vert s_t) - Q_\theta(s_t, f_\phi(\epsilon_t; s_t))]<br>  $$</li>
<li>\(f_\phi(\epsilon_t;s_t)\)是中按照策略\(\pi_\phi\)输出的均值方差通过采样（重参数化技术）得到的，可以回传梯度</li>
<li>\(\log \pi_\phi(f_\phi(\epsilon_t;s_t)\vert s_t)\)是概率密度函数\(\pi_\phi(\cdot|s_t)\)的对数，也可以回传梯度，注意这里的梯度回传包含了\(\pi_\phi\)和\(f_\phi\)都需要回传回去，由于只需要定义一个\(f_\phi\)后，\(\pi_\phi\)自然也就被定义出来了，无需重新定义参数，所以\(\pi_\phi\)和\(f_\phi\)共用了参数</li>
<li>上述损失函数的近似梯度（移除期望）可以是：<br>$$ \hat{\nabla}_\phi J_\pi(\phi) = \nabla_\phi \alpha \log \pi_\phi(\mathbf{a_t} \vert s_t) + (\nabla_{\mathbf{a_t}} \alpha \log \pi_\phi(\mathbf{a_t} \vert s_t) - \nabla_{\mathbf{a_t}} Q(s_t, \mathbf{a_t})) \nabla_\phi f_\phi(\epsilon_t; s_t) $$<ul>
<li>论文中伪代码没有显示指定\(\nabla_{\mathbf{a_t}} Q(s_t, \mathbf{a_t})\)中的Q是什么值，实际上实现时，使用的是Twin Q的最小值</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>温度系数\(\alpha\)自动更新：</strong></p>
<ul>
<li><p>将强化学习的目标改成：<br>  $$<br>  \begin{align}<br>  \max_{\pi_0, \dots, \pi_T} \mathbb{E}_{\rho_\pi} &amp;\Big[ \sum_{t=0}^T r(s_t, a_t)\Big] \\<br>  \text{s.t. } \forall t\text{, } &amp;\mathcal{H}(\pi_t) \geq \mathcal{H}_0 \\<br>  \end{align}<br>  $$</p>
</li>
<li><p>即：<br>  $$<br>  \begin{align}<br>  \max_{\pi_0, \dots, \pi_T} \mathbb{E}_{\rho_\pi} &amp;\Big[ \sum_{t=0}^T r(s_t, a_t)\Big] \\<br>  \text{s.t. } \forall t\text{, } \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi_t}}&amp;[-\log(\pi_t(a_t|s_t))] \geq \mathcal{H}_0<br>  \end{align}<br>  $$</p>
<ul>
<li>其中\(\mathcal{H}_0 \)是一个期望的最小熵阈值</li>
</ul>
</li>
<li><p>经过一系列的数学推导，可得\(\alpha\)的更新目标为：<br>  $$J(\alpha) = \mathbb{E}_{a_t \sim \pi_t} [-\alpha \log \pi_t(a_t \mid s_t) - \alpha \mathcal{H}_0]$$</p>
</li>
<li><p>实践：</p>
<ul>
<li><p>从实践经验来看，可以设置\(\mathcal{H}_0 = -dim(\mathcal{A})\)，跟动作空间维度相关（注意不是动作的数量，而是动作的维度，单维的离散动作维度应该是1，单维连续动作的维度也应该是1，比如HalfCheetah-v1动作对应六个关节的扭矩控制输入，故而动作维度是6）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">### 连续动作：</span><br><span class="line">target_entropy = - env.action_space.shape[0] </span><br><span class="line">### 离散动作：</span><br><span class="line">target_entropy = -1</span><br></pre></td></tr></table></figure>
</li>
<li><p>温度系数自动调整方案不一定优于固定温度系数方案，在不同场景下效果不同，详情见原始论文</p>
<img src="/Notes/RL/RL——SAC/SAC-Learned-Temperature.png">

</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="SAC连续动作与离散动作的实现有什么区别"><a href="#SAC连续动作与离散动作的实现有什么区别" class="headerlink" title="SAC连续动作与离散动作的实现有什么区别"></a>SAC连续动作与离散动作的实现有什么区别</h3><h4 id="建模方式"><a href="#建模方式" class="headerlink" title="建模方式"></a>建模方式</h4><ul>
<li><p><strong>策略网络</strong>：连续动作需要使用\(\mu_\phi,\sigma_\phi\)表示均值和方差，连续分布下，每个动作的概率理论上都是0，但借助概率密度函数的含义，可以通过计算概率密度来使用</p>
</li>
<li><p><strong>采样方式</strong>：采样时需要创建分布来采样，由于需要梯度回传，所以需要使用重参数法</p>
</li>
<li><p><strong>log_prob的计算</strong>：借助概率密度函数定义完成log值的抽取</p>
<ul>
<li><p>连续策略对应的动作采样和对数概率计算：1）重参数法；2）tanh转换；3）对数概率计算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">    x = F.relu(self.fc1(x))</span><br><span class="line">    mu = self.fc_mu(x)</span><br><span class="line">    std = F.softplus(self.fc_std(x))</span><br><span class="line">    dist = Normal(mu, std)</span><br><span class="line">    normal_sample = dist.rsample()  # rsample()是重参数化采样</span><br><span class="line">    log_prob = dist.log_prob(normal_sample)</span><br><span class="line">    action = torch.tanh(normal_sample)</span><br><span class="line">    # 计算tanh_normal分布的对数概率密度</span><br><span class="line">    log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)</span><br><span class="line">    action = action * self.action_bound</span><br><span class="line">    return action, log_prob</span><br></pre></td></tr></table></figure>
</li>
<li><p>这里关于<code>log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)</code>是一个推导得到的，若\(y = tanh(x)\)，那么有\(\log p(y) = \log p(x) - \log(1-(tanh(x))^2)\)</p>
</li>
<li><p>使用<code>action = torch.tanh(normal_sample)</code>的原因是想将原来的值放缩到\([-1,1]\)之间，防止原始高斯分布采样到非常离谱的动作</p>
</li>
</ul>
</li>
<li><p><strong>Critic网络</strong>：连续动作下，需要将动作放到输入侧，输出维度为1；离散动作下，可以像连续时一样处理，也可以不输入动作，同时输出维度为动作维度</p>
</li>
</ul>
<h4 id="计算损失函数"><a href="#计算损失函数" class="headerlink" title="计算损失函数"></a>计算损失函数</h4><ul>
<li><p><strong>熵+目标的计算(最大化Q值)的计算</strong>：</p>
<ul>
<li><p>连续场景下（最大化动作下的Q值，梯度直接包含在重参数法采样的动作中；熵则仅使用对数概率，因为连续分布下，难以积分，虽然状态不同，但多个动作自然一起优化，自然就有期望熵的含义了）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">new_actions, log_prob = self.actor(states) # 重参数法</span><br><span class="line">entropy = -log_prob</span><br><span class="line">q1_value = self.critic_1(states, new_actions)</span><br><span class="line">q2_value = self.critic_2(states, new_actions)</span><br><span class="line">actor_loss = torch.mean(-self.log_alpha.exp() * entropy - torch.min(q1_value, q2_value))</span><br></pre></td></tr></table></figure>
</li>
<li><p>离散场景下（用策略输出的分布计算Q值期望；熵也是完整的期望版本）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">probs = self.actor(states)</span><br><span class="line">log_probs = torch.log(probs + 1e-8)</span><br><span class="line"># 直接根据概率计算熵</span><br><span class="line">entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #</span><br><span class="line">q1_value = self.critic_1(states)</span><br><span class="line">q2_value = self.critic_2(states)</span><br><span class="line">min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value), dim=1, keepdim=True)  # 直接根据概率计算期望</span><br><span class="line">actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h4 id="SAC的连续和离散动作的思考"><a href="#SAC的连续和离散动作的思考" class="headerlink" title="SAC的连续和离散动作的思考"></a>SAC的连续和离散动作的思考</h4><ul>
<li>SAC的一个核心创新是使用玻尔兹曼分布去作为策略分布，这使得SAC可以拟合多峰情况，但是对于连续动作的场景中，SAC选择了输出一个高斯分布，也就是让高斯趋近于玻尔兹曼分布。高斯是一个单峰的分布，这意味着连续动作的SAC本质上还是单峰（Unimodal）的算法，而不是soft q-learning的多峰（Multi-modal）</li>
<li>离散情况下，确实可以拟合多峰分布</li>
</ul>
<h4 id="SAC必须是随机策略吗？"><a href="#SAC必须是随机策略吗？" class="headerlink" title="SAC必须是随机策略吗？"></a>SAC必须是随机策略吗？</h4><ul>
<li>是的，确定性策略熵太小，SAC适用于随机策略</li>
<li>问题一：如果SAC直接像DQN一样按照\(\mathop{\arg\max}_a Q(s,a)\)选择动作而不是玻尔兹曼分布会怎样？<ul>
<li>回答：确定性动作的熵为0，所以熵不再有用，整个SAC会退化成DQN</li>
</ul>
</li>
<li>问题二：如果DQN像SAC一样做玻尔兹曼分布会怎样？<ul>
<li>回答：得到的策略不再是最优的，决策到的不再是Q值最大的动作【TODO】</li>
</ul>
</li>
</ul>
<h4 id="为什么最大熵能带来收益呢？"><a href="#为什么最大熵能带来收益呢？" class="headerlink" title="为什么最大熵能带来收益呢？"></a>为什么最大熵能带来收益呢？</h4><ul>
<li><strong>所有可能的模型中熵最大的模型是最好的模型</strong>：这意味着在给定的信息下，模型的选择应该尽可能地保持不确定性，即不做不必要的假设。</li>
<li><strong>熵与正则化</strong>：机器学习/深度学习模型容易过拟合，主要原因是数据不足，建模太复杂等，增加一些正则可以降低过拟合，最大熵可以理解为一种正则</li>
<li><strong>在普通机器学习中收益来源</strong>：主要是防止过拟合（不做不必要的假设）</li>
<li><strong>在强化学习中收益来源</strong>：一方面是防止过拟合，另一方面是让探索更加充分</li>
</ul>
<h4 id="SAC-vs-AC中添加entropy-loss？"><a href="#SAC-vs-AC中添加entropy-loss？" class="headerlink" title="SAC vs AC中添加entropy loss？"></a>SAC vs AC中添加entropy loss？</h4><ul>
<li>SAC将原始AC的目标修改了，理论上目标就是让熵和reward一起最优，Critic网络和策略网络学习的目标都包含了最大化熵</li>
<li>AC 中加入 entropy loss则只是一种正则，在损失函数上对熵进行了一些约束，此时Critic网络学不到熵的信息（此时AC的Actor目标和Critic目标不完全一致）</li>
</ul>
<hr>
<h3 id="DSAC-Distributional-SAC"><a href="#DSAC-Distributional-SAC" class="headerlink" title="DSAC(Distributional SAC)"></a>DSAC(Distributional SAC)</h3><ul>
<li>背景：SAC是基于值学习的方法，值学习方法往往存在值过估计问题，即算法倾向于高估某些状态-动作对的价值，从而影响学习性能</li>
<li>DSAC通过引入值分布的方法来解决值过估计问题</li>
<li>DSAC 还引入了三个重要改进，包括：<ul>
<li>Expected Value Substituting</li>
<li>Variance-Based Critic Gradient Adjusting </li>
<li>Twin Value Distribution Learning</li>
</ul>
</li>
</ul>
<hr>
<h3 id="附录：新策略一定优于旧策略的证明"><a href="#附录：新策略一定优于旧策略的证明" class="headerlink" title="附录：新策略一定优于旧策略的证明"></a>附录：新策略一定优于旧策略的证明</h3><ul>
<li>证明目标（Lemma 2 (Soft Policy Improvement)）：<ul>
<li>若新策略\(\pi_\text{new}\)定义为:<br>  $$<br>  \pi_\text{new} = \mathop{\arg\min}_{\pi’ \in \Pi} D_\text{KL} \Big( \pi’(\cdot\vert s_t) | \frac{\exp(Q^{\pi_\text{old}}(s_t, \cdot))}{Z^{\pi_\text{old}}(s_t)} \Big)<br>  $$</li>
<li>则有：<br>  $$<br>  Q^{\pi_\text{new}}(s_t, a_t) \ge Q^{\pi_\text{old}}(s_t, a_t), \quad \forall (s_t, a_t) \in \mathcal{S} \times \mathcal{A}<br>  $$</li>
</ul>
</li>
<li>证明：<ul>
<li>由之前的定义我们有：<br>  $$<br>  \begin{align}<br>  \pi_\text{new}<br>  &amp;= \mathop{\arg\min}_{\pi’ \in \Pi} D_\text{KL} \Big( \pi’(\cdot\vert s_t) | \frac{\exp(Q^{\pi_\text{old}}(s_t, \cdot))}{Z^{\pi_\text{old}}(s_t)} \Big) \\[6pt]<br>  &amp;= \mathop{\arg\min}_{\pi’ \in \Pi} D_\text{KL} \big( \pi’(\cdot\vert s_t) | \exp(Q^{\pi_\text{old}}(s_t, \cdot) - \log Z^{\pi_\text{old}}(s_t)) \big) \\[6pt]<br>  &amp;= \mathop{\arg\min}_{\pi’ \in \Pi} \mathbb{E}_{a_t\sim\pi’} \Big[ \log \big( \frac{\pi’(a_t \vert s_t)}{\exp(Q^{\pi_\text{old}}(s_t, a_t) - \log Z^{\pi_\text{old}}(s_t))} \big) \Big] \\[6pt]<br>  &amp;= \mathop{\arg\min}_{\pi’ \in \Pi} \mathbb{E}_{a_t\sim\pi’} [ \log \pi’(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t) + \log Z^{\pi_\text{old}}(s_t) ]<br>  \end{align}<br>  $$</li>
<li>于是有:<br>  $$<br>  \mathbb{E}_{a_t\sim\pi_\text{new}} [ \log \pi_\text{new}(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t) + \log Z^{\pi_\text{old}}(s_t) ] \le \mathbb{E}_{a_t\sim\pi_\text{old}} [ \log \pi_\text{old}(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t) + \log Z^{\pi_\text{old}}(s_t) ]<br>  $$</li>
<li>由于\(\log Z^{\pi_\text{old}}(s_t)\)与策略无关，仅与状态有关，所以可以消掉，于是有：<br>  $$<br>  \begin{align}<br>  \mathbb{E}_{a_t\sim\pi_\text{new}} [ \log \pi_\text{new}(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t)] &amp;\le \mathbb{E}_{a_t\sim\pi_\text{old}} [ \log \pi_\text{old}(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t)] \\<br>  - \mathbb{E}_{a_t\sim\pi_\text{new}} [ \log \pi_\text{new}(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t)] &amp;\ge - \mathbb{E}_{a_t\sim\pi_\text{old}} [ \log \pi_\text{old}(a_t \vert s_t) - Q^{\pi_\text{old}}(s_t, a_t)] \\<br>  \mathbb{E}_{a_t\sim\pi_\text{new}} [Q^{\pi_\text{old}}(s_t, a_t) - \log \pi_\text{new}(a_t \vert s_t)] &amp;\ge \mathbb{E}_{a_t\sim\pi_\text{old}} [Q^{\pi_\text{old}}(s_t, a_t) - \log \pi_\text{old}(a_t \vert s_t) ] \\<br>  \mathbb{E}_{a_t\sim\pi_\text{new}} [Q^{\pi_\text{old}}(s_t, a_t) - \log \pi_\text{new}(a_t \vert s_t)] &amp;\ge V^{\pi_\text{old}}(s_t)\\<br>  \end{align}<br>  $$</li>
<li>于是，将\(Q^{\pi_\text{old}}(s_t, a_t)\)展开后，并将上面的式子\(V^{\pi_\text{old}}(s_t) \le \mathbb{E}_{a_t\sim\pi_\text{new}} [Q^{\pi_\text{old}}(s_t, a_t) - \log \pi_\text{new}(a_t \vert s_t)]\)不断地带入下面的式子有：<br>  $$<br>  \begin{align}<br>  Q^{\pi_\text{old}}(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V^{\pi_\text{old}}(s_t)] \\<br>  &amp;\le r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} \big[\mathbb{E}_{a_{t+1}\sim\pi_\text{new}} [Q^{\pi_\text{old}}(s_{t+1}, a_{t+1}) - \log \pi_\text{new}(a_{t+1} \vert s_{t+1})]\big] \\<br>  &amp;\cdots \\<br>  &amp;\le Q^{\pi_\text{new}}(s_t, a_t)<br>  \end{align} \\<br>  $$<ul>
<li>不断带入\(V^{\pi_\text{old}}(s_t) \le \mathbb{E}_{a_t\sim\pi_\text{new}} [Q^{\pi_\text{old}}(s_t, a_t) - \log \pi_\text{new}(a_t \vert s_t)]\)，最后所有的采样策略都会变成新策略，对应的值也就是\(Q^{\pi_\text{new}}(s_t, a_t)\)了</li>
</ul>
</li>
<li>证毕.</li>
</ul>
</li>
</ul>
<h3 id="附录：证明tanh变换后的对数概率密度"><a href="#附录：证明tanh变换后的对数概率密度" class="headerlink" title="附录：证明tanh变换后的对数概率密度"></a>附录：证明tanh变换后的对数概率密度</h3><ul>
<li>为了证明给定的关系式 \(\log p(y) = \log p(x) - \log(1-(\tanh(x))^2)\)，我们需要理解这个关系式的背景。这里 \(p(x)\) 和 \(p(y)\) 分别表示随机变量 \(X\) 和 \(Y\) 的概率密度函数（PDF），其中 \(Y = \tanh(X)\)。</li>
<li>根据概率论中的变换法则，如果我们有一个随机变量 \(X\) 和一个单调可微的函数 \(g\)，那么新的随机变量 \(Y = g(X)\) 的概率密度函数 \(p_Y(y)\) 可以通过原随机变量 \(X\) 的概率密度函数 \(p_X(x)\) 以及变换函数 \(g\) 的导数来计算。具体来说，有：<br>$$ p_Y(y) = p_X(x) \left| \frac{dx}{dy} \right| $$<ul>
<li>上面的式子可以通过概率论推导得到:<ul>
<li>概率累积分布定义：\( F_Y(y) = P(Y \leq y) = P(g(X) \leq y) \)</li>
<li>如果 \(g\) 是单调增加的，则 \(P(g(X) \leq y) = P(X \leq g^{-1}(y))\)；如果 \(g\) 是单调减少的，则 \(P(g(X) \leq y) = P(X \geq g^{-1}(y))\)。这里假设 \(g\) 是单调增加的，因此：\( F_Y(y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))\)</li>
<li>所以有：<br>  $$<br>  \begin{align}<br>  p_Y(y) &amp;= \frac{d}{dy} F_Y(y) = \frac{d}{dy} F_X(g^{-1}(y)) \\<br>  &amp;= f_X(g^{-1}(y)) \cdot \frac{d}{dy} g^{-1}(y) = f_X(x) \cdot \frac{d}{dy} x \\<br>  &amp;= f_X(x) \cdot \frac{dx}{dy}<br>  \end{align}<br>  $$<ul>
<li>其中\(x = g^{-1}(y)\)是反函数，这里的\(\frac{dx}{dy}\)中，实际上\(x\)是\(y\)的函数，即写成\(\frac{dx(y)}{dy}\)更合适</li>
<li>对于不确定 \(g\) 是否单调的情况，需要加绝对值符号</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>在这个特定的情况下，\(g(x) = \tanh(x)\)，所以 \(y = \tanh(x)\)。为了找到 \(p_Y(y)\)，我们需要计算 \(g\) 的导数，即 \(\tanh(x)\) 的导数：<br>$$ \frac{d}{dx}\tanh(x) = 1 - \tanh^2(x) $$</li>
<li>因此，我们可以将上述变换法则应用于此情况：<br>$$ p_Y(y) = p_X(x) \left| \frac{dx}{dy} \right| $$</li>
<li>因为 \(y = \tanh(x)\)，所以 \(x = \tanh^{-1}(y)\)，从而：<br>$$ \frac{dx}{dy} = \frac{d}{dy}\tanh^{-1}(y) = \frac{1}{1-y^2} $$<ul>
<li>\(\frac{dx}{dy}\)表示x对y求导，也是x对y的斜率（所以时除法），所以有\(\frac{dy}{dx} = 1 - \tanh^2(x) \)推出\(\frac{dx}{dy} = \frac{1}{1 - \tanh^2(x)} = \frac{1}{1 - y^2}\)</li>
</ul>
</li>
<li>代入上面的概率密度变换公式中，我们得到：<br>$$ p_Y(y) = p_X(x) \left| \frac{1}{1-\tanh^2(x)} \right| $$</li>
<li>由于 \(1 - \tanh^2(x)\) 总是正的（对于所有实数 \(x\)），我们可以去掉绝对值符号：<br>$$ p_Y(y) = \frac{p_X(x)}{1-\tanh^2(x)} $$</li>
<li>取对数两边，得到：<br>$$ \log p_Y(y) = \log p_X(x) - \log(1-\tanh^2(x)) $$</li>
<li>这正是我们要证明的等式：<br>$$ \log p(y) = \log p(x) - \log(1-(\tanh(x))^2) $$</li>
<li>证毕.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——TRPO.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——TRPO.html" itemprop="url">RL——TRPO</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<hr>
<h3 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h3><h4 id="TRPO目标"><a href="#TRPO目标" class="headerlink" title="TRPO目标"></a>TRPO目标</h4><p>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</p>
<ul>
<li>TRPO的目标详细推导见<a href="Notes/RL/RL%E2%80%94%E2%80%94TRPO-PPO-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%9F%BA%E7%A1%80%E6%8E%A8%E5%AF%BC.html">RL——TRPO-PPO-目标函数基础推导</a></li>
</ul>
<h4 id="TRPO推导"><a href="#TRPO推导" class="headerlink" title="TRPO推导"></a>TRPO推导</h4><ul>
<li>TRPO的目标仍然很难直接求解，所以TRPO考虑对目标做进一步的近似（近似形式的具体证明见附录）<br>$$<br>\begin{aligned}<br>\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] &amp;\approx g^T(\theta-\theta_{old}) \\<br>\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta})\right] &amp;\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})<br>\end{aligned}<br>$$<ul>
<li>\(g\)为一阶梯度：<br>  $$ g = \nabla_{\theta}\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \vert_{\theta = \theta_{\text{old}}}$$</li>
<li>\(H\)为原始KL散度在\(\theta = \theta_{\text{old}} \)处的海森矩阵（Hessian Matrix，又译作黑塞矩阵），（PS：KL散度的海森矩阵就是Fisher矩阵，一般是正定的）：<br>  $$ H = H[\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta})\right]] $$<ul>
<li>其中<br>$$ H[f(x,y)] = \begin{bmatrix}<br>\frac{\partial^2f}{\partial x^2} &amp; \frac{\partial^2f}{\partial x\partial y} \\<br>\frac{\partial^2f}{\partial x \partial y} &amp; \frac{\partial^2f}{\partial y^2}<br>\end{bmatrix}<br>$$</li>
<li>在当前场景中\(H_{ij}\)为<br>$$ H_{ij} = \frac{\partial}{\partial \theta_i}\frac{\partial}{\partial \theta_j} \mathbb{E}_{s \sim \rho_{\pi_{\theta_{\text{old}}}}} \left[D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_{\theta})\right] \vert_{\theta = \theta_{\text{old}}} $$</li>
</ul>
</li>
</ul>
</li>
<li>于是得到进一步优化的目标<br>$$<br>\begin{aligned}<br>\theta_{k+1} = \mathop{\arg\max}_\theta &amp;g^T(\theta-\theta_k)\\<br>\text{s.t. } \quad \frac{1}{2}(\theta-\theta_k)^T&amp;H(\theta-\theta_k)≤\delta<br>\end{aligned}<br>$$</li>
<li>可根据拉格朗日乘子法求解以上问题得到如下解（详细推导见附录）：<br>$$ \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g $$</li>
<li>现实场景中，计算和存储Hessian矩阵的逆矩阵\(H^{-1}\)会耗费大量时间，所以TRPO通过共轭梯度法来避免直接求解\(H^{-1}\)，核心思想就是直接计算\(x = H^{-1}g\)作为参数的更新方向</li>
<li>设定\(x = H^{-1}g\)，则原始参数更新公式可变为：<br>$$  \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{x^{T}Hx}}x  $$ </li>
<li>求解\(x = H^{-1}g\)则可转换为求方程\(Hx = g\)的解，方程\(Hx = g\)的解可通过共轭梯度法（Conjugate Gradient Method,）来求解，方法参见<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E5%92%8C%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95.html">ML——共轭梯度法和最速下降法</a><ul>
<li>其中，共轭梯度法伪代码如下<img src="/Notes/RL/RL——TRPO/CG.png">

</li>
</ul>
</li>
</ul>
<h4 id="TRPO更新步长"><a href="#TRPO更新步长" class="headerlink" title="TRPO更新步长"></a>TRPO更新步长</h4><ul>
<li>当前TRPO求解方案采用了泰勒展开的1阶近似和2阶近似，不是精准求解，新参数不一定能满足KL散度约束限制，所以在更新时，我们可以再进行一次步长搜索，使得更新后的新参数满足KL散度限制，且能够提升目标函数</li>
<li>线性搜索的具体规则，在\((0,1)\)区间内抽取K个点\(\{\alpha^i\}_{i=1}^K\)</li>
</ul>
<h4 id="TRPO训练伪代码"><a href="#TRPO训练伪代码" class="headerlink" title="TRPO训练伪代码"></a>TRPO训练伪代码</h4><ul>
<li>TRPO伪代码：<img src="/Notes/RL/RL——TRPO/TRPO.png">

</li>
</ul>
<hr>
<h3 id="附录：约束问题的泰勒展开近似推导证明"><a href="#附录：约束问题的泰勒展开近似推导证明" class="headerlink" title="附录：约束问题的泰勒展开近似推导证明"></a>附录：约束问题的泰勒展开近似推导证明</h3><ul>
<li>近似结果<br>$$<br>\begin{aligned}<br>\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] &amp;\approx g^T(\theta-\theta_{old}) \\<br>\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta})\right] &amp;\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})<br>\end{aligned}<br>$$</li>
<li>\(g\)为一阶梯度：<br>  $$ g = \nabla_{\theta}\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \vert_{\theta = \theta_{\text{old}}}$$</li>
<li>\(H\)为原始KL散度在\(\theta = \theta_{\text{old}} \)处的海森矩阵：<br>  $$<br>  H_{ij} = \frac{\partial}{\partial \theta_i}\frac{\partial}{\partial \theta_j} \mathbb{E}_{s \sim \rho_{\pi_{\theta_{\text{old}}}}} \left[D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_{\theta})\right] \vert_{\theta = \theta_{\text{old}}}<br>  $$</li>
</ul>
<h4 id="泰勒展开回顾"><a href="#泰勒展开回顾" class="headerlink" title="泰勒展开回顾"></a>泰勒展开回顾</h4><ul>
<li>泰勒展开是一种在数学分析中用于近似函数的方法，特别是当直接计算函数值较为困难时，基本思想是将一个函数在一个点附近用一个多项式来近似表示</li>
<li>对于一个在点 \(a\) 处具有 \(n+1\) 阶导数的函数 \(f(x)\)，其在 \(a\) 点的泰勒展开可以表示为：<br>$$ f(x) = f(a) + \frac{f’(a)}{1!}(x-a) + \frac{f’’(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x) $$<ul>
<li>其中，\(R_n(x)\) 是余项，表示的是泰勒多项式与实际函数之间的误差，常表示为：<br>  $$ R_n(x) = o((x-a)^n) $$<ul>
<li>这里的 \(o\) 表示当 \(x\) 趋向于 \(a\) 时，\(R_n(x)\) 相对于 \((x-a)^n\) 是高阶无穷小</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>泰勒展开在很多领域都有广泛的应用，例如物理学中的近似计算、工程学中的信号处理等。通过选择合适的 (a) 值和 (n) 阶次，可以得到不同精度的近似结果。特别地，当 (a=0) 时，这种特殊的泰勒展开被称为麦克劳林展开。</p>
<h4 id="目标的泰勒一阶近似推导"><a href="#目标的泰勒一阶近似推导" class="headerlink" title="目标的泰勒一阶近似推导"></a>目标的泰勒一阶近似推导</h4><ul>
<li>令\(f(\theta)\)为最优化目标，则：<br>$$ f(\theta) = \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] $$</li>
<li>此时对\(f(\theta)\)在\(\theta = \theta_{\text{old}}\)做泰勒展开有<br>$$ f(\theta) = f(\theta_{\text{old}}) + \nabla_\theta f(\theta)\vert_{\theta=\theta_{\text{old}}}(\theta - \theta_{\text{old}}) + o(\theta^2)$$</li>
<li>由于\(f(\theta_{\text{old}})\)与\(\theta\)无关，所以最大化\(f(\theta)\)等价于最大化\(\nabla_\theta f(\theta)\vert_{\theta=\theta_{\text{old}}}(\theta - \theta_{\text{old}}) = g^T(\theta-\theta_{old})\)</li>
</ul>
<h4 id="约束的泰勒二阶近似推导"><a href="#约束的泰勒二阶近似推导" class="headerlink" title="约束的泰勒二阶近似推导"></a>约束的泰勒二阶近似推导</h4><ul>
<li>将KL散度在\(\theta = \theta_{\text{old}}\)处二阶展开有<br>$$<br>\begin{align}<br>f(\theta) &amp;= D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_\theta) \\<br>&amp;= D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_{\theta_{\text{old}}}) + \nabla_\theta D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_\theta) \vert_{\theta=\theta_{\text{old}}} (\theta - \theta_{\text{old}}) + \frac{1}{2}(\theta - \theta_{\text{old}})^TH(\theta - \theta_{\text{old}}) + o(\theta^3)<br>\end{align}<br>$$<ul>
<li>显然，第一项\( D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_{\theta_{\text{old}}})=0 \)</li>
<li>可以证明，第二项为0，证明如下：<br>  $$<br>  \begin{align}<br>  \nabla_\theta D_{\text{KL}}(\pi_{\theta_{\text{old}}}, \pi_\theta) \vert_{\theta=\theta_{\text{old}}} &amp;= \nabla_\theta \int_a \pi_{\theta_{\text{old}}}(a|s) \log \frac{\pi_{\theta_{\text{old}}}(a|s)}{\pi_{\theta(a|s)}} da\vert_{\theta = \theta_{\text{old}}} \\<br>  &amp;= \nabla_\theta \int \pi_{\theta_{\text{old}}}(a|s) \log (\pi_{\theta_{\text{old}}}(a|s) - \pi_{\theta}(a|s)) da\vert_{\theta = \theta_{\text{old}}} \\<br>  &amp;= - \int \pi_{\theta_{\text{old}}}(a|s) \nabla_\theta \log \pi_{\theta(a|s)} da\vert_{\theta = \theta_{\text{old}}} \quad \text{积分求导互换} \\<br>  &amp;= - \int \pi_{\theta_{\text{old}}}(a|s) \frac{\nabla_\theta \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} da\vert_{\theta = \theta_{\text{old}}} \quad \text{对数概率技巧}  \\<br>  &amp;= - \nabla_\theta  \int \pi_{\theta}(a|s) da\vert_{\theta = \theta_{\text{old}}} \\<br>  &amp;= - \nabla_\theta  1  \\<br>  &amp;= 0<br>  \end{align}<br>  $$</li>
<li>证毕</li>
</ul>
</li>
</ul>
<hr>
<h3 id="附录：最优化问题求解的详细推导证明"><a href="#附录：最优化问题求解的详细推导证明" class="headerlink" title="附录：最优化问题求解的详细推导证明"></a>附录：最优化问题求解的详细推导证明</h3><ul>
<li><p>给定最优化问题<br>$$<br>\begin{aligned}<br>\theta_{k+1} = \mathop{\arg\max}_\theta &amp;g^T(\theta-\theta_k)\\<br>\text{s.t. } \quad \frac{1}{2}(\theta-\theta_k)^T&amp;H(\theta-\theta_k)≤\delta<br>\end{aligned}<br>$$</p>
</li>
<li><p>对于上述问题，Karush-Kuhn-Tucker (KKT) 条件可以表述为以下几点：</p>
<ul>
<li><strong>原始可行性</strong>：解必须满足原始约束。<br>  $$<br>  \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) \leq \delta.<br>  $$</li>
<li><strong>对偶可行性</strong>：拉格朗日乘子（或对偶变量）必须非负。<br>  $$<br>  \lambda \geq 0.<br>  $$</li>
<li><strong>互补松弛性</strong>：拉格朗日乘子与对应的不等式约束之间的乘积必须为零。<br>  $$<br>  \lambda \left( \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) - \delta \right) = 0.<br>  $$</li>
<li><strong>拉格朗日函数的梯度为零</strong>：考虑拉格朗日函数 \(L(\theta, \lambda) = - g^T(\theta-\theta_k) + \lambda \left( \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) - \delta \right)\)，其对 \(\theta\) 的偏导数应等于零。<br>  $$<br>  \nabla_\theta L = - g + \lambda H (\theta - \theta_k) = 0.<br>  $$<ul>
<li>注意：这里是因为目标是max，需要改成min后才能用\(+\lambda (\cdot)\) 的操作</li>
</ul>
</li>
</ul>
</li>
<li><p>这里，\(H\) 是一个对称矩阵（Hessian矩阵是对称的，因为\(\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}\)），\(\lambda\) 是与约束相关的拉格朗日乘子，\(\delta\) 是给定的常数</p>
</li>
<li><p>根据KKT条件中的互补松弛性条件，当 \(\lambda &gt; 0\) 时，这意味着约束 \(\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) \leq \delta\) 是紧的，即：<br>$$<br>\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) = \delta.<br>$$</p>
<ul>
<li>注意，这里无法直接求解这个问题，因为这个解问题的解不是唯一的，比如一维情况就是二次方程，解就有正负两个值，实际上，这里的解是一个以\(\theta_k\)为球心的球体（椭球体）构成的集合（一共有\(2^n\)个解？其中n是变量的维度）</li>
</ul>
</li>
<li><p>因此，当 \(\lambda &gt; 0\) 时，\(\theta\) 必须位于约束的边界上。为了确定 \(\theta\) 的具体值，我们需要同时考虑其他KKT条件，尤其是拉格朗日函数的梯度为零的条件：<br>$$<br>- g + \lambda H (\theta - \theta_k) = 0<br>$$</p>
</li>
<li><p>从这个方程中，我们可以解出 \(\theta\)：<br>$$<br>\theta - \theta_k = \frac{1}{\lambda} H^{-1} g<br>$$</p>
</li>
<li><p>将 \(\theta\) 代入互补松弛条件中可得：<br>$$<br>\frac{1}{2} \left( \frac{1}{\lambda} H^{-1} g \right)^T H \left( \frac{1}{\lambda} H^{-1} g \right) = \delta<br>$$</p>
</li>
<li><p>简化后得到：<br>$$<br>\begin{align}<br>\frac{1}{2} \left( \frac{1}{\lambda^2} g^T H^{-1} H H^{-1} g \right) &amp;= \delta \\<br>\frac{1}{2} \left( \frac{1}{\lambda^2} g^T H^{-1} g \right) &amp;= \delta \\<br>\frac{1}{2} \frac{g^T H^{-1} g}{\lambda^2} &amp;= \delta \\<br>\frac{g^T H^{-1} g}{2\delta} &amp;= \lambda^2 \\<br>\end{align}<br>$$</p>
</li>
<li><p>最终可求得：<br>$$<br>\lambda = \sqrt{\frac{g^T H^{-1} g}{2\delta}}.<br>$$</p>
</li>
<li><p>现在我们已经得到了 \(\lambda\) 的表达式，可以将其代回 \(\theta\) 的表达式中：<br>$$<br>\begin{align}<br>\theta - \theta_k &amp;= \frac{1}{\sqrt{\frac{g^T H^{-1} g}{2\delta}}} H^{-1} g = \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g<br>\end{align}<br>$$</p>
</li>
<li><p>最终，\(\theta\) 的值为：<br>$$<br>\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g.<br>$$</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——Soft-Q-Learning.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——Soft-Q-Learning.html" itemprop="url">RL——Soft-Q-Learning</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<ul>
<li>参考文献<ul>
<li>原始论文：<a href="https://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a-supp.pdf" target="_blank" rel="noopener">Reinforcement learning with deep energy-based policies.</a>，2017，ICML，UC Berkeley，作者与SAC作者是同一个</li>
<li><a href="https://zhuanlan.zhihu.com/p/430792032" target="_blank" rel="noopener">Soft Q-learning解读 - 单字卓的文章 - 知乎</a></li>
<li>读书笔记：<a href="https://stepneverstop.github.io/rl-with-deep-energy-based-policies.html" target="_blank" rel="noopener">Reinforcement Learning with Deep Energy-Based Policies</a></li>
<li><a href="https://blog.csdn.net/wxc971231/article/details/127260196" target="_blank" rel="noopener">论文理解【RL经典】—— 【SQL】Reinforcement Learning with Deep Energy-Based Policies</a>：非常详细的证明</li>
</ul>
</li>
</ul>
<hr>
<h3 id="回顾常规的强化学习场景"><a href="#回顾常规的强化学习场景" class="headerlink" title="回顾常规的强化学习场景"></a>回顾常规的强化学习场景</h3><ul>
<li>目标定义：<br>$$<br>\begin{align}<br>J(\theta) &amp;= \mathbb{E}_{\tau\sim \pi_\theta}[R(\tau)] \\<br>&amp;= \mathbb{E}_{s_0, a_0,\ldots \sim \pi_\theta}\Big[\sum\limits_{t=0}^{\infty}\gamma^t r(s_t, a_t) \Big] \\<br>&amp;= \sum_{t=0}^\infty \mathbb{E}_{(s_t,a_t) \sim \rho_{\pi_\theta}} [r(s_t, a_t)]<br>\end{align}<br>$$<ul>
<li>第一行是普通PG方法推导的写法，最贴近本质</li>
<li>第二行是TRPO文章的写法，是第一行的展开形势</li>
<li>第三行这里\(\sum_{t=0}^T \mathbb{E}_{(s_t,a_t) \sim \rho_{\pi_\theta}} [r(s_t, a_t)]\)的写法是Soft Q-Learning论文和SAC论文提出来的，和之前TRPO等文章写法都不同，这里从第二行到第三行的变换可以理解为积分和求和交换顺序的写法，从状态\(s_0\)开始，\((s_t,a_t)\)都是按照策略\(\pi_\theta\)执行下去可能遇到的概率分布</li>
</ul>
</li>
<li>Q值定义：<br>$$<br>\begin{align}<br>Q^\pi(s_t,a_t) &amp;= \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=0}^\infty\gamma^lr(s_{t+l}, a_{t+l})\Big] \vert_{(s_t,a_t)} \\<br>&amp;= r(s_t, a_t) + \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=1}^\infty\gamma^lr(s_{t+l}, a_{t+l})\Big] \vert_{(s_t,a_t)} \\<br>&amp;= r(s_t, a_t) + \sum\limits_{l=1}^\infty \mathbb{E}_{(s_{t+l},a_{t+l}) \sim \rho_\pi}[\gamma^lr(s_{t+l}, a_{t+l})] \vert_{(s_t,a_t)}<br>\end{align}<br>$$<ul>
<li>“\(\vert_{(s_t,a_t)}\)”表示条件概率，一般来说，不引起歧义的情况下，也可以省略”\(\vert_{(s_t,a_t)}\)”</li>
</ul>
</li>
<li>V值定义：<br>$$<br>\begin{align}<br>V^\pi(s_t) &amp;= \mathbb{E}_{a_t,s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=0}^\infty\gamma^lr(s_{t+l}, a_{t+l})\Big] \vert_{(s_t,a_t)} \\<br>&amp;= \mathbb{E}_{a_t \sim \pi}[Q^\pi(s_t, a_t)]<br>\end{align}<br>$$</li>
</ul>
<hr>
<h3 id="Soft-Q-Learning"><a href="#Soft-Q-Learning" class="headerlink" title="Soft Q-Learning"></a>Soft Q-Learning</h3><ul>
<li>目标定义<br>$$ J(\phi) = \sum_{t=0}^T \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi_\phi}} [r(s_t, a_t) + \alpha \mathcal{H}(\pi_\phi(.\vert s_t))] $$<ul>
<li>这里目标中增加的熵就是Soft名字的来源，这里相对标准的强化学习，仅增加了\(\alpha \mathcal{H}(\pi_\phi(.\vert s_t))\)为额外目标，后续在不引起歧义的情况下，我们也用\(\alpha \mathcal{H}(\pi_\phi(s_{t}))\)来表示，且为了方便，后续推导中常常会视为\(\alpha=1\)，这里可以通过奖励和熵同时乘以\(\frac{1}{\alpha}\)来变换得到</li>
</ul>
</li>
<li>Soft Q值定义<br>$$<br>\begin{align}<br>Q_{\text{soft}}^\pi(s_t, a_t)  &amp;= r(s_t, a_t) + \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=1}^\infty\gamma^l(r(s_{t+l}, a_{t+l}) +\alpha\mathcal{H}(\pi(s_{t+l})))\Big] \\<br>&amp;= r(s_t, a_t) + \sum\limits_{l=1}^\infty \mathbb{E}_{(s_{t+l},a_{t+l}) \sim \rho_\pi}[\gamma^l(r(s_{t+l}, a_{t+l}) + \alpha\mathcal{H}(\pi(s_{t+l})))]<br>\end{align}<br>$$<ul>
<li>对于\(Q_{\text{soft}}^\pi(s_t, a_t)\)来说，已经发生的事件是“\((s_t,a_t)\)”，此时\(a_t\)是确定的动作，对应的熵\(\mathcal{H}(\pi(s_{t}))=0\)</li>
</ul>
</li>
<li>Soft V值定义，同时推导用Soft Q值表示Soft V值<br>$$<br>\begin{align}<br>V_{\text{soft}}^\pi(s_t) &amp;= \mathbb{E}_{a_t,s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=0}^\infty\gamma^l(r(s_{t+l}, a_{t+l}) + \alpha\mathcal{H}(\pi(s_{t+l})))\Big] \\<br>&amp;= \mathbb{E}_{a_t \sim \pi}\Big[r(s_t, a_t) + \alpha\mathcal{H}(\pi(s_{t})) + \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=1}^\infty\gamma^l(r(s_{t+l}, a_{t+l}) +\alpha\mathcal{H}(\pi(s_{t+l})))\Big]\Big] \\<br>&amp;= \mathbb{E}_{a_t \sim \pi}\Big[r(s_t, a_t) + \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=1}^\infty\gamma^l(r(s_{t+l}, a_{t+l}) +\alpha\mathcal{H}(\pi(s_{t+l})))\Big] + \alpha\mathcal{H}(\pi(s_{t}))\Big] \\<br>&amp;= \mathbb{E}_{a_t \sim \pi}[Q_{\text{soft}}^\pi(s_t, a_t)] + \alpha\mathcal{H}(\pi(s_{t})) \\<br>&amp;= \mathbb{E}_{a_t \sim \pi} [Q_{\text{soft}}^\pi(s_t, a_t)] - \alpha \mathbb{E}_{a_t \sim \pi} [\log \pi(a_t \vert s_t)]\\<br>&amp;= \mathbb{E}_{a_t \sim \pi} [Q_{\text{soft}}^\pi(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)]<br>\end{align}<br>$$</li>
<li>推导用Soft V值表示Soft Q值<br>$$<br>\begin{aligned}<br>Q_{\text{soft}}^\pi(s_t, a_t)  &amp;= r(s_t, a_t) + \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=1}^\infty\gamma^l(r(s_{t+l}, a_{t+l}) +\alpha\mathcal{H}(\pi(s_{t+l})))\Big] \\<br>&amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=1}^\infty\gamma^{l-1}(r(s_{t+l}, a_{t+l}) +\alpha\mathcal{H}(\pi(s_{t+l})))\Big] \\<br>&amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}\Big[\sum\limits_{l=0}^\infty\gamma^l(r(s_{t+1+l}, a_{t+1+l}) +\alpha\mathcal{H}(\pi(s_{t+1+l})))\Big] \\<br>&amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} \Big[\mathbb{E}_{a_{t+1},s_{t+2},a_{t+2},\cdots \sim \pi}\Big[\sum\limits_{l=0}^\infty\gamma^l(r(s_{t+1+l}, a_{t+1+l}) + \alpha\mathcal{H}(\pi(s_{t+1+l})))\Big]\Big] \\<br>&amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V_{\text{soft}}^\pi(s_{t+1})] \\<br>\end{aligned}<br>$$</li>
</ul>
<hr>
<h3 id="Soft贝尔曼期望方程"><a href="#Soft贝尔曼期望方程" class="headerlink" title="Soft贝尔曼期望方程"></a>Soft贝尔曼期望方程</h3><ul>
<li>通过上面的推导，我们可以得到，<strong>Soft贝尔曼期望方程</strong>为：<br>$$<br>\begin{aligned}<br>Q_{\text{soft}}^\pi(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V_{\text{soft}}^\pi(s_{t+1})] \\<br>Q_{\text{soft}}^\pi(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [\mathbb{E}_{a_{t+1} \sim \pi} [Q_{\text{soft}}^\pi(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1} \vert s_{t+1})]] \\<br>Q_{\text{soft}}^\pi(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s), a_{t+1} \sim \pi} [Q_{\text{soft}}^\pi(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1} \vert s_{t+1})] \\<br>V_{\text{soft}}^\pi(s_t) &amp;= \mathbb{E}_{a_t \sim \pi} [Q_{\text{soft}}^\pi(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)] \\<br>V_{\text{soft}}^\pi(s_t) &amp;= \mathbb{E}_{a_t \sim \pi}[Q_{\text{soft}}^\pi(s_t, a_t)] + \alpha\mathcal{H}(\pi(s_{t}))<br>\end{aligned}<br>$$</li>
</ul>
<hr>
<h3 id="Soft贝尔曼最优方程"><a href="#Soft贝尔曼最优方程" class="headerlink" title="Soft贝尔曼最优方程"></a>Soft贝尔曼最优方程</h3><ul>
<li>首先证明，在满足如下策略改进的时候(其中\(\tilde{\pi}\))是新策略，\(\pi\)是旧策略，<br>$$<br>\tilde{\pi}(\cdot | s) \propto \exp \left(Q_{\text{soft}}^{\pi}(s, \cdot) \right), \quad \forall s.<br>$$</li>
<li>策略的Soft Q值是单调的，即：<br>$$<br>Q_{\text{soft}}^{\tilde{\pi}}(s, a) \geq Q_{\text{soft}}^{\pi_{\text{old}}}(s, a) ; \forall s, a.<br>$$</li>
<li>策略的Soft Q值是单调性的证明如下：  <img src="/Notes/RL/RL——Soft-Q-Learning/Soft-Q-Learning-Policy-Improvement.png">
<ul>
<li>以上式子证明时没有考虑温度系数\(\alpha\)，考虑温度系数\(\alpha\)时在所有的熵\(\mathcal{H}(\pi(\cdot\vert\cdot))\)时前面都加上\(\alpha\)即可</li>
<li>其中使用到以下不等式  ：<br>  $$<br>  \mathcal{H}(\pi(\cdot\vert s)) + \mathbb{E}_{a\sim \pi}[Q_{\text{soft}}^\pi(s,a)] \le \mathcal{H}(\tilde{\pi}(\cdot\vert s)) + \mathbb{E}_{a\sim \tilde{\pi}}[Q_{\text{soft}}^\pi(s,a)]<br>  $$</li>
<li>上面的不等式又使用到以下式子（下面的式子如何推出上面的式子待确认TODO）：<br>  $$<br>  \mathcal{H}(\pi(\cdot\vert s)) + \mathbb{E}_{a\sim \pi}[Q_{\text{soft}}^\pi(s,a)] = - D_{\text{KL}}(\pi(\cdot|s) || \tilde{\pi}(s, \cdot))  + \log \int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da<br>  $$<ul>
<li>以上式子证明过程如下：<br>$$<br>\begin{align}<br>\text{right}  &amp;= - \int \pi(\cdot | s) \log \frac{\pi(\cdot | s)}{\tilde{\pi}(\cdot | s)} + \log \int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da \\<br>&amp;= - \int \pi(\cdot | s) \log \pi(\cdot | s) + \int \pi(\cdot | s) \log  {\tilde{\pi}(\cdot | s)} + \int \pi(\cdot | s) \Big(\log \int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da\Big) \\<br>&amp;= - \int \pi(\cdot | s) \log \pi(\cdot | s) + \int \pi(\cdot | s) \log  \frac{\exp \left(Q_{\text{soft}}^{\pi}(s, \cdot) \right)}{\int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da} + \int \pi(\cdot | s) \Big(\log \int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da\Big) \\<br>&amp;= - \int \pi(\cdot | s) \log \pi(\cdot | s) + \int \pi(\cdot | s) \log  \exp \left(Q_{\text{soft}}^{\pi}(s, \cdot) \right) \\<br>&amp;= \mathcal{H}(\pi(\cdot\vert s)) + \mathbb{E}_{a\sim \pi}[Q_{\text{soft}}^\pi(s,a)] \\<br>&amp;= \text{left}<br>\end{align}<br>$$</li>
</ul>
</li>
</ul>
</li>
<li>其中，由式子<br>$$<br>\begin{align}<br>\mathcal{H}(\pi(\cdot\vert s)) + \mathbb{E}_{a\sim \pi}[Q_{\text{soft}}^\pi(s,a)] = - D_{\text{KL}}(\pi(\cdot|s) || \tilde{\pi}(s, \cdot))  + \log \int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da \\<br>\end{align}<br>$$<ul>
<li>其中\(\tilde{\pi}\)是按照\(\tilde{\pi}(s) = \frac{\exp \left(Q_{\text{soft}}^{\pi}(s, \cdot) \right)}{\int \exp \left(Q_{\text{soft}}^{\pi}(s, a)\right) da}\)迭代以后的策略</li>
<li>以上推导没有增加温度系数，默认温度系数为1，实际上，如果增加温度系数，有\(\tilde{\pi}(s) = \frac{\exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi}(s, \cdot) \right)}{\int \exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi}(s, a)\right) da}\)</li>
<li>问题：如果策略的熵为0或温度系数为0，此时这个最优形式还能降级到\(\pi^*(s) = \mathop{\arg\max}_a Q(s,a)\)吗？<ul>
<li>当策略的熵为0时，是可以的，此时策略必须是确定性策略，按照上述公式，动作只能取Q值最大的那一个</li>
<li>当温度系数为0时，此时Reward和DQN一致，最优解应该是\(\pi^*(s) = \mathop{\arg\max}_a Q(s,a)\)。考虑温度系数的版本\(\tilde{\pi}(s) = \frac{\exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi}(s, \cdot) \right)}{\int \exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi}(s, a)\right) da}\)，当温度系数\(\alpha\rightarrow 0\)时，策略近似取Q值最大的动作，等价于DQN</li>
</ul>
</li>
</ul>
</li>
<li>当策略收敛以后，即经过迭代以后\(\tilde{\pi} = \pi = \pi^*\)，此时有\(D_{\text{KL}}(\pi(\cdot|s) || \tilde{\pi}(s, \cdot)) = 0\)成立，即<br>$$ \mathcal{H}(\pi^*(\cdot\vert s)) + \mathbb{E}_{a\sim \pi^*}[Q_{\text{soft}}^{\pi^*}(s,a)] = \log \int \exp \left(Q_{\text{soft}}^{\pi^*}(s, a)\right) da $$</li>
<li>考虑温度系数后有：<br>$$ \alpha\mathcal{H}(\pi^*(\cdot\vert s)) + \mathbb{E}_{a\sim \pi^*}[Q_{\text{soft}}^{\pi^*}(s,a)] = \log \int \exp \left(Q_{\text{soft}}^{\pi^*}(s, a)\right) da $$</li>
<li>两边同时除以\(\alpha\)有<br>$$ \mathcal{H}(\pi^*(\cdot\vert s)) + \mathbb{E}_{a\sim \pi^*}[\frac{1}{\alpha}Q_{\text{soft}}^{\pi^*}(s,a)] = \log \int \exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi^*}(s, a)\right) da $$</li>
<li>由于<br>$$<br>V_{\text{soft}}^\pi(s_t) = \mathbb{E}_{a_t \sim \pi}[Q_{\text{soft}}^\pi(s_t, a_t)] + \alpha\mathcal{H}(\pi(s_{t}))<br>$$</li>
<li>所以有<br>$$<br>\frac{1}{\alpha}V_{\text{soft}}^\pi(s_t) = \mathbb{E}_{a_t \sim \pi}[\frac{1}{\alpha} Q_{\text{soft}}^\pi(s_t, a_t)] + \mathcal{H}(\pi(s_{t}))<br>$$</li>
<li>最优的策略\(\pi^*\)对应的V值\(V^{\pi^*}(s)\)为：<br>$$ V_{\text{soft}}^{\pi^*}(s_t) = \alpha \log \int \exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi^*}(s, a)\right) da $$</li>
</ul>
<hr>
<h3 id="贝尔曼方程总结"><a href="#贝尔曼方程总结" class="headerlink" title="贝尔曼方程总结"></a>贝尔曼方程总结</h3><h4 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h4><p>$$<br>\begin{align}<br> Q^\pi(s_t,a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p(s_{t+1}|s_t, a_t)}[V^\pi(s_{t+1})] \\<br> V^\pi(s_t) &amp;= \mathbb{E}_{a_t \sim \pi}[Q^\pi(s_t, a_t)]<br>\end{align}<br>$$</p>
<h4 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h4><p>$$<br>\begin{align}<br> Q^{\pi^*}(s_t,a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p(s_{t+1}|s_t, a_t)}[V^{\pi^*}(s_{t+1})] \\<br> V^{\pi^*}(s_t) &amp;= \max_{a_t}[Q^{\pi^*}(s_t, a_t)]<br>\end{align}<br>$$</p>
<h4 id="Soft贝尔曼方程"><a href="#Soft贝尔曼方程" class="headerlink" title="Soft贝尔曼方程"></a>Soft贝尔曼方程</h4><p>$$<br>\begin{aligned}<br>Q_{\text{soft}}^\pi(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V_{\text{soft}}^\pi(s_{t+1})] \\<br>V_{\text{soft}}^\pi(s_t) &amp;= \mathbb{E}_{a_t \sim \pi} [Q_{\text{soft}}^\pi(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)] \\<br>V_{\text{soft}}^\pi(s_t) &amp;= \mathbb{E}_{a_t \sim \pi}[Q_{\text{soft}}^\pi(s_t, a_t)] + \alpha\mathcal{H}(\pi(s_{t}))<br>\end{aligned}<br>$$</p>
<h4 id="Soft贝尔曼最优方程-1"><a href="#Soft贝尔曼最优方程-1" class="headerlink" title="Soft贝尔曼最优方程"></a>Soft贝尔曼最优方程</h4><p>$$<br>\begin{aligned}<br>Q_{\text{soft}}^{\pi^*}(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V_{\text{soft}}^{\pi^*}(s_{t+1})] \\<br>V_{\text{soft}}^{\pi^*}(s_t) &amp;= \alpha \log \int \exp \left(\frac{1}{\alpha}Q_{\text{soft}}^{\pi^*}(s, a)\right) da<br>\end{aligned}<br>$$</p>
<ul>
<li>待更新</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——TRPO-PPO-目标函数基础推导.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——TRPO-PPO-目标函数基础推导.html" itemprop="url">RL——TRPO-PPO-目标函数基础推导</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文主要介绍TRPO、PPO相关内容</em></p>
<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考链接：<ul>
<li><a href="https://www.zhihu.com/question/366605427/answer/1048153125" target="_blank" rel="noopener">如何看懂TRPO里所有的数学推导细节? - 小小何先生的回答 - 知乎</a>：知乎</li>
<li><a href="https://www.cnblogs.com/xingzheai/p/16565686.html" target="_blank" rel="noopener">从TRPO到PPO（理论分析与数学证明）</a>：博客</li>
<li><a href="https://blog.csdn.net/wdlovecjy/article/details/124192928" target="_blank" rel="noopener">TRPO公式推导</a>:笔记式推导</li>
<li><a href="https://www.cnblogs.com/kailugaji/p/15388913.html" target="_blank" rel="noopener">信赖域策略优化(Trust Region Policy Optimization, TRPO)</a>:PPT式推导</li>
<li><a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/" target="_blank" rel="noopener">RL - Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="https://blog.csdn.net/zzzzzzzzzzhy/article/details/130692382" target="_blank" rel="noopener">Trust Region Policy Optimization (TRPO) 公式推导</a>:包含一些比较细节的推导过程</li>
</ul>
</li>
</ul>
<hr>
<h3 id="相关概念理解"><a href="#相关概念理解" class="headerlink" title="相关概念理解"></a>相关概念理解</h3><h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><ul>
<li>Q值的定义<br>$$ Q_\pi(s_t,a_t) = \mathbb{E}_{s_{t+1},a_{t+1},\cdots \sim \pi}[\sum\limits_{l=0}^\infty\gamma^lr(s_{t+l}, a_{t+l})] $$<ul>
<li>说明：以上期望是条件概率下的期望，为了更准确表达，一般来说可以加上”\(\vert_{(s_t,a_t)} \)”</li>
</ul>
</li>
<li>V值的定义<br>$$ V_\pi(s_t) = \mathbb{E}_{a_t, s_{t+1},\cdots \sim \pi}[\sum \limits_{l=0}^\infty \gamma^lr(s_{t+l}, a_{t+l})] = \mathbb{E}_{a_t\sim \pi(\cdot|s_t)}[Q_\pi(s_t, a_t)] $$</li>
<li>优势函数的定义<br>$$ A_\pi(s_t,a_t) = Q_\pi(s_t,a_t) - V_\pi(s_t) $$</li>
</ul>
<h4 id="强化学习的目标定义"><a href="#强化学习的目标定义" class="headerlink" title="强化学习的目标定义"></a>强化学习的目标定义</h4><p>$$ \eta(\pi) = \mathbb{E}_{s_0, a_0,\ldots \sim \pi}[\sum\limits_{t=0}^{\infty}\gamma^t r(s_t, a_t)] $$</p>
<ul>
<li>强化学习的目标是找到一个策略\(\pi\)，能最大化以上目标函数 </li>
</ul>
<h4 id="普通策略梯度法的解法"><a href="#普通策略梯度法的解法" class="headerlink" title="普通策略梯度法的解法"></a>普通策略梯度法的解法</h4><ul>
<li>设定策略为\(\pi_\theta\)，则\(\eta(\pi)\)可表达为一个关于策略参数\(\theta\)的函数\(\eta(\theta)\)，此时可通过梯度上升法（策略梯度法）得到参数更新公式：<br>$$ \theta_{new} = \theta_{old} + \alpha\nabla_{\theta}\eta(\theta) $$</li>
</ul>
<h4 id="普通策略梯度法会遇到的问题"><a href="#普通策略梯度法会遇到的问题" class="headerlink" title="普通策略梯度法会遇到的问题"></a>普通策略梯度法会遇到的问题</h4><ul>
<li>普通策略梯度法可能存在不稳定问题：<ul>
<li>如果公式更新的步长选取过小，训练速度慢</li>
<li>如果步长选取过大，那么会导致策略参数更新步子迈得过大，如果更新过渡，可能导致策略变差，从而导致交互样本变差，差的样本进一步导致策略更差，形成恶性循环。</li>
</ul>
</li>
<li>TRPO/PPO的解法：选择一个合适的更新策略，或是如何选择一个合适的步长，使得更新过后的策略一定比当前策略更好</li>
</ul>
<h4 id="TRPO-PPO的核心思想"><a href="#TRPO-PPO的核心思想" class="headerlink" title="TRPO/PPO的核心思想"></a>TRPO/PPO的核心思想</h4><ul>
<li>TRPO/PPO的核心是使用一个约束优化问题来更新策略，这个约束保证了新策略与旧策略之间的差异不会太大</li>
</ul>
<hr>
<h3 id="TRPO-PPO的推导"><a href="#TRPO-PPO的推导" class="headerlink" title="TRPO/PPO的推导"></a>TRPO/PPO的推导</h3><h4 id="策略提升的引入"><a href="#策略提升的引入" class="headerlink" title="策略提升的引入"></a>策略提升的引入</h4><ul>
<li>回顾强化学习的目标函数<br>$$ \eta(\pi) = \mathbb{E}_{s_0, a_0,\cdots \sim \pi}[\sum\limits_{t=0}^{\infty}\gamma^t r(s_t, a_t)] $$</li>
<li>两个策略之间的关系（\(\tilde{\pi}\)是新策略，\(\pi\)是旧策略）<br>$$ \eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{s_0,a_0,\cdots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty \gamma^t A_{\pi}(s_t,a_t)] $$<ul>
<li>证明如下：<br>$$<br>\begin{aligned}<br>\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t A_\pi(s_t,a_t)] &amp;=\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t(Q_\pi(s_t,a_t)-V_\pi (s_t))]\\<br>&amp;=\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t(r(s_t)+\gamma V_\pi (s_{t+1})-V_\pi (s_t))]\\<br>&amp;=\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^{t+1} V_\pi (s_{t+1})-\sum\limits_{t=0}^\infty\gamma^{t}V_\pi (s_t) + \sum\limits_{t=0}^\infty\gamma^t r(s_t)]\\<br>&amp;=\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=1}^\infty\gamma^{t} V_\pi (s_{t})-\sum\limits_{t=0}^\infty\gamma^{t}V_\pi (s_t) + \sum\limits_{t=0}^\infty\gamma^t r(s_t)]\\<br>&amp;=\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[-V_\pi(s_0) + \sum\limits_{t=0}^\infty\gamma^t r(s_t)] \quad — \sum\limits_{t=0}^\infty\gamma^{t+1} V_\pi (s_{t+1})\\<br>&amp;=-\mathbb{E}_{s_0}[V_\pi(s_0)] + \mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t r(s_t)]\\<br>&amp;=-\eta(\pi) + \eta(\tilde{\pi})<br>\end{aligned}<br>$$</li>
</ul>
</li>
<li>显然，如果我们能找到一个策略\(\tilde{\pi}\)使得\(\mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t A_\pi(s_t,a_t)] \ge 0\)成立，即可确保策略性能（目标函数）是单调递增的<ul>
<li>但是，直接求解上式是非常困难的，因为策略\(\tilde{\pi}\)是未知的，无法用这个策略收集数据，下面我们先对这个形式进行变形，再通过其他方法近似求解</li>
</ul>
</li>
</ul>
<h4 id="策略提升的变形"><a href="#策略提升的变形" class="headerlink" title="策略提升的变形"></a>策略提升的变形</h4><ul>
<li>变形如下：<br>$$<br>\begin{aligned}<br>\eta({\tilde{\pi}}) - \eta({\pi}) &amp;= \mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t A_\pi(s_t,a_t)] \\<br>&amp;=  \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_\pi(s,a)<br>\end{aligned}<br>$$</li>
<li>其中有<br>$$ \rho_\pi(s) = P(s_0=s) + \gamma P(s_1=s) + \gamma^2 P(s_2=s) + \ldots $$</li>
<li>证明如下：<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= \mathbb{E}_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty \gamma^t A_{\pi}(s_t,a_t)]\\<br>  &amp;=\sum\limits_{t=0}^\infty\sum\limits_sP(s_t=s|\tilde{\pi})\sum\limits_a\tilde{\pi}(a|s)\gamma^tA_\pi(s,a)\\<br>  &amp;=\sum\limits_s\sum\limits_{t=0}^\infty\gamma^tP(s_t=s|\tilde{\pi})\sum\limits_a\tilde{\pi}(a|s)A_\pi(s,a)\\<br>  &amp;=\sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a)<br>\end{aligned}<br>$$</li>
<li>对于\(\sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_\pi(s,a)\)来说，我们仍然难以求解，因为策略\(\tilde{pi}\)是未知的，我们无法用这个策略收集数据，所以我们使用旧的策略\(\pi\)来替换新策略\(\tilde{\pi}\)收集数据</li>
<li>对于状态部分，当新旧策略特别接近时，他们的状态访问分布会比较接近，我们可以利用MM（Majorization-Minimization）方法构造近似目标函数，可以证明，直接优化目标函数即可优化最优:<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a) \\<br>  &amp;\approx \sum\limits_s\rho_{\pi}(s)\sum\limits_a\tilde{\pi}(a|s) A_{\pi}(s,a) - \frac{4\epsilon \gamma}{(1-\gamma)^2} \cdot D_{\text{KL}}^\max\left(\pi(\cdot|s)|| \tilde{\pi}(\cdot|s)\right)<br>\end{aligned}<br>$$<ul>
<li>其中的一些字符含义见下面的描述：在严格证明下，经过一系列推导后，我们可以得到<strong>最终优化问题</strong>是：<br>  $$ \theta = \mathop{\arg\max}_{\theta}\left[ \sum\limits_s\rho_{\pi_{\theta_{\text{old}}}}(s)\sum\limits_a\pi_\theta(a|s) A_{\pi_{\theta_{\text{old}}}}(s,a) - \frac{4\epsilon \gamma}{(1-\gamma)^2} \cdot D_{\text{KL}}^\max\left(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)\right)\right] $$<ul>
<li>其中：<br>  $$<br>  \begin{aligned}<br>  \epsilon &amp;= \max_{s,a} A_\pi(s,a) \quad — s,a是所有可行状态动作，不属于具体分布\\<br>  D_{\text{KL}}^\max(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)) &amp;= \max_s D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)) \quad — s是所有可行状态<br>  \end{aligned}<br>  $$</li>
<li>对应求解伪代码<img src="/Notes/RL/RL——TRPO-PPO-目标函数基础推导/TRPO-PPO-Optimization.png"></li>
</ul>
</li>
<li>MM方法是一种迭代优化算法，其核心思想是在每一步迭代中构造一个目标函数的下界（或上界），这个下界函数被称为“代理函数”。在每一步迭代中，不是直接优化原始的目标函数，而是优化这个更容易处理的代理函数。通过确保每次迭代都能增加（或减少）目标函数值，最终达到优化目标的目的。</li>
<li>可以通过严格的MM方法数学证明，保证这种状态分布的近似替换是正确的，即提升替换后的目标函数可以提升原始目标函数。在一些书籍或者博客中，这里可以严格证明，使用旧策略采样的状态分布后，新的目标函数是旧的目标函数的一个下界，且两者在就策略\(\pi\)处的值和梯度均相等（也就是说两者的一阶近似\(f(x) \approx f(x_0) + f’(x_0)(x-x_0)\)相同）（详细证明见：<a href="https://www.cnblogs.com/xingzheai/p/16565686.html" target="_blank" rel="noopener">从TRPO到PPO（理论分析与数学证明）</a>、<a href="https://www.zhihu.com/question/366605427/answer/1048153125" target="_blank" rel="noopener">如何看懂TRPO里所有的数学推导细节? - 小小何先生的回答 - 知乎</a>、<a href="https://www.zhihu.com/question/316004388" target="_blank" rel="noopener">强化学习TRPO模型中，L_pi(.)可逼近rho(.)的证明何解？</a>）。这个证明较为复杂，有时间可以详细看看。</li>
<li>以上是最优形式，求解比较困难，所以，可以将上面式子的约束进行放松，用KL散度来保证新旧策略之间的差异不会太大即可，之后的TRPO和PPO都是这样做的，接下来的推导（除了重要性采样以外）则都是最优形式的近似</li>
</ul>
</li>
<li>基于KL散度限制新旧策略的距离后我们得到如下的目标<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a) \\<br>  &amp;\approx \sum\limits_s\rho_{\pi}(s)\sum\limits_a\tilde{\pi}(a|s) A_{\pi}(s,a) \\<br>  \text{s.t.} \quad &amp;\mathbb{E}_{s \sim \rho_{\pi}(s)} \left[D_{\text{KL}}(\pi, \tilde{\pi})\right] \le \delta<br>\end{aligned}<br>$$</li>
<li>进一步地对于动作部分，可以用重要性采样来恢复动作分布，两步总结如下（以下\(\approx\)成立的约束是新旧策略之间的KL散度约束），此外，由于\(\eta(\tilde{\pi})\)的最大化本身与\(\eta(\pi)\)并不直接相关，所以接下来我们只需要关注他们的差值即可：<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a) \\<br>  &amp;\approx \sum\limits_s\rho_{\pi}(s)\sum\limits_a\tilde{\pi}(a|s) A_{\pi}(s,a) \quad — 限定新旧策略KL散度后可以约等于\\<br>  &amp;= \sum\limits_s\rho_{\pi}(s)\sum\limits_a q(a|s)\left[\frac{\tilde{\pi}(a|s)}{q(a|s)} A_{\pi}(s,a)\right] \\<br>  &amp;= \sum\limits_s\rho_{\pi}(s)\sum\limits_a\pi(a|s)\left[\frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_{\pi}(s,a)\right]<br>\end{aligned}<br>$$<ul>
<li>实际上，从重要性采样的视角来看，动作分布可以是基于任意策略\(q(a|s)\)采样得到的，只是一般相近策略进行重要性采样样本效率更高，所以一般都使用旧策略\(\pi(a|s)\)【PS：重要性采样也需要策略分布相近的，当策略分布之间差距过大时，也不利于重要性采样，可能出现样本采样效率低下或者数据稀疏导致的评估不准确的现象】</li>
</ul>
</li>
<li>由于相对\(\eta(\tilde{\pi})\)来说，\(\eta(\pi)\)是常数，所以有最大化\(\eta(\tilde{\pi})\)，等价于最大化\(\sum\limits_s\rho_{\pi}(s)\sum\limits_a\pi(a|s)\left[\frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_{\pi}(s,a)\right]\)即可，考虑到需要保证策略采样到的状态分布不能差距太大，我们的目标可以描述为如下的形式：<br>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad \sum\limits_s\rho_{\pi_{\theta_\text{old}}}(s)&amp;\sum\limits_a\pi_{\theta_\text{old}}(a|s)\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>\text{s.t. } \quad \quad &amp;\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</li>
<li>一般也会写成期望的等价形式：<br>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}(s), a \sim \pi_{\theta_\text{old}}(a|s)}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}(s)} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</li>
<li>或者进一步简写成：<br>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$<ul>
<li>目标是原始目标等价的期望形式</li>
<li>约束则考虑了计算KL散度时在旧策略采样的状态分布上进行验证（个人理解：这里策略之间的KL散度需要指定状态或状态分布才有意义，实际上该状态分布应该是当前策略对应的状态分布，详细展开写应该是\(\mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}(\cdot|s), \pi_{\theta_\text{new}}(\cdot|s))\right]\)，使用状态\(s \sim \rho_{\pi_{\theta_\text{old}}}\)或者\(s \sim \rho_{\pi_{\theta_\text{new}}}\)都可以，因为两者很接近）</li>
</ul>
</li>
<li>至此，目标函数中采样策略（包括状态和动作）变成了之前的旧策略，总结一下有：<ul>
<li>状态分布替换旧策略是基于新旧策略的差异不大来近似得到的，这个改动是MM（Majorization-Minimization）方法的思想，构造一个可以严格通过MM方法证明的近似目标函数\(\sum\limits_s\rho_{\pi_{\theta_{\text{old}}}}(s)\sum\limits_a\pi_\theta(a|s) A_{\pi_{\theta_{\text{old}}}}(s,a) - \frac{4\epsilon \gamma}{(1-\gamma)^2} \cdot D_{\text{KL}}^\max\left(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)\right)\)，这个目标函数的优化没有信赖域的概念，所以不是Trust Region方法</li>
<li>在构造近似目标函数后，进一步简化目标函数的等价形式为KL散度约束下的更简洁形式，此时算是Trust Region方法</li>
<li>动作分布替换旧策略是基于重要性采样实现的</li>
</ul>
</li>
</ul>
<hr>
<h3 id="TRPO简单理解"><a href="#TRPO简单理解" class="headerlink" title="TRPO简单理解"></a>TRPO简单理解</h3><h4 id="TRPO名字的由来"><a href="#TRPO名字的由来" class="headerlink" title="TRPO名字的由来"></a>TRPO名字的由来</h4><ul>
<li>TRPO（Trust Region Policy Optimization）的名字来源于其核心方法——信任域（Trust Region）优化。</li>
<li>TRPO同时包含了Trust Region算法和MM（Majorization-Minimization）算法的思想：<ul>
<li>MM算法：推导过程中，在对策略提升部分进行转换时，使用的是MM算法的思想，构造了一个近似目标函数，同时证明了该近似目标函数与原始目标函数的关系（两者的梯度和值在当前策略处相等，且近似目标函数处处小于等于原始目标函数）；</li>
<li>Trust Region算法：TRPO方法在每次迭代需要在KL散度约束内做更新优化，并且构造了一个KL散度约束的优化问题来近似求解，属于Trust Region方法的思想；</li>
</ul>
</li>
<li>补充问题：MM算法、Trust Region算法、近端梯度下降算法，这三种方法的区别和关系是什么？<ul>
<li>MM算法 vs Trust Region算法：<ul>
<li>相同点：两者都是迭代优化方法，每次迭代都通过解决一个较简单的优化问题来逼近原始问题的解。</li>
<li>异同点：<ul>
<li>构造方式: MM算法通过构造一个上界函数来近似目标函数，而Trust Region算法通过在一个信赖域内构造一个近似模型来优化目标函数。</li>
<li>信赖域: Trust Region算法明确使用信赖域来限制每次迭代的步长，而MM算法没有这种信赖域的概念。</li>
<li>适用范围: MM算法更适合处理凸优化问题，而Trust Region算法在处理非凸优化问题和大规模优化问题时表现更优。</li>
</ul>
</li>
</ul>
</li>
<li>近端梯度下降：近端梯度下降方法（Proximal Gradient Descent, PGD）是一种用于优化非光滑（nonsmooth）和复合目标函数的优化算法。它结合了梯度下降法和近端算子（proximal operator），可以有效处理带有非光滑正则化项的优化问题。该方法PPO和TRPO都没有用到</li>
</ul>
</li>
</ul>
<h4 id="TRPO解法思路"><a href="#TRPO解法思路" class="headerlink" title="TRPO解法思路"></a>TRPO解法思路</h4><ul>
<li>近似求解上述式子，用一阶梯度近似目标，用二阶梯度近似约束，从而得到一个关于参数最优化问题</li>
<li>基于共轭梯度法可以求解该问题</li>
</ul>
<h4 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h4><ul>
<li>GAE（Generalized Advantage Estimation，广义优势估计）是一种用于估计策略梯度算法中优势函数的方法。它旨在解决标准优势函数估计方法的高方差问题，通过引入一个可调参数来平衡偏差与方差之间的关系。</li>
<li>详情可参考<a href="/Notes/RL/RL%E2%80%94%E2%80%94GAE.html">RL——GAE</a></li>
</ul>
<hr>
<h3 id="PPO简单理解"><a href="#PPO简单理解" class="headerlink" title="PPO简单理解"></a>PPO简单理解</h3><h4 id="PPO名字的由来"><a href="#PPO名字的由来" class="headerlink" title="PPO名字的由来"></a>PPO名字的由来</h4><ul>
<li>PPO（Proximal Policy Optimization）名字中的“Proximal”是指“近端”约束，表示确保新策略不会偏离旧策略太远，从而保证策略更新的稳定性和有效性。跟近端梯度下降（Proximal Gradient Descent）方法没有直接关系。“Proximal”是“最接近的”或“邻近的”。在不同的上下文中，“proximal”可以有不同的具体含义，但其核心概念通常与“接近”或“邻近”有关。</li>
<li>由于PPO的优化目标推导过程与TRPO相同，都用到了近似目标函数，所以推导过程中也用到了MM的思想和Trust Region的思想，但在解决问题时仅用到了近端（“Proximal”）约束，即每次迭代策略不要更新太多（没有严格遵循Trust Region推导得到的结果），严格来说不属于Trust Region方法</li>
</ul>
<h4 id="PPO-Penalty"><a href="#PPO-Penalty" class="headerlink" title="PPO-Penalty"></a>PPO-Penalty</h4><ul>
<li>又名PPO-惩罚<br>\begin{aligned}<br>\max_{\theta}&amp;\ \  \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a) - \beta D_{KL}(\pi_{\theta_{\text{old}}}(\cdot|s), \pi_\theta(\cdot|s))\right]<br>\end{aligned}</li>
</ul>
<h4 id="PPO-Clip"><a href="#PPO-Clip" class="headerlink" title="PPO-Clip"></a>PPO-Clip</h4><ul>
<li>又名PPO截断<br>\begin{aligned}<br>\max_\theta&amp;\ \  \mathbb{E}_{s\sim \rho_{\theta_{\text{old}}},a\sim q(a|s)}\min\left(\frac{\pi_\theta(a|s)}{q(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{q(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a|s)\right)<br>\end{aligned}</li>
<li>理论上，以上采样分布可以是任意分布，实际上使用原始策略效果更好（样本利用率也更高）<br>\begin{aligned}<br>\max_\theta&amp;\ \  \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a|s)\right)<br>\end{aligned}</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/jiahong-head.png" alt="Joe Zhou">
            
              <p class="site-author-name" itemprop="name">Joe Zhou</p>
              <p class="site-description motion-element" itemprop="description">本博客主要用于记录个人学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">289</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JoeZJH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="JoeZJiahong@Foxmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joe Zhou</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
