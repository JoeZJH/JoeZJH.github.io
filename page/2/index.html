<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="En/中">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="本博客主要用于记录个人学习笔记">
<meta name="keywords" content="Python, Computer, ML, Linux, Ubuntu, NLP, Git, DL,">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiahong的个人博客">
<meta property="og:url" content="https://JoeZJH.github.io/page/2/index.html">
<meta property="og:site_name" content="Jiahong的个人博客">
<meta property="og:description" content="本博客主要用于记录个人学习笔记">
<meta property="og:locale" content="En/中">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jiahong的个人博客">
<meta name="twitter:description" content="本博客主要用于记录个人学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://JoeZJH.github.io/page/2/">





  <title>Jiahong的个人博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="En/中">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiahong的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">凡事预则立，不预则废</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——PPO.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——PPO.html" itemprop="url">RL——PPO</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><h4 id="PPO的目标"><a href="#PPO的目标" class="headerlink" title="PPO的目标"></a>PPO的目标</h4><p>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</p>
<ul>
<li>PPO目标详细推导见<a href="Notes/RL/RL%E2%80%94%E2%80%94TRPO-PPO-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%9F%BA%E7%A1%80%E6%8E%A8%E5%AF%BC.html">RL——TRPO-PPO-目标函数基础推导</a></li>
</ul>
<h4 id="PPO-Penalty"><a href="#PPO-Penalty" class="headerlink" title="PPO-Penalty"></a>PPO-Penalty</h4><ul>
<li>又名PPO-惩罚<br>\begin{aligned}<br>\max_{\theta}&amp;\ \  E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a) - \beta D_{KL}(\pi_{\theta_{\text{old}}}(\cdot|s), \pi_\theta(\cdot|s))\right]<br>\end{aligned}</li>
</ul>
<h4 id="PPO-Clip"><a href="#PPO-Clip" class="headerlink" title="PPO-Clip"></a>PPO-Clip</h4><ul>
<li>又名PPO截断<br>\begin{aligned}<br>\max_\theta&amp;\ \  E_{s\sim \rho_{\theta_{\text{old}}},a\sim q(a|s)}\min\left(\frac{\pi_\theta(a|s)}{q(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{q(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a|s)\right)<br>\end{aligned}</li>
<li>理论上，以上采样分布可以是任意分布，实际上使用原始策略效果更好（样本利用率也更高）<br>\begin{aligned}<br>\max_\theta&amp;\ \  E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a|s)\right)<br>\end{aligned}</li>
</ul>
<h4 id="PPO的训练技巧"><a href="#PPO的训练技巧" class="headerlink" title="PPO的训练技巧"></a>PPO的训练技巧</h4><ul>
<li>参考：<a href="https://zhuanlan.zhihu.com/p/512327050" target="_blank" rel="noopener">影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——TRPO-PPO-目标函数基础推导.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——TRPO-PPO-目标函数基础推导.html" itemprop="url">RL——TRPO-PPO-目标函数基础推导</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文主要介绍TRPO、PPO相关内容</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考链接：<ul>
<li><a href="https://www.zhihu.com/question/366605427/answer/1048153125" target="_blank" rel="noopener">如何看懂TRPO里所有的数学推导细节? - 小小何先生的回答 - 知乎</a>：知乎</li>
<li><a href="https://www.cnblogs.com/xingzheai/p/16565686.html" target="_blank" rel="noopener">从TRPO到PPO（理论分析与数学证明）</a>：博客</li>
<li><a href="https://blog.csdn.net/wdlovecjy/article/details/124192928" target="_blank" rel="noopener">TRPO公式推导</a>:笔记式推导</li>
<li><a href="https://www.cnblogs.com/kailugaji/p/15388913.html" target="_blank" rel="noopener">信赖域策略优化(Trust Region Policy Optimization, TRPO)</a>:PPT式推导</li>
<li><a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/" target="_blank" rel="noopener">RL - Trust Region Policy Optimization (TRPO)</a></li>
</ul>
</li>
</ul>
<h3 id="相关概念理解"><a href="#相关概念理解" class="headerlink" title="相关概念理解"></a>相关概念理解</h3><h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><ul>
<li>Q值的定义<br>$$ Q_\pi(s_t,a_t) = E_{s_{t+1},a_{t+1},\cdots}[\sum\limits_{l=0}^\infty\gamma^lr(s_{t+l})] $$</li>
<li>V值的定义<br>$$ V_\pi(s_t) = E_{a_t, s_{t+1},\cdots}[\sum \limits_{l=0}^\infty \gamma^lr(s_{t+l})] = E_{a_t\sim \pi(\cdot|s_t)}[Q_\pi(s_t, a_t)] $$</li>
<li>优势函数的定义<br>$$ A_\pi(s_t,a_t) = Q_\pi(s_t,a_t) - V_\pi(s_t) $$</li>
</ul>
<h4 id="强化学习的目标定义"><a href="#强化学习的目标定义" class="headerlink" title="强化学习的目标定义"></a>强化学习的目标定义</h4><p>$$ \eta(\pi) = E_{s_0, a_0,\ldots}[\sum\limits_{t=0}^{\infty}\gamma^t r(s_t)] $$</p>
<ul>
<li>强化学习的目标是找到一个策略\(\pi\)，能最大化以上目标函数 </li>
</ul>
<h4 id="普通策略梯度法的解法"><a href="#普通策略梯度法的解法" class="headerlink" title="普通策略梯度法的解法"></a>普通策略梯度法的解法</h4><ul>
<li>设定策略为\(\pi_\theta\)，则\(\eta(\pi)\)可表达为一个关于策略参数\(\theta\)的函数\(\eta(\theta)\)，此时可通过梯度上升法（策略梯度法）得到参数更新公式：<br>$$ \theta_{new} = \theta_{old} + \alpha\nabla_{\theta}\eta(\theta) $$</li>
</ul>
<h4 id="普通策略梯度法会遇到的问题"><a href="#普通策略梯度法会遇到的问题" class="headerlink" title="普通策略梯度法会遇到的问题"></a>普通策略梯度法会遇到的问题</h4><ul>
<li>普通策略梯度法可能存在不稳定问题：<ul>
<li>如果公式更新的步长选取过小，训练速度慢</li>
<li>如果步长选取过大，那么会导致策略参数更新步子迈得过大，如果更新过渡，可能导致策略变差，从而导致交互样本变差，差的样本进一步导致策略更差，形成恶性循环。</li>
</ul>
</li>
<li>TRPO/PPO的解法：选择一个合适的更新策略，或是如何选择一个合适的步长，使得更新过后的策略一定比当前策略更好</li>
</ul>
<h4 id="TRPO-PPO的核心思想"><a href="#TRPO-PPO的核心思想" class="headerlink" title="TRPO/PPO的核心思想"></a>TRPO/PPO的核心思想</h4><ul>
<li>TRPO/PPO的核心是使用一个约束优化问题来更新策略，这个约束保证了新策略与旧策略之间的差异不会太大</li>
</ul>
<h3 id="TRPO-PPO的推导"><a href="#TRPO-PPO的推导" class="headerlink" title="TRPO/PPO的推导"></a>TRPO/PPO的推导</h3><h4 id="策略提升的引入"><a href="#策略提升的引入" class="headerlink" title="策略提升的引入"></a>策略提升的引入</h4><ul>
<li>回顾强化学习的目标函数<br>$$ \eta(\pi) = E_{s_0, a_0,\cdots}[\sum\limits_{t=0}^{\infty}\gamma^t r(s_t)] $$</li>
<li>两个策略之间的关系（\(\tilde{\pi}\)是新策略，\(\pi\)是旧策略）<br>$$ \eta(\tilde{\pi}) = \eta(\pi) + E_{s_0,a_0,\cdots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty \gamma^t A_{\pi}(s_t,a_t)] $$<ul>
<li>证明如下：<br>$$<br>\begin{aligned}<br>E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t A_\pi(s_t,a_t)] &amp;=E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t(Q_\pi(s_t,a_t)-V_\pi (s_t))]\\<br>&amp;=E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t(r(s_t)+\gamma V_\pi (s_{t+1})-V_\pi (s_t))]\\<br>&amp;=E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^{t+1} V_\pi (s_{t+1})-\sum\limits_{t=0}^\infty\gamma^{t}V_\pi (s_t) + \sum\limits_{t=0}^\infty\gamma^t r(s_t)]\\<br>&amp;=E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=1}^\infty\gamma^{t} V_\pi (s_{t})-\sum\limits_{t=0}^\infty\gamma^{t}V_\pi (s_t) + \sum\limits_{t=0}^\infty\gamma^t r(s_t)]\\<br>&amp;=E_{s_0,a_0,\ldots\sim\tilde{\pi}}[-V_\pi(s_0) + \sum\limits_{t=0}^\infty\gamma^t r(s_t)] \quad — \sum\limits_{t=0}^\infty\gamma^{t+1} V_\pi (s_{t+1})\\<br>&amp;=-E_{s_0}[V_\pi(s_0)] + E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t r(s_t)]\\<br>&amp;=-\eta(\pi) + \eta(\tilde{\pi})<br>\end{aligned}<br>$$</li>
</ul>
</li>
<li>显然，如果我们能找到一个策略\(\tilde{\pi}\)使得\(E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t A_\pi(s_t,a_t)] \ge 0\)成立，即可确保策略性能（目标函数）是单调递增的<ul>
<li>但是，直接求解上式是非常困难的，因为策略\(\tilde{\pi}\)是未知的，无法用这个策略收集数据，下面我们先对这个形式进行变形，再通过其他方法近似求解</li>
</ul>
</li>
</ul>
<h4 id="策略提升的变形"><a href="#策略提升的变形" class="headerlink" title="策略提升的变形"></a>策略提升的变形</h4><ul>
<li>变形如下：<br>$$<br>\begin{aligned}<br>\eta({\tilde{\pi}}) - \eta({\pi}) &amp;= E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty\gamma^t A_\pi(s_t,a_t)] \\<br>&amp;=  \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_\pi(s,a)<br>\end{aligned}<br>$$</li>
<li>其中有<br>$$ \rho_\pi(s) = P(s_0=s) + \gamma P(s_1=s) + \gamma^2 P(s_2=s) + \ldots $$</li>
<li>证明如下：<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= E_{s_0,a_0,\ldots\sim\tilde{\pi}}[\sum\limits_{t=0}^\infty \gamma^t A_{\pi}(s_t,a_t)]\\<br>  &amp;=\sum\limits_{t=0}^\infty\sum\limits_sP(s_t=s|\tilde{\pi})\sum\limits_a\tilde{\pi}(a|s)\gamma^tA_\pi(s,a)\\<br>  &amp;=\sum\limits_s\sum\limits_{t=0}^\infty\gamma^tP(s_t=s|\tilde{\pi})\sum\limits_a\tilde{\pi}(a|s)A_\pi(s,a)\\<br>  &amp;=\sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a)<br>\end{aligned}<br>$$</li>
<li>对于\(\sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_\pi(s,a)\)来说，我们仍然难以求解，因为策略\(\tilde{pi}\)是未知的，我们无法用这个策略收集数据，所以我们使用旧的策略\(\pi\)来替换新策略\(\tilde{\pi}\)收集数据</li>
<li>对于状态部分，当新旧策略特别接近时，他们的状态访问分布会比较接近，我们可以利用MM（Majorization-Minimization）方法构造近似目标函数，可以证明，直接优化目标函数即可优化最优:<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a) \\<br>  &amp;\approx \sum\limits_s\rho_{\pi}(s)\sum\limits_a\tilde{\pi}(a|s) A_{\pi}(s,a) - \frac{4\epsilon \gamma}{(1-\gamma)^2} \cdot D_{\text{KL}}^\max\left(\pi(\cdot|s)|| \tilde{\pi}(\cdot|s)\right)<br>\end{aligned}<br>$$<ul>
<li>其中的一些字符含义见下面的描述：在严格证明下，经过一系列推导后，我们可以得到<strong>最终优化问题</strong>是：<br>  $$ \theta = \arg\max_{\theta}\left[ \sum\limits_s\rho_{\pi_{\theta_{\text{old}}}}(s)\sum\limits_a\pi_\theta(a|s) A_{\pi_{\theta_{\text{old}}}}(s,a) - \frac{4\epsilon \gamma}{(1-\gamma)^2} \cdot D_{\text{KL}}^\max\left(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)\right)\right] $$<ul>
<li>其中：<br>  $$<br>  \begin{aligned}<br>  \epsilon &amp;= \max_{s,a} A_\pi(s,a) \quad — s,a是所有可行状态动作，不属于具体分布\\<br>  D_{\text{KL}}^\max(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)) &amp;= \max_s D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)) \quad — s是所有可行状态<br>  \end{aligned}<br>  $$</li>
<li>对应求解伪代码<img src="/Notes/RL/RL——TRPO-PPO-目标函数基础推导/TRPO-PPO-Optimization.png"></li>
</ul>
</li>
<li>MM方法是一种迭代优化算法，其核心思想是在每一步迭代中构造一个目标函数的下界（或上界），这个下界函数被称为“代理函数”。在每一步迭代中，不是直接优化原始的目标函数，而是优化这个更容易处理的代理函数。通过确保每次迭代都能增加（或减少）目标函数值，最终达到优化目标的目的。</li>
<li>可以通过严格的MM方法数学证明，保证这种状态分布的近似替换是正确的，即提升替换后的目标函数可以提升原始目标函数。在一些书籍或者博客中，这里可以严格证明，使用旧策略采样的状态分布后，新的目标函数是旧的目标函数的一个下界，且两者在就策略\(\pi\)处的值和梯度均相等（也就是说两者的一阶近似\(f(x) \approx f(x_0) + f’(x_0)(x-x_0)\)相同）（详细证明见：<a href="https://www.cnblogs.com/xingzheai/p/16565686.html" target="_blank" rel="noopener">从TRPO到PPO（理论分析与数学证明）</a>、<a href="https://www.zhihu.com/question/366605427/answer/1048153125" target="_blank" rel="noopener">如何看懂TRPO里所有的数学推导细节? - 小小何先生的回答 - 知乎</a>）。这个证明较为复杂，有时间可以详细看看。</li>
<li>以上是最优形式，求解比较困难，所以，可以将上面式子的约束进行放松，用KL散度来保证新旧策略之间的差异不会太大即可，之后的TRPO和PPO都是这样做的，接下来的推导（除了重要性采样以外）则都是最优形式的近似</li>
</ul>
</li>
<li>基于KL散度限制新旧策略的距离后，进一步地对于动作部分，可以用重要性采样来恢复动作分布，两步总结如下（以下\(\approx\)成立的约束是新旧策略之间的KL散度约束），此外，由于\(\eta(\tilde{\pi})\)的最大化本身与\(\eta(\pi)\)并不直接相关，所以接下来我们只需要关注他们的差值即可：<br>$$<br>\begin{aligned}<br>  \eta(\tilde{\pi}) - \eta(\pi) &amp;= \sum\limits_s\rho_{\tilde{\pi}}(s)\sum\limits_a\tilde{\pi}(a|s)A_{\pi}(s,a) \\<br>  &amp;\approx \sum\limits_s\rho_{\pi}(s)\sum\limits_a\tilde{\pi}(a|s) A_{\pi}(s,a) \quad — 限定新旧策略KL散度后可以约等于\\<br>  &amp;= \sum\limits_s\rho_{\pi}(s)\sum\limits_a q(a|s)\left[\frac{\tilde{\pi}(a|s)}{q(a|s)} A_{\pi}(s,a)\right] \\<br>  &amp;= \sum\limits_s\rho_{\pi}(s)\sum\limits_a\pi(a|s)\left[\frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_{\pi}(s,a)\right]<br>\end{aligned}<br>$$<ul>
<li>实际上，从重要性采样的视角来看，动作分布可以是基于任意策略\(q(a|s)\)采样得到的，只是一般相近策略进行重要性采样样本效率更高，所以一般都使用旧策略\(\pi(a|s)\)【PS：重要性采样也需要策略分布相近的，当策略分布之间差距过大时，也不利于重要性采样，可能出现样本采样效率低下或者数据稀疏导致的评估不准确的现象】</li>
</ul>
</li>
<li>由于相对\(\eta(\tilde{\pi})\)来说，\(\eta(\pi)\)是常数，所以有最大化\(\eta(\tilde{\pi})\)，等价于最大化\(\sum\limits_s\rho_{\pi}(s)\sum\limits_a\pi(a|s)\left[\frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_{\pi}(s,a)\right]\)即可，考虑到需要保证策略采样到的状态分布不能差距太大，我们的目标可以描述为如下的形式：<br>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad \sum\limits_s\rho_{\pi_{\theta_\text{old}}}(s)&amp;\sum\limits_a\pi_{\theta_\text{old}}(a|s)\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>\text{s.t. } \quad \quad &amp;E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</li>
<li>一般也会写成期望的等价形式：<br>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;E_{s \sim \rho_{\pi_{\theta_\text{old}}}(s), a \sim \pi_{\theta_\text{old}}(a|s)}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad E_{s \sim \rho_{\pi_{\theta_\text{old}}}(s)} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</li>
<li>或者进一步简写成：<br>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$<ul>
<li>目标是原始目标等价的期望形式</li>
<li>约束则考虑了计算KL散度时在旧策略采样的状态分布上进行验证（个人理解：这里策略之间的KL散度需要指定状态或状态分布才有意义，实际上该状态分布应该是当前策略对应的状态分布，详细展开写应该是\(E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}(\cdot|s), \pi_{\theta_\text{new}}(\cdot|s))\right]\)，使用状态\(s \sim \rho_{\pi_{\theta_\text{old}}}\)或者\(s \sim \rho_{\pi_{\theta_\text{new}}}\)都可以，因为两者很接近）</li>
</ul>
</li>
<li>至此，目标函数中采样策略（包括状态和动作）变成了之前的旧策略，总结一下有：<ul>
<li>状态分布替换旧策略是基于新旧策略的差异不大来近似得到的，这个改动是MM（Majorization-Minimization）方法的思想，构造一个可以严格通过MM方法证明的近似目标函数\(\sum\limits_s\rho_{\pi_{\theta_{\text{old}}}}(s)\sum\limits_a\pi_\theta(a|s) A_{\pi_{\theta_{\text{old}}}}(s,a) - \frac{4\epsilon \gamma}{(1-\gamma)^2} \cdot D_{\text{KL}}^\max\left(\pi_{\theta_{\text{old}}}(\cdot|s)|| \pi_\theta(\cdot|s)\right)\)，这个目标函数的优化没有信赖域的概念，所以不是Trust Region方法</li>
<li>在构造近似目标函数后，进一步简化目标函数的等价形式为KL散度约束下的更简洁形式，此时算是Trust Region方法</li>
<li>动作分布替换旧策略是基于重要性采样实现的</li>
</ul>
</li>
</ul>
<h3 id="TRPO简单理解"><a href="#TRPO简单理解" class="headerlink" title="TRPO简单理解"></a>TRPO简单理解</h3><h4 id="TRPO名字的由来"><a href="#TRPO名字的由来" class="headerlink" title="TRPO名字的由来"></a>TRPO名字的由来</h4><ul>
<li>TRPO（Trust Region Policy Optimization）的名字来源于其核心方法——信任域（Trust Region）优化。</li>
<li>TRPO同时包含了Trust Region算法和MM（Majorization-Minimization）算法的思想：<ul>
<li>MM算法：推导过程中，在对策略提升部分进行转换时，使用的是MM算法的思想，构造了一个近似目标函数，同时证明了该近似目标函数与原始目标函数的关系（两者的梯度和值在当前策略处相等，且近似目标函数处处小于等于原始目标函数）；</li>
<li>Trust Region算法：TRPO方法在每次迭代需要在KL散度约束内做更新优化，并且构造了一个KL散度约束的优化问题来近似求解，属于Trust Region方法的思想；</li>
</ul>
</li>
<li>补充问题：MM算法、Trust Region算法、近端梯度下降算法，这三种方法的区别和关系是什么？<ul>
<li>MM算法 vs Trust Region算法：<ul>
<li>相同点：两者都是迭代优化方法，每次迭代都通过解决一个较简单的优化问题来逼近原始问题的解。</li>
<li>异同点：<ul>
<li>构造方式: MM算法通过构造一个上界函数来近似目标函数，而Trust Region算法通过在一个信赖域内构造一个近似模型来优化目标函数。</li>
<li>信赖域: Trust Region算法明确使用信赖域来限制每次迭代的步长，而MM算法没有这种信赖域的概念。</li>
<li>适用范围: MM算法更适合处理凸优化问题，而Trust Region算法在处理非凸优化问题和大规模优化问题时表现更优。</li>
</ul>
</li>
</ul>
</li>
<li>近端梯度下降：近端梯度下降方法（Proximal Gradient Descent, PGD）是一种用于优化非光滑（nonsmooth）和复合目标函数的优化算法。它结合了梯度下降法和近端算子（proximal operator），可以有效处理带有非光滑正则化项的优化问题。该方法PPO和TRPO都没有用到</li>
</ul>
</li>
</ul>
<h4 id="TRPO解法思路"><a href="#TRPO解法思路" class="headerlink" title="TRPO解法思路"></a>TRPO解法思路</h4><ul>
<li>近似求解上述式子，用一阶梯度近似目标，用二阶梯度近似约束，从而得到一个关于参数最优化问题</li>
<li>基于共轭梯度法可以求解该问题</li>
</ul>
<h4 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h4><ul>
<li>GAE（Generalized Advantage Estimation，广义优势估计）是一种用于估计策略梯度算法中优势函数的方法。它旨在解决标准优势函数估计方法的高方差问题，通过引入一个可调参数来平衡偏差与方差之间的关系。</li>
<li>详情可参考<a href="/Notes/RL/RL%E2%80%94%E2%80%94GAE.html">RL——GAE</a></li>
</ul>
<h3 id="PPO简单理解"><a href="#PPO简单理解" class="headerlink" title="PPO简单理解"></a>PPO简单理解</h3><h4 id="PPO名字的由来"><a href="#PPO名字的由来" class="headerlink" title="PPO名字的由来"></a>PPO名字的由来</h4><ul>
<li>PPO（Proximal Policy Optimization）名字中的“Proximal”是指“近端”约束，表示确保新策略不会偏离旧策略太远，从而保证策略更新的稳定性和有效性。跟近端梯度下降（Proximal Gradient Descent）方法没有直接关系。“Proximal”是“最接近的”或“邻近的”。在不同的上下文中，“proximal”可以有不同的具体含义，但其核心概念通常与“接近”或“邻近”有关。</li>
<li>由于PPO的优化目标推导过程与TRPO相同，都用到了近似目标函数，所以推导过程中也用到了MM的思想和Trust Region的思想，但在解决问题时仅用到了近端（“Proximal”）约束，即每次迭代策略不要更新太多（没有严格遵循Trust Region推导得到的结果），严格来说不属于Trust Region方法</li>
</ul>
<h4 id="PPO-Penalty"><a href="#PPO-Penalty" class="headerlink" title="PPO-Penalty"></a>PPO-Penalty</h4><ul>
<li>又名PPO-惩罚<br>\begin{aligned}<br>\max_{\theta}&amp;\ \  E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a) - \beta D_{KL}(\pi_{\theta_{\text{old}}}(\cdot|s), \pi_\theta(\cdot|s))\right]<br>\end{aligned}</li>
</ul>
<h4 id="PPO-Clip"><a href="#PPO-Clip" class="headerlink" title="PPO-Clip"></a>PPO-Clip</h4><ul>
<li>又名PPO截断<br>\begin{aligned}<br>\max_\theta&amp;\ \  E_{s\sim \rho_{\theta_{\text{old}}},a\sim q(a|s)}\min\left(\frac{\pi_\theta(a|s)}{q(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{q(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a|s)\right)<br>\end{aligned}</li>
<li>理论上，以上采样分布可以是任意分布，实际上使用原始策略效果更好（样本利用率也更高）<br>\begin{aligned}<br>\max_\theta&amp;\ \  E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A_{\theta_{\text{old}}}(s,a), clip\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}, 1-\epsilon, 1+\epsilon\right)A_{\theta_{\text{old}}}(a|s)\right)<br>\end{aligned}</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——策略梯度法推导.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——策略梯度法推导.html" itemprop="url">RL——策略梯度法推导</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>策略梯度法(Policy Gradient)推导，以及REINFORCE算法的介绍</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考链接：<ul>
<li>策略梯度法总结：<a href="https://paperexplained.cn/articles/article/detail/31/#5a5b5c8b-ed5f-42f4-92bd-589568e5d867" target="_blank" rel="noopener">策略梯度算法专题</a> （包含推导和总结）</li>
</ul>
</li>
</ul>
<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul>
<li><strong>策略</strong> \(\pi(a|s, \theta)\) (也可以表达为 \(\pi_{ \theta}(a|s)\))是一个从状态 \(s\) 到动作 \(a\) 概率的映射，其中 \(\theta\) 表示策略的参数。</li>
<li><strong>整个轨迹的累计回报</strong> \(R_(\tau)\)是轨迹\(\tau\)对应的回报：<br>$$<br>R(\tau) = \sum_{k=0}^{\infty} r_{k}<br>$$<ul>
<li>注意：这里没有折扣因子</li>
</ul>
</li>
<li><strong>时间t步开始的回报</strong> \(G_t\) 是从时间步 \(t\) 开始到结束的所有奖励的总和，通常定义为折扣累积奖励：<br>$$<br>G_t = \sum_{k=t}^{\infty} \gamma^k r_{k}<br>$$<br>其中 \(\gamma\) 是折扣因子，\(r_{k}\) 是在时间步 \(k\) 收到的即时奖励。</li>
<li><strong>目标</strong> 是找到参数 \(\theta\) 使得长期回报的期望值最大，即 \(\max_\theta J(\theta)\)，其中 \(J(\theta) = E_{\tau \sim p_\theta(\tau)} [R(\tau)]\)。</li>
</ul>
<h3 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h3><h4 id="优化目标："><a href="#优化目标：" class="headerlink" title="优化目标："></a>优化目标：</h4><ul>
<li><strong>目标函数</strong> \(J(\theta)\) 定义为从初始分布开始，遵循策略 \(\pi_\theta\) 时的平均回报：<br> $$<br> J(\theta) = E_{\tau \sim p_\theta(\tau)} [R(\tau)]<br> $$</li>
<li>其中 \(\tau = (s_0, a_0, r_1, s_1, a_1, \dots)\) 表示一个轨迹，\(p_\theta(\tau)\) 是在策略 \(\pi_\theta\) 下产生轨迹 \(\tau\) 的概率。</li>
</ul>
<h4 id="梯度估计："><a href="#梯度估计：" class="headerlink" title="梯度估计："></a>梯度估计：</h4><ul>
<li>我们的目标是计算目标函数关于参数 \(\theta\) 的梯度 \(\nabla_\theta J(\theta)\)。我们有：<br> $$<br> \nabla_\theta J(\theta) = \nabla_\theta \int R(\tau) p_\theta(\tau) d\tau = \int R(\tau) \nabla_\theta p_\theta(\tau) d\tau<br> $$</li>
<li>使用对数概率技巧（log derivative trick，\(\nabla_\theta log y({\theta}) = \frac{\nabla_\theta  y({\theta})}{ y({\theta}) }\)）可以将上式转换为：<br> $$<br> \nabla_\theta J(\theta) = \int R(\tau) p_\theta(\tau) \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)} d\tau = E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]<br> $$</li>
<li>如果通过蒙特卡洛采样估计上面的式子，则可以写成：<br>  $$<br>  \begin{align}<br>  \nabla_\theta J(\theta) &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)] \\<br>  &amp;\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla_\theta \log p_\theta(\tau^n)<br>  \end{align}<br>  $$<ul>
<li>上式是对原始梯度的无偏估计</li>
</ul>
</li>
</ul>
<h4 id="轨迹展开："><a href="#轨迹展开：" class="headerlink" title="轨迹展开："></a>轨迹展开：</h4><ul>
<li><p>轨迹展开后，有 \(p_\theta(\tau) = p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)\)，其中 \(p(s_0)\) 是初始状态的分布，\(p(s_{t+1}|s_t, a_t)\) 是环境的转移概率。<br>  $$<br>  \begin{align}<br>  p_\theta(\tau) &amp;= p_{\pi_\theta}(s_0, a_0, s_1, a_1,\cdots) \\<br>  &amp;= p(s_0)\pi_\theta(a_0|s_0)p(s_1|s_0,a_0)\cdots \\<br>  &amp;= p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)<br>  \end{align}<br>  $$</p>
</li>
<li><p>由于环境的输出与策略无关，即\(\nabla_\theta p(s_1|s_0,a_0) = 0\)，于是有：<br>  $$<br>  \begin{align}<br>  \nabla_\theta \log p_\theta(\tau) &amp;= \nabla_\theta p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t) \\<br>  &amp;= \nabla_\theta \log p(s_0) + \nabla_\theta \sum_t \log \pi_\theta(a_t|s_t) + \nabla_\theta  \sum_t  \log p(s_{t+1}|s_t, a_t) \\<br>  &amp;= \nabla_\theta \sum_t \log \pi_\theta(a_t|s_t) \\<br>  &amp;=  \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t)<br>  \end{align}<br>  $$</p>
</li>
<li><p>所以我们可以进一步简化梯度表达式为：<br> $$<br>  \begin{align}<br> \nabla_\theta J(\theta) &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)] = E_{\tau \sim p_\theta(\tau)} \left[\sum_t R(\tau) \nabla_\theta \log \pi_\theta(a_t|s_t)   \right] \\<br> &amp;\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla_\theta \log p_\theta(\tau^n) = \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \nabla_\theta \log \pi_\theta(a_t|s_t)<br>  \end{align}<br> $$</p>
<ul>
<li>此时，上式依然是对原始梯度的无偏估计</li>
<li>本文以上的推导主要都是在证明下面的式子：<strong>\(\nabla_\theta \int R(\tau) p_\theta(\tau) d\tau\)等价于\(E_{\tau \sim p_\theta(\tau)} \left[\sum_t R(\tau) \nabla_\theta \log \pi_\theta(a_t|s_t)  \right]\)</strong></li>
<li>在一些其他文章中，证明的重点是如下的式子：<br>   $$<br>   \begin{aligned}<br>   \nabla_\theta J(\theta)<br>   &amp;= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \\<br>   &amp;\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s)<br>   \end{aligned}<br>   $$<ul>
<li>本质上与本文的证明等价，因为\( \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) = E_{\tau \sim p_\theta(\tau)} \left[\sum_t R(\tau) \nabla_\theta \log \pi_\theta(a_t|s_t)  \right] \)</li>
</ul>
</li>
</ul>
</li>
<li><p>核心理解：</p>
<ul>
<li>\(R(\tau^n)\)表示采样得到的轨迹\(\tau^n\)对应的Reward，上述公式假设一共有N个轨迹</li>
<li>对于任意给定的轨迹\(\tau^n\)，其上面的任意样本对\((s_t,a_t)\)，均使用固定的\(R(\tau^n)\)对 \(\nabla_\theta \log \pi_\theta(a_t|s_t)\)进行加权（实际上在使用中，不会直接使用\(R(\tau^n)\)，因为轨迹中过去的Reward与当前动作无关，所以，我们仅考虑后续的轨迹上的收益即可）</li>
</ul>
</li>
</ul>
<h4 id="REINFORCE算法："><a href="#REINFORCE算法：" class="headerlink" title="REINFORCE算法："></a>REINFORCE算法：</h4><ul>
<li>考虑到轨迹中过去的Reward与当前动作无关，且后续轨迹上的收益与当前动作的关系越来越小，所以我们使用\(G_t\)来替换\(R(\tau)\)<br>  $$<br>  R(\tau) = \sum_{k=0}^{\infty} r_{k} \quad \rightarrow \quad G_t = \sum_{k=t}^{\infty} \gamma^k r_{k}<br>  $$</li>
<li>此时梯度进一步近似为：<br> $$<br>  \begin{align}<br> \nabla_\theta J(\theta) &amp;\approx \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \nabla_\theta \log \pi_\theta(a_t|s_t) \\<br> &amp;\approx \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} G_t^n \nabla_\theta \log \pi_\theta(a_t|s_t)<br>  \end{align}<br> $$</li>
<li>REINFORCE算法利用上述梯度估计来更新策略参数。具体地，对于轨迹\(R(\tau^n)\)上的状态动作样本对\((s_t,a_t)\)，参数更新规则如下：<br> $$<br> \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) G_t^n<br> $$<ul>
<li>其中 \(\alpha\) 是学习率</li>
<li>因为是累加操作，所以可以展开对每一个状态动作样本对\((s_t,a_t)\)进行累加</li>
<li>\(\frac{1}{N}\)可以不需要了，有了学习率了，可以调节到学习率中</li>
</ul>
</li>
<li>补充REINFORCE算法伪代码：


</li>
</ul>
<h4 id="减小方差："><a href="#减小方差：" class="headerlink" title="减小方差："></a>减小方差：</h4><ul>
<li><p>为了减少方差，可以在梯度估计中引入一个baseline函数 \(b(s_t)\)，它是一个与动作无关的量。更新规则变为：<br> $$<br> \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - b(s_t))<br> $$</p>
<ul>
<li>常见的选择是使用价值函数 \(V(s_t)\) 作为基线，这有助于稳定学习过程。</li>
<li>可以证明，增加 \(b(s_t)\) 后，梯度不会发生改变，上式对梯度的估计依然是无偏的</li>
</ul>
</li>
<li><p>性质一：<strong>减去一个baseline以后，依然是原始梯度的无偏估计</strong>，证明如下：<br>  $$<br>  \begin{align}<br>  \nabla_\theta J(\theta)<br>  &amp;= E_{\tau \sim p_{\theta}(\tau)} (R(\tau) - b) \nabla_\theta \log p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b E_{\tau \sim p_{\theta}(\tau)} \nabla_\theta \log p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \sum_\tau p_{\theta}(\tau) \nabla_\theta \log p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \sum_\tau \nabla_\theta p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \nabla_\theta \sum_\tau  p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \nabla_\theta 1 \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  \\<br>  \end{align}<br>  $$</p>
<ul>
<li>第三行到第四行用到了对数概率技巧：\(\nabla_\theta log y({\theta}) = \frac{\nabla_\theta  y({\theta})}{ y({\theta}) }\)</li>
<li>第四行到第五行使用了求梯度和加法交换顺序的法则</li>
</ul>
</li>
<li><p>性质二：<strong>减去一个合适的baseline函数以后，方差会变小</strong>，证明如下：</p>
<ul>
<li>方差展开<br>  $$<br>  \begin{align}<br>  &amp;\ Var_{\tau \sim p_{\theta}(\tau)} [(R(\tau) - b) \nabla \log p_{\theta}(\tau)] \\<br>  &amp;= E_{\tau \sim p_{\theta}(\tau)} [(R(\tau) - b)^2 \nabla^2 \log p_{\theta}(\tau)] - [E_{\tau \sim p_{\theta}(\tau)} [(R(\tau) - b) \nabla \log p_{\theta}(\tau)] ]^2 \\<br>  &amp;= E_{\tau \sim p_{\theta}(\tau)} [R(\tau)^2 \nabla^2 \log p_{\theta}(\tau)] - [E_{\tau \sim p_{\theta}(\tau)} [R(\tau)  \nabla \log p_{\theta}(\tau)] ]^2 - 2 b E_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla^2 \log p_{\theta}(\tau)] + b^2 E_{\tau \sim p_{\theta}(\tau)} [ \nabla^2 \log p_{\theta}(\tau)] \\<br>  &amp;= Var_{\tau \sim p_{\theta}(\tau)} [R(\tau)  \nabla \log p_{\theta}(\tau) ] - 2 b E_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla^2 \log p_{\theta}(\tau)] + b^2 E_{\tau \sim p_{\theta}(\tau)} [ \nabla^2 \log p_{\theta}(\tau)]<br>  \end{align}<br>  $$</li>
<li>进一步解的，使得上式取小值的最优\(b\)为：<br>  $$<br>  b = \frac{E_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla^2 \log p_{\theta}(\tau)] }{E_{\tau \sim p_{\theta}(\tau)} [ \nabla^2 \log p_{\theta}(\tau)]}<br>  $$</li>
<li>实际应用中，为了方便计算，通常会使用：<br>  $$<br>  \hat{b} = E_{\tau \sim p_{\theta}(\tau)} R(\tau)<br>  $$ </li>
<li>那为什么\(\hat{b} = E_{\tau \sim p_{\theta}(\tau)} R(\tau)\)是\(V_{\pi_{\theta}}(s_t)\)呢？因为两者是等价的，证明如下：<ul>
<li>对于非确定性策略来说，在状态\(s_t\)下可选的动作服从一个分布\(\pi_{\theta}(s_t)\)，按照\(E_{\tau \sim p_{\theta}(\tau)} R(\tau)\)的逻辑，该值是状态\(s_t\)下按照策略\(\pi_{\theta}(s_t)\)执行得到\(R(\tau)\)期望（注意\(a_t\)服从\(\pi_{\theta}\)分布，后续的执行动作也服从\(\pi_{\theta}\)分布），实际上就是\(V_{\pi_{\theta}}(s_t)\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="实际使用中的-R-tau"><a href="#实际使用中的-R-tau" class="headerlink" title="实际使用中的\(R(\tau)\)"></a>实际使用中的\(R(\tau)\)</h3><h4 id="原始形式"><a href="#原始形式" class="headerlink" title="原始形式"></a>原始形式</h4><p>$$ R(\tau) = \sum_{k=0}^{\infty} r_{k} $$</p>
<ul>
<li>上式中实际上是一个固定轨迹的奖励，从第0步开始</li>
<li>可基于蒙特卡洛采样得到</li>
</ul>
<h4 id="REINFORCE方法"><a href="#REINFORCE方法" class="headerlink" title="REINFORCE方法"></a>REINFORCE方法</h4><ul>
<li>迭代样本\((s_t,a_t)\)时，使用以下形式：<br>$$ G_t = \sum_{k=t}^{\infty} \gamma^k r_{k} $$<ul>
<li>丢弃掉动作之前的奖励，这些奖励与当前动作无关</li>
<li>未来越远的动作奖励越小，因为这些奖励受当前动作影响的概率越小</li>
</ul>
</li>
<li>使用baseline函数进行改进<br>$$\sum_{k=t}^{\infty} \gamma^k r_{k} - b(s_t)$$</li>
</ul>
<h4 id="用Q值替代"><a href="#用Q值替代" class="headerlink" title="用Q值替代"></a>用Q值替代</h4><ul>
<li>用\(Q(s,a)\)值代替\(R(\tau)\)</li>
<li>理由，\(Q(s,a)\)值是状态\(s\)执行\(a\)以后的\(G_t\)的期望值：<br>$$Q^{\pi_\theta}(s_t,a_t) = E_{\pi_\theta} [G_t|s_t, a_t]$$</li>
<li>使用Q值来替代可以降低方差</li>
</ul>
<h4 id="用优势函数替代"><a href="#用优势函数替代" class="headerlink" title="用优势函数替代"></a>用优势函数替代</h4><ul>
<li>用\(A(s,a) = Q(s,a) - V(s)\)来替代<ul>
<li>可以减去\(V(s)\)的理由是之前证明过减去一个baseline函数\(V(s)\)可以降低方差，且梯度无偏</li>
</ul>
</li>
<li>实际上使用时可以使用单个V网络+TD-Error实现对优势函数的估计<br>  $$ A(s,a) = r(s,a) + \gamma V(s’) - V(s) $$</li>
<li>应用场景：<ul>
<li>常规的Actor Critic方法</li>
<li>PPO方法的优势函数估计（实际的PPO中常常是GAE方式，是优势函数的一种加权）</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——TRPO.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——TRPO.html" itemprop="url">RL——TRPO</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<h3 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h3><h4 id="TRPO目标"><a href="#TRPO目标" class="headerlink" title="TRPO目标"></a>TRPO目标</h4><p>$$<br>\begin{aligned}<br>\max_{\theta_\text{new}} \quad &amp;E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta_\text{new}}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] \\<br>&amp;\text{s.t. } \quad \quad E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta_\text{new}})\right] \le \delta<br>\end{aligned}<br>$$</p>
<ul>
<li>TRPO的目标详细推导见<a href="Notes/RL/RL%E2%80%94%E2%80%94TRPO-PPO-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%9F%BA%E7%A1%80%E6%8E%A8%E5%AF%BC.html">RL——TRPO-PPO-目标函数基础推导</a></li>
</ul>
<h4 id="TRPO推导"><a href="#TRPO推导" class="headerlink" title="TRPO推导"></a>TRPO推导</h4><ul>
<li>TRPO的目标仍然很难直接求解，所以TRPO考虑对目标做进一步的近似<br>$$<br>\begin{aligned}<br>E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] &amp;\approx g^T(\theta-\theta_{old}) \\<br>E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta})\right] &amp;\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})<br>\end{aligned}<br>$$<ul>
<li>\(g\)为一阶梯度：<br>  $$ g = \nabla_{\theta}E_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_\text{old}}(a|s)} A_{\pi_{\theta_\text{old}}}(s,a)\right] $$</li>
<li>\(H\)为海森矩阵（Hessian Matrix，又译作黑塞矩阵）：<br>  $$ H = H[E_{s \sim \rho_{\pi_{\theta_\text{old}}}} \left[D_{\text{KL}}(\pi_{\theta_\text{old}}, \pi_{\theta})\right]] $$<ul>
<li>其中<br>$$ H[f(x,y)] = \begin{bmatrix}<br>\frac{\partial^2f}{\partial x^2} &amp; \frac{\partial^2f}{\partial x\partial y} \\<br>\frac{\partial^2f}{\partial x \partial y} &amp; \frac{\partial^2f}{\partial y^2}<br>\end{bmatrix}<br>$$</li>
</ul>
</li>
</ul>
</li>
<li>于是得到进一步优化的目标<br>$$<br>\begin{aligned}<br>\theta_{k+1} = \arg\max_\theta &amp;g^T(\theta-\theta_k)\\<br>\text{s.t. } \quad \frac{1}{2}(\theta-\theta_k)^T&amp;H(\theta-\theta_k)≤\delta<br>\end{aligned}<br>$$</li>
<li>可根据拉格朗日乘子法求解以上问题得到如下解（详细推导见附录）：<br>$$ \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g $$</li>
<li>现实场景中，计算和存储Hessian矩阵的逆矩阵\(H^{-1}\)会耗费大量时间，所以TRPO通过共轭梯度法来避免直接求解\(H^{-1}\)，核心思想就是直接计算\(x = H^{-1}g\)作为参数的更新方向</li>
<li>设定\(x = H^{-1}g\)，则原始参数更新公式可变为：<br>$$  \theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{x^{T}Hx}}x  $$ </li>
<li>求解\(x = H^{-1}g\)则可转换为求方程\(Hx = g\)的解，方程\(Hx = g\)的解可通过共轭梯度法来求解，方法参见<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E5%92%8C%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95.html">ML——共轭梯度法和最速下降法</a></li>
</ul>
<h4 id="TRPO更新步长"><a href="#TRPO更新步长" class="headerlink" title="TRPO更新步长"></a>TRPO更新步长</h4><ul>
<li>当前TRPO求解方案采用了泰勒展开的1阶近似和2阶近似，不是精准求解，新参数不一定能满足KL散度约束限制，所以在更新时，我们可以再进行一次步长搜索，使得更新后的新参数满足KL散度限制，且能够提升目标函数</li>
<li>线性搜索的具体规则，在\((0,1)\)区间内抽取K个点\(\{\alpha^i\}_{i=1}^K\)<img src="/Notes/RL/RL——TRPO/TRPO.png">

</li>
</ul>
<h4 id="附录：最优化问题求解的详细推导"><a href="#附录：最优化问题求解的详细推导" class="headerlink" title="附录：最优化问题求解的详细推导"></a>附录：最优化问题求解的详细推导</h4><ul>
<li><p>给定最优化问题<br>$$<br>\begin{aligned}<br>\theta_{k+1} = \arg\max_\theta &amp;g^T(\theta-\theta_k)\\<br>\text{s.t. } \quad \frac{1}{2}(\theta-\theta_k)^T&amp;H(\theta-\theta_k)≤\delta<br>\end{aligned}<br>$$</p>
</li>
<li><p>对于上述问题，Karush-Kuhn-Tucker (KKT) 条件可以表述为以下几点：</p>
<ul>
<li><strong>原始可行性</strong>：解必须满足原始约束。<br>  $$<br>  \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) \leq \delta.<br>  $$</li>
<li><strong>对偶可行性</strong>：拉格朗日乘子（或对偶变量）必须非负。<br>  $$<br>  \lambda \geq 0.<br>  $$</li>
<li><strong>互补松弛性</strong>：拉格朗日乘子与对应的不等式约束之间的乘积必须为零。<br>  $$<br>  \lambda \left( \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) - \delta \right) = 0.<br>  $$</li>
<li><strong>拉格朗日函数的梯度为零</strong>：考虑拉格朗日函数 \(L(\theta, \lambda) = - g^T(\theta-\theta_k) + \lambda \left( \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) - \delta \right)\)，其对 \(\theta\) 的偏导数应等于零。<br>  $$<br>  \nabla_\theta L = - g + \lambda H (\theta - \theta_k) = 0.<br>  $$<ul>
<li>注意：这里是因为目标是max，需要改成min后才能用\(+\lambda (\cdot)\) 的操作</li>
</ul>
</li>
</ul>
</li>
<li><p>这里，\(H\) 是一个对称矩阵（Hessian矩阵是对称的），\(\lambda\) 是与约束相关的拉格朗日乘子，\(\delta\) 是给定的常数。</p>
</li>
<li><p>根据KKT条件中的互补松弛性条件，当 \(\lambda &gt; 0\) 时，这意味着约束 \(\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) \leq \delta\) 是紧的，即：<br>$$<br>\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) = \delta.<br>$$</p>
<ul>
<li>注意，这里无法直接求解这个问题，因为这个解问题的解不是唯一的，比如一维情况就是二次方程，解就有正负两个值，实际上，这里的解是一个以\theta_k为球心的球体（椭球体）构成的集合（一共有\(2^n\)个解？其中n是变量的维度）</li>
</ul>
</li>
<li><p>因此，当 \(\lambda &gt; 0\) 时，\(\theta\) 必须位于约束的边界上。为了确定 \(\theta\) 的具体值，我们需要同时考虑其他KKT条件，尤其是拉格朗日函数的梯度为零的条件：<br>$$<br>- g + \lambda H (\theta - \theta_k) = 0<br>$$</p>
</li>
<li><p>从这个方程中，我们可以解出 \(\theta\)：<br>$$<br>\theta - \theta_k = \frac{1}{\lambda} H^{-1} g<br>$$</p>
</li>
<li><p>将 \(\theta\) 代入互补松弛条件中可得：<br>$$<br>\frac{1}{2} \left( \frac{1}{\lambda} H^{-1} g \right)^T H \left( \frac{1}{\lambda} H^{-1} g \right) = \delta<br>$$</p>
</li>
<li><p>简化后得到：<br>$$<br>\begin{align}<br>\frac{1}{2} \left( \frac{1}{\lambda^2} g^T H^{-1} H H^{-1} g \right) &amp;= \delta \\<br>\frac{1}{2} \left( \frac{1}{\lambda^2} g^T H^{-1} g \right) &amp;= \delta \\<br>\frac{1}{2} \frac{g^T H^{-1} g}{\lambda^2} &amp;= \delta \\<br>\frac{g^T H^{-1} g}{2\delta} &amp;= \lambda^2 \\<br>\end{align}<br>$$</p>
</li>
<li><p>最终可求得：<br>$$<br>\lambda = \sqrt{\frac{g^T H^{-1} g}{2\delta}}.<br>$$</p>
</li>
<li><p>现在我们已经得到了 \(\lambda\) 的表达式，可以将其代回 \(\theta\) 的表达式中：<br>$$<br>\begin{align}<br>\theta - \theta_k &amp;= \frac{1}{\sqrt{\frac{g^T H^{-1} g}{2\delta}}} H^{-1} g = \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g<br>\end{align}<br>$$</p>
</li>
<li><p>最终，\(\theta\) 的值为：<br>$$<br>\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g.<br>$$</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——TD误差和优势函数的区别.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——TD误差和优势函数的区别.html" itemprop="url">RL——TD误差和优势函数的区别</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/264806566" target="_blank" rel="noopener">TD误差 vs 优势函数 vs贝尔曼误差</a></li>
</ul>
</li>
</ul>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<h3 id="TD误差"><a href="#TD误差" class="headerlink" title="TD误差"></a>TD误差</h3><ul>
<li>时间差分误差，TD error</li>
<li>定义如下：<br>$$<br>\delta_{\theta}(s, a, s’) = R(s, a, s’) + \gamma v_{\theta}(s’) - v_{\theta}(s)<br>$$</li>
<li>\(R(s,a,s’)=r(s,a,s’)\)，表示从状态\(s\)执行\(a\)之后转移到\(s’\)获得的立即回报</li>
<li>TD error是针对确定的\(s’\)来说的</li>
</ul>
<h3 id="优势函数"><a href="#优势函数" class="headerlink" title="优势函数"></a>优势函数</h3><ul>
<li>优势函数，Advantage Function<br>$$<br>A_{\theta}(s,a) = E_{s’\sim P}[\delta_{\theta}(s, a, s’)] = E_{s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)] - v_{\theta}(s) = Q_{\theta}(s,a) - v_{\theta}(s)<br>$$</li>
<li>优势函数是TD误差关于状态\(s’\)的期望，即从状态\(s\)执行\(a\)之后关于状态\(s’\)的期望</li>
</ul>
<h3 id="贝尔曼误差"><a href="#贝尔曼误差" class="headerlink" title="贝尔曼误差"></a>贝尔曼误差</h3><ul>
<li>贝尔曼误差<br>$$<br>\epsilon_{\theta}(s) = E_{a\sim \pi} [A_{\theta}(s,a)] = E_{a\sim \pi,s’\sim P}[\delta_{\theta}(s, a, s’)] = E_{a \sim \pi, s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)] - v_{\theta}(s)<br>$$</li>
<li>贝尔曼误差是优势函数关于动作\(a\)的期望</li>
</ul>
<h3 id="期望贝尔曼误差"><a href="#期望贝尔曼误差" class="headerlink" title="期望贝尔曼误差"></a>期望贝尔曼误差</h3><ul>
<li>期望贝尔曼误差<br>$$<br>E_{s\sim \mu} [\epsilon_{\theta}(s)] = E_{s\sim \mu}[E_{a \sim \pi, s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)]] -  E_{s\sim \mu}[ v_{\theta}(s)]<br>$$</li>
<li>贝尔曼误差是优势函数关于动作\(a\)的期望</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——POMDP.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——POMDP.html" itemprop="url">RL——POMDP</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考链接：<a href="https://www.zhihu.com/zvideo/1326278888684187648?utm_source=wechat_session&utm_medium=social&utm_oi=675997552948678656" target="_blank" rel="noopener">POMDP讲解</a></li>
</ul>
<h3 id="MDP"><a href="#MDP" class="headerlink" title="MDP"></a>MDP</h3><ul>
<li><img src="/Notes/RL/RL——POMDP/MDP.png">
<ul>
<li>对于确定决策，策略的概率值为1即可<h3 id="POMDP"><a href="#POMDP" class="headerlink" title="POMDP"></a>POMDP</h3></li>
</ul>
</li>
<li><img src="/Notes/RL/RL——POMDP/POMDP.png"></li>
<li></li>
<li>
<h3 id="求解方案"><a href="#求解方案" class="headerlink" title="求解方案"></a>求解方案</h3></li>
<li><img src="/Notes/RL/RL——POMDP/MDP_Policy_Function.png"></li>
<li><img src="/Notes/RL/RL——POMDP/POMDP_Policy_Function.png"></li>
<li><img src="/Notes/RL/RL——POMDP/POMDP_Solutions.png"></li>
<li><img src="/Notes/RL/RL——POMDP/POMDP2RL.png">
<ul>
<li>RL本身并不假设知道状态转移矩阵等，所以其实可以直接使用RL求解POMDP试一下的，只是RL不保证收敛而已（PS：Q-Learning还是收敛的吧，只是DQN没有数据证明收敛）</li>
<li>如果能建模出来POMDP的整个过程，没必要用RL了</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Others/General——深刻认识URL.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Others/General——深刻认识URL.html" itemprop="url">General——深刻认识URL</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>你真的认识URL了吗？</em></p>
<h3 id="URL中的-字符"><a href="#URL中的-字符" class="headerlink" title="URL中的#字符"></a>URL中的<code>#</code>字符</h3><ul>
<li><code>#</code>在URL中与服务器无关，也就是说正常访问服务器的URL不包含<code>#</code></li>
<li><code>#</code>仅仅与本地浏览器对网页的定位相关</li>
<li><code>#</code>由于不影响对远程服务器的访问，自然也不会存在于软件包的下载连接中</li>
</ul>
<h3 id="URL的正则表达式"><a href="#URL的正则表达式" class="headerlink" title="URL的正则表达式"></a>URL的正则表达式</h3><p><em>参考博客：<a href="https://blog.csdn.net/qq_25384945/article/details/81219075" target="_blank" rel="noopener">https://blog.csdn.net/qq_25384945/article/details/81219075</a></em></p>
<ul>
<li><p>Python</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+</span><br></pre></td></tr></table></figure>
</li>
<li><p>JavaScript</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/((([A-Za-z]&#123;3,9&#125;:(?:\/\/)?)(?:[\-;:&amp;=\+\$,\w]+@)?[A-Za-z0-9\.\-]+|(?:www\.|[\-;:&amp;=\+\$,\w]+@)[A-Za-z0-9\.\-]+)((?:\/[\+~%\/\.\w\-_]*)?\??(?:[\-\+=&amp;;%@\.\w_]*)#?(?:[\.\!\/\\\w]*))?)/</span><br></pre></td></tr></table></figure>
</li>
<li><p>Java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">^(https?|ftp|file)://[-a-zA-Z0-9+&amp;@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&amp;@#/%=~_|]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python</p>
<pre><code></code></pre>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/DL/DL——多任务学习权重优化.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/DL/DL——多任务学习权重优化.html" itemprop="url">DL——多任务学习权重优化</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h3 id="loss归一化"><a href="#loss归一化" class="headerlink" title="loss归一化"></a>loss归一化</h3><hr>
<h3 id="不确定权重"><a href="#不确定权重" class="headerlink" title="不确定权重"></a>不确定权重</h3><ul>
<li>Paper: <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/425672909" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/425672909</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/65137250" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65137250</a></li>
<li><a href="https://blog.csdn.net/cv_family_z/article/details/78749992" target="_blank" rel="noopener">https://blog.csdn.net/cv_family_z/article/details/78749992</a></li>
<li><a href="https://mp.weixin.qq.com/s/BOGhU2jMqD82EMkTHGMnuw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/BOGhU2jMqD82EMkTHGMnuw</a></li>
<li>回归问题误差：正太分布，分类问题误差：玻尔兹曼分布（<a href="https://www.zhihu.com/question/274174763/answer/672202523%EF%BC%89" target="_blank" rel="noopener">https://www.zhihu.com/question/274174763/answer/672202523）</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Linux/Centos——挖矿病毒kdevtmpfsi查杀经历.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Linux/Centos——挖矿病毒kdevtmpfsi查杀经历.html" itemprop="url">Centos——挖矿病毒kdevtmpfsi查杀经历</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h3 id="挖矿病毒kdevtmpfsi查杀经历"><a href="#挖矿病毒kdevtmpfsi查杀经历" class="headerlink" title="挖矿病毒kdevtmpfsi查杀经历"></a>挖矿病毒kdevtmpfsi查杀经历</h3><h4 id="问题简介"><a href="#问题简介" class="headerlink" title="问题简介"></a>问题简介</h4><ul>
<li>服务器: 阿里云主机服务器</li>
<li>系统: Centos7</li>
<li>表现: kdevtmpfsi进程占用400%(8核心处理器)CPU</li>
<li>时间: 2020-01-07报警, 2020-01-08处理</li>
</ul>
<h4 id="查杀经过"><a href="#查杀经过" class="headerlink" title="查杀经过"></a>查杀经过</h4><ul>
<li><p>使用<code>clamscan</code>命令搜索所有文件, clamav详情见我之前的博客<a href="/Notes/Linux/Centos%E2%80%94%E2%80%94clamav%E5%AE%89%E8%A3%85%E4%B8%8E%E6%9D%80%E6%AF%92.html">clamav安装与杀毒</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup clamscan / -r --infected -l clamscan.log &gt; clamscan.out &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li>这一步比较花时间</li>
</ul>
</li>
<li><p>查看扫描结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat clamscan.log | grep FOUND</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/merged/tmp/kdevtmpfsi: Multios.Coinminer.Miner-6781728-2 FOUND<br>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/merged/tmp/red2.so: Unix.Trojan.Gafgyt-6981174-0 FOUND<br>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/upper/tmp/kdevtmpfsi: Multios.Coinminer.Miner-6781728-2 FOUND<br>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/upper/tmp/red2.so: Unix.Trojan.Gafgyt-6981174-0 FOUND</p>
</blockquote>
<ul>
<li><p>删除这四个文件,这里直接到相关目录下查看发现<code>../tmp</code>目录下往往都是病毒文件(与<code>kinsing</code>相关,全部删除)</p>
</li>
<li><p><code>top</code>查看CPU信息确定挖矿进程kdevtmpfsi的进程号[pid]</p>
</li>
<li><p>确定启动信息中启动命令,并删除(在这里查到的信息是文件已经被删除了)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /proc/[pid] -ali</span><br></pre></td></tr></table></figure>
</li>
<li><p>查找父进程进程号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status [pid]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>● docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope - libcontainer container be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161<br>   Loaded: loaded (/run/systemd/system/docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope; static; vendor preset: disabled)<br>  Drop-In: /run/systemd/system/docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope.d<br>           └─50-BlockIOAccounting.conf, 50-CPUAccounting.conf, 50-DefaultDependencies.conf, 50-Delegate.conf, 50-Description.conf, 50-MemoryAccounting.conf, 50-Slice.conf<br>   Active: active (running) since Mon 2019-11-11 11:24:17 UTC; 1 months 27 days ago<br>    Tasks: 38<br>   Memory: 2.3G<br>   CGroup: /system.slice/docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope<br>           ├─ 4475 redis-server *:6379<br>           ├─ 8528 sh -c /tmp/.ICEd-unix/vJhOU<br>           ├─ 8529 /tmp/.ICEd-unix/vJhOU<br>           └─22822 /tmp/kdevtmpfsi</p>
</blockquote>
<p>Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</p>
<ul>
<li><p>杀死不需要的相关进程,如上面的<code>4475</code>,<code>8528</code>, <code>8529</code>, <code>22822</code></p>
</li>
<li><p>查看是否还有需要杀死的进程,如果有,则杀死该进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep kinsing</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>top</code>确定挖矿进程已经被杀死</p>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>查杀病毒两个小时未发现服务器有明显异常</li>
<li>第二天出现了,需要进一步查看,重启机器后问题没有再出现</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/DL/DL——Transformer.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/DL/DL——Transformer.html" itemprop="url">DL——Transformer</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文主要介绍Transformer和Attention相关内容</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>由于LaTex中矩阵的黑体表示过于复杂，在不会引起混淆的情况下，本文中有些地方会被简写为非黑体</li>
</ul>
<hr>
<h3 id="相关论文介绍"><a href="#相关论文介绍" class="headerlink" title="相关论文介绍"></a>相关论文介绍</h3><ul>
<li>Transformer原始文章: <ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a></li>
<li>文章中介绍了一种应用Attention机制的新型特征提取器,命名为Transformer, 实验证明Transformer优于RNN(LSTM),CNN等常规的特征提取器</li>
</ul>
</li>
<li>Transformer的使用: <ul>
<li>GPT: <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li>
<li>BERT: <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li>以上两个工作都使用了Transformer作为特征提取器, 使用两阶段训练的方式实现迁移学习(Pre-Training and Fine-Training)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="相关博客介绍"><a href="#相关博客介绍" class="headerlink" title="相关博客介绍"></a>相关博客介绍</h3><ul>
<li>强烈推荐看看jalammar的博客: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">illustrated-transformer</a></li>
<li>另一篇不错的Attention和Transformer讲解<a href="https://www.sohu.com/a/226596189_500659" target="_blank" rel="noopener">自然语言处理中的自注意力机制(Self-Attention Mechanism)</a></li>
<li>一篇很多个人理解的博客 <a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读</a></li>
</ul>
<hr>
<h3 id="Transformer讲解"><a href="#Transformer讲解" class="headerlink" title="Transformer讲解"></a>Transformer讲解</h3><ul>
<li>最直观的动态图理解<img src="/Notes/DL/DL——Transformer/transform_dynamic.gif"></li>
<li>本文讲解主要按照<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a>的思路走,该论文的亮点在于:<ul>
<li>不同于以往主流机器翻译使用基于 RNN 的 Seq2Seq 模型框架，该论文用 <strong>Attention 机制代替了 RNN</strong> 搭建了整个模型框架, 这是一个从换自行车零件到把自行车换成汽车的突破</li>
<li>提出了<strong>多头注意力</strong>(Multi-Head Attention)机制方法，在编码器和解码器中大量的使用了多头自注意力机制(Multi-Head self-attention)</li>
<li>在WMT2014语料库的英德和英法语言翻译任务上取得了先进结果</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Transformer是什么"><a href="#Transformer是什么" class="headerlink" title="Transformer是什么?"></a>Transformer是什么?</h3><ul>
<li>本质上是个序列转换器 <img src="/Notes/DL/DL——Transformer/the_transformer_high_level.png"></li>
<li>进一步讲,是个 Encoder-Decoder 模型的序列转换器<img src="/Notes/DL/DL——Transformer/The_transformer_encoders_decoders.png"></li>
<li>更进一步的讲,是个 6层Encoder + 6层Decoder 结构的序列转换器<img src="/Notes/DL/DL——Transformer/The_transformer_encoder_decoder_stack.png"></li>
<li>上面的图中,每个 Encoder 是<img src="/Notes/DL/DL——Transformer/Transformer_encoder.png"></li>
<li>详细的讲, 每个Encoder是<img src="/Notes/DL/DL——Transformer/encoder_with_tensors.png"></li>
<li>展开看里面 Encoder 中的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm.png"></li>
<li>更进一步的展开看 Encoder 中的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm_2.png"></li>
<li>两层 Encoder + 两层Decoder (其中一个Decoder没有完全画出来) 的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm_3.png"></li>
<li>带细节动图查看数据流向<img src="/Notes/DL/DL——Transformer/transformer_decoding_1.gif">
</li>
<li>最后,我们给出Transformer的结构图(来自原文中)<img src="/Notes/DL/DL——Transformer/Transfomer_Architecture.png">


</li>
</ul>
<hr>
<h3 id="Transformer中的Attention"><a href="#Transformer中的Attention" class="headerlink" title="Transformer中的Attention"></a>Transformer中的Attention</h3><p><em>Transformer中使用了 Multi-Head Attention, 同时也是一种 Self Attention</em></p>
<ul>
<li>由于Transformer的Multi_Head Attention中 <strong>Query == Key == Query</strong>, 所以也是一种 <strong>Self Attention</strong><ul>
<li>即$$\boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
</li>
<li>更多关于广义Attention的理解请参考: <a href="/Notes/DL/DL%E2%80%94%E2%80%94Attention.html">DL——Attention</a></li>
</ul>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul>
<li>Muti-Head Attention，也称为多头Attention，由 \(h\) 个 Scaled Dot-Product Attention和其他线性层和Concat操作等组成<img src="/Notes/DL/DL——Transformer/Scaled_dot_product_attention_and_Multi_head_attention.png">
<ul>
<li>Scaled Dot Product Attention中Mask操作是可选的</li>
<li>Scaled Dot Product Attention数学定义为(没有Mask操作)<br>$$<br>\begin{align}<br>Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}<br>\end{align}<br>$$<ul>
<li>Softmax前除以\(\sqrt{d_k}\)的原因是防止梯度消失问题，基本思想是（原始论文脚注中有提到）：假设\(\boldsymbol{Q},\boldsymbol{K}\)中每个元素是服从均值为0，方差为1的正太分布（\(\sim N(0,1)\)），那么他们任意取两个列向量\(\boldsymbol{q}_i,\boldsymbol{k}_i\)的内积服从均值为0，方差为\(d_k\)的正太分布（\(\sim N(0,d_k)\)），具体证明可参考<a href="https://zhuanlan.zhihu.com/p/584569220" target="_blank" rel="noopener">没有比这更详细的推导 attention为什么除以根号dk——深入理解Bert系列文章</a>，过大的方差会导致softmax后梯度消失</li>
</ul>
</li>
<li>Multi-Head Attention的某个输出的数学定义为<br>$$<br>\begin{align}<br>MultiHead(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) &amp;= Concat(head_1,\dots,head_h)\boldsymbol{W}^{O} \\<br>where \quad head_i &amp;= Attention(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)<br>\end{align}<br>$$</li>
<li>注意，在一般的Attention中，没有\(\boldsymbol{W}^{O}\)这个参数，这个是用于多头Attention中，将多头的输出Concat后映射一下再输出</li>
<li>一般来说，\(head_i\)的维度是\(\frac{d_{model}}{N_{head}}=\frac{d_{model}}{h}=d_v = d_k\)，所以Multi-Head Attention的参数数量与head的数量无关，且无论多少个头，其的输出结果还是\(d_{model} = d_v * h\)维</li>
<li>原始论文中常用\(d_{model} = h * d_k = h * d_v\)，且base模型的参数设置为\(512 = 8 * 64\)</li>
</ul>
</li>
</ul>
<h5 id="有关Multi-Head-Attention的理解"><a href="#有关Multi-Head-Attention的理解" class="headerlink" title="有关Multi-Head Attention的理解"></a>有关Multi-Head Attention的理解</h5><ul>
<li>原论文的描述:</li>
</ul>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, </p>
</blockquote>
<ul>
<li>理解:<ul>
<li>所谓多头,就是多做几次(\(h\)次)同样的事情(参数\((W_i^Q, W_i^K, W_i^V)\)不共享, 即当 \(i \neq j \) 时, \((W_i^Q, W_i^K, W_i^V) \neq (W_j^Q, W_j^K, W_j^V)\)),然后把结果拼接</li>
<li>Multi-Head Attention中, 每个头(Scaled Dot-Product Attention)负责不同的子空间(subspaces at differect positions)</li>
<li>每个头权重不同, 所以他们的关注点也会不同,注意, 初始化时他们的参数不能相同, 否则会造成他们的参数永远相同, 因为他们是同构的</li>
<li>个人理解: 多头的作用可以类比于CNN中的卷积层, 负责从不同的角度提取原始数据的特征</li>
</ul>
</li>
</ul>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul>
<li>Self Attention是只 Key和Query相同的 Attention, 这里因为 Key 和 Value 也相同,所以有 <strong>Query == Key == Query</strong></li>
<li>即$$ \boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
<h4 id="Transformer中的Attention-1"><a href="#Transformer中的Attention-1" class="headerlink" title="Transformer中的Attention"></a>Transformer中的Attention</h4><ul>
<li>既是Multi-Head Attention, 也是 Self Attention</li>
<li>所以有$$\boldsymbol{Y_{AttentionOutput}} = MultiHead(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
<h4 id="Masked-Multi-Head-Attetion"><a href="#Masked-Multi-Head-Attetion" class="headerlink" title="Masked Multi-Head Attetion"></a>Masked Multi-Head Attetion</h4><ul>
<li>MaskedMHA，掩码多头Attention，用于Decoder中防止前面的token看到后面的token，Encoder中不需要MaskedMHA</li>
<li>一般性的，Masked Self-Attention是更一般的实现，不一定非要和Multi-Head绑定</li>
<li>代码实现时，主要是在计算Softmax前，按照掩码将看不到的token对应的q,k内积替换为一个大负数，比如\(-1e9\)</li>
</ul>
<h4 id="Cross-Multi-Head-Attention"><a href="#Cross-Multi-Head-Attention" class="headerlink" title="Cross Multi-Head Attention"></a>Cross Multi-Head Attention</h4><ul>
<li>CrossMHA不是Self-Attention，CrossMHA的Q,K是Encoder的输出，V来自Decoder</li>
</ul>
<hr>
<h3 id="Transformer-输入层"><a href="#Transformer-输入层" class="headerlink" title="Transformer 输入层"></a>Transformer 输入层</h3><ul>
<li>Transformer的输入层使用了 Word Embedding + Position Embedding</li>
<li>由于Transformer去除RNN的Attention机制完全不考虑词的顺序, 也就是说, 随机打乱句子中词的顺序 (也就是将键值对\((\boldsymbol{K}, \boldsymbol{V})\)对随机打乱), Transformer中Attention的结果不变</li>
<li>实际上, <strong>目前为止, Transformer中的Attention模型顶多是个非常精妙的”词袋模型”</strong> (这句话来自博客:<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">https://kexue.fm/archives/4765</a>)</li>
</ul>
<h4 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h4><ul>
<li>和之前的词嵌入一样, 将One-Hot值映射成词向量嵌入模型中</li>
</ul>
<h4 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h4><p><em>FaceBook的《Convolutional Sequence to Sequence Learning》中曾经用过Position Embedding</em></p>
<ul>
<li>在不使用RNN的情况下建模词的顺序, 弥补”词袋模型”的不足</li>
<li>用 Position Embedding来为每个位置一个向量化表示<ul>
<li>将每个位置编号，然后每个编号对应一个向量</li>
<li>通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了</li>
</ul>
</li>
<li>原始论文中, 作者提出了一种周期性位置编码的表示, 数学公式如下:<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin(pos/10000^{2i/d_{\text{model}}}) \\<br>  PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{\text{model}}})<br>  \end{align}<br>  $$</li>
<li>我觉得上述公式太丑了,转换一下写法可能更容易理解<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\<br>  PE(pos, 2i+1) &amp;= cos\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)<br>  \end{align}<br>  $$<ul>
<li>\(pos\) 是位置编号</li>
<li>\(i\) 表示位置向量的第 \(i\) 维</li>
<li>从公式来看，为什么选择\(10000^{\frac{2i}{d_{\text{model}}}}\)? <ul>
<li>\(i\)表示频率随模型embedding维度变动（模型embedding不同维度频率不同，低维度高频，高维度低频）</li>
<li>\(pos\) 表示周期，随着位置变化，每个维度的值呈现周期变化，但是不同维度的变化周期（频率）不同</li>
<li>10000是一个放缩因子，理论上可以换，在transformer原始论文实现中用了这个，且效果不错</li>
</ul>
</li>
<li>选择正弦函数的原因是假设这将允许模型学到相对位置信息<ul>
<li>因为对于固定的 \(k\), \(PE_{pos+k} = LinearFuction(PE_{pos})\), 所以这给模型提供了表达相对位置的可能性</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="与之前的Position-Embedding的区别"><a href="#与之前的Position-Embedding的区别" class="headerlink" title="与之前的Position Embedding的区别"></a>与之前的Position Embedding的区别</h5><ul>
<li>Position Embedding对模型的意义不同:<ul>
<li>以前在RNN、CNN模型中Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，因为RNN、CNN本身就能捕捉到位置信息</li>
<li>在Transformer这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段</li>
</ul>
</li>
<li>Position Embedding的向量构造方式不同<ul>
<li>在以往的Position Embedding中，基本都是根据任务训练出来的向量</li>
<li>而Google直接给出了一个构造Position Embedding的公式:<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\<br>  PE(pos, 2i+1) &amp;= cos\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)<br>  \end{align}<br>  $$</li>
<li>Google经过实验, 学到的位置嵌入和这种计算得到的位置嵌入结果很相近</li>
<li>Google选用这种嵌入方式的原因是这种方式允许模型以后可以<strong>扩展到比训练时遇到的序列长度更长的句子</strong></li>
</ul>
</li>
</ul>
<h4 id="输入层的输出-Attention的输入"><a href="#输入层的输出-Attention的输入" class="headerlink" title="输入层的输出(Attention的输入)"></a>输入层的输出(Attention的输入)</h4><ul>
<li>综合词嵌入和位置嵌入信息,我们可以得到下面的公式<br>$$<br>\begin{align}<br>\boldsymbol{x} = \boldsymbol{x}_{WE} + \boldsymbol{x}_{PE}<br>\end{align}<br>$$<ul>
<li>\(\boldsymbol{x}\) 为输入层经过词嵌入和位置嵌入后的 输出, 也就是Attention的输入</li>
<li>\(\boldsymbol{x}_{WE}\) 指词嵌入的结果</li>
<li>\(\boldsymbol{x}_{PE}\) 指位置嵌入的结果</li>
</ul>
</li>
</ul>
<h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><ul>
<li>FFN，Feed Forward Network，前馈网络层<br>$$<br>FFN(\mathbf{X}) = ReLU(\mathbf{X}\mathbf{W}^U + \mathbf{b}_1)\mathbf{W}^D + \mathbf{b}_2<br>$$</li>
<li>原始Transformer使用的是ReLU作为激活函数，现在很多时候也会选用sigmoid</li>
<li>可以看到前馈神经网络包含了两层 </li>
</ul>
<h3 id="Layer-Normaliztion"><a href="#Layer-Normaliztion" class="headerlink" title="Layer Normaliztion"></a>Layer Normaliztion</h3><ul>
<li>层归一化，是Transformer特有的一种归一化方法</li>
<li>Batch Normalization(BN)不适用与Transformer中，至少有以下原因：<ul>
<li>Transformer训练样本通常（特别是模型很大时）可能会比较小，在Batch较小时BN不再适用</li>
<li>BN是按照token维度（特征维度）来归一化的，不利于处理变长输入序列</li>
</ul>
</li>
</ul>
<p>$$<br>LayerNorm(\mathbf{x}) = \frac{\mathbf{x}-\mathbf{\mu}}{\mathbf{\sigma}}\cdot \mathbf{\gamma} + \mathbf{\beta} \\<br>\mathbf{\mu} = \frac{1}{H}\sum_{i=1}^H x_i, \quad \mathbf{\sigma} = \sqrt{\frac{1}{H}\sum_{i=1}^H(x_i-\mathbf{\mu})^2} \\<br>$$</p>
<ul>
<li>代码实现是会在分母的更号内增加一个极小量 \(\epsilon\)，防止出现除0的情况</li>
</ul>
<h4 id="LN是token维度的"><a href="#LN是token维度的" class="headerlink" title="LN是token维度的"></a>LN是token维度的</h4><ul>
<li><p>按照Transformer源码实现来看，LayerNorm是Token维度的，不是Seq维度，也就是说，token向量LayerNorm的结果只与token向量自身相关，与所在序列的其他token无关</p>
<ul>
<li>这一点是Decoder可以增量解码的关键，这一点保证了Decoder的前序词不会受到后续词的影响</li>
<li>增量解码是指：Decoder中输出下一个词时，可以使用前序词的缓存结果，由于前面的词看不到后面的词，所以增加词前后Transformer-Decoder中前序每个词的输出在每一层都不会受到影响</li>
</ul>
</li>
<li><p>一个LayerNorm的示例如下，Transformer源码中实现与这个类似</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 假设d_model=4，d_model在有些实现中为hidden_size</span><br><span class="line">layer_norm = nn.LayerNorm(4) # 这里使用LayerNorm，也可以使用RMSNorm，两者作用维度相同，只是公式不同</span><br><span class="line"># test case 1:</span><br><span class="line">input_tensor = torch.Tensor([[[1, 2, 3, 4],</span><br><span class="line">                              [2, 3, 4, 5]]])</span><br><span class="line">output_tensor = layer_norm(input_tensor)</span><br><span class="line">print(output_tensor)</span><br><span class="line"># output:</span><br><span class="line"># tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],</span><br><span class="line">#          [-1.3416, -0.4472,  0.4472,  1.3416]]],</span><br><span class="line">#        grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class="line"></span><br><span class="line"># test case 2:</span><br><span class="line">input_tensor = torch.Tensor([[[1, 2, 3, 4],</span><br><span class="line">                              [200, 3, 4, 5]]])</span><br><span class="line">output_tensor = layer_norm(input_tensor)</span><br><span class="line">print(output_tensor)</span><br><span class="line"></span><br><span class="line"># tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],</span><br><span class="line">#          [ 1.7320, -0.5891, -0.5773, -0.5655]]],</span><br><span class="line">#        grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>从示例中可以看出：</p>
<ul>
<li>修改第二个token的某个元素值，只影响第二个token的LN输出，不影响第一个token</li>
</ul>
</li>
</ul>
<h3 id="Transformer改进-LN"><a href="#Transformer改进-LN" class="headerlink" title="Transformer改进-LN"></a>Transformer改进-LN</h3><p><em>原始LN参见本文之前的内容</em></p>
<h4 id="LN的改进——RMSNorm"><a href="#LN的改进——RMSNorm" class="headerlink" title="LN的改进——RMSNorm"></a>LN的改进——RMSNorm</h4><p>$$<br>\begin{align}<br>RMSNorm(\mathbf{x}) &amp;= \frac{\mathbf{x}-\mathbf{\mu}}{RMS(\mathbf{x})}\cdot \mathbf{\gamma} \\<br>  RMS(\mathbf{x}) &amp;= \sqrt{\frac{1}{H}\sum_{i=1}^H x_i^2} \\<br>\end{align}<br>$$</p>
<ul>
<li>代码实现是会在分母的更号内增加一个极小量 \(\epsilon\)，防止出现除0的情况</li>
</ul>
<h4 id="LN的改进——DeepNorm"><a href="#LN的改进——DeepNorm" class="headerlink" title="LN的改进——DeepNorm"></a>LN的改进——DeepNorm</h4><p>$$<br>DeepNorm(\mathbf{x}) = LayerNorm(\alpha\cdot \mathbf{x} + Sublayer(\mathbf{x})) \\<br>$$</p>
<ul>
<li>这里的\(Sublayer(\mathbf{x})\)是指Transformer中的前馈神经网络层或自注意力模块（两者都会作为LN的输入）</li>
<li>实际上，原始的Transformer中，每次LN的内容都是加上残差的，这里根据归一化位置的不同还有会有不同的实现</li>
<li>原始的Transformer中，相当于\(\alpha=1\)的DeepNorm</li>
<li>这里叫做<strong>DeepNorm</strong>的原因是因为缩放残差\(\mathbf{x}\)可以扩展Transformer的深度，有论文提到利用该方法可将深度提升到1000层（<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231" target="_blank" rel="noopener">DeepNet: Scaling Transformers to 1,000 Layers</a>）</li>
</ul>
<h4 id="归一化的位置"><a href="#归一化的位置" class="headerlink" title="归一化的位置"></a>归一化的位置</h4><ul>
<li>归一化的位置包括Post-Norm、Pre-Norm和Sandwich-Norm等</li>
<li>Post-Norm<ul>
<li>原始Transformer使用的方法</li>
<li>将归一化模块使用到加法（需要把残差加到FFN/MHA的输出上）之后，详细公式是：<br>$$<br>\text{Post-Norm}(\mathbf{x}) = Norm(\mathbf{x} + Sublayer(\mathbf{x}))<br>$$</li>
</ul>
</li>
<li>Pre-Norm<ul>
<li>归一化模块放到FFN/MHA之前，详细公式是：<br>$$<br>\text{Pre-Norm}(\mathbf{x}) = \mathbf{x} + Sublayer(Norm(\mathbf{x}))<br>$$</li>
</ul>
</li>
<li>Sandwich_Norm<ul>
<li>三明治归一化，从字面意思可以知道，是两个Norm将某个层夹起来，实际上，该层是前馈神经网络层或自注意力模块<br>$$<br>\text{Sandwish-Norm}(\mathbf{x}) = \mathbf{x} + Norm(Sublayer(Norm(\mathbf{x})))<br>$$</li>
</ul>
</li>
</ul>
<h4 id="归一化位置的比较"><a href="#归一化位置的比较" class="headerlink" title="归一化位置的比较"></a>归一化位置的比较</h4><p><em><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231" target="_blank" rel="noopener">DeepNet: Scaling Transformers to 1,000 Layers</a>中也有有关Pre-Norm和Post-Norm的探讨</em></p>
<ul>
<li>一般来说，使用Post-Norm比较多，效果也更好</li>
<li>Pre-Norm在深层Transformer中容易训练（容易训练不代表效果好，Pre-Norm的拟合能力一般不如Post-Norm）<ul>
<li>所以有些模型还是会使用Pre-Norm，因为它更稳定</li>
</ul>
</li>
<li>浅层中建议使用Post-Norm</li>
<li>详情参考苏神的回答<a href="https://zhuanlan.zhihu.com/p/494661681" target="_blank" rel="noopener">为什么Pre Norm的效果不如Post Norm？</a></li>
</ul>
<h3 id="Transformer改进-激活函数"><a href="#Transformer改进-激活函数" class="headerlink" title="Transformer改进-激活函数"></a>Transformer改进-激活函数</h3><p><em>原始激活函数是ReLU(Rectified Linear Unit)</em></p>
<h4 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h4><ul>
<li>Swish, Sigmoid-weighted Linear Unit<ul>
<li><em>Swish 的全称为 “Scaled Exponential Linear Unit with Squishing Hyperbolic Tangent”,直译为“带有压缩的缩放指数线性单元”</em><br>$$<br>\text{Swish}_\beta(x) = x \cdot sigmoid(\beta x)<br>$$</li>
</ul>
</li>
<li>许多实现中常常设置\(\beta=1\)</li>
</ul>
<h4 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h4><ul>
<li>GELU, Gaussion Error Linear Unit，有时候也写作GeLU<br>$$<br>\text{GELU}(x) = 0.5x \cdot [1+erf(\frac{x}{\sqrt{2}})], \quad erf(x) = \frac{2}{\sqrt{\pi}}\int_1^x e^{-t^2} dt<br>$$</li>
<li>从公式可以看出GELU的本质是对一个正太分布的概率密度函数进行积分，实际上就是累积分布函数</li>
<li>GELU和ReLU的比较如下（图片来自<a href="https://zhuanlan.zhihu.com/p/662042707" target="_blank" rel="noopener">简单理解GELU 激活函数</a>）：<img src="/Notes/DL/DL——Transformer/GELU-ReLU.webp">

</li>
</ul>
<h4 id="补充：GLU及其变换"><a href="#补充：GLU及其变换" class="headerlink" title="补充：GLU及其变换"></a>补充：GLU及其变换</h4><ul>
<li>GLU，Gated Linear Units，是一种利用门的思想实现的激活函数，该激活函数可以理解为对输入进行门控选择，一些维度的值可以通过门，一些则不可以，门一般是一个基础的非线性激活函数</li>
<li>原始GLU形式如下：<br>$$<br>GLU = \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \odot (\mathbf{W}_2\mathbf{x} + \mathbf{b}_2)<br>$$</li>
<li>\(\sigma\)可以替换成其他非线性激活函数<ul>
<li>注意整个公式中始终只有一个非线性激活函数，其他部分都是线性映射（线性激活函数）</li>
</ul>
</li>
<li>\(\odot\)表示矩阵按照元素相乘，\(W_1,W_2,b_1,b_2\)是可学习的参数</li>
<li>该激活函数非常特殊，首先使用两个权重矩阵对输入数据进行线性变换，然后通过sigmoid激活函数进行非线性变换。这种设计使得GLU在前馈传播过程中能够更好地捕捉输入数据的非线性特征，从而提高模型的表达能力和泛化能力</li>
<li>原始论文<a href="https://arxiv.org/pdf/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer</a>中也写作下面的形式(其中\(W,V,b,c\)是可学习的参数)：<br>$$<br>GLU(\mathbf{x,W,V,b,c}) = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b}) \odot (\mathbf{V}\mathbf{x} + \mathbf{c})<br>$$</li>
<li>去掉激活函数的版本也叫作Bilinear，写作<br>$$<br>Bilinear(\mathbf{x,W,V,b,c}) = (\mathbf{W}\mathbf{x} + \mathbf{b}) \odot (\mathbf{V}\mathbf{x} + \mathbf{c})<br>$$</li>
<li>其他相关形式<br>$$<br>ReGLU(x, W, V, b, c) = max(0, xW + b) \odot (xV + c) \\<br>GEGLU(x, W, V, b, c) = GELU(xW + b) \odot (xV + c) \\<br>SwiGLU(x, W, V, b, c, \beta) = Swish_\beta(xW + b) \odot (xV + c) \\<br>$$</li>
</ul>
<h4 id="补充：FFN激活函数形式"><a href="#补充：FFN激活函数形式" class="headerlink" title="补充：FFN激活函数形式"></a>补充：FFN激活函数形式</h4><ul>
<li>FFN的ReLU激活函数形式<br>$$<br>FFN(x, W_1, W_2, b_1, b_2) = max(0, xW_1 + b_1)W_2 + b_2<br>$$</li>
<li>为了表示方便，也因为在一些文章中使用了简化，后续该形式会被简化成没有偏置项(bias)的形式：<br>$$<br>FFNReLU(x, W_1, W_2) = max(xW_1, 0)W_2<br>$$</li>
</ul>
<h4 id="FFN各种激活函数形式"><a href="#FFN各种激活函数形式" class="headerlink" title="FFN各种激活函数形式"></a>FFN各种激活函数形式</h4><ul>
<li>常用FFN的激活函数改进有，GLU,Bilinear,ReGLU,GEGLU(GeGLU),SwiGLU等<br>$$<br>\begin{align}<br>FFN_{GLU}(x, W, V, W_2) &amp;= (\sigma(xW) \odot xV )W_2 \\<br>FFN_{Bilinear}(x, W, V, W_2) &amp;= (xW \odot xV )W_2 \\<br>FFN_{ReGLU}(x, W, V, W_2) &amp;= (max(0, xW) \odot xV )W_2 \\<br>FFN_{GEGLU}(x, W, V, W_2) &amp;= (GELU(xW) \odot xV )W_2 \\<br>FFN_{SwiGLU}(x, W, V, W_2) &amp;= (Swish_1(xW) \odot xV )W_2 \\<br>\end{align}<br>$$</li>
<li>可以理解为\(\mathbf{W}^G,\mathbf{W}^U\)中包含了偏置项\(\mathbf{b}\)，有些文章/模型中则会将偏置项\(\mathbf{b}\)去掉</li>
<li>最常用的是SwiGLU</li>
<li>从形式上看，可以知道相对原始FFN激活函数形式，SwiGLU等改进增加了一个参数矩阵，为了保证原始参数数量不变，原始论文<a href="https://arxiv.org/pdf/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer</a>中提出了一种方法，通过将矩阵设置为如下的大小来保证参数数量相等<ul>
<li>原始FFN层参数为(下面\(d = d_{model}\)是模型的隐藏层大小，注意，同一层的不同token是共享FFN的):<br>$$<br>W_1^{d\times d} + W_2^{d\times d}<br>$$</li>
<li>使用SwiGLU且对齐参数数量后<br>$$<br>W^{r\times d} + V^{d\times r} + W_{2}^{r\times d}<br>$$</li>
<li>显然，当 \(r=\frac{2}{3}d\) 时，使用 SwiGLU 前后FFN层参数数量相同，都等于 \(2d^2\)</li>
</ul>
</li>
<li>一个疑问：原始的SwiGLU函数会引入两个参数矩阵 \(W,V\)，原始的FFN包含两个参数矩阵 \(W_1, W_2\)，为什么两者结合以后只剩下 \(FFN_{SwiGLU}\) 只剩三个参数\(W,V,W_2\)呢？<ul>
<li>回答：因为两个线性矩阵相乘，可以合并为 \(W = WV\)，虽然还叫做\(W\)，但实际上是多了一个矩阵乘进去的，线上训练时也只需要训练这一个矩阵即可</li>
</ul>
</li>
</ul>
<h3 id="Transformer总结"><a href="#Transformer总结" class="headerlink" title="Transformer总结"></a>Transformer总结</h3><ul>
<li>Transformer是一个特征提取能力非常强(超越LSTM)的特征提取器</li>
<li>一些讨论<ul>
<li>Transformer与CNN没关系,但是Transformer中使用多个 Scaled Dot-Product Attention 来最后拼接的方法(Multi-Head Attention), 就是CNN的多个卷积核的思想</li>
<li>Transformer论文原文中提到的残差结构也来源于CNN</li>
<li>无法对位置信息进行很好地建模，这是硬伤。尽管可以引入Position Embedding，但我认为这只是一个缓解方案，并没有根本解决问题。举个例子，用这种纯Attention机制训练一个文本分类模型或者是机器翻译模型，效果应该都还不错，但是用来训练一个序列标注模型（分词、实体识别等），效果就不怎么好了。那为什么在机器翻译任务上好？我觉得原因是机器翻译这个任务并不特别强调语序，因此Position Embedding 所带来的位置信息已经足够了，此外翻译任务的评测指标BLEU也并不特别强调语序</li>
<li>Attention如果作为一个和CNN,RNN平级的组件来使用,可能会集成到各自的优点, 而不是”口气”很大的 “Attention is All You Need”</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/jiahong-head.png" alt="Joe Zhou">
            
              <p class="site-author-name" itemprop="name">Joe Zhou</p>
              <p class="site-description motion-element" itemprop="description">本博客主要用于记录个人学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">278</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JoeZJH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="JoeZJiahong@Foxmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joe Zhou</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
