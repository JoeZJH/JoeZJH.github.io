<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="En/中">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="本博客主要用于记录个人学习笔记">
<meta name="keywords" content="Python, Computer, ML, Linux, Ubuntu, NLP, Git, DL,">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiahong的个人博客">
<meta property="og:url" content="https://JoeZJH.github.io/index.html">
<meta property="og:site_name" content="Jiahong的个人博客">
<meta property="og:description" content="本博客主要用于记录个人学习笔记">
<meta property="og:locale" content="En/中">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jiahong的个人博客">
<meta name="twitter:description" content="本博客主要用于记录个人学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://JoeZJH.github.io/">





  <title>Jiahong的个人博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="En/中">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiahong的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">凡事预则立，不预则废</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/NLP/NLP——困惑度-Perplexity.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/NLP/NLP——困惑度-Perplexity.html" itemprop="url">NLP——困惑度-Perplexity</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文主要介绍困惑度在语言模型评估中的作用</em></p>
<!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<hr>
<h3 id="如何评价一个语言模型的好坏"><a href="#如何评价一个语言模型的好坏" class="headerlink" title="如何评价一个语言模型的好坏"></a>如何评价一个语言模型的好坏</h3><h4 id="直接评估法"><a href="#直接评估法" class="headerlink" title="直接评估法"></a>直接评估法</h4><ul>
<li>将语言模型应用到实际的具体问题中去,比如机器翻译,词性标注,拼写矫正等</li>
<li>评估语言模型在这些实际问题中的具体表现</li>
</ul>
<h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>准确</li>
<li>能精确评估在具体应用场景中的效果</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>费时</li>
<li>难以操作</li>
</ul>
<h4 id="困惑度-Perplexity"><a href="#困惑度-Perplexity" class="headerlink" title="困惑度(Perplexity)"></a>困惑度(Perplexity)</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><ul>
<li>给<strong>测试集</strong>的句子赋予较高概率值的语言模型较好</li>
<li>当一个语言模型训练完成后,测试集中的句子(正常的自然语言句子)出现概率越高越好</li>
</ul>
<h5 id="困惑度的定义"><a href="#困惑度的定义" class="headerlink" title="困惑度的定义"></a>困惑度的定义</h5><ul>
<li>如果存在测试文本\(d = (w_{1}, w_{2},,,w_{N})\),那么该文本在模型Model中的困惑度为:<br>$$<br>\begin{align}<br>Perplexity(d|Model) &amp;= P(d|Model)^{-\frac{1}{N}} \\<br>&amp;= P(w_{1}, w_{2},,,w_{N}|Model)^{-\frac{1}{N}} \\<br>&amp;= \sqrt[N]{\frac{1}{P(w_{1}, w_{2},,,w_{N}|Model)}}<br>\end{align}<br>$$</li>
<li>两边取log有<br>$$<br>\begin{align}<br>Log(Perplexity(d|Model)) &amp;= -\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model) \\<br>\end{align}<br>$$</li>
<li>一般来说计算时使用公式<br>$$<br>\begin{align}<br>Perplexity(d|Model) &amp;= e^{-\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model)} \\<br>&amp;= exp\left (-\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model)\right ) \\<br>\end{align}<br>$$</li>
<li>如果在已知模型Model参数时,文档\(d\)中的词独立,即\(w_{1},w_{2},,,w_{N}\)互相独立,则有:<br>$$<br>\begin{align}<br>P(w_{1}, w_{2},,,w_{N}|Model) = \prod_{n=1}^{N}P(w_{n}|Model) \\<br>\end{align}<br>$$</li>
<li>进一步有<br>$$<br>\begin{align}<br>Perplexity(d|Model) &amp;= exp\left (-\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model)\right ) \\<br>&amp;= exp\left (-\frac{1}{N}log\prod_{n=1}^{N}P(w_{n}|Model)\right ) \\<br>&amp;= exp\left (-\frac{1}{N}\sum_{n=1}^{N}logP(w_{n}|Model)\right ) \\<br>\end{align}<br>$$</li>
<li>以上是一个文档的表述,对于多个文档\(D = (d_{1}, d_{2},,,d_{M})\)<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= P(D|Model)^{-\frac{1}{\sum_{m=1}^{M}N_{m}}} \\<br>&amp;= \prod_{m=1}^{M} P(d_{m}|Model)^{-\frac{1}{\sum_{m=1}^{M}N_{m}}} \\<br>&amp;= \sqrt[(\sum_{m=1}^{M}N_{m})]{\frac{1}{\prod_{m=1}^{M}P(d_{m}|Model)}}<br>\end{align}<br>$$</li>
<li>两边取log<br>$$<br>\begin{align}<br>Log(Perplexity(D|Model)) &amp;= -\frac{1}{\sum_{m=1}^{M}N_{m}}log(\prod_{m=1}^{M}P(d_{m}|Model)) \\<br>&amp;= -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(d_{m}|Model) \\<br>&amp;= -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \\<br>\end{align}<br>$$</li>
<li>一般来说计算公式<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= e^{-\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model)} \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \right )<br>\end{align}<br>$$</li>
<li>如果在已知Model参数的情况下,每个文档中的词都相互独立,即任取文档\(d_{m}\)有\(w_{1}^{m},w_{2}^{m},,,w_{N_{m}}^{m}\)互相独立,则有<br>$$<br>\begin{align}<br>P(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) = \prod_{n=1}^{N_{m}}P(w_{n}^{m}|Model) \\<br>\end{align}<br>$$</li>
<li>进一步可得<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}P(w_{n}^{m}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}\sum_{n=1}^{N_{m}}logP(w_{n}^{m}|Model) \right ) \\<br>\end{align}<br>$$</li>
<li>注意,多个文档的困惑度<strong>不等于</strong>所有文档困惑度的<strong>积</strong>,而是等于<strong>把所有文档合并成一个大文档,大文档的困惑度则是最终所有文档的困惑度</strong></li>
</ul>
<ul>
<li>在给定模型中,<strong>测试句子</strong>出现的概率越大,对应的困惑度越小,模型越好</li>
</ul>
<h5 id="LDA的困惑度"><a href="#LDA的困惑度" class="headerlink" title="LDA的困惑度"></a>LDA的困惑度</h5><ul>
<li>LDA中\(w_{1},w_{2},,,w_{n}\)在参数已知的情况下是互相独立的,则有<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= e^{-\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model)} \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}P(w_{n}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}\sum_{k=1}^{K}P(w_{n}=t|z_{n}=k;Model)P(z_{n}=k|d=d_{m};Model)\right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}\sum_{k=1}^{K}\theta_{m,k}\phi_{k,t}\right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}\theta_{m,:}\phi_{:,t}\right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}\sum_{n=1}^{N_{m}}log\theta_{m,:}\phi_{:,t}\right ) \\<br>\end{align}<br>$$</li>
<li>其中\(\phi_{k,t}\)表示单词t在主题k中出现的概率,\(\theta_{m,k}\)表示主题k在文档m中出现的概率</li>
<li>\(\sum_{k=1}^{K}\theta_{m,k}\phi_{k,t} = (\theta_{m,:}\phi_{:,t})\)就是单词t出现在文档m中的概率(对隐变量主题k积分)</li>
<li>上面式子中\((\theta_{m,:}\phi_{:,t})\)就是两个向量的内积,在这里:\(\theta_{m,:}\)代表行向量,表示当前文档\(d_{m}\)的主题分布,\(\phi_{:,t}\)代表列向量,表示当前每个主题生成词\(w_{t}\)的概率</li>
<li>计算公式的代码可参考L-LDA模型的实现<a href="https://github.com/JoeZJH/Labeled-LDA-Python/blob/master/model/labeled_lda.py" target="_blank" rel="noopener">GitHub仓库: Labeled-LDA-Python</a> 中的<code>perplexity</code>函数和<code>log_perplexity</code>函数</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Others/趣味题——同距运动员.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Others/趣味题——同距运动员.html" itemprop="url">趣味题——同距运动员</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><ul>
<li>有27个参加跑步的人，每3人一组，分成9组，同一组用同一个号。就是1号3个，2号3个，3号3个……现在假设第一组的赢得了比赛，每次只有一个人到达。所有人到达的时候满足规律，1号参赛者之间都间隔一个人，2号参赛者之间都间隔2个人，3号参赛者之间都间隔3个人…9号参赛者之间都间隔9人。问27个人的到达顺序是否有解？如果有，解是什么？</li>
</ul>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">def check_row(row, gap):</span><br><span class="line">    a = abs(row[0] - row[1]) == gap</span><br><span class="line">    b = abs(row[1] - row[2]) == gap</span><br><span class="line">    c = abs(row[2] - row[0]) == gap</span><br><span class="line">    abc = [a, b, c]</span><br><span class="line">    if sum([1 if e else 0 for e in abc]) != 2:</span><br><span class="line">        return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_conds(ri, rest_number):</span><br><span class="line">    size = len(rest_number)</span><br><span class="line">    gap = ri + 2</span><br><span class="line">    conds = list()</span><br><span class="line">    for i in range(size):</span><br><span class="line">        for j in range(i+1, size):</span><br><span class="line">            for k in range(j+1, size):</span><br><span class="line">                row = rest_number[i], rest_number[j], rest_number[k]</span><br><span class="line">                if check_row(row, gap):</span><br><span class="line">                    conds.append(row)</span><br><span class="line">    return conds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def backtrace(maze, ri, rest_number, all_maze):</span><br><span class="line">    if not rest_number:</span><br><span class="line">        data_ = [[e for e in row] for row in maze]</span><br><span class="line">        all_maze.append(data_)</span><br><span class="line">        return True</span><br><span class="line">    conds = generate_conds(ri, rest_number)</span><br><span class="line">    if not conds:</span><br><span class="line">        return False</span><br><span class="line">    for row in conds:</span><br><span class="line">        local_rest = [e for e in rest_number]</span><br><span class="line">        for e in row:</span><br><span class="line">            # print(&quot;local rest: %s and e: %s&quot; % (local_rest, e))</span><br><span class="line">            local_rest.remove(e)</span><br><span class="line">        maze[ri] = [e for e in row]</span><br><span class="line">        backtrace(maze, ri + 1, local_rest, all_maze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def solution():</span><br><span class="line">    maze = list()</span><br><span class="line">    for i in range(9):</span><br><span class="line">        row = [0 for _ in range(3)]</span><br><span class="line">        maze.append(row)</span><br><span class="line">    rest_number = list(range(1, 28))</span><br><span class="line">    maze[0][0] = 1</span><br><span class="line">    maze[0][1] = 3</span><br><span class="line">    maze[0][2] = 5</span><br><span class="line">    rest_number.remove(1)</span><br><span class="line">    rest_number.remove(3)</span><br><span class="line">    rest_number.remove(5)</span><br><span class="line">    all_maze = list()</span><br><span class="line">    # print(rest_number)</span><br><span class="line">    # print(maze)</span><br><span class="line">    backtrace(maze, 1, rest_number, all_maze)</span><br><span class="line">    for maze in all_maze:</span><br><span class="line">        print(maze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    solution()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——CQL.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——CQL.html" itemprop="url">RL——CQL</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考链接：<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MDU2MTM1Mg==&mid=2247485320&idx=1&sn=56be21b854a8d13e0c6d585d6121d035" target="_blank" rel="noopener">【论文分享】Conservative Q-Learning for Offline Reinforcement Learning</a>  </li>
<li><a href="https://hackmd.io/@l-7O3seyTuaQ8JjAyLP2Ng/BJWLxRcXc" target="_blank" rel="noopener">离线强化学习系列3(算法篇): 值函数约束-CQL算法详解与实现</a>：手打公式</li>
</ul>
</li>
</ul>
<hr>
<h3 id="CQL在解决什么问题？"><a href="#CQL在解决什么问题？" class="headerlink" title="CQL在解决什么问题？"></a>CQL在解决什么问题？</h3><ul>
<li><strong>分布偏移（distribution shift）</strong>：分布偏移主要是模型训练和预测时的分布存在差异，即训练数据集中的数据分布（训练）与实际决策策略下环境反馈的数据分布（预测）之间的差异。 <ul>
<li>这种差异可能导致学习到的策略在实际应用中表现不佳，因为该策略是在一个与实际环境不完全相同的数据分布上学习得到的</li>
<li>离线强化学习中这种问题会放大，特别地，会导致OOD问题</li>
</ul>
</li>
<li><strong>OOD（Out-of-distribution）</strong>：OOD通常指在训练集中没有被观察到过的样本（状态或状态-动作对），实际上，出现频率极低的样本也可以算作一定程度上的OOD样本<ul>
<li>OOD问题是由于<strong>分布偏移</strong>导致的，理论上，<strong>没有分布偏移问题就不存在OOD问题</strong>，因为模型不会遇到这些未观察过的样本</li>
</ul>
</li>
<li><strong>OOD问题容易导致值高估（Overestimation of values）问题</strong>：强化学习的迭代公式一般都是找时的动作最大的动作，这本身导致的容易值高估的现象</li>
</ul>
<hr>
<h3 id="CQL相关推导"><a href="#CQL相关推导" class="headerlink" title="CQL相关推导"></a>CQL相关推导</h3><h4 id="一些定义"><a href="#一些定义" class="headerlink" title="一些定义"></a>一些定义</h4><ul>
<li>数据集\(\mathcal{D}  \sim d^{\pi_{\beta}}(\mathbf{s})\pi_{\beta}(\mathbf{a} \mid \mathbf{s})\)，即数据是行为策略\(\pi_\beta\)与环境交互来得到的</li>
<li>对于任意\((s_0, a_0) \in \mathcal{D}\)，有经验行为策略（empirical behavior policy）为：<br>$$ \hat{\pi}_\beta(a_0\vert s_0) = \frac{\sum_{(s,a) \in \mathcal{D}}\mathbf{1}(s=s_0,a=a_0)}{\sum_{s\in \mathcal{D}}\mathbf{1}(s=s_0)} $$</li>
</ul>
<h4 id="常规的学习方法"><a href="#常规的学习方法" class="headerlink" title="常规的学习方法"></a>常规的学习方法</h4><ul>
<li>Q-Learning的迭代公式（通过迭代如下Bellman Optimality Operator实现）：<br>$$ \mathcal{B}^{*} Q(\mathbf{s}, \mathbf{a})=r(\mathbf{s}, \mathbf{a})+\gamma \mathbb{E}_{\mathbf{s}^{\prime} \sim P\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)}\left[\max _{\mathbf{a}^{\prime}} Q\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right] $$</li>
<li>Actor-Critic的迭代思路  <img src="/Notes/RL/RL——CQL/CQL-1.png">
<ul>
<li>以上公式来自论文，实际上，上式中\(a \sim \pi^k(a|s)\)应该改成\(a \sim \pi(a|s)\)更合适，因为argmax的目标参数是\(\pi\)</li>
<li>其实AC方法中，包含了DQN的迭代思路，\(a’\)始终取上一轮Q值最大的动作即可</li>
</ul>
</li>
</ul>
<h4 id="改进一"><a href="#改进一" class="headerlink" title="改进一"></a>改进一</h4><ul>
<li>\(\mu(s, a)=d^{\pi_\beta}(s) \mu(a \mid s)\)<br>$$\hat{Q}^{k+1} \leftarrow \arg \min_{Q} \alpha \mathbb{E}_{\mathbf{s} \sim \mathcal{D}, \mathbf{a} \sim \mu(\mathbf{a} \mid \mathbf{s})}[Q(\mathbf{s}, \mathbf{a})]+\frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a} \sim \mathcal{D}}\left[\left(Q(\mathbf{s}, \mathbf{a})-\hat{\mathcal{B}}^{\pi} \hat{Q}^{k}(\mathbf{s}, \mathbf{a})\right)^{2}\right]$$<h4 id="改进二"><a href="#改进二" class="headerlink" title="改进二"></a>改进二</h4></li>
</ul>
<h4 id="改进三"><a href="#改进三" class="headerlink" title="改进三"></a>改进三</h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——BCQ.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——BCQ.html" itemprop="url">RL——BCQ</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/369524801" target="_blank" rel="noopener">【笔记】BCQ详解</a><ul>
<li>原作者的PPT</li>
</ul>
</li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/272152582" target="_blank" rel="noopener">BCQ姊妹篇：Discrete BCQ - Metaqiang的文章 - 知乎</a></li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/206489894" target="_blank" rel="noopener">【代码速读】（RL）1.BCQ - 一条的文章 - 知乎</a></li>
</ul>
<hr>
<h3 id="BCQ整体介绍"><a href="#BCQ整体介绍" class="headerlink" title="BCQ整体介绍"></a>BCQ整体介绍</h3><ul>
<li>BCQ（Batch-Constrained deep Q-learning）分为连续版本（<a href="https://proceedings.mlr.press/v97/fujimoto19a/fujimoto19a.pdf" target="_blank" rel="noopener">Off-Policy Deep Reinforcement Learning without Exploration</a>，2019年8月）和离散版本（<a href="https://arxiv.org/pdf/1910.01708" target="_blank" rel="noopener">Benchmarking   Batch Deep Reinforcement Learning Algorithms</a>，2019年10月）两篇文章，一作是同一个作者  </li>
<li><strong>外推误差(Extrapolation Error)的定义：off-policy值学习中，当前策略真实状态动作访问分布和数据集中的状态动作分布不匹配导致的一种误差</strong><blockquote>
<p>Extrapolation error is an error in off-policy value learning which is introduced by the mismatch between the dataset and true state-action visitation of the current policy</p>
</blockquote>
</li>
<li>背景：off-policy的策略理论上可以从任意行为策略采样的数据中学习最优策略，但是直接将off-policy策略应用到Offline RL（也称为Batch RL）场景中可能面临，<strong>Absent Data</strong>（状态动作对缺失），<strong>Training Mismatch</strong>（训练预测分布不一致），<strong>Model Bias</strong>（随机MDP的状态转移概率有偏差）等问题</li>
<li>BCQ的基本思想：采取保守策略，让学到的策略对应的状态动作访问空间尽量只在出现过的数据集上，或者相近的数据上<ul>
<li>基本方法：主要通过限制\(Q(s’,\pi(s’))\)中的\(s’\)不要偏离数据集太多来实现</li>
</ul>
</li>
</ul>
<hr>
<h3 id="BCQ连续版本"><a href="#BCQ连续版本" class="headerlink" title="BCQ连续版本"></a>BCQ连续版本</h3><h4 id="关键实验"><a href="#关键实验" class="headerlink" title="关键实验"></a>关键实验</h4><ul>
<li><p>实验设置：</p>
<ul>
<li>第一个实验(Final Buffer)，使用DDPG算法在线训练一个智能体，将<strong>智能体训练过程中与环境交互的所有数据</strong>保存下来，利用这些数据训练另一个离线DDPG智能体</li>
<li>第二个实验(Concurrent)，使用DDPG算法在线训练一个智能体，训练时每次从经验回放池中采样，并用相同的数据<strong>同步训练</strong>离线DDPG智能体，甚至保持训练时使用的数据和数据顺序都完全相同</li>
<li>第三个实验(Imitation)，使用DDPG算法在线训练一个智能体，将该智能体作为专家，与环境交互采集大量数据，利用这些数据训练另一个离线DDPG智能体</li>
</ul>
</li>
<li><p>实验结果：</p>
  <img src="/Notes/RL/RL——BCQ/BCQ-DDPG-Experiments.png" title height="50%" width="50%">
  <!-- <img src="/Notes/RL/RL——BCQ/BCQ-DDPG-Experiments.png"> -->

<ul>
<li>第三个实验中使用的样本最好，但是训练得到的离线智能体效果最差，原因分析主要是<strong>外推误差</strong>导致</li>
<li>三个实验的离线DDPG智能体都有不同情况的Q值高估问题，其中第三个实验的Q值高估问题最为严重（注意图2中看起来高估问题大于图1中，其实不是，是因为图二的量纲较小导致的）</li>
</ul>
</li>
</ul>
<h4 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h4><ul>
<li>对于给定的真实MDP和数据集\(\mathcal{B}\)，定义外推误差<br>$$\epsilon_\text{MDP}(s,a) = Q^\pi(s,a) - Q_{\mathcal{B}}^\pi(s,a)$$</li>
<li>则有外推误差可推导得到如下结论：<img src="/Notes/RL/RL——BCQ/BCQ-Extrapolation-Error.png" title height="80%" width="80%">
<!-- <img src="/Notes/RL/RL——BCQ/BCQ-Extrapolation-Error.png"> -->

</li>
</ul>
<h4 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h4><ul>
<li><p>整体流程概览:</p>
<img src="/Notes/RL/RL——BCQ/BCQ-Algorithm.png" title height="80%" width="80%">
<!-- <img src="/Notes/RL/RL——BCQ/BCQ-Algorithm.png"> -->
</li>
<li><p>训练流程解释：</p>
<ul>
<li>训练时，使用4个Q网络（其中两个是Target Q网络），1个策略网络和扰动网络</li>
<li>两个Q网络的用途是在计算Q值目标时做他们最大最小值的凸组合（实际上就是最大最小值的加权平均），类似Twin Q中取两个Q的最小值的方法，\(y\)值计算方法（流程中公式(13)）<br>$$ y = r + \gamma \max_{a_i}\Big[ \lambda \min_{j=1,2}Q_{\theta_j}(s’,a_i) + (1-\lambda)\max_{j=1,2}Q_{\theta_j}(s’,a_i) \Big] $$</li>
</ul>
</li>
</ul>
<h4 id="Serving步骤"><a href="#Serving步骤" class="headerlink" title="Serving步骤"></a>Serving步骤</h4><ul>
<li>给定一个状态\(s\)</li>
<li>\(\{ a_i \sim G_w(s) \}_{i=1}^n\)：通过conditional VAE网络 \(G_w(s)\) 采样\(n\)个动作</li>
<li>\(\xi_{\phi}(s,a_i,\Phi)\)：将这些状态和动作经过扰动网络，扰动网络输出是在\([-\Phi,\Phi]\)内的，得到的扰动值</li>
<li>将扰动添加到原始动作上，再将动作经过Q网络，选取能使Q value最大的动作</li>
<li>最终总结如下：<br>$$ \pi(s) = \mathop{\arg\max}_{a_i + \xi_{\phi}(s,a_i,\Phi)} Q_\theta(s, a_i + \xi_{\phi}(s,a_i,\Phi)), \quad with \quad \{ a_i \sim G_w(s) \}_{i=1}^n $$</li>
</ul>
<hr>
<h3 id="BCQ离散版本"><a href="#BCQ离散版本" class="headerlink" title="BCQ离散版本"></a>BCQ离散版本</h3><h4 id="训练流程-1"><a href="#训练流程-1" class="headerlink" title="训练流程"></a>训练流程</h4><ul>
<li>整体流程概览:<img src="/Notes/RL/RL——BCQ/Discrete-BCQ-Algorithm.png">

</li>
</ul>
<h4 id="Serving步骤-1"><a href="#Serving步骤-1" class="headerlink" title="Serving步骤"></a>Serving步骤</h4><ul>
<li>按照如下策略决策：</li>
</ul>
<p>$$ \pi(s) = \mathop{\arg\max}_{a\vert\frac{G_w(a|s)}{\max_\hat{a} G_w(\hat{a}|s)} \gt \tau} Q_\theta(s,a) $$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Hadoop/Hadoop——hdfs+dfs和hadoop+fs的区别.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Hadoop/Hadoop——hdfs+dfs和hadoop+fs的区别.html" itemprop="url">Hadoop——hdfs+dfs和hadoop+fs的区别</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>在管理HDFS文件时，我们常用的命令有三个 <code>hadoop fs</code>，<code>hadoop dfs</code>和<code>hdfs dfs</code>【注意没有<code>hdfs fs</code>】</em><br><em>参考：<a href="https://blog.csdn.net/u013019431/article/details/78485555" target="_blank" rel="noopener">https://blog.csdn.net/u013019431/article/details/78485555</a></em></p>
<hr>
<h3 id="比较三种命令的区别"><a href="#比较三种命令的区别" class="headerlink" title="比较三种命令的区别"></a>比较三种命令的区别</h3><ul>
<li><p>hadoop fs:</p>
<ul>
<li>FS relates to a generic file system which can point to any file systems like local, HDFS etc. So this can be used when you are dealing with different file systems such as Local FS, HFTP FS, S3 FS, and others</li>
<li>意思是说该命令可以用于其他文件系统，不止是hdfs文件系统内，也就是说该命令的使用范围更广</li>
</ul>
</li>
<li><p>hadoop dfs</p>
<ul>
<li>专门针对hdfs分布式文件系统</li>
</ul>
</li>
<li><p>hdfs dfs</p>
<ul>
<li>和上面的命令作用相同，相比于上面的命令更为推荐，并且当使用hadoop dfs时内部会被转为hdfs dfs命令</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Java/Java——Logger变量命名规则.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Java/Java——Logger变量命名规则.html" itemprop="url">Java——Logger变量命名规则</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h3 id="一般Java常量命名规范"><a href="#一般Java常量命名规范" class="headerlink" title="一般Java常量命名规范"></a>一般Java常量命名规范</h3><ul>
<li>Java中的常量名称一般用全大写，比如美团，阿里等公司均有相关要求，详情参考<a href="/Backup/Backup/Java/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4Java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C.pdf">阿里巴巴Java开发手册.pdf</a></li>
</ul>
<hr>
<h3 id="一个特殊的例子——Logger"><a href="#一个特殊的例子——Logger" class="headerlink" title="一个特殊的例子——Logger"></a>一个特殊的例子——Logger</h3><h4 id="特殊写法"><a href="#特殊写法" class="headerlink" title="特殊写法"></a>特殊写法</h4><ul>
<li><p>Spring中Logger对象的名称使用的是小写</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">private static final Logger logger= LoggerFactory.getLogger(BeanFactory.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他很多公司或者开源工具的代码也跟着这样用，比如美团的RPC开源框架<code>mtthrift</code></p>
</li>
</ul>
<h4 id="Logger对象使用final的原因"><a href="#Logger对象使用final的原因" class="headerlink" title="Logger对象使用final的原因"></a>Logger对象使用final的原因</h4><ul>
<li>定义成static final,logger变量不可变，读取速度快</li>
<li>static 修饰的变量是不管创建了new了多少个实例，也只创建一次，节省空间，如果每次都创建Logger的话比较浪费内存；final修饰表示不可更改，常量</li>
<li>将域定义为static,每个类中只有一个这样的域。而每一个对象对于所有的实例域却都有自己的一份拷贝，用static修饰既节约空间，效率也好。final 是本 logger 不能再指向其他 Logger 对象</li>
</ul>
<h4 id="为什么不适用大写"><a href="#为什么不适用大写" class="headerlink" title="为什么不适用大写"></a>为什么不适用大写</h4><ul>
<li>Spring开发者有自己的编程规范<ul>
<li>常量引用不用大写？</li>
<li><code>private</code>修饰的常量不用大写？</li>
<li>Logger太特殊了，使用特殊定义，仅此一个，别无其他</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/Linux/Linux——终端登录其他用户打开图形化窗口.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/Linux/Linux——终端登录其他用户打开图形化窗口.html" itemprop="url">Linux——多用户问题</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>在Linux系统有多个用户时，我们可能需要从一个用户界面打开终端登录另一个用户，从而使用该用户的环境和软件</em></p>
<hr>
<h3 id="多用户打开各自软件问题"><a href="#多用户打开各自软件问题" class="headerlink" title="多用户打开各自软件问题"></a>多用户打开各自软件问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><ul>
<li>在一个用户登录图形界面后，需要以另一个用户的身份打开一个图形化软件，此时直接打开图形化软件可能遇到如下错误</li>
</ul>
<blockquote>
<p>No protocol specified</p>
</blockquote>
<h4 id="问题发生原因"><a href="#问题发生原因" class="headerlink" title="问题发生原因"></a>问题发生原因</h4><ul>
<li>因为Xserver默认情况下禁止别的用户图形程序运行在当前用户图形界面上</li>
</ul>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li>在当前用户下执行命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xhost +</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——AC、A2C和A3C.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——AC、A2C和A3C.html" itemprop="url">RL——AC、A2C和A3C</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<hr>
<h3 id="策略梯度法的推导结论"><a href="#策略梯度法的推导结论" class="headerlink" title="策略梯度法的推导结论"></a>策略梯度法的推导结论</h3><ul>
<li>策略梯度法推导结论是<br>  $$<br>  \begin{align}<br>  \nabla_\theta J(\theta) &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)] \\<br>  &amp; = E_{\tau \sim p_\theta(\tau)} \left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)  \right] \\<br>  &amp; = E_{\tau \sim p_\theta(\tau)} \left[\sum_t \psi_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\<br>  \end{align}<br>  $$<ul>
<li>推导详情见<a href="/Notes/RL/RL%E2%80%94%E2%80%94%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95.html">RL——策略梯度法</a></li>
<li>\(\psi_t\)来替换\(R(\tau)\)的理由是可以考虑用一个与时间相关的变量来替换与时间无关的累计收益</li>
</ul>
</li>
<li>进一步地，其中的\(\psi_t\)可以换成不同的形式，包括：<ul>
<li>\(\sum_{t’=0}^T \gamma^t r_{t’}\)：完成的轨迹收益</li>
<li>\(\sum_{t’=t}^T \gamma^{t’-t} r_{t’}\)：从第\(t\)步开始的轨迹收益，理由是策略主要影响的是\(t\)步开始收益，对前面步骤的收益影响不大</li>
<li>\(\sum_{t’=t}^T \gamma^{t’-t} r_{t’} - b(s_t)\)：进一步降低方差，详细证明见<a href="/Notes/RL/RL%E2%80%94%E2%80%94%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95.html">RL——策略梯度法</a></li>
<li>\(Q^{\pi_\theta}(s_t,a_t)\)：动作价值函数，是\(\sum_{t’=t}^T \gamma^{t’-t} r_{t’}\)的估计值</li>
<li>\(A^{\pi_\theta}(s_t,a_t)\)：优势函数，仅考虑当前状态\(s_t\)下不同动作带来的收益，忽略状态本身的价值</li>
<li>\( r_t + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_{t}) \)：V值的TD-Error形式，即贝尔曼残差，本质等价于优势函数</li>
</ul>
</li>
<li>问题：将\(\psi_t\)换成\( r_t + \gamma Q^{\pi_\theta}(s_{t+1}, a_{t+1}) - Q^{\pi_\theta}(s_{t}, a_t) \)可以吗？<ul>
<li>答案是不可以。理由是\( r_t + \gamma Q^{\pi_\theta}(s_{t+1}, a_{t+1}) - Q^{\pi_\theta}(s_{t}, a_t) \)本身不是优势函数\(A(s,a)\)，也不是\(Q(s,a)\)没有别的含义，只有Q值自身更新时的TD-Error</li>
<li>证明：如果\(a_{t+1} \sim \pi_\theta\)，则\(Q^{\pi_\theta}(s_{t+1}, a_{t+1}) = V^{\pi_\theta}(s_{t+1})\)；但是因为\(Q^{\pi_\theta}(s_{t}, a_t) \neq V^{\pi_\theta}(s_{t})\)（因为\(a_t\)是已经发生的事实），故而\( r_t + \gamma Q^{\pi_\theta}(s_{t+1}, a_{t+1}) - Q^{\pi_\theta}(s_{t}, a_t) \)本身不是优势函数，也能等价于优势函数</li>
<li>改进：如果非要用Q值来作为Actor Critic的价值网络，则需要求解策略\(\pi_{\theta^*} = \arg\max_{\pi_\theta} E_{a_t \sim \pi_\theta(\cdot|s_t)} [Q(s_t, a_t)]\)，比如DDPG就是如此</li>
</ul>
</li>
</ul>
<hr>
<h3 id="AC算法"><a href="#AC算法" class="headerlink" title="AC算法"></a>AC算法</h3><ul>
<li>普通AC（Actor Critic）算法一般是直接使用\(Q(s_t,a_t)\)来替换\(\psi_t\)</li>
</ul>
<h4 id="Critic网络的更新"><a href="#Critic网络的更新" class="headerlink" title="Critic网络的更新"></a>Critic网络的更新</h4><p>$$<br>Loss_{\text{critic}} = \sum (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ </p>
<ul>
<li>这里要求\(a_{t+1} \sim \pi_\theta\)，\(Q^{w}\)值拟合的目标是策略\(\pi_\theta\)对应的Q值\(Q^{\pi_\theta}(s_{t}, a_t)\)</li>
</ul>
<h4 id="Actor网络的更新"><a href="#Actor网络的更新" class="headerlink" title="Actor网络的更新"></a>Actor网络的更新</h4><p>$$<br>\begin{align}<br>Loss_{\text{actor}} &amp;= \sum Q^{w}(s_{t}, a_t) \log \pi_\theta(a_t|s_t) \\<br>Q^{w}(s_{t}, a_t) &amp;= \text{Stop_Gradient}(Q^{w}(s_{t}, a_t))<br>\end{align}<br>$$</p>
<ul>
<li>这里面的\(Q^{w}(s_{t}, a_t)\)是不参与Actor参数的更新的</li>
<li>\(a_t \sim \pi_\theta(\cdot|s_t)\)</li>
</ul>
<h4 id="AC算法的问题"><a href="#AC算法的问题" class="headerlink" title="AC算法的问题"></a>AC算法的问题</h4><ul>
<li>AC算法虽然是直接优化策略的，但是由于它是on-policy的，样本利用率很低，导致训练缓慢</li>
</ul>
<hr>
<h3 id="A2C算法"><a href="#A2C算法" class="headerlink" title="A2C算法"></a>A2C算法</h3><ul>
<li>A2C（Advantage Actor Critic）算法是使用\( r_t + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_{t}) \)来替换\(\psi_t\)的方法</li>
<li>很多书籍里面会直接使用A2C算法</li>
</ul>
<h4 id="Critic网络的更新-1"><a href="#Critic网络的更新-1" class="headerlink" title="Critic网络的更新"></a>Critic网络的更新</h4><p>$$<br>Loss_{\text{critic}} = \sum (r_t + \gamma V^{\bar{w}}(s_{t+1}) - V^{w}(s_{t})) ^ 2<br>$$ </p>
<ul>
<li>这里V值拟合的目标是策略\(\pi_\theta\)对应的V值\(V^{\pi_\theta}\)</li>
</ul>
<h4 id="Actor网络的更新-1"><a href="#Actor网络的更新-1" class="headerlink" title="Actor网络的更新"></a>Actor网络的更新</h4><p>$$<br>\begin{align}<br>Loss_{\text{actor}} &amp;= \sum \delta \log \pi_\theta(a_t|s_t) \\<br>\delta &amp;= \text{Stop_Gradient}(r_t + \gamma V^{w}(s_{t+1}) - V^{w}(s_{t}))<br>\end{align}<br>$$</p>
<ul>
<li>这里面的\(r_t + \gamma V^{w}(s_{t+1}) - V^{w}(s_{t})\)是不参与Actor参数的更新的</li>
</ul>
<h4 id="A2C算法的问题"><a href="#A2C算法的问题" class="headerlink" title="A2C算法的问题"></a>A2C算法的问题</h4><ul>
<li>A2C算法是同步更新的，需要每个Worker都收集完成数据才能执行一次更新，很多时候数据交互完成的时间是很不一致的，训练也较慢</li>
</ul>
<hr>
<h3 id="A3C算法"><a href="#A3C算法" class="headerlink" title="A3C算法"></a>A3C算法</h3><ul>
<li>A3C（Asynchronous Advantage Actor Critic），是Actor Critic方法的异步版本</li>
</ul>
<h4 id="A3C的主要优化点"><a href="#A3C的主要优化点" class="headerlink" title="A3C的主要优化点"></a>A3C的主要优化点</h4><h5 id="异步训练框架优化："><a href="#异步训练框架优化：" class="headerlink" title="异步训练框架优化："></a>异步训练框架优化：</h5><ul>
<li>一个全局网络（包括V网络和Actor网络）和多个相互独立的Local网络（即Worker）</li>
<li>训练时：使用多个Worker并行的和环境分别交互，各自收集数据、计算梯度、异步更新全局网络参数</li>
<li>inference时：仅适用全局网络中的Actor网络就可以</li>
</ul>
<h5 id="策略更新损失增加熵"><a href="#策略更新损失增加熵" class="headerlink" title="策略更新损失增加熵"></a>策略更新损失增加熵</h5><p>$$<br>\begin{align}<br>Loss_{\text{actor}} &amp;= \sum \delta \log \pi_\theta(a_t|s_t) + c H(\pi_\theta(a|s)) \\<br>\delta &amp;= \text{Stop_Gradient}(r_t + \gamma V^{w}(s_{t+1}) - V^{w}(s_{t}))<br>\end{align}<br>$$</p>
<ul>
<li>增加熵相当于是一种正则，与SAC思想相似</li>
</ul>
<hr>
<h3 id="AC-A2C-A3C的区别"><a href="#AC-A2C-A3C的区别" class="headerlink" title="AC/A2C/A3C的区别"></a>AC/A2C/A3C的区别</h3><ul>
<li>AC（Actor-Critic）、A2C（Advantage Actor-Critic）和A3C（Asynchronous Advantage Actor-Critic）三者可以从名字上看出来算法的区别</li>
</ul>
<h4 id="AC（Actor-Critic）"><a href="#AC（Actor-Critic）" class="headerlink" title="AC（Actor-Critic）"></a>AC（Actor-Critic）</h4><ul>
<li>Actor-Critic 方法结合了值函数方法和策略梯度方法的优点。它由两部分组成：一个称为“actor”的网络负责学习采取什么行动；另一个称为“critic”的网络评估采取的动作的好坏，即这个动作的价值</li>
</ul>
<h4 id="A2C（Advantage-Actor-Critic）"><a href="#A2C（Advantage-Actor-Critic）" class="headerlink" title="A2C（Advantage Actor-Critic）"></a>A2C（Advantage Actor-Critic）</h4><ul>
<li><strong>关键词：Advantage</strong></li>
<li>A2C 是 Actor-Critic 方法的一个变种，它引入了优势函数（advantage function）的概念来代替直接使用价值函数</li>
<li>优势函数 (A(s, a)) 定义为执行特定动作相对于遵循当前策略下的平均行为所能获得的优势或劣势。这种方法有助于更准确地估计哪些动作比其他动作更好，从而提高学习效率</li>
</ul>
<ul>
<li><strong>更新方式</strong>：A2C 使用同步的方式更新参数，可以考虑使用多个网Actor分别于环境进行交互，然后共同的样本一起<strong>同步更新主网络</strong>，这意味着所有代理（agents）共享同一个模型，并且在每个训练步骤结束时同步更新模型权重</li>
</ul>
<h4 id="A3C（Asynchronous-Advantage-Actor-Critic）"><a href="#A3C（Asynchronous-Advantage-Actor-Critic）" class="headerlink" title="A3C（Asynchronous Advantage Actor-Critic）"></a>A3C（Asynchronous Advantage Actor-Critic）</h4><ul>
<li><strong>关键词：Asynchronous</strong></li>
<li>A3C 是 A2C 的异步版本，它允许多个代理同时在不同的环境中学习</li>
<li><strong>更新方式</strong>：每个代理都有自己的环境副本和局部模型副本，它们独立地探索环境并收集经验。然后，这些经验被用来异步地更新全局模型，这样可以增加数据多样性并加快学习速度</li>
<li><strong>优点</strong>：通过异步更新，A3C 可以有效利用多核处理器的能力，实现更快的学习速度和更好的性能</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——GAE.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——GAE.html" itemprop="url">RL——GAE</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<p><em>GAE，Generalized advantage estimation，平衡了强化学习中的方差与偏差，常用于TRPO和PPO中，用于评估优势函数</em></p>
<h3 id="GAE的推导"><a href="#GAE的推导" class="headerlink" title="GAE的推导"></a>GAE的推导</h3><ul>
<li>推导过程（基于多步时序差分的思想）  <img src="/Notes/RL/RL——GAE/GAE.png">
<ul>
<li>\(\lambda\) 是一个介于0和1之间的参数，用于控制偏差-方差的权衡。当 \(\lambda = 0\)时，GAE退化为单步TD误差；当\(\lambda = 1\)时，GAE等价于无限步回报</li>
<li>\(\gamma\) 是折扣因子</li>
</ul>
</li>
<li>GAE本质上是对不同步数的优势函数进行加权求和，最终得到一个的优势函数估计值</li>
<li>GAE可通过参数调整控制方差和偏差的关系</li>
</ul>
<h3 id="附录：强化学习中的方差与偏差"><a href="#附录：强化学习中的方差与偏差" class="headerlink" title="附录：强化学习中的方差与偏差"></a>附录：强化学习中的方差与偏差</h3><ul>
<li>参考链接：<a href="/Notes/RL/RL%E2%80%94%E2%80%94%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html">方差与偏差</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://JoeZJH.github.io/Notes/RL/RL——DDPG和TD3.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joe Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/jiahong-head.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiahong的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Notes/RL/RL——DDPG和TD3.html" itemprop="url">RL——DDPG和TD3</a></h1>
        

        <div class="post-meta">
          

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/384497349" target="_blank" rel="noopener">强化学习之图解PPO算法和TD3算法</a>  <!-- <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"  type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


</li>
</ul>
</li>
</ul>
<hr>
<h3 id="DPG"><a href="#DPG" class="headerlink" title="DPG"></a>DPG</h3><ul>
<li>原始论文：<a href="https://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">Deterministic Policy Gradient Algorithms</a>（论文中详细描述了SGD，DPG两种算法的off-policy，on-policy版本的分析）  </li>
</ul>
<h4 id="Stochastic-Actor-Critic-Algorithms"><a href="#Stochastic-Actor-Critic-Algorithms" class="headerlink" title="Stochastic Actor-Critic Algorithms"></a>Stochastic Actor-Critic Algorithms</h4><ul>
<li>Critic网络损失函数<br>$$<br>Loss_{\text{critic}} = \sum_t (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ <ul>
<li>这里要求\(a_{t+1} \sim \pi_\theta\)，\(Q^w(s_t,a_t)\)值拟合的目标是策略\(\pi_\theta\)对应的Q值\(Q^{\pi_\theta}(s_{t}, a_t)\)</li>
<li>这里训练使用的\((s_t,a_t,s_{t+1})\)是当前策略采样到的数据（实际上，Q值的学习样本保证\(a_{t+1}\)的采样策略即可，样本可以是任意策略采样的，当然，用当前策略采样的会更好些）</li>
</ul>
</li>
<li>Actor网络优化梯度：<br>$$<br>\nabla_\theta J(\pi_\theta) = E_{s\sim \rho^{\pi},a\sim\pi_\theta}\left[ \nabla_\theta \log \pi_\theta(a|s)Q^w(s,a) \right]<br>$$<ul>
<li>问题：\(s\sim \rho^{\pi}\)中的\(\pi\)必须是\(\pi_\theta\)吗？<ul>
<li>回答：是的，原始推导中，回合\(\tau\)必须是来自当前策略的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Off-Policy-Actor-Critic"><a href="#Off-Policy-Actor-Critic" class="headerlink" title="Off-Policy Actor Critic"></a>Off-Policy Actor Critic</h4><ul>
<li>Critic网络损失函数<br>$$<br>Loss_{\text{critic}} = \sum_t (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ <ul>
<li>这里要求\(a_{t+1} \sim \pi_\theta\)，\(Q^w(s_t,a_t)\)值拟合的目标是策略\(\pi_\theta\)对应的Q值\(Q^{\pi_\theta}(s_{t}, a_t)\)</li>
<li>这里训练使用的\((s_t,a_t,s_{t+1})\)是当前其他行为策略采样到的数据</li>
</ul>
</li>
<li>Actor网络优化梯度：<br>$$<br>\nabla_\theta J(\pi_\theta) = E_{s\sim \rho^\beta,a\sim \beta}\left[ \frac{\pi_\theta(a|s)}{\beta_\theta(a|s)} \nabla_\theta \log \pi_\theta(a|s)Q^w(s,a) \right]<br>$$<ul>
<li>问题：<strong>动作的偏移通过重要性采样\(\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)} \)来解决，但是状态也可以吗？</strong> <ul>
<li>回答：不可以，这里状态采用行为策略采样是因为off-policy场景下，策略梯度的目标就是在行为策略采样的状态上最大化目标函数（这样得到的不是最优策略，线上serving时的状态分布肯定与当前行为策略采样的状态不一致，所以是一个妥协的次优解）</li>
</ul>
</li>
<li>思考：<strong>off-policy AC 与 DQN 的区别</strong><ul>
<li>对于DQN，我们通过在每一个状态上让Q值拟合到最优策略对应的Q值（与状态分布无关，任意的状态我们都可以找到最优策略对应的Q值），然后通过\(\arg\max_a Q(s,a)\)来找到最优策略。</li>
<li>对于off-policy AC，如果不考虑状态的分布，这里带来的偏差是从优化目标上出现的，即off-policy AC最大化的目标是，在行为策略采样的状态分布下，寻找一个策略，最大化累计策略收益期望。这里的目标显然与on-policy的原始目标不同了，状态分布线上线下不一致问题会导致天然的偏差。</li>
<li>问题：为什么不可以理解为与DQN一样？任意给定的状态我都做到策略最大化了，实际上就已经求到了最优策略了？（按照这个理解，除了off-policy都会遇到的训练评估数据分布不一致外，没有别的问题？）<ul>
<li>回答：不可以，因为<strong>DQN的目标是拟合\(Q^*(s,a)\)，与状态分布无关</strong>；而<strong>策略梯度法的目标找到一个最优策略\(\pi^*\)，最大化策略该策略下的累计收益，这里要求状态分布和动作分布均来自求解到的最优策略\(\pi^*\)</strong>，off-policy AC下的状态分布是行为策略的，存在偏差</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>off-policy AC方法不常用，因为从目标上天然旧带着偏差</li>
</ul>
<h5 id="Off-Policy-AC如何对混合策略采样的样本进行重要性采样？"><a href="#Off-Policy-AC如何对混合策略采样的样本进行重要性采样？" class="headerlink" title="Off-Policy AC如何对混合策略采样的样本进行重要性采样？"></a>Off-Policy AC如何对混合策略采样的样本进行重要性采样？</h5><ul>
<li>在Replay Buffer中记录下每个样本的采样策略（<a href="https://github.com/Kaixhin/ACER/blob/master/train.py#L194" target="_blank" rel="noopener">代码示例</a>），并在更新时逐个样本计算动作概率比值（<a href="https://github.com/Kaixhin/ACER/blob/master/train.py#L89-L92" target="_blank" rel="noopener">代码示例</a>），参见：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## 策略记录</span><br><span class="line">memory.append(state, action, reward, policy.detach()) </span><br><span class="line"></span><br><span class="line">## 动作概率比值计算</span><br><span class="line">if off_policy:</span><br><span class="line">    rho = policies[i].detach() / old_policies[i]</span><br><span class="line">else:</span><br><span class="line">    rho = torch.ones(1, action_size)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="On-Policy-Deterministic-Actor-Critic"><a href="#On-Policy-Deterministic-Actor-Critic" class="headerlink" title="On-Policy Deterministic Actor-Critic"></a>On-Policy Deterministic Actor-Critic</h4><ul>
<li>优化目标<br>$$ J(\theta) = \int_\mathcal{S} \rho^{\mu_\theta}(s) Q^{\mu_\theta}(s, \mu_\theta(s)) ds $$ <ul>
<li>其中\(\rho^{\mu_\theta}(s’) = \int_\mathcal{S} \sum_{k=1}^\infty \gamma^{k-1} \rho_0(s) \rho^\mu(s \to s’, k) ds\)</li>
</ul>
</li>
<li>确定性梯度定理：<br>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)<br>&amp;= \int_\mathcal{S} \rho^{\mu_\theta}(s) \nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ds \\<br>&amp;= E_{s \sim \rho^{\mu_\theta}} [\nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}]<br>\end{aligned}<br>$$</li>
<li>确定性策略看作是随机策略的一种特殊形式，也就是策略的概率分布仅在某一个动作上有非零概率(该动作概率为1)。实际上，在DPG的论文中，作者指出：如果对随机策略，通过确定性策略和一个随机变量进行重参数化(re-parameterize)，那么随机策略最终会在方差\(\sigma=0\)时与确定性策略等价。由于随机策略需要对整个状态和动作空间进行积分，我们可以预计随机策略的学习需要比确定性策略更多的样本（这里只是猜测，没有证据证明）</li>
</ul>
<h4 id="Off-Policy-Deterministic-Actor-Critic"><a href="#Off-Policy-Deterministic-Actor-Critic" class="headerlink" title="Off-Policy Deterministic Actor-Critic"></a>Off-Policy Deterministic Actor-Critic</h4><ul>
<li><p>优化目标<br>$$ J_\beta(\theta) = \int_\mathcal{S} \rho^\beta(s) Q^{\mu_\theta}(s, \mu_\theta(s)) ds $$ </p>
<ul>
<li>其中\(\rho^\beta(s)\)是行为策略上采样得到的样本状态分布，这里直接导致了优化目标不是在最优策略下的回合（回合包含状态和动作）分布下的奖励期望最大，相对on-policy Deterministic AC算是次优解</li>
</ul>
</li>
<li><p>推导结果：<br>$$<br>\begin{aligned}<br>\nabla_\theta J_\beta(\theta) &amp;= E_{s \sim \rho^\beta(s)} \left[\nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s)  \rvert_{a=\mu_\theta(s)} \right]<br>\end{aligned}<br>$$</p>
</li>
<li><p>Critic网络更新（TD-Error）<br>$$<br>Loss_{\text{critic}} = \sum_t (r_t + \gamma Q^{\bar{w}}(s_{t+1}, a_{t+1}) - Q^{w}(s_{t}, a_t)) ^ 2<br>$$ </p>
<ul>
<li>这里要求\(a_{t+1} = \mu_\theta(s_{t+1})\)，\(Q^w(s_t,a_t)\)值拟合的目标是策略\(\mu_\theta\)对应的Q值\(Q^{\mu_\theta}(s_{t}, a_t)\)，实际更新中常使用Target网络\(\bar{\theta}\)</li>
<li>这里训练使用的\((s_t,a_t,s_{t+1})\)是行为策略采样到的数据（Q值\(Q^{\mu_\theta}(s_{t}, a_t)\)的学习样本保证\(a_{t+1}\)的采样策略是\(\mu_\theta\)即可，样本可以是任意策略采样的，当然，用当前策略采样的会更好些）</li>
</ul>
</li>
<li><p>Actor网络更新<br>$$<br>\begin{aligned}<br>Loss_{\text{actor}} = - E_{s_t \sim \rho^\beta(s)} [Q_w(s_t,\mu_\theta(s_t))]<br>\end{aligned}<br>$$</p>
<ul>
<li>上面的Loss求导就可以得到梯度是\(E_{s \sim \rho^\beta(s)} \left[\nabla_a Q^{\mu_\theta}(s, a) \nabla_\theta \mu_\theta(s)  \rvert_{a=\mu_\theta(s)} \right]\)，与之前推导结论一致</li>
<li>直观上理解，这里的目标是对于任意给定的状态\(s_t\)下（这个状态样本是行为策略采样得到的），找到一个最大最大化当前\(Q_w(s_t,\mu_\theta(s_t)) \)的动作参数\(\mu_\theta\)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h3><ul>
<li>Deep Deterministic Policy Gradient Algorithms，是DPG的Deep网络版本，原始论文地址<a href="https://arxiv.org/pdf/1509.02971" target="_blank" rel="noopener">CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</a>  </li>
</ul>
<h4 id="DDPG训练流程"><a href="#DDPG训练流程" class="headerlink" title="DDPG训练流程"></a>DDPG训练流程</h4><ul>
<li>伪代码：<img src="/Notes/RL/RL——DDPG和TD3/DDPG-Algorithm.png">
<ul>
<li>其中\(\theta^{\mu’}\)和\(\theta^{Q’}\)分别表示策略\(\mu’\)和价值\(Q’\)的参数</li>
<li><strong>随机探索</strong>：做选择动作\(a_t\)时，添加一个随机噪声，可以增强探索能力，使得模型更加鲁棒，如果没有随机噪声，可能会很快收敛到局部最优</li>
<li><strong>软更新</strong>：Target网络的更新选择软更新，DQN中是硬更新</li>
</ul>
</li>
</ul>
<hr>
<h3 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h3><ul>
<li>TD3是对DDPG的改进，全称为Twin Delayed Deep Deterministic Policy Gradient Algorithm，原始论文：<a href="https://arxiv.org/pdf/1802.09477" target="_blank" rel="noopener">Addressing Function Approximation Error in Actor-Critic Methods</a>  </li>
<li>有两个改进包含在名字中，Twin和Delayed</li>
<li>其他改进是在Actor 的target网络输出中，增加噪声</li>
</ul>
<h4 id="TD3训练流程"><a href="#TD3训练流程" class="headerlink" title="TD3训练流程"></a>TD3训练流程</h4><ul>
<li>伪代码<img src="/Notes/RL/RL——DDPG和TD3/TD3-Algorithm.png">
<ul>
<li>\(t \text{mod} d\)表示策略更新比Q值更新慢一些，\(d\)次Q值更新对应一次策略更新</li>
</ul>
</li>
</ul>
<h4 id="改进1：Twin"><a href="#改进1：Twin" class="headerlink" title="改进1：Twin"></a>改进1：Twin</h4><ul>
<li>采用双Critic网络（训练网络和target网络均为双网络），缓解Q值高估问题</li>
</ul>
<h4 id="改进2：Delayed"><a href="#改进2：Delayed" class="headerlink" title="改进2：Delayed"></a>改进2：Delayed</h4><ul>
<li>Actor的目标是在Q值更新时，寻找最优的策略，如果Q值更新太快，容易波动，可以让Q值比较稳定了再更新Actor网络</li>
<li>具体做法，Critic网络更新\(d\)次再更新一次Actor</li>
</ul>
<h4 id="改进3：Target策略网络增加噪声"><a href="#改进3：Target策略网络增加噪声" class="headerlink" title="改进3：Target策略网络增加噪声"></a>改进3：Target策略网络增加噪声</h4><ul>
<li>在Actor 的target策略网络输出的策略中，增加噪声，可以缓解Q值高估问题</li>
</ul>
<hr>
<h3 id="TD3-BC（for-Offline-RL）"><a href="#TD3-BC（for-Offline-RL）" class="headerlink" title="TD3+BC（for Offline RL）"></a>TD3+BC（for Offline RL）</h3><ul>
<li>TD3+BC，在TD3的基础上，增加策略模仿，即对策略进行迭代时，损失函数中增加\(loss_{\text{BC}} = (\pi_{\theta}(s) - a)^2\)</li>
<li>DDPG是Online RL的算法，TD3+BC是Offline RL的算法</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/jiahong-head.png" alt="Joe Zhou">
            
              <p class="site-author-name" itemprop="name">Joe Zhou</p>
              <p class="site-description motion-element" itemprop="description">本博客主要用于记录个人学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">283</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JoeZJH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="JoeZJiahong@Foxmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joe Zhou</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
