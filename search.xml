<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LaTex——排版</title>
    <url>/Notes/LaTeX/LaTeX%E2%80%94%E2%80%94%E6%8E%92%E7%89%88.html</url>
    <content><![CDATA[<h3 id="vspace"><a href="#vspace" class="headerlink" title="vspace"></a>vspace</h3><ul>
<li>增加或减少行间距</li>
<li>例如，减少图片和正文的间距0.1cm采用下面的表达，单位可以是cm,pt,mm等<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;figure&#125;[h]</span><br><span class="line">  \centering</span><br><span class="line">  \vspace&#123;-0.1cm&#125;</span><br><span class="line">  \includegraphics[width=\linewidth]&#123;Q-Network-Page-2&#125;</span><br><span class="line">  \caption&#123;Q-Network of RL-MPCA&#125;</span><br><span class="line">  \Description&#123;&#125;</span><br><span class="line">  \label&#123;fig:Q-Network&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="列表缩进"><a href="#列表缩进" class="headerlink" title="列表缩进"></a>列表缩进</h3><ul>
<li>参考链接：<ul>
<li><a href="https://tex.stackexchange.com/questions/170525/itemize-left-margin" target="_blank" rel="noopener">https://tex.stackexchange.com/questions/170525/itemize-left-margin</a></li>
<li><a href="https://www.cxyzjd.com/article/robert_chen1988/83179571" target="_blank" rel="noopener">https://www.cxyzjd.com/article/robert_chen1988/83179571</a></li>
</ul>
</li>
<li>缩进设置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% \usepackage&#123;enumitem&#125;</span><br><span class="line">% \setlist&#123;leftmargin=5.5mm&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="footnote编号"><a href="#footnote编号" class="headerlink" title="footnote编号"></a>footnote编号</h3><ul>
<li><code>\footnote{}</code>默认会自动编号，整片文章按照顺序排列编号</li>
<li><code>\footnote[3]{}</code>则允许手动设置编号，可以按照自己的意愿随意设置编号</li>
</ul>
<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><ul>
<li>参考链接：</li>
<li>文档内部超链接<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;hyperref&#125;</span><br><span class="line">\hyperlink&#123;thesentence&#125;&#123;[1]&#125;</span><br><span class="line">\hypertarget&#123;thesentence&#125;&#123;[1]&#125;.</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTex——省略号</title>
    <url>/Notes/LaTeX/LaTeX%E2%80%94%E2%80%94%E7%9C%81%E7%95%A5%E5%8F%B7.html</url>
    <content><![CDATA[<h3 id="省略号在LaTeX中的使用"><a href="#省略号在LaTeX中的使用" class="headerlink" title="省略号在LaTeX中的使用"></a>省略号在LaTeX中的使用</h3><ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/104112163" target="_blank" rel="noopener">LaTeX入门（6）</a></li>
<li>常见的省略号有：“\ldots”，“\cdots”，“\vdots”，“\ddots”等</li>
<li>对于常规的“\ldots”（下），“\cdots”（中），一般来说，使用“\dots”更好些，能够自动适配选择位置</li>
<li>“\vdots”，“\ddots”则用在特殊情况下，比如矩阵等</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>趣味题——同距运动员</title>
    <url>/Notes/Others/%E8%B6%A3%E5%91%B3%E9%A2%98%E2%80%94%E2%80%94%E5%90%8C%E8%B7%9D%E8%BF%90%E5%8A%A8%E5%91%98.html</url>
    <content><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><ul>
<li>有27个参加跑步的人，每3人一组，分成9组，同一组用同一个号。就是1号3个，2号3个，3号3个……现在假设第一组的赢得了比赛，每次只有一个人到达。所有人到达的时候满足规律，1号参赛者之间都间隔一个人，2号参赛者之间都间隔2个人，3号参赛者之间都间隔3个人…9号参赛者之间都间隔9人。问27个人的到达顺序是否有解？如果有，解是什么？</li>
</ul>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def check_row(row, gap):</span><br><span class="line">    a = abs(row[0] - row[1]) == gap</span><br><span class="line">    b = abs(row[1] - row[2]) == gap</span><br><span class="line">    c = abs(row[2] - row[0]) == gap</span><br><span class="line">    abc = [a, b, c]</span><br><span class="line">    if sum([1 if e else 0 for e in abc]) != 2:</span><br><span class="line">        return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_conds(ri, rest_number):</span><br><span class="line">    size = len(rest_number)</span><br><span class="line">    gap = ri + 2</span><br><span class="line">    conds = list()</span><br><span class="line">    for i in range(size):</span><br><span class="line">        for j in range(i+1, size):</span><br><span class="line">            for k in range(j+1, size):</span><br><span class="line">                row = rest_number[i], rest_number[j], rest_number[k]</span><br><span class="line">                if check_row(row, gap):</span><br><span class="line">                    conds.append(row)</span><br><span class="line">    return conds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def backtrace(maze, ri, rest_number, all_maze):</span><br><span class="line">    if not rest_number:</span><br><span class="line">        data_ = [[e for e in row] for row in maze]</span><br><span class="line">        all_maze.append(data_)</span><br><span class="line">        return True</span><br><span class="line">    conds = generate_conds(ri, rest_number)</span><br><span class="line">    if not conds:</span><br><span class="line">        return False</span><br><span class="line">    for row in conds:</span><br><span class="line">        local_rest = [e for e in rest_number]</span><br><span class="line">        for e in row:</span><br><span class="line">            # print(&quot;local rest: %s and e: %s&quot; % (local_rest, e))</span><br><span class="line">            local_rest.remove(e)</span><br><span class="line">        maze[ri] = [e for e in row]</span><br><span class="line">        backtrace(maze, ri + 1, local_rest, all_maze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def solution():</span><br><span class="line">    maze = list()</span><br><span class="line">    for i in range(9):</span><br><span class="line">        row = [0 for _ in range(3)]</span><br><span class="line">        maze.append(row)</span><br><span class="line">    rest_number = list(range(1, 28))</span><br><span class="line">    maze[0][0] = 1</span><br><span class="line">    maze[0][1] = 3</span><br><span class="line">    maze[0][2] = 5</span><br><span class="line">    rest_number.remove(1)</span><br><span class="line">    rest_number.remove(3)</span><br><span class="line">    rest_number.remove(5)</span><br><span class="line">    all_maze = list()</span><br><span class="line">    # print(rest_number)</span><br><span class="line">    # print(maze)</span><br><span class="line">    backtrace(maze, 1, rest_number, all_maze)</span><br><span class="line">    for maze in all_maze:</span><br><span class="line">        print(maze)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    solution()</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——BCQ</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94BCQ.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/369524801" target="_blank" rel="noopener">【笔记】BCQ详解</a><ul>
<li>原作者的PPT</li>
</ul>
</li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/272152582" target="_blank" rel="noopener">BCQ姊妹篇：Discrete BCQ - Metaqiang的文章 - 知乎</a></li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/206489894" target="_blank" rel="noopener">【代码速读】（RL）1.BCQ - 一条的文章 - 知乎</a></li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop——hdfs+dfs和hadoop+fs的区别</title>
    <url>/Notes/Hadoop/Hadoop%E2%80%94%E2%80%94hdfs+dfs%E5%92%8Chadoop+fs%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p><em>在管理HDFS文件时，我们常用的命令有三个 <code>hadoop fs</code>，<code>hadoop dfs</code>和<code>hdfs dfs</code>【注意没有<code>hdfs fs</code>】</em><br><em>参考：<a href="https://blog.csdn.net/u013019431/article/details/78485555" target="_blank" rel="noopener">https://blog.csdn.net/u013019431/article/details/78485555</a></em></p>
<hr>
<h3 id="比较三种命令的区别"><a href="#比较三种命令的区别" class="headerlink" title="比较三种命令的区别"></a>比较三种命令的区别</h3><ul>
<li><p>hadoop fs:</p>
<ul>
<li>FS relates to a generic file system which can point to any file systems like local, HDFS etc. So this can be used when you are dealing with different file systems such as Local FS, HFTP FS, S3 FS, and others</li>
<li>意思是说该命令可以用于其他文件系统，不止是hdfs文件系统内，也就是说该命令的使用范围更广</li>
</ul>
</li>
<li><p>hadoop dfs</p>
<ul>
<li>专门针对hdfs分布式文件系统</li>
</ul>
</li>
<li><p>hdfs dfs</p>
<ul>
<li>和上面的命令作用相同，相比于上面的命令更为推荐，并且当使用hadoop dfs时内部会被转为hdfs dfs命令</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Java——Logger变量命名规则</title>
    <url>/Notes/Java/Java%E2%80%94%E2%80%94Logger%E5%8F%98%E9%87%8F%E5%91%BD%E5%90%8D%E8%A7%84%E5%88%99.html</url>
    <content><![CDATA[<hr>
<h3 id="一般Java常量命名规范"><a href="#一般Java常量命名规范" class="headerlink" title="一般Java常量命名规范"></a>一般Java常量命名规范</h3><ul>
<li>Java中的常量名称一般用全大写，比如美团，阿里等公司均有相关要求，详情参考<a href="/Backup/Backup/Java/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4Java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C.pdf">阿里巴巴Java开发手册.pdf</a></li>
</ul>
<hr>
<h3 id="一个特殊的例子——Logger"><a href="#一个特殊的例子——Logger" class="headerlink" title="一个特殊的例子——Logger"></a>一个特殊的例子——Logger</h3><h4 id="特殊写法"><a href="#特殊写法" class="headerlink" title="特殊写法"></a>特殊写法</h4><ul>
<li><p>Spring中Logger对象的名称使用的是小写</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static final Logger logger= LoggerFactory.getLogger(BeanFactory.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他很多公司或者开源工具的代码也跟着这样用，比如美团的RPC开源框架<code>mtthrift</code></p>
</li>
</ul>
<h4 id="Logger对象使用final的原因"><a href="#Logger对象使用final的原因" class="headerlink" title="Logger对象使用final的原因"></a>Logger对象使用final的原因</h4><ul>
<li>定义成static final,logger变量不可变，读取速度快</li>
<li>static 修饰的变量是不管创建了new了多少个实例，也只创建一次，节省空间，如果每次都创建Logger的话比较浪费内存；final修饰表示不可更改，常量</li>
<li>将域定义为static,每个类中只有一个这样的域。而每一个对象对于所有的实例域却都有自己的一份拷贝，用static修饰既节约空间，效率也好。final 是本 logger 不能再指向其他 Logger 对象</li>
</ul>
<h4 id="为什么不适用大写"><a href="#为什么不适用大写" class="headerlink" title="为什么不适用大写"></a>为什么不适用大写</h4><ul>
<li>Spring开发者有自己的编程规范<ul>
<li>常量引用不用大写？</li>
<li><code>private</code>修饰的常量不用大写？</li>
<li>Logger太特殊了，使用特殊定义，仅此一个，别无其他</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——强化学习与动态规划</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.html</url>
    <content><![CDATA[<ul>
<li>参考文献<ul>
<li><a href="https://blog.csdn.net/weixin_44389191/article/details/117261395?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_aa&utm_relevant_index=2" target="_blank" rel="noopener">【强化学习】个人总结03——动态规划寻找最优策略</a></li>
<li><a href="https://zhiqianghe.blog.csdn.net/article/details/104401240?spm=1001.2101.3001.6650.10&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-10.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-10.pc_relevant_default&utm_relevant_index=16" target="_blank" rel="noopener">手把手教你强化学习 (四)动态规划与策略迭代、值迭代</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/361955692" target="_blank" rel="noopener">第四章 动态规划（DP）、蒙特卡罗（MC）、时间差分（TD） - 臭皮匠的文章 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/333661254" target="_blank" rel="noopener">David Silver强化学习之动态规划寻找最优策略理论与实战(三) - CristianoC的文章 - 知乎</a></li>
</ul>
</li>
</ul>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>动态规划 (Dynamic Programming, DP) 就是先把复杂问题分解为若干的子问题，再通过求解这些子问题来得到原问题的解。这适合解决满足如下两个性质的问题：<br>最优子结构 (optimal substructure)：一个原问题可以拆分成一个个的小问题，解决这些小问题后能够通过组合小问题的解来得到原问题的最优解。<br>重叠子问题 (overlapping subproblems)：子问题出现多次，并且子问题的解能被存储起来重复使用。</p>
<p>马尔科夫决策过程正好满足动态规划的这两个要求：贝尔曼方程把问题分解成一个递归的结构来求解子问题，价值函数可以存储并复用它的最佳解。因此我们就可以使用动态规划的方法来求解马尔科夫决策过程的核心问题：预测和控制。</p>
<p>预测 (prediction)：已知一个马尔科夫决策过程 MDP 和一个策略 π，或者是给定一个马尔科夫奖励过程 MRP，求解基于该策略的价值函数 vπ。（评估一个给定的策略）<br>控制 (control)：已知一个马尔科夫决策过程 MDP，求解最优价值函数 v∗ 和最优策略 π∗。（搜索最佳策略）<br>这两种问题的区别在于，预测问题是策略已给，我们需要确定它的价值函数是多少，而控制问题是要在没有策略的前提下确定最优的价值函数以及对应的策略。两者之间存在递进关系，在强化学习中，我们通过解决预测问题，进而解决控制问题。</p>
<h2 id="1-同步动态规划-Synchronous-Dynamic-Programming"><a href="#1-同步动态规划-Synchronous-Dynamic-Programming" class="headerlink" title="1. 同步动态规划 (Synchronous Dynamic Programming)"></a>1. 同步动态规划 (Synchronous Dynamic Programming)</h2><p>同步动态规划算法中，每一次迭代都更新所有状态的价值</p>
<ul>
<li>参考链接：<a href="https://blog.csdn.net/panglinzhuo/article/details/77752574" target="_blank" rel="noopener">策略迭代与值迭代的区别</a><h3 id="1-1-策略评估-policy-evaluation"><a href="#1-1-策略评估-policy-evaluation" class="headerlink" title="1.1 策略评估 (policy evaluation)"></a>1.1 策略评估 (policy evaluation)</h3><h3 id="1-2-策略迭代-policy-iteration"><a href="#1-2-策略迭代-policy-iteration" class="headerlink" title="1.2 策略迭代 (policy iteration)"></a>1.2 策略迭代 (policy iteration)</h3><h3 id="1-3-价值迭代-value-iteration"><a href="#1-3-价值迭代-value-iteration" class="headerlink" title="1.3 价值迭代 (value iteration)"></a>1.3 价值迭代 (value iteration)</h3></li>
</ul>
<h2 id="2-异步动态规划-Asynchronous-Dynamic-Programming"><a href="#2-异步动态规划-Asynchronous-Dynamic-Programming" class="headerlink" title="2. 异步动态规划 (Asynchronous Dynamic Programming)"></a>2. 异步动态规划 (Asynchronous Dynamic Programming)</h2><p>在异步动态规划算法中，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性地更新部分状态的价值，这种算法能显著节约计算资源，并且只要所有状态能够得到持续的访问更新，那么也能确保算法收敛至最优解。<br>下面分别介绍比较常用的异步动态规划思想：<br>原位动态规划 (in-place dynamic programming)：直接利用当前状态的后续状态的价值来更新当前状态的价值。<br>优先级动态规划 (prioritised sweeping)：对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。<br>实时动态规划 (real-time dynamic programming)：直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。</p>
<h2 id="3-动态规划"><a href="#3-动态规划" class="headerlink" title="3. 动态规划"></a>3. 动态规划</h2><p>动态规划算法使用全宽度(full-width)的回溯机制来进行状态价值的更新,也就是说,无论是同步还是异步动态规划,在每一次回溯更新某一个状态的价值时,都要追溯到该状态的所有可能的后续状态,并结合已知的马尔科夫决策过程定义的状态转换矩阵和奖励来更新该状态的价值.这种全宽度的价值更新方式对于状态数在百万级别及以下的中等规模的马尔科夫决策问题还是比较有效的,但是当问题规模继续变大时,动态规划算法将会因贝尔曼维度灾难而无法使用,每一次的状态回溯更新都要消耗非常昂贵的计算资源.为此需要寻找其他有效的算法,这就是后文将要介绍的采样回溯.这类算法的一大特点是不需要知道马尔科夫决策过程的定义,也就是不需要了解状态转移概率矩阵以及奖励函数,而是使用采样产生的奖励和状态转移概率.这类算法通过采样避免了维度灾难,其回溯的计算时间消耗是常数级的.由于这类算法具有非常可观的优势,在解决大规模实际问题时得到了广泛的应用.</p>
<ul>
<li>动态规划算法是model-based算法，因为需要回溯当前状态的所有后续状态（full-width回溯机制）</li>
<li>model-free算法则无法使用动态规划算法（只能使用类似采样回溯等方法）<ul>
<li>蒙特卡罗法就是采样回溯法</li>
<li>model-free方法一般迭代公式中都包含着学习率超参数（有时候会1/N，用于模拟期望），而model-based算法可以直接获取到真实的概率转移情况，无需学习率超参数</li>
</ul>
</li>
<li>TD（temporal difference）则是结合了蒙特卡罗法和动态规划法的方法<ul>
<li>类似于蒙特克罗法，需要模拟交互序列<ul>
<li>不同于蒙特卡罗法，不需要积累很多交互数据求均值</li>
</ul>
</li>
<li>类似于动态规划算法，求解公式里面使用当前回报和下一时刻的价值预估来更新当前时刻的价值<ul>
<li>不同于动态规划算法，不需要知道状态转移矩阵，也不需要full-width回溯</li>
</ul>
</li>
</ul>
</li>
<li>Q-learning的学习率为1，且model-based中状态转移矩阵取值为{0,1}时，Q-learning的迭代公式与价值迭代的迭代公式完全一致<ul>
<li>直观理解（不一定严谨）：由于Q-learning中，状态转移矩阵未知（下个状态的期望收益不可信），也就不一定是full-width回溯（一次更新所有状态）的，学习时需要增加学习率参数，否则很容易走偏；而model-based的价值迭代（动态规划方案）中，可以知道状态转移矩阵，求得的收益期望是可信的，而且是full-width回溯的，不需要增加学习率超参数（或者学习率为1）<ul>
<li>这里full-width的作用是什么呢？如果价值迭代中不是full-width的是否可以收敛呢？理论上是可以的吧【待确定】</li>
<li>无法使用full-width的原因也就是状态转移矩阵未知，但是如果已知状态转移矩阵取值为{0,1}，那么理论上每次执行时都可以认为状态转移矩阵是已知的吧，因为环境返回的数据包含了下一个状态信息，而状态转移矩阵只能取P(S’|S)=1，此时Q-learning更像是一个异步动态规划算法（real-time 动态规划算法）</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——多用户问题</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E7%BB%88%E7%AB%AF%E7%99%BB%E5%BD%95%E5%85%B6%E4%BB%96%E7%94%A8%E6%88%B7%E6%89%93%E5%BC%80%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%AA%97%E5%8F%A3.html</url>
    <content><![CDATA[<p><em>在Linux系统有多个用户时，我们可能需要从一个用户界面打开终端登录另一个用户，从而使用该用户的环境和软件</em></p>
<hr>
<h3 id="多用户打开各自软件问题"><a href="#多用户打开各自软件问题" class="headerlink" title="多用户打开各自软件问题"></a>多用户打开各自软件问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><ul>
<li>在一个用户登录图形界面后，需要以另一个用户的身份打开一个图形化软件，此时直接打开图形化软件可能遇到如下错误</li>
</ul>
<blockquote>
<p>No protocol specified</p>
</blockquote>
<h4 id="问题发生原因"><a href="#问题发生原因" class="headerlink" title="问题发生原因"></a>问题发生原因</h4><ul>
<li>因为Xserver默认情况下禁止别的用户图形程序运行在当前用户图形界面上</li>
</ul>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li>在当前用户下执行命令<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xhost +</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——A3C</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94A3C.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<ul>
<li><a href="https://www.cnblogs.com/pinard/p/10334127.html" target="_blank" rel="noopener">强化学习(十五) A3C</a>：其中的流程值的参考</li>
<li><a href="https://zhuanlan.zhihu.com/p/110998399" target="_blank" rel="noopener">理解Actor-Critic的关键是什么？(附代码及代码分析)</a></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——PPO&amp;TD3</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94PPO-TD3.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/384497349" target="_blank" rel="noopener">强化学习之图解PPO算法和TD3算法</a><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


</li>
</ul>
</li>
</ul>
<h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><h4 id="PPO的训练技巧"><a href="#PPO的训练技巧" class="headerlink" title="PPO的训练技巧"></a>PPO的训练技巧</h4><ul>
<li>参考：<a href="https://zhuanlan.zhihu.com/p/512327050" target="_blank" rel="noopener">影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）</a></li>
</ul>
<h4 id="为什么说PPO算法是on-policy的？"><a href="#为什么说PPO算法是on-policy的？" class="headerlink" title="为什么说PPO算法是on-policy的？"></a>为什么说PPO算法是on-policy的？</h4><ul>
<li><p>首先引入一个其他博主的理解：</p>
<blockquote>
<p>PPO：依赖于importance sampling实现的off-policy算法在面对太大的策略差异时将无能为力（正在训练的policy与实际与环境交互时的policy差异过大），所以学者们认为PPO其实是一种on-policy的算法，这类算法在训练时需要保证生成训练数据的policy与当前训练的policy一致，对于过往policy生成的数据难以再利用，所以在sample efficiency这条衡量强化学习（Reinforcement Learning, RL）算法的重要标准上难以取得优秀的表现。</p>
</blockquote>
</li>
<li><p>在推导TRPO和PPO的过程中</p>
<ul>
<li>在将新策略\(\pi\)上的状态访问频率\(\rho_{\pi}(\bf{s})\)替换成旧策略的状态访问频率\(\rho_{\pi_{old}}(\bf{s})\)时，要求\(\pi\)与\(pi_{old}\)相聚不能太远，这就要求采样的样本不能是太早的策略，详情见《强化学习精要》P247（注意：此处\(\pi\)表达与书中相反）</li>
<li>在将新策略\(\pi\)上的动作采样替换为就策略\(\pi_{old}\)上的动作采样时，需要进行Importance Sampling，这要求了采样到的数据应该都是来源于同一个旧策略\(\pi_{old}\)<ul>
<li><strong>来源于同一个旧策略</strong>说明：最好是更新一次参数清空一次Buffer，根据本人对一些PPO实现的观察，实际实现时做不到这样，一般一个episode更新一次Buffer，而当Batch Size小于episode的步数时，在一次episode中可能会进行多次更新，一种理解是，同一个episode中的多次更新策略不会变化太大，实际上分布也比较接近，可以看做是同一个？</li>
</ul>
</li>
</ul>
</li>
<li><p>TRPO和PPO均是从较新的策略中采样样本，然后通过Importance Sampling将数据分布误差进行修正，从而对当前策略进行更新，本质上可以看做是</p>
</li>
<li><p>PPO策略原本是需要当前策略采样的样本的，但是使用了Importance Sampling来减少on-policy方法的采样要求，但是PPO实际上还是需要当前策略产生的数据才能进行有效学习，为此，我们一般会使用一个Clip方法来限制PPO当前策略和旧策略的偏差，以保证数据的有效性</p>
</li>
<li><p>一些其他off-policy的方法也会使用Importance Sampling，但这些策略往往是从固定策略\(\mu\)采样的</p>
<ul>
<li>这些方法的损失函数中会将样本权重按照\(\frac{\pi}{\mu}\)来进行修正动作的概率分布</li>
<li>这些off-policy方法与PPO方法最大的不同在于这些方法不需要限制当前策略与行为策略的距离（KL散度）<ul>
<li>问题：为什么这些off-policy方法不需要保证行为策略下的状态访问频率\(\rho_{\mu}(\bf{s})\)和目标策略下的状态访问频率\(\rho_{\pi}(\bf{s})\)一致？</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h3><ul>
<li>TD3是对DDPG的改进，全称为Twin Delayed Deep Deterministic Policy Gradient Algorithm</li>
<li>有两个改进包含在名字中，Twin和Delayed</li>
<li>其他改进是在Actor 的target网络输出中，增加噪声</li>
</ul>
<h4 id="改进1：Twin"><a href="#改进1：Twin" class="headerlink" title="改进1：Twin"></a>改进1：Twin</h4><ul>
<li>采用双Critic网络（训练网络和target网络均为双网络），缓解Q值高估问题</li>
</ul>
<h4 id="改进2：Delayed"><a href="#改进2：Delayed" class="headerlink" title="改进2：Delayed"></a>改进2：Delayed</h4><ul>
<li>Actor的目标是在Q值更新时，寻找最优的策略，如果Q值更新太快，容易波动，可以让Q值比较稳定了再更新Actor网络</li>
<li>具体做法，Critic网络更新\(d\)次再更新一次Actor</li>
</ul>
<h4 id="改进3：增加噪声"><a href="#改进3：增加噪声" class="headerlink" title="改进3：增加噪声"></a>改进3：增加噪声</h4><ul>
<li>在Actor 的target网络输出中，增加噪声，可以缓解Q值高估问题</li>
</ul>
<h4 id="其他扩展"><a href="#其他扩展" class="headerlink" title="其他扩展"></a>其他扩展</h4><ul>
<li>TD3+BC，在TD3的基础上，增加策略模仿，即对策略进行迭代时，损失函数中增加\(loss_{BC} = (\pi_{\theta}(s) - a)^2\)</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——GAE</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94GAE.html</url>
    <content><![CDATA[<p><em>GAE，Generalized advantage estimation，平衡了强化学习中的方差与偏差，常用于AC中</em></p>
<ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/549145459" target="_blank" rel="noopener">六、GAE 广义优势估计 - 万理的探求者的文章 - 知乎</a><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


</li>
</ul>
</li>
</ul>
<h3 id="强化学习中的方差与偏差"><a href="#强化学习中的方差与偏差" class="headerlink" title="强化学习中的方差与偏差"></a>强化学习中的方差与偏差</h3><ul>
<li>参考链接：<a href="/Notes/RL/RL%E2%80%94%E2%80%94%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html">方差与偏差</a></li>
</ul>
<h3 id="GAE、-lambda-return、TD-lambda-的区别是什么？"><a href="#GAE、-lambda-return、TD-lambda-的区别是什么？" class="headerlink" title="GAE、\(\lambda\)-return、TD(\(\lambda\))的区别是什么？"></a>GAE、\(\lambda\)-return、TD(\(\lambda\))的区别是什么？</h3><ul>
<li><p>参考链接：</p>
<ul>
<li><p><a href="https://www.zhihu.com/question/476110046/answer/2029179166" target="_blank" rel="noopener">强化学习中，GAE和TD(lambda)的区别是什么？ - 动词大词动的回答 - 知乎</a></p>
<blockquote>
<p>TD lambda: 用 n-step future reward估计value<br>  GAE: 用n-step TD residual of value估计advantage</p>
</blockquote>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/260778552" target="_blank" rel="noopener">lambda return 和 TD(lambda)的关系 - 啊啊啊的文章 - 知乎</a></p>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——TD误差和优势函数的区别</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94TD%E8%AF%AF%E5%B7%AE%E5%92%8C%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/264806566" target="_blank" rel="noopener">TD误差 vs 优势函数 vs贝尔曼误差</a></li>
</ul>
</li>
</ul>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<h3 id="TD误差"><a href="#TD误差" class="headerlink" title="TD误差"></a>TD误差</h3><ul>
<li>时间差分误差，TD error</li>
<li>定义如下：<br>$$<br>\delta_{\theta}(s, a, s’) = R(s, a, s’) + \gamma v_{\theta}(s’) - v_{\theta}(s)<br>$$</li>
<li>\(R(s,a,s’)=r(s,a,s’)\)，表示从状态\(s\)执行\(a\)之后转移到\(s’\)获得的立即回报</li>
<li>TD error是针对确定的\(s’\)来说的</li>
</ul>
<h3 id="优势函数"><a href="#优势函数" class="headerlink" title="优势函数"></a>优势函数</h3><ul>
<li>优势函数，Advantage Function<br>$$<br>A_{\theta}(s,a) = E_{s’\sim P}[\delta_{\theta}(s, a, s’)] = E_{s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)] - v_{\theta}(s) = Q_{\theta}(s,a) - v_{\theta}(s)<br>$$</li>
<li>优势函数是TD误差关于状态\(s’\)的期望，即从状态\(s\)执行\(a\)之后关于状态\(s’\)的期望</li>
</ul>
<h3 id="贝尔曼误差"><a href="#贝尔曼误差" class="headerlink" title="贝尔曼误差"></a>贝尔曼误差</h3><ul>
<li>贝尔曼误差<br>$$<br>\epsilon_{\theta}(s) = E_{a\sim \pi} [A_{\theta}(s,a)] = E_{a\sim \pi,s’\sim P}[\delta_{\theta}(s, a, s’)] = E_{a \sim \pi, s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)] - v_{\theta}(s)<br>$$</li>
<li>贝尔曼误差是优势函数关于动作\(a\)的期望</li>
</ul>
<h3 id="期望贝尔曼误差"><a href="#期望贝尔曼误差" class="headerlink" title="期望贝尔曼误差"></a>期望贝尔曼误差</h3><ul>
<li>期望贝尔曼误差<br>$$<br>E_{s\sim \mu} [\epsilon_{\theta}(s)] = E_{s\sim \mu}[E_{a \sim \pi, s’\sim P}[R(s, a, s’) + \gamma v_{\theta}(s’)]] -  E_{s\sim \mu}[ v_{\theta}(s)]<br>$$</li>
<li>贝尔曼误差是优势函数关于动作\(a\)的期望</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——SAC</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94SAC.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/385658411" target="_blank" rel="noopener">强化学习之图解SAC算法</a></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——策略梯度法</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95.html</url>
    <content><![CDATA[<p><em>策略梯度法(Policy Gradient)推导，以及REINFORCE算法的介绍</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul>
<li><strong>策略</strong> \(\pi(a|s, \theta)\) (也可以表达为 \(\pi_{ \theta}(a|s)\))是一个从状态 \(s\) 到动作 \(a\) 概率的映射，其中 \(\theta\) 表示策略的参数。</li>
<li><strong>整个轨迹的累计回报</strong> \(R_(\tau)\)是轨迹\(\tau\)对应的回报：<br>$$<br>R(\tau) = \sum_{k=0}^{\infty} r_{k}<br>$$<ul>
<li>注意：这里没有折扣因子</li>
</ul>
</li>
<li><strong>时间t步开始的回报</strong> \(G_t\) 是从时间步 \(t\) 开始到结束的所有奖励的总和，通常定义为折扣累积奖励：<br>$$<br>G_t = \sum_{k=t}^{\infty} \gamma^k r_{k}<br>$$<br>其中 \(\gamma\) 是折扣因子，\(r_{k}\) 是在时间步 \(k\) 收到的即时奖励。</li>
<li><strong>目标</strong> 是找到参数 \(\theta\) 使得长期回报的期望值最大，即 \(\max_\theta J(\theta)\)，其中 \(J(\theta) = E[G_t]\)。</li>
</ul>
<h3 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h3><h4 id="优化目标："><a href="#优化目标：" class="headerlink" title="优化目标："></a>优化目标：</h4><ul>
<li><strong>目标函数</strong> \(J(\theta)\) 定义为从初始分布开始，遵循策略 \(\pi_\theta\) 时的平均回报：<br> $$<br> J(\theta) = E_{\tau \sim p_\theta(\tau)} [R(\tau)]<br> $$</li>
<li>其中 \(\tau = (s_0, a_0, r_1, s_1, a_1, \dots)\) 表示一个轨迹，\(p_\theta(\tau)\) 是在策略 \(\pi_\theta\) 下产生轨迹 \(\tau\) 的概率。</li>
</ul>
<h4 id="梯度估计："><a href="#梯度估计：" class="headerlink" title="梯度估计："></a>梯度估计：</h4><ul>
<li><p>我们的目标是计算目标函数关于参数 \(\theta\) 的梯度 \(\nabla_\theta J(\theta)\)。我们有：<br> $$<br> \nabla_\theta J(\theta) = \nabla_\theta \int R(\tau) p_\theta(\tau) d\tau = \int R(\tau) \nabla_\theta p_\theta(\tau) d\tau<br> $$</p>
</li>
<li><p>使用对数概率技巧（log derivative trick，\(\nabla_\theta log y({\theta}) = \frac{\nabla_\theta  y({\theta})}{ y({\theta}) }\)）可以将上式转换为：<br> $$<br> \nabla_\theta J(\theta) = \int R(\tau) p_\theta(\tau) \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)} d\tau = E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]<br> $$</p>
</li>
<li><p>如果通过蒙特卡洛采样估计上面的式子，则可以写成：<br>  $$<br>  \begin{align}<br>  \nabla_\theta J(\theta) &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)] \\<br>  &amp;\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla_\theta \log p_\theta(\tau^n)<br>  \end{align}<br>  $$</p>
<ul>
<li>上式是对原始梯度的无偏估计<h4 id="轨迹展开："><a href="#轨迹展开：" class="headerlink" title="轨迹展开："></a>轨迹展开：</h4></li>
</ul>
</li>
<li><p>轨迹展开后，有 \(p_\theta(\tau) = p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)\)，其中 \(p(s_0)\) 是初始状态的分布，\(p(s_{t+1}|s_t, a_t)\) 是环境的转移概率。<br>  $$<br>  \begin{align}<br>  p_\theta(\tau) &amp;= p_{\pi_\theta}(s_0, a_0, s_1, a_1,\cdots) \\<br>  &amp;= p(s_0)\pi_\theta(a_0|s_0)p(s_1|s_0,a_0)\cdots \\<br>  &amp;= p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)<br>  \end{align}<br>  $$</p>
</li>
<li><p>由于环境的输出与策略无关，即\(\nabla_\theta p(s_1|s_0,a_0) = 0\)，于是有：<br>  $$<br>  \begin{align}<br>  \nabla_\theta \log p_\theta(\tau) &amp;= \nabla_\theta p(s_0) \prod_t \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t) \\<br>  &amp;= \nabla_\theta \log p(s_0) + \nabla_\theta \sum_t \log \pi_\theta(a_t|s_t) + \nabla_\theta  \sum_t  \log p(s_{t+1}|s_t, a_t) \\<br>  &amp;= \nabla_\theta \sum_t \log \pi_\theta(a_t|s_t) \\<br>  &amp;=  \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t)<br>  \end{align}<br>  $$</p>
</li>
<li><p>所以我们可以进一步简化梯度表达式为：<br> $$<br>  \begin{align}<br> \nabla_\theta J(\theta) &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)] = E_{\tau \sim p_\theta(\tau)} \left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau^n)  \right] \\<br> &amp;\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla_\theta \log p_\theta(\tau^n) = \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \nabla_\theta \log \pi_\theta(a_t|s_t)<br>  \end{align}<br> $$</p>
<ul>
<li>此时，上式依然是对原始梯度的无偏估计</li>
</ul>
</li>
<li><p>核心理解：</p>
<ul>
<li>\(R(\tau^n)\)表示采样得到的轨迹\(\tau^n\)对应的Reward，上述公式假设一共有N个轨迹</li>
<li>对于任意给定的轨迹\(\tau^n\)，其上面的任意样本对\((s_t,a_t)\)，均使用固定的\(R(\tau^n)\)对 \(\nabla_\theta \log \pi_\theta(a_t|s_t)\)进行加权（实际上在使用中，不会直接使用\(R(\tau^n)\)，因为轨迹中过去的Reward与当前动作无关，所以，我们仅考虑后续的轨迹上的收益即可）</li>
</ul>
</li>
</ul>
<h4 id="REINFORCE算法："><a href="#REINFORCE算法：" class="headerlink" title="REINFORCE算法："></a>REINFORCE算法：</h4><ul>
<li>考虑到轨迹中过去的Reward与当前动作无关，且后续轨迹上的收益与当前动作的关系越来越小，所以我们使用\(G_t\)来替换\(R(\tau)\)<br>  $$<br>  R(\tau) = \sum_{k=0}^{\infty} r_{k} \quad \rightarrow \quad G_t = \sum_{k=t}^{\infty} \gamma^k r_{k}<br>  $$</li>
<li>此时梯度进一步近似为：<br> $$<br>  \begin{align}<br> \nabla_\theta J(\theta) &amp;\approx \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n) \nabla_\theta \log \pi_\theta(a_t|s_t) \\<br> &amp;\approx \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} G_t^n \nabla_\theta \log \pi_\theta(a_t|s_t)<br>  \end{align}<br> $$</li>
<li>REINFORCE算法利用上述梯度估计来更新策略参数。具体地，对于轨迹\(R(\tau^n)\)上的状态动作样本对\((s_t,a_t)\)，参数更新规则如下：<br> $$<br> \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) G_t^n<br> $$<ul>
<li>其中 \(\alpha\) 是学习率</li>
<li>因为是累加操作，所以可以展开对每一个状态动作样本对\((s_t,a_t)\)进行累加</li>
<li>\(\frac{1}{N}\)可以不需要了，有了学习率了，可以调节到学习率中</li>
</ul>
</li>
<li>补充REINFORCE算法伪代码：<img src="/Notes/RL/RL——策略梯度法/PG-1.png">
<img src="/Notes/RL/RL——策略梯度法/PG-2.png">

</li>
</ul>
<h4 id="减小方差："><a href="#减小方差：" class="headerlink" title="减小方差："></a>减小方差：</h4><ul>
<li><p>为了减少方差，可以在梯度估计中引入一个baseline \(b(s_t)\)，它是一个与动作无关的量。更新规则变为：<br> $$<br> \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - b(s_t))<br> $$</p>
<ul>
<li>常见的选择是使用价值函数 \(V(s_t)\) 作为基线，这有助于稳定学习过程。</li>
<li>可以证明，增加 \(b(s_t)\) 后，梯度不会发生改变，上式对梯度的估计依然是无偏的</li>
</ul>
</li>
<li><p>性质一：<strong>减去一个baseline以后，依然是原始梯度的无偏估计</strong>，证明如下：<br>  $$<br>  \begin{align}<br>  \nabla_\theta J(\theta)<br>  &amp;= E_{\tau \sim p_{\theta}(\tau)} (R(\tau) - b) \nabla_\theta \log p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b E_{\tau \sim p_{\theta}(\tau)} \nabla_\theta \log p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \sum_\tau p_{\theta}(\tau) \nabla_\theta \log p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \sum_\tau \nabla_\theta p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \nabla_\theta \sum_\tau  p_{\theta}(\tau) \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  - b \nabla_\theta 1 \\<br>  &amp;= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla_\theta \log p_\theta(\tau)]  \\<br>  \end{align}<br>  $$</p>
<ul>
<li>第三行到第四行用到了对数概率技巧：\(\nabla_\theta log y({\theta}) = \frac{\nabla_\theta  y({\theta})}{ y({\theta}) }\)</li>
<li>第四行到第五行使用了求梯度和加法交换顺序的法则</li>
</ul>
</li>
<li><p>性质二：<strong>减去一个合适的baseline以后，方差会变小</strong>，证明如下：</p>
<ul>
<li>方差展开<br>  $$<br>  \begin{align}<br>  &amp;\ Var_{\tau \sim p_{\theta}(\tau)} [(R(\tau) - b) \nabla \log p_{\theta}(\tau)] \\<br>  &amp;= E_{\tau \sim p_{\theta}(\tau)} [(R(\tau) - b)^2 \nabla^2 \log p_{\theta}(\tau)] - [E_{\tau \sim p_{\theta}(\tau)} [(R(\tau) - b) \nabla \log p_{\theta}(\tau)] ]^2 \\<br>  &amp;= E_{\tau \sim p_{\theta}(\tau)} [R(\tau)^2 \nabla^2 \log p_{\theta}(\tau)] - [E_{\tau \sim p_{\theta}(\tau)} [R(\tau)  \nabla \log p_{\theta}(\tau)] ]^2 - 2 b E_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla^2 \log p_{\theta}(\tau)] + b^2 E_{\tau \sim p_{\theta}(\tau)} [ \nabla^2 \log p_{\theta}(\tau)] \\<br>  &amp;= Var_{\tau \sim p_{\theta}(\tau)} [R(\tau)  \nabla \log p_{\theta}(\tau) ] - 2 b E_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla^2 \log p_{\theta}(\tau)] + b^2 E_{\tau \sim p_{\theta}(\tau)} [ \nabla^2 \log p_{\theta}(\tau)]<br>  \end{align}<br>  $$</li>
<li>进一步解的，使得上式取小值的最优\(b\)为：<br>  $$<br>  b = \frac{E_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla^2 \log p_{\theta}(\tau)] }{E_{\tau \sim p_{\theta}(\tau)} [ \nabla^2 \log p_{\theta}(\tau)]}<br>  $$</li>
<li>实际应用中，为了方便计算，通常会使用：<br>  $$<br>  \hat{b} = E_{\tau \sim p_{\theta}(\tau)} R(\tau)<br>  $$ </li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——多阶段决策-贯序决策-马尔科夫决策</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E5%A4%9A%E9%98%B6%E6%AE%B5%E5%86%B3%E7%AD%96-%E8%B4%AF%E5%BA%8F%E5%86%B3%E7%AD%96-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96.html</url>
    <content><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul>
<li>区分多阶段决策-贯序决策-马尔科夫决策之间的区别和联系</li>
</ul>
<h3 id="决策过程分析"><a href="#决策过程分析" class="headerlink" title="决策过程分析"></a>决策过程分析</h3><ul>
<li>百度百科<blockquote>
<p>马尔可夫决策过程（Markov Decision Process, MDP）是序贯决策（sequential decision）的数学模型，用于在系统状态具有马尔可夫性质的环境中模拟智能体可实现的随机性策略与回报<br>多阶段决策是指决策者在整个决策过程中做出时间上先后有别的多项决策。它通常比只需做出一项决策的单阶段决策要复杂，它或是要决策者一次确定各阶段应选择的一串最优策略，或是找出表示一个过程内连续变化的一条控制变量曲线，或是确定适合不同状态的灵活策略。<br>序贯决策是指按时间顺序排列起来，以得到按顺序的各种决策(策略)，是用于随机性或不确定性动态系统最优化的决策方法。</p>
</blockquote>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——强化学习中的方差与偏差</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html</url>
    <content><![CDATA[<hr>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h3 id="MC和TD的方差分析"><a href="#MC和TD的方差分析" class="headerlink" title="MC和TD的方差分析"></a>MC和TD的方差分析</h3><ul>
<li>MC<ul>
<li>方差大，偏差小</li>
</ul>
</li>
<li>TD<ul>
<li>偏差大，方差小</li>
</ul>
</li>
<li>理解：<ul>
<li>与模型训练类似，方差与偏差是指同一个模型(bagging中所有模型共同组成一个模型)的输出是随着<strong>数据集/时间</strong>变化的</li>
<li>方差大表达的是多次评估结果之间差别大</li>
<li>偏差大则表示多次评估结果的均值与真实值差别大</li>
<li>MC采样每次都需要重新采样不同路径集合，在不同路径集合下，相同状态价值的评估结果差别大，但是结果的期望是符合真实情况的（甚至是无偏的）</li>
<li>TD方案则每次都使用下一个状态的相关估值，方差不会太大，但是下个状态的估值不一定符合真实值，所以偏差较大</li>
</ul>
</li>
<li>参考链接：<ul>
<li><a href="https://www.zhihu.com/question/345835710/answer/845950090" target="_blank" rel="noopener">强化学习，方差比较大是说什么的方差大，为啥方差比较大？ - Discover的回答 - 知乎</a> </li>
<li><a href="https://www.zhihu.com/question/345835710/answer/1265400871" target="_blank" rel="noopener">强化学习，方差比较大是说什么的方差大，为啥方差比较大？ - 无非尔耳的回答 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25913410" target="_blank" rel="noopener">强化学习入门 第四讲 时间差分法（TD方法）-知乎-天津包子馅儿</a></li>
</ul>
</li>
</ul>
<h3 id="Multi-Step-TD"><a href="#Multi-Step-TD" class="headerlink" title="Multi-Step TD"></a>Multi-Step TD</h3><ul>
<li>Multi-Step TD，也称为n-step TD</li>
<li>将TD中的即时奖励替换成采用未来多个时间片的奖励和，同时\(\gamma V(s_{t+1})\)值替换成\(\gamma^n V(s_{t+n})\)</li>
<li>详情如下：<img src="/Notes/RL/RL——强化学习中的方差与偏差/n-step-TD.png">


</li>
</ul>
<h3 id="lambda-return"><a href="#lambda-return" class="headerlink" title="\(\lambda\)-return"></a>\(\lambda\)-return</h3><ul>
<li>\(\lambda\)-return是在Forward视角下，对n-step TD的各个值进行加权求和<br>$$<br>G^{\lambda}_t = (1-\lambda) \sum^{N-1}_1 G_t^{(n)} + \lambda^{N-1} G_t^{(N)}<br>$$</li>
<li>其中 \(0\le \lambda \le 1\)，当 \(\lambda=0\)且\(N=1\) 即为TD算法，当\(\lambda=1\)即为MC算法。</li>
<li>注意：\((1-\lambda)\)是为了配平整个式子，保证加权平均的权重和为1</li>
</ul>
<h3 id="TD-lambda"><a href="#TD-lambda" class="headerlink" title="TD(\(\lambda\))"></a>TD(\(\lambda\))</h3><ul>
<li>TD(\(\lambda\))方法是在Backward视角下，对n-step TD的各个值进行加权求和<ul>
<li>TD(\(\lambda\))可以解决\(\lambda\)-return需要等到episode结束才能获得状态估计量的缺点？</li>
</ul>
</li>
<li>资格迹【TODO】：<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/54936262" target="_blank" rel="noopener">强化学习导论（十二）- 资格迹-知乎-张万鹏</a></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——模仿学习</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<a href="https://blog.csdn.net/caozixuan98724/article/details/103765605?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2" target="_blank" rel="noopener">模仿学习(Imitation Learning)概述</a></li>
</ul>
<hr>
<h3 id><a href="#" class="headerlink" title></a></h3>]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——模仿学习</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB.html</url>
    <content><![CDATA[<hr>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/104224859" target="_blank" rel="noopener">强化学习路线图-知乎-岳路飞</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/46600521" target="_blank" rel="noopener">强化学习论文汇总-知乎-张楚珩</a> </li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——Gym安装</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94Gym%E5%AE%89%E8%A3%85.html</url>
    <content><![CDATA[<ul>
<li>安装是使用<code>pip install gym</code></li>
<li>如果安装后使用<code>env.render()</code>无法显示可视化图像，可考虑安装<code>0.21.0</code>版本<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install gym==0.21.0</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——POMDP</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94POMDP.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<a href="https://www.zhihu.com/zvideo/1326278888684187648?utm_source=wechat_session&utm_medium=social&utm_oi=675997552948678656" target="_blank" rel="noopener">POMDP讲解</a></li>
</ul>
<h3 id="MDP"><a href="#MDP" class="headerlink" title="MDP"></a>MDP</h3><ul>
<li><img src="/Notes/RL/RL——POMDP/MDP.png">
<ul>
<li>对于确定决策，策略的概率值为1即可<h3 id="POMDP"><a href="#POMDP" class="headerlink" title="POMDP"></a>POMDP</h3></li>
</ul>
</li>
<li><img src="/Notes/RL/RL——POMDP/POMDP.png"></li>
<li></li>
<li>
<h3 id="求解方案"><a href="#求解方案" class="headerlink" title="求解方案"></a>求解方案</h3></li>
<li><img src="/Notes/RL/RL——POMDP/MDP_Policy_Function.png"></li>
<li><img src="/Notes/RL/RL——POMDP/POMDP_Policy_Function.png"></li>
<li><img src="/Notes/RL/RL——POMDP/POMDP_Solutions.png"></li>
<li><img src="/Notes/RL/RL——POMDP/POMDP2RL.png">
<ul>
<li>RL本身并不假设知道状态转移矩阵等，所以其实可以直接使用RL求解POMDP试一下的，只是RL不保证收敛而已（PS：Q-Learning还是收敛的吧，只是DQN没有数据证明收敛）</li>
<li>如果能建模出来POMDP的整个过程，没必要用RL了</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——REM</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94REM.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/137001028" target="_blank" rel="noopener">【强化学习 120】SimpleBatchRL (REM)</a></li>
<li>单纯形(Simplex)介绍：<a href="https://blog.csdn.net/zhaoyin214/article/details/99090386" target="_blank" rel="noopener">学习笔记 - 单纯形</a><ul>
<li><img src="/Notes/RL/RL——REM/N_Dim_Simplex.png"></li>
</ul>
</li>
<li>概率单纯形（概率单纯形）介绍：<a href="https://zhuanlan.zhihu.com/p/479892005" target="_blank" rel="noopener"></a><ul>
<li><img src="/Notes/RL/RL——REM/Probability_Simplex.png"></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——进化算法</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/150946035" target="_blank" rel="noopener">进化策略及其在深度学习中的应用</a></li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>General——深刻认识URL</title>
    <url>/Notes/Others/General%E2%80%94%E2%80%94%E6%B7%B1%E5%88%BB%E8%AE%A4%E8%AF%86URL.html</url>
    <content><![CDATA[<p><em>你真的认识URL了吗？</em></p>
<h3 id="URL中的-字符"><a href="#URL中的-字符" class="headerlink" title="URL中的#字符"></a>URL中的<code>#</code>字符</h3><ul>
<li><code>#</code>在URL中与服务器无关，也就是说正常访问服务器的URL不包含<code>#</code></li>
<li><code>#</code>仅仅与本地浏览器对网页的定位相关</li>
<li><code>#</code>由于不影响对远程服务器的访问，自然也不会存在于软件包的下载连接中</li>
</ul>
<h3 id="URL的正则表达式"><a href="#URL的正则表达式" class="headerlink" title="URL的正则表达式"></a>URL的正则表达式</h3><p><em>参考博客：<a href="https://blog.csdn.net/qq_25384945/article/details/81219075" target="_blank" rel="noopener">https://blog.csdn.net/qq_25384945/article/details/81219075</a></em></p>
<ul>
<li><p>Python</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+</span><br></pre></td></tr></table></figure>
</li>
<li><p>JavaScript</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/((([A-Za-z]&#123;3,9&#125;:(?:\/\/)?)(?:[\-;:&amp;=\+\$,\w]+@)?[A-Za-z0-9\.\-]+|(?:www\.|[\-;:&amp;=\+\$,\w]+@)[A-Za-z0-9\.\-]+)((?:\/[\+~%\/\.\w\-_]*)?\??(?:[\-\+=&amp;;%@\.\w_]*)#?(?:[\.\!\/\\\w]*))?)/</span><br></pre></td></tr></table></figure>
</li>
<li><p>Java</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">^(https?|ftp|file)://[-a-zA-Z0-9+&amp;@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&amp;@#/%=~_|]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python</p>
<pre><code></code></pre>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——多任务学习权重优化</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%9D%83%E9%87%8D%E4%BC%98%E5%8C%96.html</url>
    <content><![CDATA[<hr>
<h3 id="loss归一化"><a href="#loss归一化" class="headerlink" title="loss归一化"></a>loss归一化</h3><hr>
<h3 id="不确定权重"><a href="#不确定权重" class="headerlink" title="不确定权重"></a>不确定权重</h3><ul>
<li>Paper: <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/425672909" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/425672909</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/65137250" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65137250</a></li>
<li><a href="https://blog.csdn.net/cv_family_z/article/details/78749992" target="_blank" rel="noopener">https://blog.csdn.net/cv_family_z/article/details/78749992</a></li>
<li><a href="https://mp.weixin.qq.com/s/BOGhU2jMqD82EMkTHGMnuw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/BOGhU2jMqD82EMkTHGMnuw</a></li>
<li>回归问题误差：正太分布，分类问题误差：玻尔兹曼分布（<a href="https://www.zhihu.com/question/274174763/answer/672202523%EF%BC%89" target="_blank" rel="noopener">https://www.zhihu.com/question/274174763/answer/672202523）</a></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos——挖矿病毒kdevtmpfsi查杀经历</title>
    <url>/Notes/Linux/Centos%E2%80%94%E2%80%94%E6%8C%96%E7%9F%BF%E7%97%85%E6%AF%92kdevtmpfsi%E6%9F%A5%E6%9D%80%E7%BB%8F%E5%8E%86.html</url>
    <content><![CDATA[<hr>
<h3 id="挖矿病毒kdevtmpfsi查杀经历"><a href="#挖矿病毒kdevtmpfsi查杀经历" class="headerlink" title="挖矿病毒kdevtmpfsi查杀经历"></a>挖矿病毒kdevtmpfsi查杀经历</h3><h4 id="问题简介"><a href="#问题简介" class="headerlink" title="问题简介"></a>问题简介</h4><ul>
<li>服务器: 阿里云主机服务器</li>
<li>系统: Centos7</li>
<li>表现: kdevtmpfsi进程占用400%(8核心处理器)CPU</li>
<li>时间: 2020-01-07报警, 2020-01-08处理</li>
</ul>
<h4 id="查杀经过"><a href="#查杀经过" class="headerlink" title="查杀经过"></a>查杀经过</h4><ul>
<li><p>使用<code>clamscan</code>命令搜索所有文件, clamav详情见我之前的博客<a href="/Notes/Linux/Centos%E2%80%94%E2%80%94clamav%E5%AE%89%E8%A3%85%E4%B8%8E%E6%9D%80%E6%AF%92.html">clamav安装与杀毒</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup clamscan / -r --infected -l clamscan.log &gt; clamscan.out &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li>这一步比较花时间</li>
</ul>
</li>
<li><p>查看扫描结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat clamscan.log | grep FOUND</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/merged/tmp/kdevtmpfsi: Multios.Coinminer.Miner-6781728-2 FOUND<br>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/merged/tmp/red2.so: Unix.Trojan.Gafgyt-6981174-0 FOUND<br>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/upper/tmp/kdevtmpfsi: Multios.Coinminer.Miner-6781728-2 FOUND<br>/var/lib/docker/overlay/bdd049c71596d743907224a8dd6fdb3fb4ca76e3af8dfd6eee2d034de2be45a1/upper/tmp/red2.so: Unix.Trojan.Gafgyt-6981174-0 FOUND</p>
</blockquote>
<ul>
<li><p>删除这四个文件,这里直接到相关目录下查看发现<code>../tmp</code>目录下往往都是病毒文件(与<code>kinsing</code>相关,全部删除)</p>
</li>
<li><p><code>top</code>查看CPU信息确定挖矿进程kdevtmpfsi的进程号[pid]</p>
</li>
<li><p>确定启动信息中启动命令,并删除(在这里查到的信息是文件已经被删除了)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls /proc/[pid] -ali</span><br></pre></td></tr></table></figure>
</li>
<li><p>查找父进程进程号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status [pid]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>● docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope - libcontainer container be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161<br>   Loaded: loaded (/run/systemd/system/docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope; static; vendor preset: disabled)<br>  Drop-In: /run/systemd/system/docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope.d<br>           └─50-BlockIOAccounting.conf, 50-CPUAccounting.conf, 50-DefaultDependencies.conf, 50-Delegate.conf, 50-Description.conf, 50-MemoryAccounting.conf, 50-Slice.conf<br>   Active: active (running) since Mon 2019-11-11 11:24:17 UTC; 1 months 27 days ago<br>    Tasks: 38<br>   Memory: 2.3G<br>   CGroup: /system.slice/docker-be9fcab033e6158f8ff7d6ac07d28cfd918375178c27e016aa800cbeef985161.scope<br>           ├─ 4475 redis-server *:6379<br>           ├─ 8528 sh -c /tmp/.ICEd-unix/vJhOU<br>           ├─ 8529 /tmp/.ICEd-unix/vJhOU<br>           └─22822 /tmp/kdevtmpfsi</p>
</blockquote>
<p>Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</p>
<ul>
<li><p>杀死不需要的相关进程,如上面的<code>4475</code>,<code>8528</code>, <code>8529</code>, <code>22822</code></p>
</li>
<li><p>查看是否还有需要杀死的进程,如果有,则杀死该进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef | grep kinsing</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>top</code>确定挖矿进程已经被杀死</p>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>查杀病毒两个小时未发现服务器有明显异常</li>
<li>第二天出现了,需要进一步查看,重启机器后问题没有再出现</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——BCB出价推导</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94BCB%E5%87%BA%E4%BB%B7%E6%8E%A8%E5%AF%BC.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<p><em>本文主要记录BCB出价的推导</em></p>
<ul>
<li>其他参考链接：<a href="https://zhuanlan.zhihu.com/p/532453764" target="_blank" rel="noopener">智能出价——BCB求解</a></li>
</ul>
<h3 id="BCB介绍"><a href="#BCB介绍" class="headerlink" title="BCB介绍"></a>BCB介绍</h3><ul>
<li>预算约束的出价，Budget Constrained Bidding（简称BCB），广告主的出价目标是设置一定预算，拿到最多的流量（点击或者订单）</li>
</ul>
<hr>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><ul>
<li>一般BCB优化问题的定义:</li>
</ul>
<p>$$<br>\begin{align}<br>\max_{x_{ij}} \sum^N_{i=1} x_{ij}  v_{ij} \\<br>s.t. \sum^N_{i=1} x_{ij}  c_{ij} \le B \\<br>      \sum^M_{j=1} x_{ij} = 1 \\<br>      x_{ij} \in {0, 1}<br>\end{align}<br>$$</p>
<h3 id="解法一"><a href="#解法一" class="headerlink" title="解法一"></a>解法一</h3><p>可以解得，最终结果为(求解过程参见RL-MPCA论文):<br>$$ j^* = \arg\max_{j} (v_{ij} - \lambda c_{ij}) $$</p>
<h3 id="解法二"><a href="#解法二" class="headerlink" title="解法二"></a>解法二</h3><ul>
<li>如果<strong>只有一个广告位置，且使用二价计费</strong>，原始问题可化简为</li>
</ul>
<p>$$<br>\begin{align}<br>\max_{x_{i}} \sum^N_{i=1} x_{i} v_{i} \\<br>s.t. \sum^N_{i=1} x_{i} c_{i} \le B \\<br>      x_{i} \in {0, 1}<br>\end{align}<br>$$</p>
<ul>
<li>最终可解得结果为(求解参考链接：<a href="https://zhuanlan.zhihu.com/p/532453764" target="_blank" rel="noopener">智能出价——BCB求解</a>)：<br>$$ bid = v_i / \lambda $$</li>
</ul>
<h3 id="关于两种解法的结果分析"><a href="#关于两种解法的结果分析" class="headerlink" title="关于两种解法的结果分析"></a>关于两种解法的结果分析</h3><ul>
<li>本质上，两种解法结果应该是完全相同的，第一种解法中，如果只有一个广告位置，且使用二价计费，求解到的结果本质于第二种解法结果一致。<ul>
<li>当 \(v_{i} &gt; \lambda \cdot Price_{win} \)时(\(Price_{win} = c_{i}\),表示净胜价格), 此时选择\(bid &gt; Price_{win}\)的出价即可获得本次竞拍，此时不管是解法一(取\(v_{ij} - \lambda * c_{ij}\) 最大的动作)还是解法二(\(bid=v_i/\lambda\))都会选择执行竞拍动作。</li>
<li>反之，当当\(v_{i} &lt; \lambda \cdot Price_{win}\)时, 此时选择\(bid &lt; Price_{win}\)的出价即可获得本次竞拍，此时不管是解法一还是解法二都会选择执行不竞拍动作。</li>
</ul>
</li>
<li>解法一适用于任何场景，解法二则仅适用于<strong>只有一个广告位置，且使用二价计费</strong>的场景</li>
<li>解法二的结果使用起来会更简单：<ul>
<li>解法一需要预估不同出价下的收益，实际上，在<strong>只有一个广告位置，且使用二价计费</strong>的场景，不竞争当前广告位置的价值为0，计费为0，若竞争，则价值为\(v_i\)，计费为\(c_i\)，所以仅需要预估价值\(v_i\)，计费\(c_i\)。</li>
<li>解法二则可直接预估一个值\(v_i\)即可，不需要预估\(c_i\)，即在线不需要预估净胜价格<ul>
<li>但是离线流量回放求解\(\lambda\)时，理论上也需要预估一个净胜价格，或者在给出一个价格时，需要知道是否会竞争成功。\(c_i\)已经隐含在求解到的\(\lambda\)中</li>
</ul>
</li>
<li>若对于任意竞拍，都给定净胜价格，那么解法一和解法二等价，都只需要预估\(v_i\)一个值即可</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——WideAndDeep</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94WideAndDeep.html</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>一般来说，模型最重要的是 Memorization 和 Generalization <ul>
<li>Memorization: 可以不精确地定义为学习 items 或者特征的频繁共现，并且从历史数据里面开发(exploiting)</li>
<li>Generalization: 基于相关性的传递性(transitivity of correlation), 探索发现一些新的特征组合(从我出现或很少出现的组合)</li>
</ul>
</li>
</ul>
<h3 id="Wide"><a href="#Wide" class="headerlink" title="Wide"></a>Wide</h3><ul>
<li>注重记忆能力</li>
<li>手动特征交叉组合(实现低阶的特征组合)</li>
</ul>
<h3 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h3><ul>
<li>泛化能力</li>
<li>高阶特征</li>
<li>低维稠密embedding</li>
</ul>
<h3 id="Wide-and-Deep"><a href="#Wide-and-Deep" class="headerlink" title="Wide and Deep"></a>Wide and Deep</h3><ul>
<li>结合了Wide和Deep两个模型</li>
<li>不是模型的集成(Ensemble)，这里称为联合训练(Joint Training)<ul>
<li>集成的两个模型是分开独立训练的，然后预测的时候合并了两个模型的预测</li>
<li>联合训练是两个模型同时训练，训练时是加权相加的，每次训练时损失是一起传递</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——多任务学习总结</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>多任务学习，multi-task learning</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h3 id="多任务学习的结构"><a href="#多任务学习的结构" class="headerlink" title="多任务学习的结构"></a>多任务学习的结构</h3><h4 id="ESMM及扩展"><a href="#ESMM及扩展" class="headerlink" title="ESMM及扩展"></a>ESMM及扩展</h4><h4 id="MMoE及相关变体"><a href="#MMoE及相关变体" class="headerlink" title="MMoE及相关变体"></a>MMoE及相关变体</h4><h3 id="多任务学习的损失函数权重设置"><a href="#多任务学习的损失函数权重设置" class="headerlink" title="多任务学习的损失函数权重设置"></a>多任务学习的损失函数权重设置</h3><h4 id="基于人工经验的人工优化"><a href="#基于人工经验的人工优化" class="headerlink" title="基于人工经验的人工优化"></a>基于人工经验的人工优化</h4><ul>
<li>一般思想是对齐损失函数均值，并按照业务偏好有所倾斜，如果愿意花时间尝试，往往能拿到不错的结果</li>
</ul>
<h4 id="基于贝叶斯推论的权重优化"><a href="#基于贝叶斯推论的权重优化" class="headerlink" title="基于贝叶斯推论的权重优化"></a>基于贝叶斯推论的权重优化</h4><ul>
<li>基于不确定性的权重设置方法（Uncertainty Weighting）</li>
<li>基本公式<br>$$<br>L(\mathbf{W}, \sigma_1, \sigma_2,…,\sigma_K) = \sum_{k=1}^K \frac{1}{2\sigma^2}L(\mathbf{W}) + \log \sigma^2<br>$$<ul>
<li>上述公式可通过推导得出<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</a><ul>
<li>可以推导，无论是回归问题还是分类问题（也可以是分类和回归问题的混合），都可以按照上面的方法设置损失函数（分类问题证明中会使用到一个近似值，不是严格推导）</li>
<li>推导是在假设</li>
</ul>
</li>
<li>\(\sigma\)是可学习的参数，初始设置固定值，然后使用梯度更新学习即可</li>
<li>使用简单，实际使用时效果也确实不错，建议人工调参也可以在先使用该方案拿到权重量级后继续</li>
</ul>
</li>
</ul>
<h4 id="帕累托最优权重优化"><a href="#帕累托最优权重优化" class="headerlink" title="帕累托最优权重优化"></a>帕累托最优权重优化</h4><ul>
<li><p>原始论文：<a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf" target="_blank" rel="noopener">Multi-Task Learning as Multi-Objective Optimization</a></p>
<img src="/Notes/ComputationalAdvertising/CA——多任务学习总结/pareto-optimazation-for-MTL.png"></li>
<li><p>内积的含义：向量A到向量B的投影长度与向量B长度的乘积</p>
</li>
<li><p>Algorithm1展示的是，对于两个任务的情况，图示展示了二维向量的情况，可以通过判断向量之间的关系确定求解\(\alpha\)的方式</p>
<ul>
<li>Algorithm2中的FrankWolfeSlover算法则是对Algorithm1的多任务扩展</li>
</ul>
</li>
<li><p>其他参考，阿里巴巴多任务学习帕累托最优论文：<a href="https://yongfeng.me/attach/lin-recsys2019.pdf" target="_blank" rel="noopener">A Pareto-Efficient Algorithm for Multiple Objective Optimization in E-Commerce Recommendation</a></p>
</li>
</ul>
<h3 id="损失函数优化"><a href="#损失函数优化" class="headerlink" title="损失函数优化"></a>损失函数优化</h3><h4 id="损失函数归一化"><a href="#损失函数归一化" class="headerlink" title="损失函数归一化"></a>损失函数归一化</h4><p><em>可用于解决由于不同任务损失函数量级差异带来的问题</em></p>
<h5 id="普通版本"><a href="#普通版本" class="headerlink" title="普通版本"></a>普通版本</h5><p>$$<br>L_{norm} = \frac{L_k(\mathbf{W})}{L_0(\mathbf{W_0})}<br>$$</p>
<ul>
<li>使用各个任务自己的第一次输出的损失函数作为基础损失函数，其中\(L_0(\mathbf{W_0}\)为第一次计算loss的到的损失函数</li>
</ul>
<h5 id="滑动平均版本"><a href="#滑动平均版本" class="headerlink" title="滑动平均版本"></a>滑动平均版本</h5><p>$$<br>L_{base} = \alpha L_{base} + (1-\alpha) L_k \\<br>L_{norm} = \frac{L_k(\mathbf{W})}{L_{base}} \\<br>$$</p>
<ul>
<li>使用滑动平均来记录基础损失函数，该方案可进一步减少由于初始化损失误差过大带来的问题</li>
</ul>
<h4 id="梯度归一化（GradNorm）"><a href="#梯度归一化（GradNorm）" class="headerlink" title="梯度归一化（GradNorm）"></a>梯度归一化（GradNorm）</h4><ul>
<li>原始论文：<a href="https://proceedings.mlr.press/v80/chen18a/chen18a.pdf" target="_blank" rel="noopener">GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</a><img src="/Notes/ComputationalAdvertising/CA——多任务学习总结/grad-norm-definition-for-MTL.png">
<img src="/Notes/ComputationalAdvertising/CA——多任务学习总结/grad-norm-for-MTL.png"></li>
<li>核心思想是对各任务的损失函数进行加权(\(w_i\))求和得到更新共享参数的损失函数</li>
<li>“GradNorm”这个名字的由来是因为权重\(w_i\)是与梯度2范数的期望等有关的？</li>
<li>公式中\(w_i\)是各个任务损失函数对共享参数损失函数的权重，该权重初始值为1，在训练过程中逐步更新，每一步最后都保持该权重加和为\(T\)（\(T\)为任务数量，即保证权重均值为1）</li>
<li>问题：文中没有明确各个任务各自的参数如何更新，猜测各自更新即可</li>
</ul>
<h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><ul>
<li>一般情况下，根据业务特点，尽量使用类似于ESMM结构</li>
<li>权重设置尝试次序：<ul>
<li>对损失函数进行归一化（梯度归一化好像效果一般？）</li>
<li>权重设置时，先使用不确定性权重（Uncertainty Weighting）跑一版，得到基线</li>
<li>在Uncertainty Weighting的基础上，人工可以根据业务需要微调，可以偏向于需要的任务</li>
<li>帕累托最优实现复杂，且不一定有收益<ul>
<li>复杂体现在：需要在每个batch上重新求解最优化问题，得到当前的loss权重（用上一个batch的梯度求解这一个batch的最优权重）</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——Google-Ad-Click-Prediction</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94Google-Ad-Click-Prediction.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<ul>
<li>参考文献：<a href="https://dl.acm.org/doi/pdf/10.1145/2487575.2488200?download=true" target="_blank" rel="noopener">Google KDD 2013，Ad Click Prediction: a View from the Trenches</a>(引用量500+)</li>
<li>介绍了截止到2013年之前的点击率预估常用算法，FTRL是Google三年的结晶（2010-2013）</li>
<li><strong>在线学习</strong>优化算法的发展历程：SGD-&gt;TG-&gt;FOBOS-&gt;RDA-&gt;FTRL-Proximal</li>
</ul>
<h3 id="核心贡献"><a href="#核心贡献" class="headerlink" title="核心贡献"></a>核心贡献</h3><ul>
<li>基于传统的逻辑回归算法(Regularized Logistic Regression，正则化的逻辑回归)在点击率预估时的不足，提出一种<strong>在线逻辑回归</strong>算法，FTRL(Follow The Regularized Leader)</li>
<li>per-coordinate learning rate</li>
</ul>
<h3 id="一些模型的比较和介绍"><a href="#一些模型的比较和介绍" class="headerlink" title="一些模型的比较和介绍"></a>一些模型的比较和介绍</h3><ul>
<li>传统逻辑回归算法中使用OGD(Online Gradient Descent)是非常高效的，使用很小的计算资源就能得到较好的精确度.</li>
<li>但是OGD在生成<strong>稀疏</strong>模型方面表现不好(OGD + L1)</li>
<li>其他在稀疏性方面表现良好的方法有FOBOS, 截断梯度（Truncated Gradient）和 RDA（Regularized Dual Averaging)<ul>
<li>RDA在精确度和稀疏性方面做tradeoff， 效果好于FOBOS</li>
</ul>
</li>
<li>FTRL-Proximal号称可以同时获得RDA的稀疏性和OGD的精确度</li>
<li>RDA模型是微软提出的一种在线优化算法，与OGD完全不同，能得到更加稀疏的模型，但是精确度不如OGD</li>
</ul>
<h3 id="FTRL-Proximal-Learning-Online-Learning-And-Sparsity"><a href="#FTRL-Proximal-Learning-Online-Learning-And-Sparsity" class="headerlink" title="FTRL-Proximal Learning (Online Learning And Sparsity)"></a>FTRL-Proximal Learning (Online Learning And Sparsity)</h3><ul>
<li>也是通过L1正则化控制模型的稀疏度</li>
</ul>
<h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><h5 id="FTL-Follow-The-Leader-的介绍"><a href="#FTL-Follow-The-Leader-的介绍" class="headerlink" title="FTL(Follow The Leader)的介绍"></a>FTL(Follow The Leader)的介绍</h5><h5 id="OGD的更新方式"><a href="#OGD的更新方式" class="headerlink" title="OGD的更新方式"></a>OGD的更新方式</h5><ul>
<li>更新规则：<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;= \mathbf{w}_t-\eta_t\mathbf{g}_t<br>\end{align}<br>$$</li>
</ul>
<h5 id="FTLR-Follow-The-Regularized-Leader"><a href="#FTLR-Follow-The-Regularized-Leader" class="headerlink" title="FTLR(Follow The Regularized Leader)"></a>FTLR(Follow The Regularized Leader)</h5><p><em>加上正则项的FTL</em></p>
<ul>
<li>更新规则：<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;= \arg\min_{\mathbf{w}}\left ( \mathbf{g}_{1:t}\cdot \mathbf{w} + \frac{1}{2}\sum_{s=1}^t \sigma_s || \mathbf{w} - \mathbf{w}_s||_2^2 + \lambda_1||\mathbf{w}||_1 \right ) \\<br>\mathbf{g}_{1:t} &amp;= \sum_{i=1}^t\mathbf{g}_i<br>\end{align}<br>$$<ul>
<li>第一项是对损失函数梯度的贡献的一个估计</li>
<li>第二项是控制参数\(\mathbf{w}\)在每次迭代中变化不要太大</li>
<li>第三项是L1正则化，用于使模型变得稀疏（除了L1正则化项以外，也可以再加上L2正则化）</li>
<li>去掉正则化项就是FTL（Follow The Leader）</li>
<li>\(\sigma_s\)是学习速率</li>
<li>这个学习速率可以用Per-Coordinate Learning Rate:<br>$$<br>\begin{align}<br>\eta_{t, i} &amp;= \frac{\alpha}{\beta + \sqrt{\sum_{s=1}^t g_{s,i}^2}} \\<br>\mathbf{g}_s &amp;= \nabla l_s(\mathbf{w})<br>\end{align}<br>$$</li>
</ul>
</li>
</ul>
<h3 id="Per-Coordinate-Learning-Rates"><a href="#Per-Coordinate-Learning-Rates" class="headerlink" title="Per-Coordinate Learning Rates"></a>Per-Coordinate Learning Rates</h3><ul>
<li>对参数的每一维度分开训练，每个维度有自己的学习率</li>
<li>某个特征出现的次数越多，说明当前该特征对应的参数值越可信，学习率就应该越小</li>
<li>考虑了数据在每个特征上的分布不均匀性<ul>
<li>参数某个维度上的样本数越少，这些样本就会得到越大的利用(具体表现就是该特征的学习率会比较大)</li>
</ul>
</li>
</ul>
<h3 id="一个思考"><a href="#一个思考" class="headerlink" title="一个思考"></a>一个思考</h3><ul>
<li>问题：为什么机器学习中的学习率都是越来越小？</li>
<li>答案：因为刚开始训练时，参数的值不太可信（也就是说最终参数与当前参数的置信度比较低），所以更新时应该更新的步骤大一些，让当前的参数变化大一些，训练到后来，随着参数的值越来越可信（当前参数的置信度比较高），更新的步骤就应该小一些，让当前的变化小一些</li>
</ul>
<h3 id="一些工程上的Trick"><a href="#一些工程上的Trick" class="headerlink" title="一些工程上的Trick"></a>一些工程上的Trick</h3><h4 id="Saving-Memory-at-Massive-Scale"><a href="#Saving-Memory-at-Massive-Scale" class="headerlink" title="Saving Memory at Massive Scale"></a>Saving Memory at Massive Scale</h4><h5 id="Probabilistic-Feature-Inclusion"><a href="#Probabilistic-Feature-Inclusion" class="headerlink" title="Probabilistic Feature Inclusion"></a>Probabilistic Feature Inclusion</h5><ul>
<li>在高维数据中，大量的特征是出现频率非常低的(rare)，半数的唯一特征甚至只出现一次</li>
<li>统计这些特征的代价是非常昂贵的，有些特征可能永远不会被实际使用(这里如何理解昂贵？也就是说训练了也没用？)</li>
<li>额外的读写数据是昂贵的，如果能丢弃一部分出现评论特别低的特征(比如出现频率低于k次)</li>
<li>实现稀疏可以使用L1正则化，也可以使用Probabilistic Feature Inclusion</li>
<li>关于Probabilistic Feature Inclusion的做法<ul>
<li>当一个特征第一次出现时，以一定的概率接受这个新特征</li>
<li>效果作用于数据预处理阶段，但是可以在在线执行中设置</li>
</ul>
</li>
</ul>
<h6 id="Possion-Inclusion"><a href="#Possion-Inclusion" class="headerlink" title="Possion Inclusion"></a>Possion Inclusion</h6><ul>
<li>对于新的特征，以概率p添加入模型</li>
<li>对于已经存在模型中的特征，正常更新其系数</li>
</ul>
<h6 id="Bloom-Filter-Inclusion"><a href="#Bloom-Filter-Inclusion" class="headerlink" title="Bloom Filter Inclusion"></a>Bloom Filter Inclusion</h6><ul>
<li>用布隆过滤器仅仅保留出现次数在n次以上的特征</li>
</ul>
<img src="/Notes/ComputationalAdvertising/CA——Google-Ad-Click-Prediction/inclusion_methods.png">

<h5 id="Encoding-Values-with-Fewer-Bits"><a href="#Encoding-Values-with-Fewer-Bits" class="headerlink" title="Encoding Values with Fewer Bits"></a>Encoding Values with Fewer Bits</h5><ul>
<li>常用的OGD使用32或者64位浮点数编码来存储模型的系数</li>
<li>Google提出可以使用16位浮点数来存储系数，同时加上一些策略</li>
<li>实验结果：将64位的浮点值换为为系数存储节省了75%的RAM内存空间(这还用实验？直接计算就得到了啊)</li>
</ul>
<h5 id="Training-Many-Similar-Models"><a href="#Training-Many-Similar-Models" class="headerlink" title="Training Many Similar Models"></a>Training Many Similar Models</h5><ul>
<li>同时训练多个模型是超参数选择常用的方法</li>
<li>将可以共享的东西共享</li>
<li>在节省内存的同时，还可以节约网络带宽(存储一份Per-coordinate学习率，节省内存和带宽等)，CPU(用同一个hash表检索特征，节省CPU)和硬盘空间</li>
</ul>
<h5 id="A-Single-Value-Structure"><a href="#A-Single-Value-Structure" class="headerlink" title="A Single Value Structure"></a>A Single Value Structure</h5><ul>
<li>有时候我们希望评估大量的模型在只有少量的特征组(groups of features)添加或者删除时的变化</li>
<li>对于每一维度(coordinate),仅仅存储一个系数值而不是多个(正常应该为每个模型存储一个)</li>
<li>存储该维度对应特征组的模型共享该系数</li>
<li>对每个特征，训练时每个模型都会计算自己的值，然后所有模型的取平均作为所有包含该维度特征模型的共享</li>
</ul>
<h5 id="Computing-Learning-Rates-with-Counts"><a href="#Computing-Learning-Rates-with-Counts" class="headerlink" title="Computing Learning Rates with Counts"></a>Computing Learning Rates with Counts</h5><ul>
<li>对于每个特征来说，<strong>梯度和</strong>以及<strong>梯度平方和</strong>是必须计算的</li>
<li>梯度的计算必须准确，但是计算学习率却是可以粗糙计算的</li>
<li>仅仅统计样本出现次数(Counts)就能大概计算学习率</li>
</ul>
<h6 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h6><ul>
<li>假设包含一个给定特征的所有事件(events)具有有相同的概率(一般来说，这个近似是不可能的，但是在这个目标里面是可行的)<ul>
<li>如何理解这个假设的意义呢？<strong>对于具有某个特征的所有样本，其取值为(正负例)是的概率是相等的，正例(click)概率都为\(p\)</strong></li>
</ul>
</li>
<li>进一步假设模型已经精确地学习到了概率</li>
<li>如果有分别有\(P，N\)个正负样本(events),则有\(p=\frac{P}{N+P}\)</li>
<li>则有对于逻辑回归(\(p(1-p)\))来说，正例的梯度为\(p-1\),负例的梯度为\(p\)<br>$$<br>\begin{align}<br>\sum g_{t,i}^2 &amp;= \sum_{positive \ events}(1-p_t)^2 + \sum_{negative \ events}p_t^2 \\<br>&amp;\approx P(1-\frac{P}{N+P})^2 + N(\frac{P}{N+P})^2 \\<br>&amp;= \frac{PN}{N+P}<br>\end{align}<br>$$</li>
<li>也就是说，为了近似计算\(\sum g_{t,i}^2\)，我们仅需要记录\(P，N\)即可</li>
</ul>
<h5 id="Subsampling-Traning-Data"><a href="#Subsampling-Traning-Data" class="headerlink" title="Subsampling Traning Data"></a>Subsampling Traning Data</h5><ul>
<li>典型的CTRs是远远低于50%，数据偏差很大，正例(点击)的样本很稀疏</li>
<li>在模型训练中正例样本相对而言更有价值</li>
<li>使用下采样：很大程度上降低数据量，同时保证对精度最小程度的影响</li>
</ul>
<h6 id="采样方法："><a href="#采样方法：" class="headerlink" title="采样方法："></a>采样方法：</h6><ul>
<li>保留所有至少被点击过一次的请求(Query，也就是样本)</li>
<li>以一定比例\(r\in(0, 1]\)采样没有被点击过的请求</li>
<li>因为包含通用的特征(Feature Phase)计算，所以这种方法是合理的，但是需要纠偏(直接用采样后的样本训练会造成预测偏差)</li>
<li>加入一个重要性权重\(w_t\)<ul>
<li>\(w_t = 1\) for clicked queries</li>
<li>\(w_t = \frac{1}{r}\) for queries with no clicks</li>
<li>本质上可以理解为这里是将采样时造成的负例比例偏差补齐</li>
</ul>
</li>
</ul>
<h3 id="模型性能评估"><a href="#模型性能评估" class="headerlink" title="模型性能评估"></a>模型性能评估</h3><h4 id="Progressive-Validation"><a href="#Progressive-Validation" class="headerlink" title="Progressive Validation"></a>Progressive Validation</h4><ul>
<li>Progressive验证又称为在线损失(online loss)</li>
<li>与交叉验证(cross-validation)或留出法(hold out)验证是不同的</li>
<li>在服务查询中，在线损失能很好的代表我们的精度，因为在训练模型前，它仅仅在最新数据上评估其性能(因为这准确的模拟了当模型进行服务查询时发生了什么)</li>
<li>由于用了100%的数据作为训练和测试，在线损失比留出法验证有更好的统计数据</li>
<li>绝对指标往往会带来误导<ul>
<li>即使预测是完美的，对数损失和其他指标的差异也依赖着问题的困难程度</li>
<li>不同的国家，不同请求的点击率不同(同为对数损失的最佳实践，50%的点击率会好于2%的点击率)</li>
</ul>
</li>
<li>所以使用相对变化，看指标相对于base line改变了多少</li>
<li>从经验来看，相对指标在整个时间段内都很稳定</li>
<li>相同的指标计算应该对应在完全相同的数据(比如不同时段的损失比较是没有意义的)</li>
</ul>
<h3 id="置信度评估-Confidence-Estimates"><a href="#置信度评估-Confidence-Estimates" class="headerlink" title="置信度评估(Confidence Estimates)"></a>置信度评估(Confidence Estimates)</h3><ul>
<li>对很多应用来说，除了评估广告的CTR，还要量化预测的期望精确度(the expected accuracy of the prediction)</li>
</ul>
<h3 id="校正预测-Calibrating-Predictions"><a href="#校正预测-Calibrating-Predictions" class="headerlink" title="校正预测(Calibrating Predictions)"></a>校正预测(Calibrating Predictions)</h3><ul>
<li>系统偏差(Systematic Bias)指平均预测CTR(Average Predicted CTR)和观测CTR(Observed CTR)的差异</li>
<li>造成系统偏差的原因包括：<ul>
<li>不精确的模型假设</li>
<li>学习算法的缺陷</li>
<li>在训练或者服务(预测)时不可用的隐藏特征</li>
</ul>
</li>
<li>解决方案：<ul>
<li>添加一个校正层将预测CTRs与观测CTR做匹配(match predicted CTRSs to observed click-through rates)</li>
<li>暂时不能提供有效的保证，保证校正影响的有效</li>
<li>校正的本质是找到(拟合)一个函数映射\(g\),使得模型输出值与真实概率值一一对应</li>
<li>逻辑回归中的sigmoid函数可以看做是一个校正预测的函数吗？</li>
</ul>
</li>
<li>更多参考<ul>
<li><a href="https://blog.csdn.net/fzcoolbaby/article/details/99174601?utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">风险模型 - 概率校准</a></li>
<li><a href="https://www.cnblogs.com/lc1217/p/7069000.html" target="_blank" rel="noopener">机器学习：概率校准</a>, 文中有代码示例</li>
<li><a href="https://blog.csdn.net/fjsd155/article/details/84382838?utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">概率校准 Probability Calibration</a></li>
</ul>
</li>
</ul>
<h4 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h4><p><em>参考博客：<a href="https://blog.csdn.net/fzcoolbaby/article/details/99174601?utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">https://blog.csdn.net/fzcoolbaby/article/details/99174601?utm_source=distribute.pc_relevant.none-task</a></em></p>
<ul>
<li>概率模型的搭建过程中，由于抽样与正则化等原因，导致模型的输出概率值明显偏离真实概率值<ul>
<li>[待更新：为什么抽样和正则化会影响模型的输出概率发生变化？]</li>
</ul>
</li>
<li>此时的模型输出概率值仅仅有排序的意义，其绝对值没有意义(定序值，而非定距数值)</li>
<li>校正预测的过程就是把模型输出概率值的校正成真实的概率的过程，使得校正后的概率有绝对值意义</li>
</ul>
<h3 id="自动特征管理-Automated-Feature-Management"><a href="#自动特征管理-Automated-Feature-Management" class="headerlink" title="自动特征管理(Automated Feature Management)"></a>自动特征管理(Automated Feature Management)</h3><ul>
<li>将特征空间描绘成上下文和语义信号，每个信号都可以被翻译成一个在学习时有真实值的特征集合</li>
<li>[待更新]</li>
</ul>
<h3 id="一些不成功的实验记录-Unsuccessful-Experiments"><a href="#一些不成功的实验记录-Unsuccessful-Experiments" class="headerlink" title="一些不成功的实验记录(Unsuccessful Experiments)"></a>一些不成功的实验记录(Unsuccessful Experiments)</h3><p><em>本节记录google的一些失败的尝试方向(有些可能会让人很惊讶)，这些方向模型没有明显收益</em></p>
<h4 id="Aggressive-Feature-Hashing"><a href="#Aggressive-Feature-Hashing" class="headerlink" title="Aggressive Feature Hashing"></a>Aggressive Feature Hashing</h4><p><em>关于特征哈希(Feature Hashing)的相关知识可参考<a href="/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94Feature-Hashing.html">Feature-Hashing</a></em></p>
<ul>
<li><a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" target="_blank" rel="noopener">Feature Hashing for Large Scale Multitask Learning</a>论文指出，Feature Hashing方法效果显著<ul>
<li>报告显示使用特征hashing技巧，可以能将学习一个垃圾邮件过滤模型的特征空间映射成包含\(2^24\)个特征的特征空间(疑问：这里的特征原来不止\(2^24\)个吗？)</li>
</ul>
</li>
<li>但是实验证明，使用 Feature Hashing 方法并不能提升我们的方法，所以建议保留 interpretable(non-hashed)的特征向量</li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><ul>
<li>Google用网格搜索的方法测试了从0.1到0.5的Dropout特征丢弃概率</li>
<li>所有情况均没有带来任何收益(包括精度指标和泛化能力)，还往往给模型带来损害(detriment)</li>
<li>Google给出的一个解释是：Dropout在图像领域取得较好收益的原因是因为图像领域的数据特征分布与计算广告领域不同<ul>
<li>图像领域：稠密特征，此时Dropout把结果(effect)从强相关的特征中分离开来，从而得到泛化效果更好的分类器</li>
<li>计算广告：稀疏特征，且有噪音</li>
</ul>
</li>
</ul>
<h4 id="Feature-Bagging"><a href="#Feature-Bagging" class="headerlink" title="Feature Bagging"></a>Feature Bagging</h4><ul>
<li>对特征进行overlapping采样(注意，样本Bagging和特征Bagging不同)，然后训练多个独立的模型，最后取平均值</li>
<li>实验证明模型的的AucLoss降低了0.1%-0.6%</li>
</ul>
<h4 id="Feature-Vector-Normalization"><a href="#Feature-Vector-Normalization" class="headerlink" title="Feature Vector Normalization"></a>Feature Vector Normalization</h4><ul>
<li>\(\mathbf{x} = \frac{\mathbf{x}}{||\mathbf{x}||}\)</li>
<li>开始有一点精度上的收益，但是后面也出现了一定程度的detriment</li>
<li>Google的解释是可能是由于per-coordinate learning rates和正则化的影响</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo——Next主题搜索窗口无法弹出</title>
    <url>/Notes/Hexo/Hexo%E2%80%94%E2%80%94Next%E4%B8%BB%E9%A2%98%E6%90%9C%E7%B4%A2%E7%AA%97%E5%8F%A3%E6%97%A0%E6%B3%95%E5%BC%B9%E5%87%BA.html</url>
    <content><![CDATA[<ul>
<li>参考博客： <a href="https://www.sqlsec.com/2017/12/hexosearch.html" target="_blank" rel="noopener">https://www.sqlsec.com/2017/12/hexosearch.html</a></li>
</ul>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>有时候会遇到在Mac上Next主题窗口无法弹出的问题</li>
<li>问题分为两类<ul>
<li>一类为找不到 <code>search.xml</code> 文件：加载 <code>search.xml</code> 时错误为404</li>
<li>另一类为文章中有特殊字符：加载 <code>search.xml</code> 检查时错误为304</li>
</ul>
</li>
</ul>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="404类"><a href="#404类" class="headerlink" title="404类"></a>404类</h4><ul>
<li><p>修改最外层配置文件<code>./_config.yml</code>,添加以下语句(实测这一步非必须)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装搜索插件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="304类"><a href="#304类" class="headerlink" title="304类"></a>304类</h4><ul>
<li>304状态说明是加载文件存在，但是无法正常解析文件</li>
<li>直接用浏览器访问 <code>search.xml</code> 文件链接，然后查看文件解析异常出现在哪个地方，然后删除特殊字符即可<ul>
<li>个人理解，从哪个文件不能搜索，特殊字符就出现在哪个文件中</li>
<li>说明：目前还没遇到过这种情况，后面遇到会再补充</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——Feature-Hashing</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94Feature-Hashing.html</url>
    <content><![CDATA[<ul>
<li>参考文献: <a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" target="_blank" rel="noopener">Feature Hashing for Large Scale Multitask Learning</a></li>
<li>特征哈希(Feature Hashing)常用于数据特征降维，同时尽量保留原始特征的表达能力<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<h3 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h3></li>
<li>给出了一种高维数据降维方法，特征哈希(Feature Hashing)</li>
<li>严格证明了特征哈希的可用性</li>
</ul>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>一种构造组合特征的方法是笛卡尔乘积</li>
<li>计算广告领域往往有数十亿的高维特征</li>
<li>一种表达方式是使用词表法，对每个特征从词表里面进行查询<ul>
<li>缺陷一：拓展问题，每次拓展词表时难度较大(难以进行模型升级，因为特征维度在变化)</li>
<li>缺陷二：无法处理词表中不存在的特征(Unknown特征)</li>
</ul>
</li>
<li>一般的降维方法容易带来特征表达能力的损失</li>
</ul>
<h3 id="特征哈希"><a href="#特征哈希" class="headerlink" title="特征哈希"></a>特征哈希</h3><ul>
<li><p>哈希函数定义(参考自博客：<a href="https://blog.csdn.net/qjf42/article/details/82387559" target="_blank" rel="noopener">https://blog.csdn.net/qjf42/article/details/82387559</a>)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def feature_hashing(features, m):</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	Args:</span><br><span class="line">		features: 输入特征序列，可以是数字，字符串(本质还是数字)</span><br><span class="line">		m: 输出的特征维度，通常是2**26(vw),2**20(scikit-learn)</span><br><span class="line">	Returns:</span><br><span class="line">		m维的（稀疏）向量</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	# 这里的x一般是稀疏表示的（如：scipy.sparse.csr_matrix），这里是为了方便说明</span><br><span class="line">    x = np.zeros(m)</span><br><span class="line">    for feature in features:</span><br><span class="line">    	# hash_func_1保证散列尽量平均且散列速度快</span><br><span class="line">        idx = hash_func_1(feature) % m </span><br><span class="line">        # 这里在原始论文中</span><br><span class="line">        sign = hash_func_2(feature) % 2</span><br><span class="line">        if sign == 1:</span><br><span class="line">            x[idx] += 1</span><br><span class="line">        else:</span><br><span class="line">            x[idx] -= 1</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<ul>
<li>输出特征维度一般为\(m = 2^24\)等，是一个认为给定的数字</li>
</ul>
</li>
<li><p>与词表法比较：</p>
<ul>
<li>解决了模型升级时的特征拓展问题(增加新特征时，特征的维度不会变化)</li>
<li>解决了Unknown特征问题(个人理解：因为hash函数不管是什么特征，都可以一视同仁)</li>
<li>无需查表，节省了查表的时间(个人理解：其实查表时一般实现方式也是用哈希表构建索引，所以这里不能算是优势)</li>
<li>完成了降维(这里是把字典法里面对邮件或者文档的id也算作一个特征，这个特征one hot表示一下将会造成数据维度变得非常大？但是id算做特征有什么意义吗？)</li>
</ul>
</li>
</ul>
<h3 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h3><ul>
<li>冲突总会发生，也就是说不同一个特征总有一定的概率被映射到同一个维度(即两个不同特征的<code>idx</code>值可能相等)上</li>
<li>Paper中的垃圾邮件过滤模型实验证明：冲突造成的损失漏召率在\(m=2^22\)时影响约为1%，接近不做hash时的效果(特征维度在不做hash约为\(2^{25}\))且在\(m=2^{18}\)时为1.12%，也只升高了一点点</li>
<li>另外：无论如何，总有特殊情况，比如重要的特征如用户的性别特征“男”和“女”二者可能被映射到同一个维度上<ul>
<li>这里编码时是男:<code>(1, 0)</code>, 女<code>(0, 1)</code>这样的，所以如果二者映射到同一个维度上，那么这两个特征丢失了原本的表达能力</li>
<li>真实环境中如果遇到这些问题将会很难调试，如果找到了相关的问题，可以通过修改映射函数的输入参数字符串等方式来错开两个特征</li>
</ul>
</li>
<li>特征哈希本身可以类比于机器学习中的核技巧,所以特征哈希也称为哈希技巧</li>
</ul>
<h3 id="一些理解"><a href="#一些理解" class="headerlink" title="一些理解"></a>一些理解</h3><h4 id="知乎用户"><a href="#知乎用户" class="headerlink" title="知乎用户"></a>知乎用户</h4><ul>
<li>参考<a href="https://www.zhihu.com/people/ainika-peng" target="_blank" rel="noopener">Ainika Peng</a>的博客：<a href="https://www.zhihu.com/question/264165760/answer/279676705" target="_blank" rel="noopener">https://www.zhihu.com/question/264165760/answer/279676705</a></li>
<li>一般而言这类技术是为了解决两个问题：<ul>
<li>一是<strong>将categorical的特征编码为模型可理解的格式</strong>, 这是个基础问题。One-Hot Serializing就可以达到这个效果，例如将训练样本中出现过的的每个deviceid按顺序递增编号（比如deviceid@xxx:1 -&gt; feature@10000:1）。<ul>
<li>缺点1：这个映射表需要传递给引擎端，在prediction的时候照着再查一遍，数据量大且数据结构设计不好的时候很费时间；</li>
<li>缺点2：有些频次很低的特征置信度不高，单独出现对模型无益（甚至over-fitting）。这时候可以使用按频次截断技术进行降维。比如微软在deep crossing中提到的特征工程方法：只保留曝光最多的10k个campaign id编码为0-9999，其余的id全部编码为10000，但辅以poCTR等连续特征作为辅助。事实上这是一种手工的Hashing方法。</li>
</ul>
</li>
<li>二是<strong>尽可能在保留有效信息的基础上降低训练和预测过程的时间复杂度</strong></li>
</ul>
</li>
<li>自动Hashing方法的好处是：<ul>
<li>只要训练和预测时使用的hashing方法一致，对同一个特征的编码结果即一致，因此引擎预测或提供数据dump的时候无需查找编码表。所以其最大优点在于数据一致性和速度提升，这在极大规模特征和在线学习中至关重要。</li>
</ul>
</li>
<li>我们说的Hashing算法一般而言均特意设计为低碰撞率。<ul>
<li>因此一般hashing算法本身不会大幅降低特征维度，自然也不会大幅损失特征信息。真正可能存在问题的是hashing之后的降维过程。</li>
<li>一个非常常见的陷阱是string哈希到int64后取模m，试图将特征压缩至m维one-hot空间。在这种情况下，对于不知情的随机hashing过程，不同特征的碰撞率为1/m。举个例子，对于“性别”特征，将male哈希为一个int64，female哈希为另一个int64，很难发生碰撞；但如果你试图使用mod2将其压缩，如果你的算法哈希出的这两个int64奇偶性相同，则会导致特征失效。在你有很多feature需要哈希而又不清楚hashing算法细节的情况下，这在概率意义上是很容易发生的。<ul>
<li>这里的mod2是极端举例，其实m的取值小于原始维度的情况下都有一定概率造成冲突</li>
</ul>
</li>
</ul>
</li>
<li>因此我们会更倾向于所谓的embedding算法<ul>
<li>例如将70万维的userid通过weight embedding到32维的连续值特征上（不同于传统hashing的低维离散值特征）。这意味着训练过程更加复杂（有更多的weight需要optimize）；但对于预测过程，其特征性能十分良好且时间复杂度得以降低。同时，由于连续值特征空间的表达能力大幅高于离散值特征空间，整个模型的表达能力并不会明显下降，也基本不会发生离散hashing的碰撞问题。</li>
<li>当然，如果是<strong>FM这类倾向于接受离散值的模型，手工serializing+精心设计的hashing是较好的选择</strong>。</li>
</ul>
</li>
<li>优点：<ul>
<li>训练和预测的时间复杂度大幅降低；</li>
<li>数据的一致性强，不存在同一个特征今天编码成这个、明天编码成那个的情况，便于跟踪单特征效果；</li>
<li>对new feature可以直接编码并加入训练，无需等待编码表统计并反馈；</li>
<li>降低feature space大小，（精心设计可以）降低over-fitting的几率。</li>
</ul>
</li>
<li>缺点<ul>
<li>在不清楚hashing function细节的情况下，容易导致特征碰撞失效，且难以排查；</li>
<li>难以通过hashing出的特征反推源特征；</li>
<li>embedding会降低模型的可解释性。</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>General——各种压缩方式总结</title>
    <url>/Notes/Others/General%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E5%8E%8B%E7%BC%A9%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<hr>
<h3 id="tar-gz"><a href="#tar-gz" class="headerlink" title=".tar.gz"></a>.tar.gz</h3><h3 id="tar-bz2"><a href="#tar-bz2" class="headerlink" title=".tar.bz2"></a>.tar.bz2</h3><h3 id="tar-xz"><a href="#tar-xz" class="headerlink" title=".tar.xz"></a>.tar.xz</h3><h3 id="tgz"><a href="#tgz" class="headerlink" title=".tgz"></a>.tgz</h3>]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>General——各种包的管理总结</title>
    <url>/Notes/Others/General%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E5%8C%85%E7%9A%84%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<hr>
<h3 id="编程语言相关"><a href="#编程语言相关" class="headerlink" title="编程语言相关"></a>编程语言相关</h3><ul>
<li>参考链接: <a href="https://help.github.com/en/github/managing-packages-with-github-packages/about-github-packages#supported-clients-and-formats" target="_blank" rel="noopener">https://help.github.com/en/github/managing-packages-with-github-packages/about-github-packages#supported-clients-and-formats</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">Package client</th>
<th align="left">Language</th>
<th align="left">Package format</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">npm</td>
<td align="left">JavaScript</td>
<td align="left">package.json</td>
<td align="left">Node package manager</td>
</tr>
<tr>
<td align="left">gem</td>
<td align="left">Ruby</td>
<td align="left">Gemfile</td>
<td align="left">RubyGems package manager</td>
</tr>
<tr>
<td align="left">mvn</td>
<td align="left">Java</td>
<td align="left">pom.xml</td>
<td align="left">Apache Maven project management and comprehension tool</td>
</tr>
<tr>
<td align="left">gradle</td>
<td align="left">Java</td>
<td align="left">build.gradle or build.gradle.kts</td>
<td align="left">Gradle build automation tool for Java</td>
</tr>
<tr>
<td align="left">docker</td>
<td align="left">N/A</td>
<td align="left">Dockerfile</td>
<td align="left">Docker container management platform</td>
</tr>
<tr>
<td align="left">nuget</td>
<td align="left">.NET</td>
<td align="left">nupkg</td>
<td align="left">NuGet package management for .NET</td>
</tr>
<tr>
<td align="left">pip</td>
<td align="left">Python</td>
<td align="left">requirements.txt</td>
<td align="left">use <code>pip install -r requirements.txt</code></td>
</tr>
</tbody></table>
<hr>
<h3 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h3><ul>
<li>参考链接: <a href="https://www.iteye.com/blog/justcoding-1937171" target="_blank" rel="noopener">https://www.iteye.com/blog/justcoding-1937171</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">软件管理方式</th>
<th align="left">线下安装命令</th>
<th align="left">线上安装命令</th>
<th align="left">distribution 操作系统</th>
</tr>
</thead>
<tbody><tr>
<td align="left">RPM</td>
<td align="left">rpm, rpmbuild</td>
<td align="left">yum</td>
<td align="left">Red Hat/Fedora</td>
</tr>
<tr>
<td align="left">DPKG</td>
<td align="left">dpkg</td>
<td align="left">apt, apt-get</td>
<td align="left">Debian/Ubuntu</td>
</tr>
</tbody></table>
<hr>
<h3 id="rpm和dpkg常用命令总结"><a href="#rpm和dpkg常用命令总结" class="headerlink" title="rpm和dpkg常用命令总结"></a>rpm和dpkg常用命令总结</h3><ul>
<li>参考链接: <a href="http://cha.homeip.net/blog/archives/2005/08/rpm_vs_dpkg.html" target="_blank" rel="noopener">http://cha.homeip.net/blog/archives/2005/08/rpm_vs_dpkg.html</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">操作描述</th>
<th align="center">rpm</th>
<th align="center">dpkg</th>
</tr>
</thead>
<tbody><tr>
<td align="left">安装指定套件</td>
<td align="center">rpm -i pkgfile.rpm</td>
<td align="center">dpkg -i pkgfile.deb</td>
</tr>
<tr>
<td align="left">显示所有已安装的套件名称</td>
<td align="center">rpm -qa</td>
<td align="center">dpkg -l</td>
</tr>
<tr>
<td align="left">显示套件包含的所有档案</td>
<td align="center">rpm -ql [softwarename]</td>
<td align="center">dpkg -L [softwarename]</td>
</tr>
<tr>
<td align="left">显示特定档案所属套件名称</td>
<td align="center">rpm -qf [/path/to/file]</td>
<td align="center">dpkg -S [/path/to/file]</td>
</tr>
<tr>
<td align="left">显示制定套件是否安装</td>
<td align="center">rpm -q [softwarename]</td>
<td align="center">dpkg -l [softwarename], -s或-p显示详细咨询, -l只列出简洁咨询</td>
</tr>
<tr>
<td align="left">移除指定套件</td>
<td align="center">rpm -e [softwarename]</td>
<td align="center">dpkg -r softwarename, -r 留下套件设定, -P完全移除</td>
</tr>
</tbody></table>
<hr>
<h3 id="apt和yum常用命令总结"><a href="#apt和yum常用命令总结" class="headerlink" title="apt和yum常用命令总结"></a>apt和yum常用命令总结</h3><ul>
<li>参考博客: <a href="https://cnblogs.com/lanbosm/p/9130211.html" target="_blank" rel="noopener">https://cnblogs.com/lanbosm/p/9130211.html</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">操作描述</th>
<th align="center">yum</th>
<th align="center">apt</th>
</tr>
</thead>
<tbody><tr>
<td align="left">软件源配置文件路径</td>
<td align="center">/etc/yum.conf</td>
<td align="center">/etc/apt/sources.list</td>
</tr>
<tr>
<td align="left">安装软件包</td>
<td align="center">yum install [package]</td>
<td align="center">apt-get install [package]</td>
</tr>
<tr>
<td align="left">删除软件包</td>
<td align="center">yum uninstall [package]</td>
<td align="center">apt-get remove [package]</td>
</tr>
<tr>
<td align="left">删除有依赖关系的软件包和配置文件</td>
<td align="center">yum uninstall [package]</td>
<td align="center">apt-get autoremove [package] –purge</td>
</tr>
<tr>
<td align="left">查看安装包信息</td>
<td align="center">yum info [package]</td>
<td align="center">apt-cache show [package]</td>
</tr>
<tr>
<td align="left">更新软件包列表</td>
<td align="center">yum update</td>
<td align="center">apt-get update</td>
</tr>
<tr>
<td align="left">清空缓存</td>
<td align="center">yum clean</td>
<td align="center">apt-get clean</td>
</tr>
<tr>
<td align="left">搜索包名</td>
<td align="center">yum</td>
<td align="center">apt-cahce search</td>
</tr>
</tbody></table>
<hr>
<h3 id="一些特殊命令"><a href="#一些特殊命令" class="headerlink" title="一些特殊命令"></a>一些特殊命令</h3><h4 id="apt"><a href="#apt" class="headerlink" title="apt"></a>apt</h4><ul>
<li><p>列出所有可用包名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-cache pkgnames</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过描述列出包名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-cache search [keys]</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定包的版本号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get install [package]=[version]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h4><ul>
<li><p>搜索包的可用版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum --showduplicates list [package] | expand</span><br></pre></td></tr></table></figure>

<ul>
<li><code>expand</code>命令用于将文件的制表符<code>tab</code>转换成空格符<code>space</code><ul>
<li>默认一个<code>tab</code>对应8个<code>space</code></li>
<li>若不指定文件名(或者文件名为<code>-</code>), 则<code>expand</code>会从标准输入读取数据</li>
</ul>
</li>
<li><code>unexpand</code>命令与expand相反</li>
</ul>
</li>
<li><p>安装时指定包的版本号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install [package]-[version]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="yum和apt安装的常用参数"><a href="#yum和apt安装的常用参数" class="headerlink" title="yum和apt安装的常用参数"></a>yum和apt安装的常用参数</h4><ul>
<li><code>-y</code>: 指定在询问是否安装时均选择<code>yes</code></li>
<li><code>-q</code>: <code>quiet</code>,安装途中不打印log信息</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos——clamav安装与杀毒</title>
    <url>/Notes/Linux/Centos%E2%80%94%E2%80%94clamav%E5%AE%89%E8%A3%85%E4%B8%8E%E6%9D%80%E6%AF%92.html</url>
    <content><![CDATA[<p><em>clam是一款Linux上免费的杀毒软件</em></p>
<hr>
<h3 id="安装与配置"><a href="#安装与配置" class="headerlink" title="安装与配置"></a>安装与配置</h3><h4 id="命令行安装"><a href="#命令行安装" class="headerlink" title="命令行安装"></a>命令行安装</h4><p><em>命令行安装clamav会自动创建clamav用户和clamav组</em></p>
<ul>
<li><p>Centos</p>
</li>
<li><p>在Centos7亲测*</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install –y clamav clamav-update</span><br></pre></td></tr></table></figure>
</li>
<li><p>ubuntu</p>
</li>
<li><p>在Ubuntu16.04上亲测*</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get install clamav</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h4><p><em>如果因为某些原因无法从命令行安装,可以尝试用源码安装,此时需要首先手动创建相关组和用户</em></p>
<h5 id="安装前的配置"><a href="#安装前的配置" class="headerlink" title="安装前的配置"></a>安装前的配置</h5><ul>
<li><p>创建clamav组</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">groupadd clamav</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户并添加到clamav组</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd -g clamav clamav</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="源码下载和安装"><a href="#源码下载和安装" class="headerlink" title="源码下载和安装"></a>源码下载和安装</h5><ul>
<li><p>下载</p>
<ul>
<li>下载链接: <a href="http://www.clamav.net/downloads" target="_blank" rel="noopener">http://www.clamav.net/downloads</a>, 因为版本可能会有更新,这里我们直接给出网站下载地址,可以随时查看版本信息</li>
<li>找到软件包下载链接后,使用wget下载即可,比如<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://www.clamav.net/downloads/production/clamav-0.102.0.tar.gz</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>安装</p>
<ul>
<li><p>解压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xf clamav-0.102.0.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>切换目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd clamav-0.102.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum/apt install gcc openssl openssl-devel</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="升级病毒库"><a href="#升级病毒库" class="headerlink" title="升级病毒库"></a>升级病毒库</h4><ul>
<li><p>升级命令</p>
<ul>
<li><p>命令行安装后更新命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">freshclam</span><br></pre></td></tr></table></figure>
</li>
<li><p>源码安装后更新命令, 也可以建立软连接后直接使用上面的命令行启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/local/clamav/bin/freshclam</span><br></pre></td></tr></table></figure>
</li>
<li><p>建立链接指令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln -s [source path] [target path]</span><br></pre></td></tr></table></figure>


</li>
</ul>
</li>
</ul>
<h4 id="查找病毒文件"><a href="#查找病毒文件" class="headerlink" title="查找病毒文件"></a>查找病毒文件</h4><ul>
<li><p>常用命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup clamscan / -r --infected -l clamscan.log &gt; clamscan.out &amp;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-r</code> 指明递归查杀</li>
<li><code>--infected</code> 表示仅仅输出被感染的部分文件, 否则没有被感染的文件会输出<code>文件名: OK</code>这样无用的信息</li>
<li><code>-l</code> 指明日志文件路径</li>
<li><code>/</code> 是查找的目标目录,如果是整个机器查找则使用<code>/</code></li>
<li>由于查杀病毒需要很长时间,所以建议使用后台进程进行, 如果是远程, 建议使用<code>nohup</code></li>
<li>由于输出非常多,所以一般我们使用<code>clamscan.out</code>和<code>clamscan.log</code>分别存储输出和日志</li>
</ul>
</li>
<li><p>列出被感染文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat clamscan.out | grep FOUND</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Others</tag>
        <tag>Centos</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——COEC特征</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94COEC%E7%89%B9%E5%BE%81.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考： <a href="http://d0evi1.com/positionbias/" target="_blank" rel="noopener">http://d0evi1.com/positionbias/</a></li>
<li>COEC（Click on Expected Click）：<strong>点击与期望点击的比值</strong></li>
</ul>
<hr>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>在评价一个广告的质量好坏时，单纯使用广告的点击率作为指标是不可行的</li>
<li>广告的点击率与广告的曝光位置相关【包括slot或position引起的问题】<ul>
<li>从position上来讲，越靠前的广告越容易被点击</li>
</ul>
</li>
</ul>
<hr>
<h3 id="COEC"><a href="#COEC" class="headerlink" title="COEC"></a>COEC</h3><ul>
<li>为了抵消广告曝光位置对广告的点击率的影响，我们引入期望点击</li>
<li>在计算商家的点击率时，使用<strong>点击与期望点击的比值</strong>作为商家点击率质量的衡量指标<br>$$<br>\begin{align}<br>COEC = \frac{\sum_{i=1}^N isClick_i}{\sum_{i=1}^N expCTR_i}<br>\end{align}<br>$$<ul>
<li>\(isClick_i\)为0或1，表示真实点击情况</li>
<li>\(expCTR_i\)为广告真实曝光位置的期望点击率,不同slot和position对应的期望点击率不同</li>
</ul>
</li>
</ul>
<hr>
<h3 id="COEC特征"><a href="#COEC特征" class="headerlink" title="COEC特征"></a>COEC特征</h3><ul>
<li>在广告相关指标预估模型中，使用COEC作为广告的特征可提升模型的效果</li>
<li>由于COEC的分母上考虑了位置因素，使得COEC更能真实的反应广告实际点击率的真实质量</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>CA——术语和专有名词总结</title>
    <url>/Notes/ComputationalAdvertising/CA%E2%80%94%E2%80%94%E6%9C%AF%E8%AF%AD%E5%92%8C%E4%B8%93%E6%9C%89%E5%90%8D%E8%AF%8D%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>本文主要总结计算广告(computational advertising)中的术语和专有名词</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<p>*&lt;&lt;计算广告&gt;&gt;中后面附录有很多计算广告相关的专有名词相关记录*<br><em>本文将长期持续更新,记录平时遇到计算广告专有名词</em></p>
<hr>
<h3 id="英文简写"><a href="#英文简写" class="headerlink" title="英文简写"></a>英文简写</h3><ul>
<li>IOE计算架构: IBM, Oracle, EMC(适用于规模不大但对于一致性和实时性要求较高的场景)</li>
<li>BI: Business Intelligence(商业智能)</li>
<li>DM: Direct Marketing(直接营销)</li>
<li>ROI: Return on Investment(投入产出比 = 总产出/总投入)</li>
<li>EDM: Email Direct Marketing(邮件营销广告)</li>
<li>GD: Guaranteed Delivery(担保式投送)</li>
<li>CPM: Cost per Mille(按千次展示付费)</li>
<li>CPC: Cost per Click</li>
<li>CPS: Cost per Sale</li>
</ul>
<hr>
<h3 id="常见中英文对照"><a href="#常见中英文对照" class="headerlink" title="常见中英文对照"></a>常见中英文对照</h3><ul>
<li>付费内容: sponsored content</li>
<li>商业化: monetization</li>
<li>个性化推荐: personalized recommendation</li>
<li>直接效果广告: direct response</li>
<li>品牌广告: brand awareness</li>
<li>横幅广告: banner ad</li>
<li>交互式广告: playalble ad</li>
<li>激励广告: incentive ad</li>
<li>展示广告: display advertising</li>
<li>合约广告: agreement-based advertising</li>
<li>定向广告: targeted advertising</li>
<li>受众定向: audience targeting</li>
<li>在线分配: online allocation</li>
<li>带约束优化: constrained optimization</li>
</ul>
]]></content>
      <tags>
        <tag>CA</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP——UNILM论文阅读笔记</title>
    <url>/Notes/NLP/NLP%E2%80%94%E2%80%94UNILM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>本文介绍了MSRA今年的一篇文章: <a href="https://arxiv.org/abs/1905.03197" target="_blank" rel="noopener">UNILM: Unified Language Model Pre-training for Natural Language Understanding and Generation</a></em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="UNILM基于三个预训练目标"><a href="#UNILM基于三个预训练目标" class="headerlink" title="UNILM基于三个预训练目标"></a>UNILM基于三个预训练目标</h3><ul>
<li>Unidirectional LM: </li>
<li>Bidirectional LM: </li>
<li>Sequence2Sequence LM: </li>
</ul>
<hr>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><h4 id="和BERT对比"><a href="#和BERT对比" class="headerlink" title="和BERT对比"></a>和BERT对比</h4><ul>
<li>BERT是双向模型,所以自然语言的生成(NLG)任务上不适用</li>
<li>UNILM采用三种(无监督的)LM目标,其中的Sequence2Sequence LM能够解决文本生成问题</li>
</ul>
<h4 id="其他一些优点"><a href="#其他一些优点" class="headerlink" title="其他一些优点"></a>其他一些优点</h4><ul>
<li>仅使用一个LM(由Transformer构成), 在三个不同的预训练任务上共享参数, 泛化能力强<ul>
<li>参数共享: 不需要在不同的语言模型(对应不同的预训练任务)上使用不同的模型参数</li>
<li>泛化能力强: 多个训练目标同时优化模型,使得模型能够避开因为训练目标引起的过拟合问题</li>
</ul>
</li>
<li></li>
</ul>
<hr>
<h3 id="模型结构介绍"><a href="#模型结构介绍" class="headerlink" title="模型结构介绍"></a>模型结构介绍</h3><h4 id="结构图"><a href="#结构图" class="headerlink" title="结构图"></a>结构图</h4><img src="/Notes/NLP/NLP——UNILM论文阅读笔记/UNILM_Overview.png">
<ul>
<li>输入向量为 \(\vec{x} = (x_1, x_2,,,x_{|x|}\))</li>
<li>输入向量表征与BERT相同,由Embedding层(三个Embedding层之和, 和BERT一样):<ul>
<li>Token Embedding, WordPiece</li>
<li>Position Embedding</li>
<li>Segment Embedding, 由于UNILM会使用多个LM任务训练,所以Segment Embedding在模型中也扮演着LM标识的作用(对不同的LM目标使用不同的Segment Embeddings)</li>
</ul>
</li>
<li>主要网络(Backbone Network)为多层Transformer:<ul>
<li>每两个Transformer Blocks之间的Self-Attention Masks: For Each Transformer Block, 使用多头Self-Attention来聚合之前层出现的输入向量.(这里的Self-Attention使用了)</li>
<li>Transformer之间的Self-Attention机制决定了模型什么语言模型(任务)</li>
<li>如图所示:<ul>
<li>Bidirectional LM 对应的 Self-Attention Masks为全0的矩阵, 表示所有的Attention都不屏蔽 (attend to all tokens)</li>
<li>Left-to-Right LM 对应的 Self-Attention Masks为拼屏蔽右上三角的矩阵(左下三角全为0)</li>
<li>Right-to-Left LM 对应的 Self-Attention Masks为拼屏蔽左下三角的矩阵(右上三角全为0)</li>
<li>Seq-to-Seq LM 对应的 Self-Attention为 <ul>
<li>\(S_1\)-\(S_1\) 为全0(0表示开放), 对应为Bidirectional</li>
<li>\(S_1\)-\(S_2\) 为负无穷(负无穷表示屏蔽)</li>
<li>\(S_2\)-\(S_1\) 为全0</li>
<li>\(S_2\)-\(S_2\) 为屏蔽右上三角的矩阵(这里与Left-to-Right的情况相同)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="按照结构图分析数据流"><a href="#按照结构图分析数据流" class="headerlink" title="按照结构图分析数据流"></a>按照结构图分析数据流</h4><ul>
<li>对于一组输入向量 \(\{\vec{x}\}_{i=1}^{|x|} \), 初始编码为 \(\mathbf{H}^0 = [\vec{x}_1, \vec{x}_2,,,\vec{x}_{|\vec{x}|}]\)</li>
<li>第一层后编码为上下文表征\(\mathbf{H}^1 = [\vec{h}_1^1, \vec{h}_2^1,,,\vec{h}_{|\vec{x}|}^1]\)</li>
<li>第 \(l\) 层后编码为上下文表征\(\mathbf{H}^l = [\vec{h}_1^l, \vec{h}_2^l,,,\vec{h}_{|\vec{x}|}^l]\)</li>
<li>每一层的 Transformer Block, 使用 multiple self-attention heads去聚合上一层的输出: L-layer的 Transformer对应的数学表达式为 \(\mathbf{H}^l = Transformer_l(\mathbf{H}^{l-1}), l \in [1, L]\)</li>
<li>Self-Attention Head \(\mathbf{A}_l\)的详细计算公式如下:<br>$$<br>\begin{align}<br>\mathbf{H} = \mathbf{H}^{l-1}\mathbf{W}_l^Q \\<br>\mathbf{K} = \mathbf{H}^{l-1}\mathbf{W}_l^K \\<br>\mathbf{V} = \mathbf{H}^{l-1}\mathbf{W}_l^V \\<br>\mathbf{A} = softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}} + \mathbf{M})\mathbf{V}_l \\<br>where, \quad \mathbf{M}_{ij} = 0 \ or \ -\infty<br>\end{align}<br>$$<ul>
<li>\(\mathbf{M}\)中的值<ul>
<li>\(-\infty\) 表示屏蔽Attention (prevent from Attention)</li>
<li>0 表示允许 Attention (allow to Attention)</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——QR-DQN</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94QR-DQN.html</url>
    <content><![CDATA[<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/40681570" target="_blank" rel="noopener">用 Quantile Regression 分析变量相关性</a></li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/138091493" target="_blank" rel="noopener">【DRL-7】Distributional DQN: Quantile Regression-DQN</a></li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——混合精度训练</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83.html</url>
    <content><![CDATA[<p><em>混合精度 (Automatically Mixed Precision, AMP)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h3 id="数字类型"><a href="#数字类型" class="headerlink" title="数字类型"></a>数字类型</h3><h4 id="FP32"><a href="#FP32" class="headerlink" title="FP32"></a>FP32</h4><ul>
<li>FP32，Single-precision floating-point</li>
<li>4B, 32位，符号位1位，指数位8位，尾数位23位<ul>
<li>符号位：用于表示数值的正负。</li>
<li>指数位：用于表示数值的范围。</li>
<li>尾数位（Fraction）：也称为小数位，用于表示数值的精度。</li>
</ul>
</li>
<li>数字表达范围：\([-3e^{38},-1e^{-45}] \bigcup [1e^{-45}, 3e^{38}]\)</li>
<li>下溢精度：\(1e^{-38}\)</li>
</ul>
<h4 id="FP16"><a href="#FP16" class="headerlink" title="FP16"></a>FP16</h4><ul>
<li>FP16，Half-precision floating-point</li>
<li>2B, 16位，符号位1位，指数位5位，尾数位10位</li>
<li>数字表达范围：\([-65504,-5.9e^{-8}] \bigcup [5.9e^{-8}, 65504]\)</li>
<li>下溢精度：\(5.9e^{-8}\)</li>
</ul>
<h4 id="BF16"><a href="#BF16" class="headerlink" title="BF16"></a>BF16</h4><ul>
<li>BF16，Brain Floating Point</li>
<li>2B，16位，符号位1位，指数位8位，尾数位7位</li>
<li>数字表达范围：\([−3e^{38}, -9.2^{-41}],[9.2^{-41}, 3e^{38}]\)</li>
<li>下溢精度：\(9.2^{-41}\)</li>
</ul>
<h4 id="FP32-vs-FP16"><a href="#FP32-vs-FP16" class="headerlink" title="FP32 vs FP16"></a>FP32 vs FP16</h4><ul>
<li>能表达的数字范围和精度远小于FP32</li>
<li>浮点数都有个上下溢问题：<ul>
<li>上/下溢出：FP16 的表示范围不大，非常容易溢出</li>
<li>超过\(6.5e^4\)的数字会上溢出变成 inf，小于\(5.9e^{-8}\)的数字会下溢出变成 0</li>
</ul>
</li>
</ul>
<h4 id="FP16-vs-BF16"><a href="#FP16-vs-BF16" class="headerlink" title="FP16 vs BF16"></a>FP16 vs BF16</h4><ul>
<li>都是2B存储</li>
<li>BF16指数位更多：可以表示更大范围的数值</li>
<li>FP16尾数位更多：可以表示更精确的数值，如果都是小数值，用FP16更好<ul>
<li>注意：下溢精度不等于精度，下溢精度与指数关系更大</li>
</ul>
</li>
</ul>
<h3 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h3><ul>
<li>最早论文：<a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Mixed precision training</a><ul>
<li>作者：百度，英伟达</li>
<li>一次迭代过程<img src="/Notes/DL/DL——混合精度训练/mixed-precision-training-iteration-for-a-layer.png"></li>
<li>基本思路：保持原始参数还是fp32的情况下，将计算梯度等所有流程都使用fp16进行，节省内存/显存的同时提升训练速度</li>
<li>fp16会损失精度，所以在过程中需要用到scaling操作</li>
</ul>
</li>
<li>混合精度训练的优点<ul>
<li>减少显存占用：FP16 的显存占用只有 FP32 的一半，这使得我们可以用更大的 batch size；<ul>
<li>混合精度训练下，需要存储的变量为：FP16的梯度，FP16的参数，FP32的参数；好像并没有减少显存啊？</li>
</ul>
</li>
<li>加速训练：使用 FP16，模型的训练速度几乎可以提升 1 倍。</li>
</ul>
</li>
<li>FP16的下溢值这么大，梯度一般都很小，为什么能存储梯度？<ul>
<li>通过loss scaling技术，对loss进行缩放（放大很多倍）可以确保梯度不会下溢，在更新时转换成FP32再unscale回去即可（需要FP32）</li>
</ul>
</li>
<li>FP32有什么用？（为什么不能只使用fp16呢？）<ul>
<li>防止FP16导致误差过大：将模型权重、激活值、梯度等数据用 FP16 来存储，同时维护一份 FP32 的模型权重副本用于更新。在反向传播得到 FP16 的梯度以后，将其转化成 FP32 并 unscale，最后更新 FP32 的模型权重。因为整个更新过程是在 FP32 的环境中进行的，所以不会出现舍入误差。 </li>
</ul>
</li>
<li>为了节省存储和加快训练速度，特别是大模型时代，越来越重要</li>
</ul>
<h3 id="混合精度的使用"><a href="#混合精度的使用" class="headerlink" title="混合精度的使用"></a>混合精度的使用</h3><ul>
<li>更新最新参考资料：<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247550159&idx=5&sn=f5db2afa547970bc429112e32d2e7daf&chksm=ebb73c1bdcc0b50d0e85039bd5d8349a23330e3e0f138a7dd2da218a20174d0965837682dd14&scene=27" target="_blank" rel="noopener">由浅入深的混合精度训练教程</a></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>IDEA——lombok包使用说明</title>
    <url>/Notes/Java/IDEA%E2%80%94%E2%80%94lombok%E5%8C%85%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html</url>
    <content><![CDATA[<p><em>参考链接：<a href="https://blog.csdn.net/sunayn/article/details/85252507" target="_blank" rel="noopener">https://blog.csdn.net/sunayn/article/details/85252507</a></em></p>
<ul>
<li>在Java使用lombok包提供Setter和Getter等注解可以简化编程</li>
</ul>
<hr>
<h3 id="lombok包使用流程"><a href="#lombok包使用流程" class="headerlink" title="lombok包使用流程"></a>lombok包使用流程</h3><ul>
<li><p>在pom中导入lombok包依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--lombok 注解--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.projectlombok&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;lombok&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;1.16.10&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用注解</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.example.springboot.model;</span><br><span class="line"> </span><br><span class="line">import lombok.Getter;</span><br><span class="line">import lombok.Setter;</span><br><span class="line"> </span><br><span class="line">import java.util.Date;</span><br><span class="line"> </span><br><span class="line">@Setter</span><br><span class="line">@Getter</span><br><span class="line">public class User &#123;</span><br><span class="line">    private Integer id;</span><br><span class="line"> </span><br><span class="line">    private String name;</span><br><span class="line"> </span><br><span class="line">    private Integer age;</span><br><span class="line"> </span><br><span class="line">    private String sex;</span><br><span class="line"> </span><br><span class="line">    private Date birthday;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>上面的例子是为整个类的属性加上了注解，但更多的使用时针对类里面的某个属性单独添加这@Setter和@Getter注解</li>
</ul>
</li>
</ul>
<h3 id="可能存在的问题"><a href="#可能存在的问题" class="headerlink" title="可能存在的问题"></a>可能存在的问题</h3><h4 id="IDEA中无法识别注解"><a href="#IDEA中无法识别注解" class="headerlink" title="IDEA中无法识别注解"></a>IDEA中无法识别注解</h4><ul>
<li>这可能导致代码找不到Getter和Setter方法而报错</li>
<li>此时IDEA需要安装lombok插件才能正常使用</li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——样本不均衡问题</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<hr>
<h3 id="关于样本不均衡的解决"><a href="#关于样本不均衡的解决" class="headerlink" title="关于样本不均衡的解决"></a>关于样本不均衡的解决</h3><ul>
<li>相关实践: 某次比赛中, 训练集中正负样本数量分别为20663和569877,占比分别为3.499%和96.501%, 差别很大</li>
</ul>
<h4 id="基于数据的解决方案"><a href="#基于数据的解决方案" class="headerlink" title="基于数据的解决方案"></a>基于数据的解决方案</h4><h5 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h5><ul>
<li>增加负样本的数量<ul>
<li>简单复制样本多次: 容易过拟合,模型变得复杂</li>
<li>使用<strong>SMOTE</strong>算法采样: 对少数样本集\(S_{min}\)中的每一个样本 \(x\),从他的K近邻中随机选取一个样本\(y\), 然后在 \(x, y\) 之间随机选取一个新的点作为新合成的样本<ul>
<li>能降低过拟合风险</li>
<li>需要一个样本间距离定义的函数,且对于少数样本多时选取最近邻的复杂度太大</li>
<li>由于为每个样本都采样了,所以可能会增大类间重叠度(生成的样本可能是符合多数样本类别的,很容易造成生成没有意义甚至是噪声)</li>
</ul>
</li>
<li>使用<strong>Borderline-SMOTE</strong>算法优化<strong>SMOTE</strong>算法: <ul>
<li>只在分类边界上的少数样本进行采样</li>
</ul>
</li>
<li>使用<strong>ADASYN</strong>算法优化<strong>SMOTE</strong>算法:<ul>
<li>给不同的少数类样本合成不同个数的新样本</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h5><ul>
<li>减少正样本数量,让分类器更重视负样本<ul>
<li>Easy Ensemble算法: 每次从 \(S_{maj}\) 中随机抽取子集 \(E\), 然后用 \(E + S_{min}\), 最后融合多个分类模型的结果<ul>
<li>融合模型时简单的可以对模型输出做均值(评估方式为AUC分数)或者投票(评估方式为分类精度)</li>
</ul>
</li>
<li>Balance Cascade算法: 级联结构, 在每一级中从 \(S_{maj}\) 中随机抽取子集 \(E\), 然后用 \(E + S_{min}\) 训练当前级的分类其,然后将 \(S_{maj}\) 能正确被当前分类器正确分类的样本剔除, 剩下不能正确分类的样本进行下一级操作,重复若干次后得到级联结构,最终的输出结果是各级分类器结果的融合<ul>
<li>这里有点像是Boosting方法,对样本权重进行修改,使得模型更重视上次分类错误的样本</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="基于算法的解决方案"><a href="#基于算法的解决方案" class="headerlink" title="基于算法的解决方案"></a>基于算法的解决方案</h4><ul>
<li>修改模型的训练<strong>目标函数</strong><ul>
<li>比如使用 AUC_ROC 的负数作为损失函数</li>
<li>深度学习中可以使用Focal Loss代替传统的SGD</li>
</ul>
</li>
<li>也可以将问题转化为<strong>基于单类学习的异常检测</strong>问题</li>
<li><strong>单类学习</strong>(One Class Learning), <strong>异常检测</strong>(Anomaly Detection) *<ul>
<li><strong>单类学习</strong>(One Class Learning): 训练数据只有一个类别,学习一个能够远离这个类别样本的Boundary,比如<ul>
<li>单类别SVM(One Class SVM)</li>
<li>K近邻非参数方法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="一个重要的特殊说明"><a href="#一个重要的特殊说明" class="headerlink" title="一个重要的特殊说明"></a>一个重要的特殊说明</h3><ul>
<li>在深度神经网络中,如果样本类别不平衡,不要使用BN, 否则会出现问题</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>KG——图嵌入综述</title>
    <url>/Notes/KnowledgeGraph/KG%E2%80%94%E2%80%94%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%BB%BC%E8%BF%B0.html</url>
    <content><![CDATA[<p><em>本文主要介绍图嵌入(Graph Embedding)的发展和方法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="图-Graph"><a href="#图-Graph" class="headerlink" title="图(Graph)"></a>图(Graph)</h3><ul>
<li>结点和边的集合</li>
<li>经典图论中的图, 知识图谱, 概率图模型中的图等</li>
<li>传统的图算法包括(图论中的):<ul>
<li>最小生成树算法: Prim算法, Kruskal算法等</li>
<li>最短路径算法: Dijkstra算法, Floyed算法等</li>
</ul>
</li>
<li>图神经网络算法包括<ul>
<li><strong>图嵌入</strong>(Graph Embedding): 基于<strong>随机游走</strong>(Random Walk)生成路径</li>
<li><strong>图卷积神经网络</strong>(Graph CNN, GCN): 基于<strong>邻居汇聚</strong>实现</li>
</ul>
</li>
</ul>
<hr>
<h3 id="图嵌入"><a href="#图嵌入" class="headerlink" title="图嵌入"></a>图嵌入</h3><ul>
<li>用低维的向量来表示结点, 同时要求任意在图中相似的节点在向量空间中也接近.</li>
<li>得到的节点的向量表示可以用来解决节点分类等下游任务</li>
</ul>
<hr>
<h3 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h3><ul>
<li>论文链接: <em><a href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf" target="_blank" rel="noopener">DeepWalk KDD 2014</a></em></li>
<li>核心思想: 通过将游走路径(walks)当做句子(sentences), 用从截断随机游走(truncated random walks)中得到的局部信息来学习隐式表示<ul>
<li>类似于Word2Vec, node对应word, walks对应sentence</li>
</ul>
</li>
<li>核心方法: <ul>
<li>随机游走方法进行采样</li>
<li>使用Skip-Gram方法训练采样的样本</li>
</ul>
</li>
<li>实验结果:<ul>
<li>论文中展示了DeepWalk在多个多标签网络分类任务中的隐式表示, 比如BlogCatalog, Flickr和YouTube</li>
<li>在某些实验中,仅仅用60%的训练数据即可到达(超过)所有Baseline方法</li>
<li>在稀疏标记数据上F1分数表现良好</li>
</ul>
</li>
</ul>
<h4 id="随机游走"><a href="#随机游走" class="headerlink" title="随机游走"></a>随机游走</h4><ul>
<li>随机游走方法: 一种可重复访问(有放回采样)的深度优先遍历算法(DFS)<ul>
<li>给定起始访问节点A</li>
<li>从A的邻居中随机采样一个节点B作为下一个节点</li>
<li>从B的邻居中随机采样一个节点C作为下一个节点</li>
<li>….</li>
<li>直到序列长度满足truncated条件, 得到一个walk</li>
</ul>
</li>
</ul>
<h4 id="Skip-Gram-训练"><a href="#Skip-Gram-训练" class="headerlink" title="Skip-Gram 训练"></a>Skip-Gram 训练</h4><ul>
<li>对随机游走采样到的数据进行Skip-Gram训练</li>
<li>最终得到每个节点的表示向量</li>
</ul>
<hr>
<h3 id="Node2Vec"><a href="#Node2Vec" class="headerlink" title="Node2Vec"></a>Node2Vec</h3><ul>
<li>论文链接: <em><a href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf" target="_blank" rel="noopener">Node2Vec KDD 2016</a></em></li>
<li>核心思想: 综合考虑DFS和BFS的图嵌入方法, 可以视为DeepWalk的一种扩展(DeepWalk的随机游走仅仅是考虑DFS的,不考虑BFS)<img src="/Notes/KnowledgeGraph/KG——图嵌入综述/DFS_BFS_for_Node2Vec.png">

</li>
</ul>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><ul>
<li><p>Node2Vec要解决的问题: 找到一种从节点到embedding向量的映射函数\(f\), 最大化整体后验概率(乘积). 每个节点的后验概率为: 给定某个节点, 相邻节点出现的概率 \(Pr(N_S(u)|f(u))\)<br>$$<br>\begin{align}<br>\max_f\sum_{u \in V} log Pr(N_S(u)|f(u))<br>\end{align}<br>$$</p>
</li>
<li><p>为了简化上述问题,作者引入两个假设</p>
<ul>
<li><strong>条件独立性</strong>(Conditional independence):<br>$$<br>\begin{align}<br>Pr(N_S(u)|f(u)) = \prod_{n_i \in N_S(u)} Pr(n_i|f(u))<br>\end{align}<br>$$</li>
<li><strong>特征空间中的对称性</strong>(Symmetry in feature space): 假设源节点和邻居节点在特征空间中有对称<br>$$<br>\begin{align}<br>Pr(n_i|f(u)) = \frac{exp(f(n_i)\cdot f(u))}{\prod_{v\in V}exp(f(v)\cdot f(u))}<br>\end{align}<br>$$<ul>
<li>本质上表达的是: <strong>一个节点作为源节点或者邻近节点时都使用同一个特征向量表示</strong></li>
<li>理解: 特征向量的点乘(内积)表示两个点之间的关联程度?</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>Bash——Shell中的环境变量高级解析方法</title>
    <url>/Notes/Linux/Bash%E2%80%94%E2%80%94Shell%E4%B8%AD%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%AB%98%E7%BA%A7%E8%A7%A3%E6%9E%90%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<p><em>参考链接：<a href="https://www.cnblogs.com/flintlovesam/p/6677037.html" target="_blank" rel="noopener">https://www.cnblogs.com/flintlovesam/p/6677037.html</a></em></p>
<h3 id="Shell中的-、-和-使用范例"><a href="#Shell中的-、-和-使用范例" class="headerlink" title="Shell中的${}、##和%%使用范例"></a>Shell中的${}、##和%%使用范例</h3><ul>
<li><p>假设我们定义了一个变量为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">file=/dir1/dir2/dir3/my.file.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以用${ }分别替换得到不同的值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;file#*/&#125;：删掉第一个 / 及其左边的字符串：dir1/dir2/dir3/my.file.txt</span><br><span class="line">$&#123;file##*/&#125;：删掉最后一个 /  及其左边的字符串：my.file.txt</span><br><span class="line">$&#123;file#*.&#125;：删掉第一个 .  及其左边的字符串：file.txt</span><br><span class="line">$&#123;file##*.&#125;：删掉最后一个 .  及其左边的字符串：txt</span><br><span class="line">$&#123;file%/*&#125;：删掉最后一个  /  及其右边的字符串：/dir1/dir2/dir3</span><br><span class="line">$&#123;file%%/*&#125;：删掉第一个 /  及其右边的字符串：(空值)</span><br><span class="line">$&#123;file%.*&#125;：删掉最后一个  .  及其右边的字符串：/dir1/dir2/dir3/my.file</span><br><span class="line">$&#123;file%%.*&#125;：删掉第一个  .   及其右边的字符串：/dir1/dir2/dir3/my</span><br></pre></td></tr></table></figure>
</li>
<li><p>记忆的方法为：</p>
<ul>
<li><code>%</code>和<code>#</code>分别在<code>$</code>的右边和左边</li>
<li><code>#</code>是去掉左边（键盘上<code>#</code>在 <code>$</code> 的左边）</li>
<li><code>%</code>是去掉右边（键盘上<code>%</code> 在<code>$</code> 的右边）</li>
<li>单一符号是最小匹配；两个符号是最大匹配<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;file:0:5&#125;：提取最左边的 5 个字节：/dir1</span><br><span class="line">$&#123;file:5:5&#125;：提取第 5 个字节右边的连续5个字节：/dir2</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>也可以对变量值里的字符串作替换：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;file/dir/path&#125;：将第一个dir 替换为path：/path1/dir2/dir3/my.file.txt</span><br><span class="line">$&#123;file//dir/path&#125;：将全部dir 替换为 path：/path1/path2/path3/my.file.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>利用 ${ } 还可针对不同的变数状态赋值(沒设定、空值、非空值)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;file-my.file.txt&#125; ：假如 $file 沒有设定，則使用 my.file.txt 作传回值。(空值及非空值時不作处理)</span><br><span class="line">$&#123;file:-my.file.txt&#125; ：假如 $file 沒有設定或為空值，則使用 my.file.txt 作傳回值。 (非空值時不作处理)</span><br><span class="line">$&#123;file+my.file.txt&#125; ：假如 $file 設為空值或非空值，均使用 my.file.txt 作傳回值。(沒設定時不作处理)</span><br><span class="line">$&#123;file:+my.file.txt&#125; ：若 $file 為非空值，則使用 my.file.txt 作傳回值。 (沒設定及空值時不作处理)</span><br><span class="line">$&#123;file=my.file.txt&#125; ：若 $file 沒設定，則使用 my.file.txt 作傳回值，同時將 $file 賦值為 my.file.txt 。 (空值及非空值時不作处理)</span><br><span class="line">$&#123;file:=my.file.txt&#125; ：若 $file 沒設定或為空值，則使用 my.file.txt 作傳回值，同時將 $file 賦值為my.file.txt 。 (非空值時不作处理)</span><br><span class="line">$&#123;file?my.file.txt&#125; ：若 $file 沒設定，則將 my.file.txt 輸出至 STDERR。 (空值及非空值時不作处理)</span><br><span class="line">$&#123;file:?my.file.txt&#125; ：若 $file 没设定或为空值，则将 my.file.txt 输出至 STDERR。 (非空值時不作处理)</span><br><span class="line">$&#123;#var&#125; 可计算出变量值的长度：</span><br><span class="line">$&#123;#file&#125; 可得到 27 ，因为/dir1/dir2/dir3/my.file.txt 是27个字节</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>KG——Neo4j使用笔记</title>
    <url>/Notes/KnowledgeGraph/KG%E2%80%94%E2%80%94Neo4j%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>本文主要记录Neo4j使用过程中的遇到的问题和解决方案等</em></p>
<hr>
<h3 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ul>
<li>安装Neo4j前,一般需要安装Java环境, Ubuntu自带的版本不行的话需要重新下载安装新的版本并设置环境变量</li>
</ul>
<h5 id="自动安装"><a href="#自动安装" class="headerlink" title="自动安装"></a>自动安装</h5><ul>
<li>Ubuntu上安装和卸载命令<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install neo4j</span><br><span class="line">sudo apt-get remove neo4j</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="手动安装"><a href="#手动安装" class="headerlink" title="手动安装"></a>手动安装</h5><ul>
<li>下载neo4j已经编译好的文件, 官网为<a href="http://www.neo4j.com/" target="_blank" rel="noopener">http://www.neo4j.com/</a><ul>
<li>不要直接点download neo4j, 从官网选中products-&gt;neo4j database-&gt;拉到最下面选择Community Edition_&gt;选择对应的release版本(.tar文件)下载即可</li>
</ul>
</li>
<li>将下载到的.tar文件解压到安装目录中(一般选择/usr/local/neo4j/)</li>
<li>现已经安装成功,直接进入安装目录即可使用<code>./bin/neo4j console</code>启动neo4j数据库</li>
<li>初始的账户和密码都是neo4j, 第一次登录需要重新设置密码</li>
</ul>
<h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><ul>
<li><p>使用console启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/neo4j console</span><br></pre></td></tr></table></figure>

<ul>
<li>terminal将输出实时运行log信息,关闭terminal,neo4j数据库随之关闭</li>
<li>此时用其他电脑不能访问,只能本地电脑localhost:7474/browser访问</li>
</ul>
</li>
<li><p>查看启动状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/neo4j status</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/neo4j start</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/neo4j stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>默认访问端口: 7474, 比如本地访问网址为: localhost:7474/browser</p>
</li>
</ul>
<hr>
<h3 id="Neo4J导入Turtle文件"><a href="#Neo4J导入Turtle文件" class="headerlink" title="Neo4J导入Turtle文件"></a>Neo4J导入Turtle文件</h3><h4 id="Turtle简介"><a href="#Turtle简介" class="headerlink" title="Turtle简介"></a>Turtle简介</h4><p><em>参考博客: <a href="https://blog.csdn.net/u011801161/article/details/78833958" target="_blank" rel="noopener">https://blog.csdn.net/u011801161/article/details/78833958</a></em></p>
<ul>
<li>Turtle是最常用的RDF序列化方式, 比RDF/XML更紧凑, 可读性比N-Triples更好</li>
<li>其他序列化方式包括:<ul>
<li>RDF/XML: 用XML格式来表示RDF数据</li>
<li>N-Triples: 用多个三元组来表示RDF数据集合,是最直观的表示方法,每一行表示一个三元组,方便机器解析和处理,<a href="https://wiki.dbpedia.org/" target="_blank" rel="noopener">DBpedia</a> 是按照这个方式来发布数据的</li>
<li>RDFa: (The Resource Description Framework in Attributes)</li>
<li>JSON-LD</li>
</ul>
</li>
</ul>
<h4 id="安装neosemantics插件"><a href="#安装neosemantics插件" class="headerlink" title="安装neosemantics插件"></a>安装neosemantics插件</h4><p><em>注意,这里要求neo4j安装方式是手动安装的,(自动安装的neo4j本人找不到<neo_home>/plugins目录),手动安装方式参考前面的安装流程</neo_home></em></p>
<ul>
<li><p>下载插件release版本, 项目地址: <a href="https://github.com/neo4j-labs/neosemantics" target="_blank" rel="noopener">https://github.com/neo4j-labs/neosemantics</a></p>
<ul>
<li>注意,下载时一定要查看版本与已经安装的neo4j数据库是否兼容,否则可能造成运行时异常,或者找不到方法名等</li>
</ul>
</li>
<li><p>按照项目中的README.md安装插件</p>
<ul>
<li><p>复制release版本到<neo_home>/plugins目录下</neo_home></p>
</li>
<li><p>修改<neo_home>/conf/neo4j.conf文件,添加一行</neo_home></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dbms.unmanaged_extension_classes=semantics.extension=/rdf</span><br></pre></td></tr></table></figure>
</li>
<li><p>重启neo4j服务器</p>
</li>
<li><p>使用<code>call dbms.procedures()</code>测试是否安装成功</p>
</li>
</ul>
</li>
</ul>
<h4 id="导入Turtle文件"><a href="#导入Turtle文件" class="headerlink" title="导入Turtle文件"></a>导入Turtle文件</h4><ul>
<li><p>导入云端文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CALL semantics.importRDF(&quot;https://raw.githubusercontent.com/jbarrasa/neosemantics/3.5/docs/rdf/nsmntx.ttl&quot;,&quot;Turtle&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入本地文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CALL semantics.importRDF(&quot;file:///home/jiahong/neosemantics/3.5/docs/rdf/nsmntx.ttl&quot;,&quot;Turtle&quot;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="彻底清空Neo4J数据库"><a href="#彻底清空Neo4J数据库" class="headerlink" title="彻底清空Neo4J数据库"></a>彻底清空Neo4J数据库</h3><p><em>参考链接: <a href="https://blog.csdn.net/u012485480/article/details/83088818" target="_blank" rel="noopener">https://blog.csdn.net/u012485480/article/details/83088818</a></em></p>
<h4 id="使用Cypher语句"><a href="#使用Cypher语句" class="headerlink" title="使用Cypher语句"></a>使用Cypher语句</h4><ul>
<li>直接使用下面的Cypher语句即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (n) detach delete n</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ul>
<li>无需操作文件</li>
<li>无需停止和启动服务</li>
<li>对于数据量大的情况下删除很慢(需要先查询再删除,内存可能会溢出)</li>
</ul>
<h4 id="删除数据库文件"><a href="#删除数据库文件" class="headerlink" title="删除数据库文件"></a>删除数据库文件</h4><ul>
<li><p>停止neo4j服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ./bin/neo4j stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除<code>./data/databases/graph.db</code>目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rm -rf ./data/database/graph.db</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动neo4j服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ./bin/neo4j start</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul>
<li>需要删除文件操作</li>
<li>需要停止和启动服务</li>
<li>对于数据量大的情况下删除速度也非常快速</li>
</ul>
<hr>
<h3 id="Neo4j约束"><a href="#Neo4j约束" class="headerlink" title="Neo4j约束"></a>Neo4j约束</h3><h4 id="查看当前数据库中的所有约束"><a href="#查看当前数据库中的所有约束" class="headerlink" title="查看当前数据库中的所有约束"></a>查看当前数据库中的所有约束</h4><ul>
<li>Cypher查询语句CQL<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:schema</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="创建约束"><a href="#创建约束" class="headerlink" title="创建约束"></a>创建约束</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create constraint on (p:Person) assert p.name is unique</span><br></pre></td></tr></table></figure>

<h4 id="删除约束"><a href="#删除约束" class="headerlink" title="删除约束"></a>删除约束</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">drop constraint on (p:Person) assert p.name is unique</span><br></pre></td></tr></table></figure>

<h4 id="节点的唯一性约束"><a href="#节点的唯一性约束" class="headerlink" title="节点的唯一性约束"></a>节点的唯一性约束</h4><ul>
<li><p>为某个标签在某个属性上创建唯一性约束</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create constraint on (p:Person) assert p.name is unique</span><br></pre></td></tr></table></figure>

<ul>
<li>创建唯一性约束后会自动为该标签对应的属性创建索引Index</li>
<li>这里的索引为<code>ON :Person(name) ONLINE (for uniqueness constraint)</code></li>
<li>理解,因为要确保唯一性,所以需要索引加快每次插入节点前检索的效率</li>
<li>手动创建索引(为了加快某个标签的某个属性的检索效率)的方法为:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create index on :Person(name)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>唯一性约束设置后,当写入重复的数据时,会报错</p>
<blockquote>
<p>Neo.ClientError.Schema.ConstraintValidationFailed<br>Node(19718935) already exists with label <code>Person</code> and property <code>name</code> = ‘Joe’</p>
</blockquote>
</li>
</ul>
<hr>
<h3 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h3><h4 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h4><ul>
<li><p>创建结点方式如下,其中p</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create (p:Person&#123;name:&apos;Joe&apos;&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>上面的句子创建了一个结点</p>
</li>
<li><p>结点标签为: Person</p>
<ul>
<li>如果之前没有Person标签则新建Person标签</li>
<li>如果没有添加索引,那么这个标签在所有Person标签的节点都被删除后也会自动消失</li>
<li>如果添加了索引,则删除所有结点和相关索引后该标签会自动消失</li>
</ul>
</li>
<li><p>结点名称为: p</p>
<ul>
<li>p本质上在这里是一个变量</li>
<li>如果当前执行语句中对当前结点没有更多操作, 甚至可以省略节点名称p<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create (:Person&#123;name: &apos;Joe&apos;&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>结点属性<code>name</code>的值为: ‘Joe’</p>
<ul>
<li>这个属性很有用, 可以在显示结点的时候直接在结点中显示出来”Joe”, 方便查看</li>
<li>测试: 换成其他属性,比如属性<code>a</code>后, 在Neo4j可视化结点时是不显示的</li>
<li><code>name</code>本身也可以省略<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create (:Person)</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="删除结点"><a href="#删除结点" class="headerlink" title="删除结点"></a>删除结点</h4><ul>
<li>删除标签为Person且名字为”Joe”的所有结点<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (p:Person&#123;name:&apos;Joe&apos;&#125;) delete p</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="标签操作"><a href="#标签操作" class="headerlink" title="标签操作"></a>标签操作</h3><ul>
<li>Neo4j中一般为节点创建一个标签即可,通常一些标准的知识图谱还会为同一个节点创建多个标签,说明这个节点属于多个标签</li>
<li>节点的标签数量可以为0个,1个或多个</li>
<li>没有标签的结点可通过id获取到<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (n) where id(n)=&lt;node-id&gt; return n</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="直接创建标签"><a href="#直接创建标签" class="headerlink" title="直接创建标签"></a>直接创建标签</h4><ul>
<li><p>单个标签创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create (&lt;node-name&gt;:&lt;label-name&gt;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>多个标签创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create (&lt;node-name&gt;:&lt;label-name1&gt;:&lt;label-name2&gt;:...:&lt;label-nameN&gt;)</span><br></pre></td></tr></table></figure>

<ul>
<li>从很多知识图谱的例子来看,标签之间并不是完全的从属关系</li>
<li>从属关系: Person:Student</li>
<li>并列关系: Man:Student</li>
</ul>
</li>
</ul>
<h4 id="给已有的节点添加标签"><a href="#给已有的节点添加标签" class="headerlink" title="给已有的节点添加标签"></a>给已有的节点添加标签</h4><ul>
<li>使用<code>set</code>关键字添加标签标签<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (p:Person&#123;name:&apos;Joe&apos;&#125;) set p:Student</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="移除已有结点的标签"><a href="#移除已有结点的标签" class="headerlink" title="移除已有结点的标签"></a>移除已有结点的标签</h4><ul>
<li>使用<code>remove</code>关键字删除标签<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (p:Person&#123;name:&apos;Joe&apos;&#125;) remove p:Student</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="属性操作"><a href="#属性操作" class="headerlink" title="属性操作"></a>属性操作</h3><ul>
<li>属性操作与标签操作类似, 使用的也是<code>REMOVE</code>和<code>SET</code>指令</li>
</ul>
<h4 id="直接创建属性"><a href="#直接创建属性" class="headerlink" title="直接创建属性"></a>直接创建属性</h4><ul>
<li>使用<code>CREATE</code>指令<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create (p:Person&#123;name:&apos;Joe&apos;&#125;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="添加属性"><a href="#添加属性" class="headerlink" title="添加属性"></a>添加属性</h4><ul>
<li>使用<code>SET</code>指令<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (p:Person&#123;name:&quot;Joe&quot;&#125;) set p.sex=&quot;male&quot;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="移除属性"><a href="#移除属性" class="headerlink" title="移除属性"></a>移除属性</h4><ul>
<li>使用<code>REMOVE</code>指令<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">match (p:Person&#123;name:&quot;Joe&quot;&#125;) remove p.sex</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="各种操作命令总结"><a href="#各种操作命令总结" class="headerlink" title="各种操作命令总结"></a>各种操作命令总结</h3><ul>
<li><code>DELETE</code>和<code>CREATE</code>指令用于删除节点和关联关系</li>
<li><code>REMOVE</code>和<code>SET</code>指令用于删除标签和属性</li>
</ul>
<hr>
<h3 id="Neo4j同时创建多个数据库"><a href="#Neo4j同时创建多个数据库" class="headerlink" title="Neo4j同时创建多个数据库"></a>Neo4j同时创建多个数据库</h3><ul>
<li>Neo4j中无法同时创建多个数据库,但是我们可以通过硬性和软性的方法分别实现等价功能</li>
</ul>
<h4 id="硬件上实现多个数据库"><a href="#硬件上实现多个数据库" class="headerlink" title="硬件上实现多个数据库"></a>硬件上实现多个数据库</h4><ul>
<li>Neo4j的数据库文件为<code>./data/databases/graph.db</code><ul>
<li>我们可以手动修改该文件的名称,然后重新创建文件实现</li>
</ul>
</li>
<li>Neo4j的数据库配置文件为<code>./conf/neo4j.conf</code><ul>
<li>可以修改<code>#dbms.active_database=graph.db</code></li>
<li>修改方法为将注释取消并且修改数据库为对应的数据库名称</li>
</ul>
</li>
</ul>
<h4 id="软件上实现多个数据库"><a href="#软件上实现多个数据库" class="headerlink" title="软件上实现多个数据库"></a>软件上实现多个数据库</h4><ul>
<li>为不同数据库的每个结点分别指定同一个数据库名称对应的标签<ul>
<li>比如”Docker”和”School”分别对应Docker知识图谱和学校知识图谱</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Py2neo中结点如何被图识别"><a href="#Py2neo中结点如何被图识别" class="headerlink" title="Py2neo中结点如何被图识别?"></a>Py2neo中结点如何被图识别?</h3><ul>
<li>每个Python结点对象都有个唯一的标识符ID<ul>
<li>对应属性为<code>identity</code></li>
<li>对于从Graph中读出的结点,该属性为一个唯一的数值,与图数据库中结点的数值一致</li>
<li>对于Python直接初始化的结点对象,该属性是<code>None</code></li>
</ul>
</li>
<li>只要<code>identity</code>属性指定了,其他属性与数据库中的结点不同也可以的<ul>
<li>使用<code>Graph.push(local_node)</code>可以把本地结点更新到远处数据库中</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Neo4j和JVM版本兼容问题"><a href="#Neo4j和JVM版本兼容问题" class="headerlink" title="Neo4j和JVM版本兼容问题"></a>Neo4j和JVM版本兼容问题</h3><h4 id="报错"><a href="#报错" class="headerlink" title="报错:"></a>报错:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ./bin/neo4j start</span><br></pre></td></tr></table></figure>

<blockquote>
<p>ERROR! Neo4j cannot be started using java version 1.8.0_222. </p>
</blockquote>
<ul>
<li>Please use Oracle(R) Java(TM) 11, OpenJDK(TM) 11 to run Neo4j.</li>
<li>Please see <a href="https://neo4j.com/docs/" target="_blank" rel="noopener">https://neo4j.com/docs/</a> for Neo4j installation instructions.</li>
</ul>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li><p>安装对应版本的Java虚拟机(这里不会修改操作系统中原来的JAVA_HOME)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum search jdk</span><br><span class="line">sudo yum install java-11-openjdk</span><br></pre></td></tr></table></figure>
</li>
<li><p>将对应的JAVA_HONE配置到Neo4j中(不修改原来系统中的JAVA_HOME)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim ./conf/neo4j.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>在文件最后一行添加</p>
</li>
</ul>
<blockquote>
<p>JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.5.10-0.el7_7.x86_64</p>
</blockquote>
<ul>
<li><p>重新启动neo4j</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ./bin/neo4j start</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意: 以上方法都不会影响系统的JAVA_HOME和JAVA环境</p>
</li>
</ul>
<hr>
<h3 id="Neo4j服务器配置远程访问功能"><a href="#Neo4j服务器配置远程访问功能" class="headerlink" title="Neo4j服务器配置远程访问功能"></a>Neo4j服务器配置远程访问功能</h3><ul>
<li><p>打开配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim ./conf/neo4j.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>将下面的语句注释取消</p>
</li>
</ul>
<blockquote>
<p>dbms.connectors.default_listen_address=0.0.0.0</p>
</blockquote>
<ul>
<li>注意: 如果是服务器有防火墙,则需要把以下端口打开<ul>
<li>7474: http</li>
<li>7687: bolt</li>
</ul>
</li>
</ul>
<h3 id="Neo4j-dump数据"><a href="#Neo4j-dump数据" class="headerlink" title="Neo4j dump数据"></a>Neo4j dump数据</h3><ul>
<li>参考链接：<a href="https://www.jianshu.com/p/8c501b49adb7" target="_blank" rel="noopener">https://www.jianshu.com/p/8c501b49adb7</a></li>
<li>dump原始数据库为<code>.dump</code>文件<br><code>bin/neo4j-admin dump --database graph.db --to [dir]</code><ul>
<li>将数据库graph.db中的数据dump为<code>.dump</code>文件，文件名字自动生成为<code>对应的数据库名称.dump</code></li>
</ul>
</li>
<li>将<code>.dump</code>文件导入到库中，（库需要停掉，并且库名不能有相同的名字）<br><code>bin/neo4j-admin load --from graph.db.dump</code><ul>
<li>相当于是dump的逆向操作，数据库文件名称自动生成为前缀(注意不能与已经存在的数据库产生冲突)</li>
</ul>
</li>
<li>当不同版本库相互倒数据时需要把该参数开启，在conf/neo4j.conf中<br><code>dbms.allow_format_migration=true</code></li>
<li>这个dump命令只有在3.2.0才有的</li>
</ul>
<hr>
<h3 id="Neo4j-4-0-0以后"><a href="#Neo4j-4-0-0以后" class="headerlink" title="Neo4j 4.0.0以后"></a>Neo4j 4.0.0以后</h3><ul>
<li>数据库文件夹变化了,不能像之前一样修改文件夹名为<code>graph.db</code>来更改数据库</li>
</ul>
<hr>
<h3 id="使用Cypher查询数据库时的效率问题"><a href="#使用Cypher查询数据库时的效率问题" class="headerlink" title="使用Cypher查询数据库时的效率问题"></a>使用Cypher查询数据库时的效率问题</h3><ul>
<li>应该把Cypher语句和MySQL的查询语句联系起来看</li>
<li>Cypher从第一句开始匹配，然后依次匹配相关的每一句</li>
<li>如果存在两条匹配过程的路径，而又需要把这两个路径联系起来，那么需要使用<code>WHERE</code>子句<ul>
<li><code>WHERE</code>语句非常有用，能避免很多不必要的问题，还能加入与或非的逻辑</li>
</ul>
</li>
<li>但是一定要注意，<code>WHERE</code>语句使用方便，但是容易造成检索慢的问题</li>
</ul>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><ul>
<li><p>尽量不要写出如下语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MATCH (image:Image), (base_image:BaseImage) </span><br><span class="line">WHERE image.name = &quot;ubuntu:latest&quot; and </span><br><span class="line">	(image)-[:hasBaseImage]-&gt;(base_image)</span><br><span class="line">RETURN base_image</span><br></pre></td></tr></table></figure>

<ul>
<li>上述语句将匹配所有的Image对象</li>
<li>然后匹配所有BaseImage对象</li>
<li>接着执行WHERE子句过滤</li>
<li>最后再返回</li>
</ul>
</li>
<li><p>上面的句子可以换成如下语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MATCH (image:Image&#123;name:&quot;ubuntu:latest&quot;), (image)-[:hasBaseImage]-&gt;(base_image)</span><br><span class="line">RETURN base_image</span><br></pre></td></tr></table></figure>

<ul>
<li>上面的句子直接找到名称为”ubuntu:latest“的镜像</li>
<li>然后直接从改对象开始搜索相关关系的BaseImage</li>
<li>最后返回</li>
</ul>
</li>
<li><p>实验表明：后面一句比前一句速度快很多</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>KG</tag>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title>CSDN——打印博客</title>
    <url>/Notes/Others/CSDN%E2%80%94%E2%80%94%E6%89%93%E5%8D%B0%E5%8D%9A%E5%AE%A2.html</url>
    <content><![CDATA[<p><em>本文描述了如何打印干净的CSDN博客</em><br>参考博客: <a href="https://blog.csdn.net/sinat_42483341/article/details/89354222" target="_blank" rel="noopener">https://blog.csdn.net/sinat_42483341/article/details/89354222</a></p>
<hr>
<h3 id="使用JavaScript脚本"><a href="#使用JavaScript脚本" class="headerlink" title="使用JavaScript脚本"></a>使用JavaScript脚本</h3><p><em>本方法适用在Chrome中</em></p>
<ul>
<li>在Chrome的调试窗口中,Console处输入下面的脚本即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(function()&#123;</span><br><span class="line">$(&quot;#side&quot;).remove();</span><br><span class="line">$(&quot;#comment_title, #comment_list, #comment_bar, #comment_form, .announce, #ad_cen, #ad_bot&quot;).remove();</span><br><span class="line">$(&quot;.nav_top_2011, #header, #navigator&quot;).remove();</span><br><span class="line">$(&quot;.p4course_target, .comment-box, .recommend-box, #csdn-toolbar, #tool-box&quot;).remove();</span><br><span class="line">$(&quot;aside&quot;).remove();</span><br><span class="line">$(&quot;.tool-box&quot;).remove();</span><br><span class="line">$(&quot;main&quot;).css(&apos;display&apos;,&apos;content&apos;); </span><br><span class="line">$(&quot;main&quot;).css(&apos;float&apos;,&apos;left&apos;); </span><br><span class="line">window.print();</span><br><span class="line"></span><br><span class="line">$(&quot;tool-box&quot;).remove();</span><br><span class="line">&#125;)();</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="使用油猴插件"><a href="#使用油猴插件" class="headerlink" title="使用油猴插件"></a>使用油猴插件</h3><p><em>没搜索到相关脚本,有时间的时候自己实现一份</em></p>
<ul>
<li>基本功能<ul>
<li>在CSDN网站访问时添加一个”打印博客”按钮,点击即可打印</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——DDIM</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94DDIM.html</url>
    <content><![CDATA[<p><em>文本介绍DDIM（Denoising Diffusion Implicit Models）的理论介绍</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考链接：<ul>
<li>原始论文：<a href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf" target="_blank" rel="noopener">DENOISING DIFFUSION IMPLICIT MODELS, ICLR 2021</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/627616358" target="_blank" rel="noopener">一文读懂DDIM凭什么可以加速DDPM的采样效率</a></li>
<li><a href="https://www.bilibili.com/video/BV1Ra4y1F73C/" target="_blank" rel="noopener">一个视频看懂DDIM凭什么加速采样|扩散模型相关</a></li>
</ul>
</li>
</ul>
<h3 id="DDPM为什么慢？"><a href="#DDPM为什么慢？" class="headerlink" title="DDPM为什么慢？"></a>DDPM为什么慢？</h3><ul>
<li>采样步数不能太小，否则单次破坏力度过大，不容易恢复？</li>
<li>不能跳步，否则不遵循马尔科夫过程</li>
</ul>
<h3 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h3><ul>
<li>核心是在DDPM的基础上，增加解决马尔科夫问题</li>
<li>直观理解<img src="/Notes/DL/DL——DDIM/DDIM-P1.png"></li>
<li>推导结果，推导流程有时间再补充<img src="/Notes/DL/DL——DDIM/DDIM-P2.png">
<ul>
<li>当采样方差\(\sigma\)满足一定条件时，上面的式子会满足马尔科夫过程，即等价于DDPM</li>
</ul>
</li>
<li>图示跳步采样（生成）的原理：<img src="/Notes/DL/DL——DDIM/DDIM-P3.png">

</li>
</ul>
<h3 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h3><h4 id="DDPM的训练和推理"><a href="#DDPM的训练和推理" class="headerlink" title="DDPM的训练和推理"></a>DDPM的训练和推理</h4><img src="/Notes/DL/DL——DDIM/ddpm-training-and-sampling.png">

<h4 id="DDIM的训练和推理"><a href="#DDIM的训练和推理" class="headerlink" title="DDIM的训练和推理"></a>DDIM的训练和推理</h4><ul>
<li><p>训练过程与DDPM基本一致，但DDIM只需要采样固定间隔的步数即可</p>
<ul>
<li>如果已经有训练好的DDPM，可以直接用，因为DDPM的训练时间步包含了DDIM的训练时间步</li>
</ul>
</li>
<li><p>推理过程</p>
<img src="/Notes/DL/DL——DDIM/DDIM-P4.png">
</li>
<li><p>推理时，一般会设置\(\sigma=0\)，即DDIM是确定性的，也就是DDIM中I(Implicit)区别于DDPM中(Probabilistic)的原因</p>
</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul>
<li>实验设置<img src="/Notes/DL/DL——DDIM/DDIM-P5-0.png"></li>
<li>结果展示<img src="/Notes/DL/DL——DDIM/DDIM-P5.png"></li>
<li>结果分析<img src="/Notes/DL/DL——DDIM/DDIM-P6.png"></li>
<li>从图中可以得出结论<ul>
<li>\(\sigma\)越小（\(\eta\)越小，方差越小），即方差越小，效果越好，DDIM最好（DDIM对应\(\eta=0\)且\(\sigma=0\)）</li>
<li>当\(T=1000\)时（或者\(T\)非常大时），DDPM效果最好（DDPM对应\(\eta=1\)且\(\sigma = \hat{\sigma}\)）</li>
<li>小节：当采样步数少时（即间隔大时），使用DDIM效果更好，DDPM效果非常差；当采样步数很大时，DDPM效果微微好于DDIM</li>
</ul>
</li>
</ul>
<h3 id="代码亲测"><a href="#代码亲测" class="headerlink" title="代码亲测"></a>代码亲测</h3><ul>
<li><p>各种\(\alpha,\beta\)的定义技巧：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num_steps = 100</span><br><span class="line">betas = torch.linspace(-6,6,num_steps) </span><br><span class="line">betas = torch.sigmoid(betas)*(0.5e-2 - 1e-5)+1e-5 # beta逐步递增</span><br><span class="line">alphas = 1-betas</span><br><span class="line">print(&quot;alphas: %s&quot; % alphas)</span><br><span class="line">alphas_prod = torch.cumprod(alphas,0) # 连乘</span><br><span class="line">alphas_bar = alphas_prod</span><br><span class="line">alphas_prod_p = torch.cat([torch.tensor([1]).float(),alphas_prod[:-1]],0) # previous连乘</span><br><span class="line">alphas_bar_sqrt = torch.sqrt(alphas_bar)</span><br><span class="line">one_minus_alphas_bar_log = torch.log(1 - alphas_bar)</span><br><span class="line">one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_bar)</span><br><span class="line"></span><br><span class="line"># 模型定义：model(x, t)</span><br><span class="line"># 采样方式：torch.randn_like(x)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现细节(为了清晰表达，对原始采样公式有所修改)：<br>$$<br>x_s = \sqrt{\bar{\alpha}_s}\left(\frac{x_k-\sqrt{1-\bar{\alpha}_k}\epsilon_{\theta}(x_k,k)}{\bar{\alpha}_k}\right) + \sqrt{1-\bar{\alpha}_s-a_1\sigma_k^2}\epsilon_{\theta}(x_k,k) + a_2\sigma_k \epsilon<br>$$</p>
<ul>
<li>其中：\(\sigma_k^2 = \beta_t\)</li>
<li>注意：\(a_1,a_2\)是新加的，拆开\(a_1,a_2\)的原因是实验发现两者可以设置不同值，且有以下现象：<ul>
<li>\(a_2=0,a_1=1.0\)时，此时表示不采样，整个生成过程中没有添加随机值（除了初始样本为随机值外），不影响样本的生成质量，相对有随机值生成效果甚至更好</li>
<li>\(a_1=0\)时，无论\(a_2\)值为多少，生成的样本均是类似乱码的图</li>
<li>\(a_1&gt;0.5\)时，无论\(a_2\)值是否为0，生成的样本均不错（\(a_1\)的值不能太小，否则效果会不好）</li>
</ul>
</li>
</ul>
</li>
<li><p>亲测实验结果与论文有偏差的原因分析:</p>
<ul>
<li>如果采样步数较少，效果也不好，但当采样间隔为2时，即跳一步采样，效果还可以（不如间隔为1）<ul>
<li>采样步数越多，效果越好（与原始论文结果一样）</li>
</ul>
</li>
<li>为什么本人实现的DDIM采样步数不能太少？（跳步10步时效果较差，与原始论文结果不一致）<ul>
<li>DDIM可能对模型要求很高，本人尝试环境中模型过于简单？训练样本过少？</li>
</ul>
</li>
<li>为什么\(a_1=0\)时，效果非常差？<ul>
<li>待解答</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="DDIM对比DDPM"><a href="#DDIM对比DDPM" class="headerlink" title="DDIM对比DDPM"></a>DDIM对比DDPM</h3><ul>
<li>DDIM训练流程与DDPM完全相同，只是推理(采样)过程不同</li>
<li>DDIM主要解决DDPM采样慢的问题<ul>
<li>DDPM有符合马尔科夫假设，需要一步步采样，效率慢</li>
<li>DDIM没有马尔科夫假设，可以跳步生成图片，且可以不采样（标准的DDIM就是不采样的，论文中，噪音的方差越小，得到的效果越好）</li>
</ul>
</li>
</ul>
<h3 id="一些问题和解答"><a href="#一些问题和解答" class="headerlink" title="一些问题和解答"></a>一些问题和解答</h3><ul>
<li>DDPM采样时不能去掉噪音，为什么DDIM可以？<ul>
<li>回答：因为DDIM采样中，噪音的方差\(\sigma\)是通过实验发现效果比较好的</li>
<li>具体原因？</li>
</ul>
</li>
<li>为什么DDPM和DDIM训练逻辑基本一致，但DDPM推断必须遵循马尔科夫性，而DDIM不需要？<ul>
<li>回答：<ul>
<li>训练时：DDIM和DDPM训练过程都遵循马尔科夫过程。DDIM可以按照一定间隔采样时间步，但实际上也是经过在满足马尔科夫过程的情况下推导出来的，DDPM和DDIM的训练采样公式都是因为方差可以累加实现跳跃采样的</li>
<li>推断时：因为DDPM的采样公式是在满足马尔科夫过程情况下推导出来的， 而DDIM的采样公式是在非马尔科夫过程情况下推导出来的，所以使用DDPM采样公式时，不能跳步；使用DDIM采样公式时（注意：实际上DDIM不采样，直接确定性生成），可以跳步</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——DDPM</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94DDPM.html</url>
    <content><![CDATA[<p><em>文本介绍DDPM（Denoising Diffusion Probabilistic Models）的理论介绍</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考链接：<ul>
<li>原始论文：<a href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf" target="_blank" rel="noopener">Denoising Diffusion Probabilistic Models, NeurIPS 2020</a></li>
<li><a href="https://www.bilibili.com/video/BV1b541197HX/" target="_blank" rel="noopener">54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读</a><ul>
<li>视频中有部分小bug，评论区已修复</li>
</ul>
</li>
<li><a href="https://zhuanlan.zhihu.com/p/624851115" target="_blank" rel="noopener">一文解决你关于扩散模型ddpm的所有疑惑</a>，回答了很多疑惑<ul>
<li><a href="https://www.bilibili.com/video/BV1p24y1K7Pf/" target="_blank" rel="noopener">一个视频看懂扩散模型DDPM原理推导|AI绘画底层模型</a>，</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h3><img src="/Notes/DL/DL——DDPM/ddpm-derivation.webp">

<h3 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h3><img src="/Notes/DL/DL——DDPM/ddpm-training-and-sampling.png">

<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ul>
<li>待更新</li>
</ul>
<h3 id="一些问题和解答"><a href="#一些问题和解答" class="headerlink" title="一些问题和解答"></a>一些问题和解答</h3><ul>
<li>关于\(\epsilon_{\theta}(x_t, t)\)的含义<ul>
<li>训练时预估的\(\epsilon_{\theta}(x_t, t)\)的含义是什么？学到的是从\(x_0\)到\(x_t\)加的噪音，还是从\(x_{t-1}\)到\(x_t\)加的噪音？<ul>
<li>回答：是从\(x_0\)到\(x_t\)加的噪音，因为从训练流程的公式就可以看出，\(\epsilon_{\theta}(x_t, t)\)中的\(x_t=\sqrt{\bar{\alpha}_{t}}x_0 + \sqrt{1-\bar{\alpha}_{t}}\epsilon\)，其中\(\epsilon\)就是从\(x_0\)到\(x_t\)加的噪音，而损失函数的目标就是让\(\epsilon_{\theta}-&gt;\epsilon\)。</li>
</ul>
</li>
<li>在推理时，既然\(\epsilon_{\theta}(x_t, t)\)是从\(x_0\)到\(x_t\)加的累计噪音，为什么可以使用\(\epsilon_{\theta}(x_t, t)\)来完成从\(x_{t}\)到\(x_{t-1}\)的过程？<ul>
<li>回答：因为这里从\(x_{t}\)到\(x_{t-1}\)的本质也是分两步的，第一步是从\(x_{t}\)到\(x_{0}\)(由\(x_t=\sqrt{\bar{\alpha}_{t}}x_0 + \sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta}\)反推即可得到\(x_0 = \frac{1}{\sqrt{\bar{\alpha}_{t}}}(x_t-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta})\)，注意这里得到的\(x_0\)可能质量不太好，不能直接作为最终结果)；第二步是已知从\(x_{t}\)和\(x_{0}\)后，可以得到\(x_{t-1}\)的分布\(p(x_{t-1}|x_t,x_0)\)，进一步对分布进行采样，就能得到一个\(x_{t-1}\)的实例。两步合并以后就是DDPM的采样伪代码中的公式</li>
</ul>
</li>
<li>既然为什么不能一步到位得到\(x_0 = \frac{1}{\sqrt{\bar{\alpha}_{t}}}(x_t-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta})\)?<ul>
<li>回答：这样做是不可以的，在整个推导过程中，我们有\(q(x_t|x_0) = N(x_t;\sqrt{\bar{\alpha}_{t}}x_0, (1-\bar{\alpha}_{t})\mathbf{I})\)（这里是通过高斯过程的叠加实现的，整个过程遵循马尔科夫过程），所以才有公式\(x_t=\sqrt{\bar{\alpha}_{t}}x_0 + \sqrt{1-\bar{\alpha}_{t}}\epsilon\)，这不代表我们可以通过\(x_0 = \frac{1}{\sqrt{\bar{\alpha}_{t}}}(x_t-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta})\)来得到\(x_0\)，因为此时的\(x_0\)是无法一步导出的（即\(q(x_0|x_t)\)是未知的，未知的原因是不遵循马尔科夫过程，训练时使用的损失函数是在满足马尔科夫过程假设的情况下推导出来的，推理时也不能违背该假设），必须遵循马尔科夫过程（即\(q(x_{t-1}|x_t)\)）<ul>
<li>\(q(x_t|x_0)\)已知但是\(q(x_0|x_t)\)未知的原因是因为采样是不可逆的</li>
<li>既然\(x_0 = \frac{1}{\sqrt{\bar{\alpha}_{t}}}(x_t-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta})\)不准确（不遵循马尔科夫过程），为什么按照\(x_0 = \frac{1}{\sqrt{\bar{\alpha}_{t}}}(x_t-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta})\)的到的\(x_0\)可以作为中间变量来帮助生成\(x_{t-1}\)呢？公式推导带入的时候使用了这个式子，本质上也说明推导过程不遵循马尔科夫过程了吧？</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>DDPM可以不加入噪声吗？<ul>
<li>实践：不可以，图片生成质量会特别差</li>
<li>回答：因为如果不加入噪声，原因未知，猜测是采样次数多导致中间步骤的偏差被放大？(来源：<a href="https://www.bilibili.com/video/BV1Ra4y1F73C/" target="_blank" rel="noopener">一个视频看懂DDIM凭什么加速采样|扩散模型相关</a>)【这个观点无法解释DDIM为什么可以直接生成，DDIM（DDIM生成时不加入噪声）生成1000步效果也不会太差】</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——xDeepFM</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94xDeepFM.html</url>
    <content><![CDATA[<p><em>文本介绍xDeepFM的理论介绍,包括阅读论文后自己的理解</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>上篇博客参考: <a href="/Notes/DL/DL%E2%80%94%E2%80%94DeepFM.html">DeepFM</a></li>
<li>原始论文: <a href="https://arxiv.org/pdf/1803.05170.pdf" target="_blank" rel="noopener">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems, KDD 2018</a></li>
</ul>
<h3 id="xDeepFM结构图"><a href="#xDeepFM结构图" class="headerlink" title="xDeepFM结构图"></a>xDeepFM结构图</h3><img src="/Notes/DL/DL——xDeepFM/xDeepFM_overview.png">
<ul>
<li>显然, 上图中除了 CIN 部分以外,其他跟 DeepFM 基本是相同的, 所以我们本文主要讲述 CIN 组件部分, 其他的嵌入层等可参考我之前的博客<a href="/Notes/DL/DL%E2%80%94%E2%80%94DeepFM.html">DeepFM</a></li>
</ul>
<h4 id="CIN组件"><a href="#CIN组件" class="headerlink" title="CIN组件"></a>CIN组件</h4><p><em>Compressed Interaction Network</em></p>
<ul>
<li>论文中首先提出的就是这个CIN网络(Compressed Interaction Network)</li>
<li>结构图如下:<img src="/Notes/DL/DL——xDeepFM/cin_overview.png"></li>
<li>理解<ul>
<li>每个隐藏层都与一个池化操作连接到一起</li>
<li>特征阶数与网络层数相关</li>
<li>可以与 RNN 对应着看, 当前网络层由上一个隐藏层和一个额外输入确定</li>
<li>确切的说: CIN 中当前层输入是前一层的隐藏层 + 原来的特征向量</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>KG——知识图谱的描述</title>
    <url>/Notes/KnowledgeGraph/KG%E2%80%94%E2%80%94%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E6%8F%8F%E8%BF%B0.html</url>
    <content><![CDATA[<p><em>参考博客: <a href="https://blog.csdn.net/u011801161/article/details/78833958" target="_blank" rel="noopener">https://blog.csdn.net/u011801161/article/details/78833958</a></em></p>
<hr>
<h3 id="RDF"><a href="#RDF" class="headerlink" title="RDF"></a>RDF</h3><ul>
<li>Resource Description Framework</li>
<li>资源描述框架</li>
<li>本质是一个数据模型</li>
<li>提供了统一的描述实体和资源的标准</li>
<li>形式上表现为主谓宾(SPO, Subject-Predication-Object)三元组, 也称为一条语句(Statement), 知识图谱中称为一条知识</li>
</ul>
<h4 id="RDF的序列化方法"><a href="#RDF的序列化方法" class="headerlink" title="RDF的序列化方法"></a>RDF的序列化方法</h4><p><em>参考博客: <a href="https://blog.csdn.net/u011801161/article/details/78833958" target="_blank" rel="noopener">https://blog.csdn.net/u011801161/article/details/78833958</a></em></p>
<ul>
<li><p>RDF/XML: 用XML格式来表示RDF数据</p>
</li>
<li><p>N-Triples: 用多个三元组来表示RDF数据集合,是最直观的表示方法,每一行表示一个三元组,方便机器解析和处理,<a href="https://wiki.dbpedia.org/" target="_blank" rel="noopener">DBpedia</a> 是按照这个方式来发布数据的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/chineseName&gt; &quot;罗纳尔多·路易斯·纳萨里奥·德·利马&quot;^^string.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/career&gt; &quot;足球运动员&quot;^^string.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/fullName&gt; &quot;Ronaldo Luís Nazário de Lima&quot;^^string.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/birthDate&gt; &quot;1976-09-18&quot;^^date.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/height&gt; &quot;180&quot;^^int.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/weight&gt; &quot;98&quot;^^int.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/nationality&gt; &quot;巴西&quot;^^string.</span><br><span class="line">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/hasBirthPlace&gt; &lt;http://www.kg.com/place/10086&gt;.</span><br><span class="line">&lt;http://www.kg.com/place/10086&gt; &lt;http://www.kg.com/ontology/address&gt; &quot;里约热内卢&quot;^^string.</span><br><span class="line">&lt;http://www.kg.com/place/10086&gt; &lt;http://www.kg.com/ontology/coordinate&gt; &quot;-22.908333, -43.196389&quot;^^string.</span><br></pre></td></tr></table></figure>
</li>
<li><p>RDFa: (The Resource Description Framework in Attributes)</p>
</li>
<li><p>Turtle是最常用的RDF序列化方式, 比RDF/XML更紧凑, 可读性比N-Triples更好</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Example2 Turtle:</span><br><span class="line"></span><br><span class="line">@prefix person: &lt;http://www.kg.com/person/&gt; .</span><br><span class="line">@prefix place: &lt;http://www.kg.com/place/&gt; .</span><br><span class="line">@prefix : &lt;http://www.kg.com/ontology/&gt; .</span><br><span class="line"></span><br><span class="line">person:1 :chineseName &quot;罗纳尔多·路易斯·纳萨里奥·德·利马&quot;^^string.</span><br><span class="line">person:1 :career &quot;足球运动员&quot;^^string.</span><br><span class="line">person:1 :fullName &quot;Ronaldo Luís Nazário de Lima&quot;^^string.</span><br><span class="line">person:1 :birthDate &quot;1976-09-18&quot;^^date.</span><br><span class="line">person:1 :height &quot;180&quot;^^int. </span><br><span class="line">person:1 :weight &quot;98&quot;^^int.</span><br><span class="line">person:1 :nationality &quot;巴西&quot;^^string. </span><br><span class="line">person:1 :hasBirthPlace place:10086.</span><br><span class="line">place:10086 :address &quot;里约热内卢&quot;^^string.</span><br><span class="line">place:10086 :address &quot;-22.908333, -43.196389&quot;^^string.</span><br></pre></td></tr></table></figure>

<ul>
<li>同一个实体拥有多个属性（数据属性）或关系（对象属性），我们可以只用一个subject来表示，使其更紧凑。我们可以将上面的Turtle改为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Example3 Turtle:</span><br><span class="line"></span><br><span class="line">@prefix person: &lt;http://www.kg.com/person/&gt; .</span><br><span class="line">@prefix place: &lt;http://www.kg.com/place/&gt; .</span><br><span class="line">@prefix : &lt;http://www.kg.com/ontology/&gt; .</span><br><span class="line"></span><br><span class="line">person:1 :chineseName &quot;罗纳尔多·路易斯·纳萨里奥·德·利马&quot;^^string;</span><br><span class="line">         :career &quot;足球运动员&quot;^^string;</span><br><span class="line">         :fullName &quot;Ronaldo Luís Nazário de Lima&quot;^^string;</span><br><span class="line">         :birthDate &quot;1976-09-18&quot;^^date;</span><br><span class="line">         :height &quot;180&quot;^^int;</span><br><span class="line">         :weight &quot;98&quot;^^int;</span><br><span class="line">         :nationality &quot;巴西&quot;^^string; </span><br><span class="line">         :hasBirthPlace place:10086.</span><br><span class="line">place:10086 :address &quot;里约热内卢&quot;^^string;</span><br><span class="line">            :address &quot;-22.908333, -43.196389&quot;^^string.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>JSON-LD: 即“JSON for Linking Data”，用键值对的方式来存储RDF数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;@context&quot;: &quot;https://json-ld.org/contexts/person.jsonld&quot;,</span><br><span class="line">  &quot;@id&quot;: &quot;http://dbpedia.org/resource/John_Lennon&quot;,</span><br><span class="line">  &quot;name&quot;: &quot;John Lennon&quot;,</span><br><span class="line">  &quot;born&quot;: &quot;1940-10-09&quot;,</span><br><span class="line">  &quot;spouse&quot;: &quot;http://dbpedia.org/resource/Cynthia_Lennon&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="RDF的缺点"><a href="#RDF的缺点" class="headerlink" title="RDF的缺点"></a>RDF的缺点</h4><ul>
<li>表达能力有限<ul>
<li>无法区分雷和对象</li>
<li>无法定义和描述类的关系/属性</li>
</ul>
</li>
</ul>
<hr>
<h3 id="RDFS-OWL"><a href="#RDFS-OWL" class="headerlink" title="RDFS/OWL"></a>RDFS/OWL</h3><ul>
<li>是RDF的一种扩展</li>
<li>是用来描述RDF数据的</li>
<li>本质上是一些预定义词汇(Vocabulary)构成的集合</li>
<li>用于对RDF进行类似的类定义以及属性的定义</li>
</ul>
<h4 id="RDFS-OWL的序列化方法"><a href="#RDFS-OWL的序列化方法" class="headerlink" title="RDFS/OWL的序列化方法"></a>RDFS/OWL的序列化方法</h4><ul>
<li>RDFS/OWL序列化方式和RDF没什么不同，其实在表现形式上，它们就是RDF</li>
<li>常用的方式主要是RDF/XML，Turtle</li>
</ul>
<h4 id="RDFS"><a href="#RDFS" class="headerlink" title="RDFS"></a>RDFS</h4><ul>
<li>Resource Description Framework Schema</li>
<li>是RDF的一种扩展</li>
<li>RDFS几个比较重要，常用的词汇： <ul>
<li>rdfs:Class. 用于定义类。 </li>
<li>rdfs:domain. 用于表示该属性属于哪个类别。 </li>
<li>rdfs:range. 用于描述该属性的取值类型。 </li>
<li>rdfs:subClassOf. 用于描述该类的父类。比如，我们可以定义一个运动员类，声明该类是人的子类。 </li>
<li>rdfs:subProperty. 用于描述该属性的父属性。比如，我们可以定义一个名称属性，声明中文名称和全名是名称的子类。</li>
<li>其实rdf:Property和rdf:type也是RDFS的词汇，因为RDFS本质上就是RDF词汇的一个扩展。我们在这里不罗列进去，是不希望读者混淆, 更多RDFS词汇的用法参考<a href="https://www.w3.org/TR/rdf-schema" target="_blank" rel="noopener">W3C官方文档</a></li>
</ul>
</li>
<li>举例<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .</span><br><span class="line">@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .</span><br><span class="line">@prefix : &lt;http://www.kg.com/ontology/&gt; .</span><br><span class="line"></span><br><span class="line">### 这里我们用词汇rdfs:Class定义了“人”和“地点”这两个类。</span><br><span class="line">:Person rdf:type rdfs:Class.</span><br><span class="line">:Place rdf:type rdfs:Class.</span><br><span class="line"></span><br><span class="line">### rdfs当中不区分数据属性和对象属性，词汇rdf:Property定义了属性，即RDF的“边”。</span><br><span class="line">:chineseName rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:career rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:fullName rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:birthDate rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:date .</span><br><span class="line"></span><br><span class="line">:height rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:int .</span><br><span class="line"></span><br><span class="line">:weight rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:int .</span><br><span class="line"></span><br><span class="line">:nationality rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:hasBirthPlace rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range :Place .</span><br><span class="line"></span><br><span class="line">:address rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Place;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:coordinate rdf:type rdf:Property;</span><br><span class="line">        rdfs:domain :Place;</span><br><span class="line">        rdfs:range xsd:string .</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="OWL"><a href="#OWL" class="headerlink" title="OWL"></a>OWL</h4><ul>
<li><p>Web Ontology Language</p>
</li>
<li><p>是对RDFS的一个扩展，添加了额外的预定义词汇</p>
</li>
<li><p>提供快速，灵活的数据建模能力</p>
</li>
<li><p>高效的自动推理能力</p>
</li>
<li><p>描述属性特征的词汇 </p>
<ul>
<li>owl:TransitiveProperty. 表示该属性具有传递性质。例如，我们定义“位于”是具有传递性的属性，若A位于B，B位于C，那么A肯定位于C。 </li>
<li>owl:SymmetricProperty. 表示该属性具有对称性。例如，我们定义“认识”是具有对称性的属性，若A认识B，那么B肯定认识A。 </li>
<li>owl:FunctionalProperty. 表示该属性取值的唯一性。 例如，我们定义“母亲”是具有唯一性的属性，若A的母亲是B，在其他地方我们得知A的母亲是C，那么B和C指的是同一个人。 </li>
<li>owl:inverseOf. 定义某个属性的相反关系。例如，定义“父母”的相反关系是“子女”，若A是B的父母，那么B肯定是A的子女。</li>
</ul>
</li>
<li><p>本体映射词汇（Ontology Mapping） </p>
<ul>
<li>owl:equivalentClass. 表示某个类和另一个类是相同的。 </li>
<li>owl:equivalentProperty. 表示某个属性和另一个属性是相同的。 </li>
<li>owl:sameAs. 表示两个实体是同一个实体。</li>
</ul>
</li>
<li><p>举例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .</span><br><span class="line">@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .</span><br><span class="line">@prefix : &lt;http://www.kg.com/ontology/&gt; .</span><br><span class="line">@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .</span><br><span class="line"></span><br><span class="line">### 这里我们用词汇owl:Class定义了“人”和“地点”这两个类。</span><br><span class="line">:Person rdf:type owl:Class.</span><br><span class="line">:Place rdf:type owl:Class.</span><br><span class="line"></span><br><span class="line">### owl区分数据属性和对象属性（对象属性表示实体和实体之间的关系）。词汇owl:DatatypeProperty定义了数据属性，owl:ObjectProperty定义了对象属性。</span><br><span class="line">:chineseName rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:career rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:fullName rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:birthDate rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:date .</span><br><span class="line"></span><br><span class="line">:height rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:int .</span><br><span class="line"></span><br><span class="line">:weight rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:int .</span><br><span class="line"></span><br><span class="line">:nationality rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:hasBirthPlace rdf:type owl:ObjectProperty;</span><br><span class="line">        rdfs:domain :Person;</span><br><span class="line">        rdfs:range :Place .</span><br><span class="line"></span><br><span class="line">:address rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Place;</span><br><span class="line">        rdfs:range xsd:string .</span><br><span class="line"></span><br><span class="line">:coordinate rdf:type owl:DatatypeProperty;</span><br><span class="line">        rdfs:domain :Place;</span><br><span class="line">        rdfs:range xsd:string .</span><br></pre></td></tr></table></figure>
</li>
<li><p>举个例子体现对两个不同知识图谱的融合</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://www.zhangsan.com/ontology/Person rdf:type owl:Class . </span><br><span class="line">http://www.lisi.com/ontology/Human rdf:type owl:Class . </span><br><span class="line">http://www.zhangsan.com/ontology/Person owl:equivalentClass http://www.lisi.com/ontology/Human .</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——类的__class__属性与isinstance函数的用法</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94isinstance%E7%9A%84%E5%9B%B0%E5%A2%83.html</url>
    <content><![CDATA[<hr>
<h3 id="不同文件为入口文件时"><a href="#不同文件为入口文件时" class="headerlink" title="不同文件为入口文件时"></a>不同文件为入口文件时</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># file: fruit.py</span><br><span class="line"></span><br><span class="line">class Apple:</span><br><span class="line">	def __init__(self):</span><br><span class="line">		name = &quot;HongFuShi&quot;</span><br><span class="line"></span><br><span class="line">apple = Apple()</span><br><span class="line">print apple.__class__</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># file: run.py</span><br><span class="line">import fruit</span><br></pre></td></tr></table></figure>

<ul>
<li>考虑一个文件名为<em>fruit.py</em>的文件夹中定义了一个类<code>Apple</code>,同时初始化一个对象<code>apple</code><ul>
<li>若执行<code>python fruit.py</code>: 输入”<strong>main</strong>.Apple”</li>
<li>若将当前文件导入到另一个文件<em>run.py</em>中,然后执行<code>python run.py</code>: 输出”fruit.Apple”</li>
<li>也就是说,执行不同文件,类<code>Apple</code>的前缀不同</li>
</ul>
</li>
</ul>
<hr>
<h3 id="isinstance的困境"><a href="#isinstance的困境" class="headerlink" title="isinstance的困境"></a>isinstance的困境</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># file: fruit.py</span><br><span class="line">class Apple(object):</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    from run import get_apple</span><br><span class="line">    apple1 = Apple()</span><br><span class="line">    print apple1.__class__</span><br><span class="line">    apple2 = get_apple()</span><br><span class="line">    print apple2.__class__</span><br><span class="line">    print isinstance(apple2, Apple)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># file: run.py</span><br><span class="line">from fruit import Apple</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_apple():</span><br><span class="line">    apple = Apple()</span><br><span class="line">    print apple.__class__</span><br><span class="line">    return apple</span><br></pre></td></tr></table></figure>

<ul>
<li><p>此时执行<code>python run.py</code>, 无任何输出</p>
</li>
<li><p>若执行<code>python fruit.py</code>, 则输出如下:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;class &apos;__main__.Apple&apos;&gt;</span><br><span class="line">&lt;class &apos;fruit.Apple&apos;&gt;</span><br><span class="line">&lt;class &apos;fruit.Apple&apos;&gt;</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

<ul>
<li>此时<code>fruit.py</code>是程序的入口文件</li>
<li>在入口文件中执行<code>apple1 = Apple()</code>后得到的类将是<code>__main__.Apple</code></li>
<li>在入口文件被导入到<code>run.py</code>文件中后,执行<code>apple2 = Apple()</code>后得到的类将是<code>fruit.Apple</code></li>
<li>此时,由于<code>apple2</code>的类别是<code>fruit.Apple</code>且<code>Apple</code>在<code>fruit.py</code>中是<code>__main__.Apple</code>,造成<code>isinstance(apple2, Apple)</code>返回<code>False</code></li>
<li>isinstance的困境: <strong>看起来是同一个类,但执行isinstance后返回False</strong></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>RL——学习笔记</title>
    <url>/Notes/RL/RL%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<hr>
<h3 id="为什么Off-Policy在于不与环境交互时效果不好？"><a href="#为什么Off-Policy在于不与环境交互时效果不好？" class="headerlink" title="为什么Off-Policy在于不与环境交互时效果不好？"></a>为什么Off-Policy在于不与环境交互时效果不好？</h3><ul>
<li><strong>Training Mismatch: 网络更新用的期望梯度与数据集中的观测不一致</strong><ul>
<li><strong>与环境交互时能成功的原因</strong>：在使用off-policy方法（包括DQN和A3C等）进行Policy-Network更新或Q-Network更新时，梯度更新本质上都需要根据下一时刻的奖励期望/价值函数/Q函数的期望（期望应该与状态无关）来进行，但是为了方便，我们一般都是用了当前观测状态等进行梯度更新，这里能够成功的本质原因与SGD类似，因为状态是从当前策略的分布中采样出来的，所以经过多次尝试可以逼近期望梯度，换句话说，当前的观测是从当前策略分布中采样来的，也就是对当前策略的一种无偏估计</li>
<li><strong>不交互时失败的原因</strong>：在固定数据集中使用off-policy方法时，则该数据集与off-policy方案对应的数据是无法对齐的，也就是说数据集中的期望与策略对应的期望不同，也就不能简单的使用该期望对应的梯度更新网络！</li>
</ul>
</li>
<li><strong>Absent Data: 数据集中可能存在某些状态是从来没有出现过的？</strong><ul>
<li>某些state-action对的缺失？</li>
<li>缺失会导致在该状态和动作上的错误估计</li>
</ul>
</li>
<li><strong>Model Bias: 数据集本身可能分布与MDP分布不一致</strong><ul>
<li>比如某个状态转移概率在MDP中是1/10,但在Model Bias中可能为1/5，这样就导致了模型错误估计了环境的转态转移概率</li>
</ul>
</li>
</ul>
<h3 id="为什么online-RL中不能使用BN？"><a href="#为什么online-RL中不能使用BN？" class="headerlink" title="为什么online RL中不能使用BN？"></a>为什么online RL中不能使用BN？</h3><ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/210761985" target="_blank" rel="noopener">强化学习需要批归一化(Batch Norm) 或归一化吗？</a></li>
<li><a href="https://www.bilibili.com/video/BV12d4y1f74C" target="_blank" rel="noopener">[5分钟深度学习] #06 批量归一化 Batch Normalization</a></li>
</ul>
</li>
<li>BN能work的主要原因是，每个Batch的数据是从整体数据中随机采样得到的，每个Batch的均值和方差不会差别太大</li>
<li>在online RL中，随着Agent的探索，数据分布一直在变化，进入模型的不同Batch无法保证能代表整体样本，随着Agent的迭代数据分布逐步变化，模型来不及学到固定的均值和方差，导致无法收敛</li>
<li>offline RL中，数据是提前固定的，则没有这个问题，所以offline RL中，可以使用BN</li>
</ul>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——数字范围边界等问题</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E6%95%B0%E5%AD%97%E8%8C%83%E5%9B%B4%E8%BE%B9%E7%95%8C%E7%AD%89%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p><em>C++中不同类型的数字有自己不同的边界和范围,Python中呢?如何判断边界问题?</em></p>
<hr>
<h3 id="最大最小整数"><a href="#最大最小整数" class="headerlink" title="最大最小整数"></a>最大最小整数</h3><h4 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int minInt = 0x80000000;</span><br><span class="line">int maxInt = 0xffffffff</span><br></pre></td></tr></table></figure>

<h4 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">minInt = -0xffffffff</span><br><span class="line">maxInt = 0xffffffff</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Python中int大小为24个字节,数字太大时不会越界,会变为long类型,long类型的字节占位可以非常大(24以下为int,之后为long,分别可以为36,44,52,60等,每次8位递加?),不会越界</p>
<ul>
<li>测试:当一个数字太大时,使用int(a)强制字符转换也不能将数字转换为int类型,将一直为long类型</li>
<li>测试: Python中24个字节存储一个int类型对象,但是并不是所有空间都存值,只有一部分用来存储数值<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MinInt = int(0x80000000000000000)</span><br><span class="line">print MinInt</span><br><span class="line">import sys</span><br><span class="line">print sys.getsizeof(MinInt)</span><br><span class="line">print type(int(MinInt))</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">147573952589676412928</span><br><span class="line">36</span><br><span class="line">&lt;type &apos;long&apos;&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Python中定义最小负数时可以使用float最小值或者是很大的整数的负数,而不是像C++一样</p>
</li>
</ul>
<hr>
<h3 id="最大最小浮点数"><a href="#最大最小浮点数" class="headerlink" title="最大最小浮点数"></a>最大最小浮点数</h3><h4 id="Python-1"><a href="#Python-1" class="headerlink" title="Python"></a>Python</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">minFloat = float(&quot;-inf&quot;)</span><br><span class="line">maxFloat = float(&quot;inf&quot;)</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——主成分分析-PCA</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA.html</url>
    <content><![CDATA[<hr>
<h3 id="与TSVD的对比"><a href="#与TSVD的对比" class="headerlink" title="与TSVD的对比"></a>与TSVD的对比</h3><ul>
<li>关于SVD的进一步了解可参考<a href="/Notes/Others/Math%E2%80%94%E2%80%94%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3-SVD.html">Math——奇异值分解-SVD</a></li>
<li>PCA与TSVD目标不同</li>
<li>TSVD奇异值与PCA分解得到的对角矩阵元素意义不同<ul>
<li>PCA得到矩阵对角元素的是该维度的方差</li>
<li>TSVD得到的是某种重要的隐形意义(注意,不是方差)</li>
</ul>
</li>
<li>PCA等价于下面两个步骤:<ul>
<li>对数据X中心化</li>
<li>对数据做TSVD分解</li>
</ul>
</li>
</ul>
<hr>
<h3 id="与ICA的对比"><a href="#与ICA的对比" class="headerlink" title="与ICA的对比"></a>与ICA的对比</h3><ul>
<li>ICA得到的变量满足独立性</li>
<li>PCA得到的变量满足不相关性</li>
<li>独立与不相关的关系<ul>
<li>变量独立\(=&gt;\)变量不相关</li>
<li>变量不相关\(\neq&gt;\)变量独立</li>
<li>当变量是正态分布时:变量独立\(&lt;=&gt;\)变量不相关</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Tips——一些有用的tips总结</title>
    <url>/Notes/Others/Tips%E2%80%94%E2%80%94%E4%B8%80%E4%BA%9B%E6%9C%89%E7%94%A8%E7%9A%84tips%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>本文对一些程序员日常可能用到的小tips进行总结和记录</em></p>
<hr>
<h3 id="海量字符串的合并"><a href="#海量字符串的合并" class="headerlink" title="海量字符串的合并"></a>海量字符串的合并</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><ul>
<li><p>将大量的(现实遇到的是18W+级别的句子,句子长度平均在100个字符以上)字符串需要合并为一个字符串</p>
</li>
<li><p>如果直接迭代并使用下面的语句合并,花费很多时间,随着字符串的增大,合并速度越来越慢</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">all_texts += &quot; %s&quot; % text</span><br><span class="line">all_texts = &quot;%s %s&quot; % (all_text, text)</span><br></pre></td></tr></table></figure>
</li>
<li><p>现实生活中发现到了5000个字符串以上时速度变得极慢</p>
</li>
</ul>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li>分批次合并,将词语分批次分别合并为一个比较小的,最后再合并到一起<ul>
<li>实际中我按照2000个句子一份合并完成,再最终合并,速度提升了非常多</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——队列和栈使用</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E9%98%9F%E5%88%97%E5%92%8C%E6%A0%88%E4%BD%BF%E7%94%A8.html</url>
    <content><![CDATA[<p><em>本文从总结Python中栈和队列的基本使用</em><br><strong>Python 中queue模块是线程安全的,为多线程任务设计的,没有peek()操作</strong></p>
<ul>
<li>双端队列(deque)是一个具有栈和队列性质的数据结构，可以从两端弹出</li>
</ul>
<hr>
<h3 id="普通的栈和队列"><a href="#普通的栈和队列" class="headerlink" title="普通的栈和队列"></a>普通的栈和队列</h3><h4 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h4><h5 id="list实现栈"><a href="#list实现栈" class="headerlink" title="list实现栈"></a>list实现栈</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># init</span><br><span class="line">stack = list()</span><br><span class="line"># push</span><br><span class="line">stack.append(1)</span><br><span class="line"># pop</span><br><span class="line">stack.pop()</span><br><span class="line"># peek</span><br><span class="line">top = stack[-1]</span><br><span class="line"># determine whether it is empty</span><br><span class="line">if len(stack) == 0:</span><br><span class="line">	print(&quot;stack is empty&quot;)</span><br></pre></td></tr></table></figure>

<h5 id="deque实现栈"><a href="#deque实现栈" class="headerlink" title="deque实现栈"></a>deque实现栈</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line"># init</span><br><span class="line">stack = deque([1, 2, 3])</span><br><span class="line"># push</span><br><span class="line">stack.append(4)</span><br><span class="line"># pop</span><br><span class="line">stack.pop()</span><br><span class="line"># peek</span><br><span class="line">top = stack[-1]</span><br><span class="line"># determine whether it is empty</span><br><span class="line">if len(stack) == 0:</span><br><span class="line">	print(&quot;stack is empty&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h4><h5 id="list实现栈-1"><a href="#list实现栈-1" class="headerlink" title="list实现栈"></a>list实现栈</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># init</span><br><span class="line">queue = [1, 2, 3]</span><br><span class="line"># push</span><br><span class="line">queue.append(4)</span><br><span class="line"># pop</span><br><span class="line">queue.pop(0)</span><br><span class="line"># peek</span><br><span class="line">first = queue[0]</span><br><span class="line">last = queue[-1]</span><br><span class="line"># determine whether it is empty</span><br><span class="line">if len(queue) == 0:</span><br><span class="line">	print(&quot;queue is empty&quot;)</span><br></pre></td></tr></table></figure>

<h5 id="deque实现队列"><a href="#deque实现队列" class="headerlink" title="deque实现队列"></a>deque实现队列</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line"># init</span><br><span class="line">queue = deque([1, 2, 3])</span><br><span class="line"># push</span><br><span class="line">queue.append(4)</span><br><span class="line"># pop</span><br><span class="line">queue.popleft()</span><br><span class="line"># peek</span><br><span class="line">first = queue[0]</span><br><span class="line">last = queue[-1]</span><br><span class="line"># determine whether it is empty</span><br><span class="line">if len(queue) == 0:</span><br><span class="line">	print(&quot;queue is empty&quot;)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="线程安全的栈和队列"><a href="#线程安全的栈和队列" class="headerlink" title="线程安全的栈和队列"></a>线程安全的栈和队列</h3><h4 id="queue模块实现队列和栈"><a href="#queue模块实现队列和栈" class="headerlink" title="queue模块实现队列和栈"></a>queue模块实现队列和栈</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import queue</span><br><span class="line"># init, stack and queue</span><br><span class="line">sstack = queue.LifoQueue()</span><br><span class="line">squeue = queue.Queue()</span><br><span class="line"># push</span><br><span class="line">sstack.put(item)</span><br><span class="line"># pop</span><br><span class="line">sstack.get()</span><br><span class="line"># determine whether it is empty</span><br><span class="line">if sstack.empty():</span><br><span class="line">	print(&quot;sstack is empty&quot;)</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Stack</tag>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos——硬盘操作-分区和挂载</title>
    <url>/Notes/Linux/Centos%E2%80%94%E2%80%94%E7%A1%AC%E7%9B%98%E6%93%8D%E4%BD%9C-%E5%88%86%E5%8C%BA%E5%92%8C%E6%8C%82%E8%BD%BD.html</url>
    <content><![CDATA[<ul>
<li>参考博客:<a href="https://www.cnblogs.com/lizhangshu/p/9719018.html" target="_blank" rel="noopener">https://www.cnblogs.com/lizhangshu/p/9719018.html</a></li>
</ul>
<hr>
<h3 id="磁盘分区类型"><a href="#磁盘分区类型" class="headerlink" title="磁盘分区类型"></a>磁盘分区类型</h3><ul>
<li>三种分区<ul>
<li>主分区</li>
<li>扩展分区</li>
<li>逻辑分区</li>
</ul>
</li>
<li>分区规则<ul>
<li>主分区 + 扩展分区的数量不能超过4个</li>
<li>扩展分区只能有1个</li>
<li>逻辑分区要在扩展分区之上进行划分，逻辑分区没有数量限制，可以任意个</li>
<li>扩展分区是不能直接用的，他是以逻辑分区的方式来使用的，所以说扩展分区可分成若干逻辑分区。他们的关系是包含的关系，所有的逻辑分区都是扩展分区的一部分。</li>
</ul>
</li>
<li>硬盘的容量<ul>
<li>硬盘容量 = 主分区的容量 + 扩展分区的容量</li>
<li>扩展分区的容量 = 各个逻辑分区的容量之和</li>
</ul>
</li>
<li>为什么 主分区 + 扩展分区数量不能超过4个<ul>
<li>主分区就是普通磁盘分盘，但是由于磁盘设备由大量的扇区组成，一个扇区的容量为512字节。磁盘的第一个扇区最为重要，记录了主引导记录与分区表信息。就第一个扇区而言，主引导信息记录需要占用466个字节，分区表64个字节，结束符占用2个字节；其中分区表中每记录一个分区信息就需要16个字节，所以最多只有4个分区信息可以记录在第一个扇区中，所以主分区+扩展分区的数量不能超过4个。但是为了创建更多的分区，就使用扩展分区做份下若干个分区的指针，划分若干个逻辑分区，来满足分区数大于4个的需求。扩展分区不需要挂载，但是可以格式化。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="分区操作"><a href="#分区操作" class="headerlink" title="分区操作"></a>分区操作</h3><ul>
<li><p>查看当前设备上的磁盘信息及分区信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fdisk -l</span><br></pre></td></tr></table></figure>
</li>
<li><p>选取需要分区的磁盘</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fdisk /dev/[disk_name]</span><br></pre></td></tr></table></figure>

<ul>
<li>注意这里disk_name是硬盘名不是分区名称</li>
<li>一般来说硬盘名都是<code>sda</code>, <code>sdb</code>, <code>vda</code>, <code>vdb</code>等</li>
<li>一般来说硬盘<code>sda</code>上的分区名成为<code>sda1</code>,<code>sda2</code>等</li>
</ul>
</li>
<li><p>在进入分区磁盘后打印操作帮助</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">m</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">n</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择是主分区还是扩展分区</p>
<ul>
<li><p>主分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">p</span><br></pre></td></tr></table></figure>
</li>
<li><p>拓展分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">e</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>选择分区号[1-4]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>
</li>
<li><p>起始扇区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2048</span><br></pre></td></tr></table></figure>

<ul>
<li>一般默认值即可</li>
</ul>
</li>
<li><p>结束扇区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+200G</span><br></pre></td></tr></table></figure>

<ul>
<li>结束扇区在起始扇区基础上+200G, 表示该分区大小为200G</li>
<li>默认是全部分配给当前分区</li>
</ul>
</li>
</ul>
<hr>
<h3 id="格式化分区"><a href="#格式化分区" class="headerlink" title="格式化分区"></a>格式化分区</h3><ul>
<li><p>先输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkfs.</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>Tab</code>键,查看所有可能的命令</p>
</li>
</ul>
<blockquote>
<p>mkfs.bfs     mkfs.exfat   mkfs.ext3    mkfs.fat     mkfs.msdos   mkfs.vfat<br>mkfs.cramfs  mkfs.ext2    mkfs.ext4    mkfs.minix   mkfs.ntfs </p>
</blockquote>
<ul>
<li><p>用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkfs.ntfs</span><br></pre></td></tr></table></figure>
</li>
<li><p>说明:</p>
<ul>
<li>mkfs.ntfs命令可以将文件分区为</li>
</ul>
</li>
</ul>
<hr>
<h3 id="挂载分区"><a href="#挂载分区" class="headerlink" title="挂载分区"></a>挂载分区</h3><ul>
<li><p>新建挂载文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /mnt/data</span><br></pre></td></tr></table></figure>

<ul>
<li>建议新建在<code>/mnt/</code>文件夹下</li>
<li>比如我本地的机器为<code>/mnt/SSD</code>和<code>/mnt/HDD</code></li>
</ul>
</li>
</ul>
<h4 id="临时挂载"><a href="#临时挂载" class="headerlink" title="临时挂载"></a>临时挂载</h4><ul>
<li><p>分区临时挂载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount /dev/[part_name] /mnt/[dir_name]</span><br></pre></td></tr></table></figure>

<ul>
<li>没有文件系统的分区不能挂载</li>
</ul>
</li>
<li><p>分区临时卸载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">umount /dev/[part_name]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="开机重启自动挂载"><a href="#开机重启自动挂载" class="headerlink" title="开机重启自动挂载"></a>开机重启自动挂载</h4><h5 id="修改文件"><a href="#修改文件" class="headerlink" title="修改文件"></a>修改文件</h5><ul>
<li><p>查看想要挂载分区的UUID</p>
<ul>
<li><p>查看所有UUID</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blkid</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看某个分区的UUID</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blkid /dev/[part_name]</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>编辑文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/fstab</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">UUID=[UUID_NUMBER]   /data/[dir_name]  ext4  defaults   0 0</span><br><span class="line">分区临时卸 	挂载路径 		分区格式 	 	参数     是否备份 		引导分区相关(引导分区为1,其他分区为0或者2)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="修改生效的两种方式"><a href="#修改生效的两种方式" class="headerlink" title="修改生效的两种方式"></a>修改生效的两种方式</h5><ul>
<li><p>使用命令重新加载<code>/etc/fstab</code>的内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount -a</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新启动机器</p>
<ul>
<li>如果配置有错机器可能无法正常进入系统,但是会进入Emergency模式,我们可以在Emergency模式下修改<code>/etc/fstab</code>然后重新启动来修复问题</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——NFS服务器和客户端的配置</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94NFS%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E9%85%8D%E7%BD%AE.html</url>
    <content><![CDATA[<p><em>以Centos7为例, Ubuntu相似</em></p>
<hr>
<h3 id="NFS-Server"><a href="#NFS-Server" class="headerlink" title="NFS Server"></a>NFS Server</h3><ul>
<li><p>安装nfs所需的所有组件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum -y install nfs*</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置开机启动nfs和rpcbind服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable rpcbind.service</span><br><span class="line">systemctl enable nfs-server.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动nfs和rpcbind服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start rpcbind.service</span><br><span class="line">systemctl start nfs-server.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置exports文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/exports</span><br><span class="line">&gt;&gt;&gt; input</span><br><span class="line">/home/jiahong/SharedTest *(rw,no_root_squash,no_all_squash,sync)</span><br><span class="line">/home/jiahong/SharedTest 123.45.6.7(rw,no_root_squash,no_all_squash,sync)</span><br><span class="line">&gt;&gt;&gt; input done</span><br></pre></td></tr></table></figure>

<ul>
<li>这里的*号可以使用ip,表示只有这个ip可以访问共享文件</li>
<li>使用*则表示所有ip均可访问,设置多个ip可以访问则可使用多行</li>
</ul>
</li>
<li><p>使exports的配置生效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exportfs -rv</span><br></pre></td></tr></table></figure>

<ul>
<li>-r生效</li>
<li>-v显示结果</li>
</ul>
</li>
<li><p>查看是否生效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exportfs</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="NFS-Client"><a href="#NFS-Client" class="headerlink" title="NFS Client"></a>NFS Client</h3><ul>
<li><p>安装nfs所需的所有组件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum -y install nfs*</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置开机启动rpcbind服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable rpcbind.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动rpcbind服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start rpcbind.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看服务器哪些目录可以共享</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">showmount -e serverip</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建文件夹以作为mount目标</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p /mnt/nfs/shared_dir</span><br></pre></td></tr></table></figure>
</li>
<li><p>挂载操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount -t nfs serverip:server_dir client_dir</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看挂载情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure>
</li>
<li><p>解除挂载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">umount client_dir</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><ul>
<li><p>客户端出现以下情况时, 一般是服务器防火墙有问题, 解决方案是下面的解开防火墙的命令</p>
<ul>
<li>mount.nfs: No route to host</li>
<li>mount.nfs: Connection timed out<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --permanent --add-service=rpc-bind</span><br><span class="line">firewall-cmd --permanent --add-service=mountd</span><br><span class="line">firewall-cmd --permanent --add-port=2049/tcp</span><br><span class="line">firewall-cmd --permanent --add-port=2049/udp</span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>客户端出现以下情况时,说明客户端未umount但服务器解除文件夹了</p>
<ul>
<li>mount.nfs: Stale file handle</li>
<li>umount: xx/xx: Stale file handle<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">umount -lf /xx/xx</span><br><span class="line"></span><br><span class="line"># then mount /xx/xx again</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——文件操作</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C.html</url>
    <content><![CDATA[<p><em>本文记录一些Linux相关文件操作的常见问题</em></p>
<hr>
<h3 id="mv操作中断"><a href="#mv操作中断" class="headerlink" title="mv操作中断"></a>mv操作中断</h3><ul>
<li>由于mv操作等价于先执行cp然后执行rm操作<ul>
<li>还在cp阶段,原始数据是完整的,删除目标文档就行</li>
<li>如果已经进入rm阶段,那么说明目标文件时完整的,删除原始文件就行</li>
</ul>
</li>
</ul>
<hr>
<h3 id="硬链接与软链接"><a href="#硬链接与软链接" class="headerlink" title="硬链接与软链接"></a>硬链接与软链接</h3><p><em>Linux中链接分为两类:硬链接(hard link)和软链接(soft link),软链接又称为符号链接(symbolic link)</em></p>
<ul>
<li>如果一个文件有多个硬链接,那么需要所有硬连接都被删除,当前文件才会被删除<ul>
<li>原始文件与硬链接是同一个物理地址的两个不同名字</li>
<li>硬链接是相互的(个人理解: 一个普通的文件就可以理解为一个硬链接)</li>
</ul>
</li>
<li>如果一个文件有一个硬链接和多个软链接(符号链接),那么删除符号链接不影响原始文件<ul>
<li>只有文件的所有硬链接都没删除后文件才会被删除</li>
<li>文件被删除后软链接也会自动失效,链接路径链接不上</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——服务器防火墙配置</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%98%B2%E7%81%AB%E5%A2%99%E9%85%8D%E7%BD%AE.html</url>
    <content><![CDATA[<p><em>本文介绍Linux下服务器防火墙的设置</em><br>更详细的描述可以参考博客:<a href="https://blog.51cto.com/andyxu/2137046" target="_blank" rel="noopener">https://blog.51cto.com/andyxu/2137046</a></p>
<hr>
<h3 id="防火墙总结"><a href="#防火墙总结" class="headerlink" title="防火墙总结"></a>防火墙总结</h3><ul>
<li>iptables:内核层面的netfilter网络过滤器来处理</li>
<li>firewalld: 交由内核层面的nftables包过滤框架处理</li>
</ul>
<hr>
<h3 id="Centos6"><a href="#Centos6" class="headerlink" title="Centos6"></a>Centos6</h3><p><em>Centos6默认使用iptable作为防火墙</em></p>
<h4 id="查看防火墙状态"><a href="#查看防火墙状态" class="headerlink" title="查看防火墙状态"></a>查看防火墙状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service iptable status</span><br></pre></td></tr></table></figure>

<h4 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h4><ul>
<li><p>临时</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">servcie iptables stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>永久</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="打开防火墙"><a href="#打开防火墙" class="headerlink" title="打开防火墙"></a>打开防火墙</h4><ul>
<li><p>临时</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">servcie iptables start</span><br></pre></td></tr></table></figure>
</li>
<li><p>永久</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chkconfig iptables on</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Centos7"><a href="#Centos7" class="headerlink" title="Centos7"></a>Centos7</h3><p><em>Centos7默认使用的时firewall作为防火墙, 默认使用systemctl管理服务,接下来介绍systemctl管理firewall服务的操作,其他服务也可用systemctl以类似方法管理,只需将filewall名称换成其他服务名称即可</em></p>
<ul>
<li>关于服务名称的命名<ul>
<li>一般来说都是正常名称后加上一位’d.service’,比如’firewalld.service’,’mysqld.service’等</li>
</ul>
</li>
</ul>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ul>
<li>Centos7自带firewalld</li>
<li>Ubuntu:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install firewalld</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><ul>
<li>/usr/lib/firewalld/<ul>
<li>系统配置,尽量不修改</li>
</ul>
</li>
<li>/etc/firewalld/<ul>
<li>用户配置地址</li>
</ul>
</li>
</ul>
<h4 id="关于systemctl的使用"><a href="#关于systemctl的使用" class="headerlink" title="关于systemctl的使用"></a>关于systemctl的使用</h4><ul>
<li><p>列出所有服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl list-unit-files</span><br></pre></td></tr></table></figure>
</li>
<li><p>列出所有打开的服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl list-unit-files|grep enabled</span><br></pre></td></tr></table></figure>
</li>
<li><p>列出某个服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl list-unit-files|grep [service name]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="查看防火墙状态-1"><a href="#查看防火墙状态-1" class="headerlink" title="查看防火墙状态"></a>查看防火墙状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --state</span><br></pre></td></tr></table></figure>

<ul>
<li>输出<code>not running</code>或者<code>running</code></li>
</ul>
<h4 id="查看防火墙服务"><a href="#查看防火墙服务" class="headerlink" title="查看防火墙服务"></a>查看防火墙服务</h4><ul>
<li><p>方法一</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl list-unit-files|grep firewalld.service</span><br></pre></td></tr></table></figure>

<ul>
<li>输出<code>firewalld.service disabled</code>或者<code>firewalld.service enabled</code></li>
</ul>
</li>
<li><p>方法二</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure>

<ul>
<li>输出更详细的信息</li>
</ul>
</li>
</ul>
<h4 id="开机启动"><a href="#开机启动" class="headerlink" title="开机启动"></a>开机启动</h4><ul>
<li><p>禁止</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>允许</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable firewalld.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl is-enabled firewalld.service;echo $?</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="启动防火墙服务"><a href="#启动防火墙服务" class="headerlink" title="启动防火墙服务"></a>启动防火墙服务</h4><ul>
<li><p>启动防火墙后默认只开放22端口,其他端口都关闭</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start firewalld.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>不能启动的解决方案</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl unmask firewalld.service </span><br><span class="line">systemctl start firewalld.service</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="关闭防火墙服务"><a href="#关闭防火墙服务" class="headerlink" title="关闭防火墙服务"></a>关闭防火墙服务</h4><ul>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld.service</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="端口相关操作命令"><a href="#端口相关操作命令" class="headerlink" title="端口相关操作命令"></a>端口相关操作命令</h4><ul>
<li><p>查看所有以开放端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --list-ports</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看某个端口是否开启</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --query-port=8080/tcp</span><br></pre></td></tr></table></figure>
</li>
<li><p>开放端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=80/tcp --permanent</span><br></pre></td></tr></table></figure>

<ul>
<li>命令含义：<ul>
<li>–zone #作用域</li>
<li>–add-port=80/tcp #添加端口，格式为：端口/通讯协议</li>
<li>–permanent #永久生效，没有此参数重启后失效</li>
</ul>
</li>
</ul>
</li>
<li><p>重启服务</p>
</li>
<li><p>开放端口后需要重启服务才能生效*</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure>
</li>
<li><p>移除指定端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --permanent --remove-port=8080/tcp</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="防火墙的域"><a href="#防火墙的域" class="headerlink" title="防火墙的域"></a>防火墙的域</h4><h5 id="域的作用"><a href="#域的作用" class="headerlink" title="域的作用"></a>域的作用</h5><ul>
<li>一共9种,常用的就一种public,开放时把所有访问该端口的用户当做公共人员,不完全信任,trusted为完全信任:<ul>
<li>block dmz drop external home internal public trusted work</li>
<li>下面图片来自博客:<a href="https://blog.51cto.com/13503302/2095633" target="_blank" rel="noopener">https://blog.51cto.com/13503302/2095633</a><img src="/Notes/Linux/Linux——服务器防火墙配置/firewalld-zones.png"></li>
</ul>
</li>
<li>在开放端口时,可以为其添加域(默认为public),不同的域代表不同的信任</li>
</ul>
<h5 id="域的操作"><a href="#域的操作" class="headerlink" title="域的操作"></a>域的操作</h5><ul>
<li><p>查看默认zone</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --get-default-zone</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改默认zone</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --set-default-zone=public</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="防火墙与SSH登录"><a href="#防火墙与SSH登录" class="headerlink" title="防火墙与SSH登录"></a>防火墙与SSH登录</h4><p><em>默认启动firewall后,防火墙不会打开22端口[已测试],但为何还能正常ssh登录呢?</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --list-all</span><br></pre></td></tr></table></figure>

<blockquote>
<p>public<br>  target: default<br>  icmp-block-inversion: no<br>  interfaces:<br>  sources:<br>  services: ssh dhcpv6-client<br>  ports:<br>  protocols:<br>  masquerade: no<br>  forward-ports:<br>  source-ports:<br>  icmp-blocks:<br>  rich rules: </p>
</blockquote>
<ul>
<li>从上面的输出中可以看出来<code>ssh</code>服务默认被开启防火墙了,所以无需开启22端口即可使用ssh指令登录到服务器</li>
<li>同时开启的还有<code>dhcpv6-client</code>服务,这个服务用于ipv6的DHCP服务(为什么ipv4不需要这个服务呢?详情参考问答:<a href="https://unix.stackexchange.com/questions/176717/what-is-dhcpv6-client-service-in-firewalld-and-can-i-safely-remove-it" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/176717/what-is-dhcpv6-client-service-in-firewalld-and-can-i-safely-remove-it</a> )</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——查看服务器内核和系统版本</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E6%9F%A5%E7%9C%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%86%85%E6%A0%B8%E5%92%8C%E7%B3%BB%E7%BB%9F%E7%89%88%E6%9C%AC.html</url>
    <content><![CDATA[<p><em>本文主要介绍Linux系统的内核版本和系统版本等信息用命令行如何查看</em></p>
<hr>
<h3 id="系统类型和版本"><a href="#系统类型和版本" class="headerlink" title="系统类型和版本"></a>系统类型和版本</h3><h4 id="Centos"><a href="#Centos" class="headerlink" title="Centos"></a>Centos</h4><ul>
<li><p>文件存在表示为Centos</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat /etc/redhat-release</span><br></pre></td></tr></table></figure>
</li>
<li><p>上述指令同时会输出Centos版本</p>
</li>
</ul>
<h4 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h4><ul>
<li><p>命令可以执行表示为Ubuntu</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lsb_release -a</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行下面命令可以看出Ubuntu的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat /etc/issue</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="更进一步的内核信息"><a href="#更进一步的内核信息" class="headerlink" title="更进一步的内核信息"></a>更进一步的内核信息</h3><ul>
<li><p>内核名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uname -s</span><br></pre></td></tr></table></figure>
</li>
<li><p>结点名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uname -n</span><br></pre></td></tr></table></figure>
</li>
<li><p>内核发行号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uname -r</span><br></pre></td></tr></table></figure>
</li>
<li><p>处理器类型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uname -p</span><br></pre></td></tr></table></figure>
</li>
<li><p>操作系统</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uname -o</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu——自动加载bashrc</title>
    <url>/Notes/Linux/Ubuntu%E2%80%94%E2%80%94%E8%87%AA%E5%8A%A8%E5%8A%A0%E8%BD%BDbashrc.html</url>
    <content><![CDATA[<p><em>本文描述了如何为Ubuntu用户创建默认的<del>/.bashrc并设置自动加载</del>/.bashrc</em></p>
<hr>
<h3 id="创建-bashrc"><a href="#创建-bashrc" class="headerlink" title="创建.bashrc"></a>创建.bashrc</h3><ul>
<li><p>如果.bashrc存在,则无需创建,很多程序安装时可能会自动创建,否则,需要我们复制一个</p>
</li>
<li><p>复制命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp /etc/skel/.bashrc ~/</span><br></pre></td></tr></table></figure>
</li>
<li><p>亲测,无需复制,创建一个新的即可</p>
</li>
</ul>
<hr>
<h3 id="使-bashrc生效"><a href="#使-bashrc生效" class="headerlink" title="使.bashrc生效"></a>使.bashrc生效</h3><h4 id="暂时生效"><a href="#暂时生效" class="headerlink" title="暂时生效"></a>暂时生效</h4><p><em>用户重新登录不会生效</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<h4 id="永久生效"><a href="#永久生效" class="headerlink" title="永久生效"></a>永久生效</h4><p><em>用户登录后默认生效</em></p>
<ul>
<li><p>新建或打开文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加下面的语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ~/.profile: executed by Bourne-compatible login shells.</span><br><span class="line"> </span><br><span class="line">if [ &quot;$BASH&quot; ]; then</span><br><span class="line">  if [ -f ~/.bashrc ]; then</span><br><span class="line">    . ~/.bashrc</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"> </span><br><span class="line">mesg n</span><br></pre></td></tr></table></figure>
</li>
<li><p>保存后退出,以后默认的,我们登录后<code>~/.bashrc</code>即可生效</p>
</li>
<li><p>马上生效<code>~/.profile</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——rpm和deb包的区别</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94rpm%E5%92%8Cdeb%E5%8C%85%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p><em>Linux系统管理之rpm命令的使用</em></p>
<hr>
<h3 id="帮助信息"><a href="#帮助信息" class="headerlink" title="帮助信息"></a>帮助信息</h3><ul>
<li>执行<code>man rpm</code>可获取rpm命令的详细帮助信息</li>
</ul>
<hr>
<h3 id="Centos"><a href="#Centos" class="headerlink" title="Centos"></a>Centos</h3><ul>
<li>yum是用于安装和管理RPM包的</li>
<li>RPM包是一种预先在linux机器上被打包好的文件,文件后缀为<code>.rpm</code>,类似于Ubuntu上的deb</li>
</ul>
<h4 id="yum和rpm的区别"><a href="#yum和rpm的区别" class="headerlink" title="yum和rpm的区别"></a>yum和rpm的区别</h4><ul>
<li>yum和rpm都是管理RPM包的</li>
<li>yum可以联网下载需要的RPM包</li>
<li>yum自己可以处理依赖</li>
</ul>
<h4 id="Centos安装deb包"><a href="#Centos安装deb包" class="headerlink" title="Centos安装deb包"></a>Centos安装deb包</h4><ul>
<li><p>安装alien</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># download alien source code</span><br><span class="line"># uncompress source code</span><br><span class="line">tar -zxvf alien_x.xx.tar.gz</span><br><span class="line">cd alien</span><br><span class="line"># compile</span><br><span class="line">make </span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>转换deb包为rpm包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># generate a rpm package with name xxx.rpm</span><br><span class="line">alien -r xxx.deb</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装rpm包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh xxx.rpm</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><ul>
<li>apt-get是用于管理deb包的</li>
</ul>
<h4 id="Ubuntu上安装rpm包"><a href="#Ubuntu上安装rpm包" class="headerlink" title="Ubuntu上安装rpm包"></a>Ubuntu上安装rpm包</h4><ul>
<li><p>安装alien</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install alien</span><br></pre></td></tr></table></figure>
</li>
<li><p>转换</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># generate a deb package with name xxx.deb</span><br><span class="line">sudo alien xxx.rpm</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i xxx.deb</span><br></pre></td></tr></table></figure>
</li>
<li><p>说明</p>
<ul>
<li>不是所有的RPM包都能通过alien成功转换成deb包并成功安装的,能找到deb包的最好使用deb包安装</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Ubuntu使用deb包(apt-get, dpkg),Centos使用RPM包(yum, rpm)</li>
<li>deb包和RPM包可互相转换(使用alien包转换即可)</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——BERT</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94BERT.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考博客:<ul>
<li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/48612853" target="_blank" rel="noopener">BERT详解</a></li>
</ul>
</li>
<li>BERT论文: <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
</ul>
<hr>
<h3 id="BERT之前"><a href="#BERT之前" class="headerlink" title="BERT之前"></a>BERT之前</h3><h4 id="Word2Vec的缺点"><a href="#Word2Vec的缺点" class="headerlink" title="Word2Vec的缺点"></a>Word2Vec的缺点</h4><ul>
<li><strong>多义词问题</strong>: 传统的Word Embedding无法识别多义词<ul>
<li>确切的说是所有词的固定表征的方式(静态方式)的缺点</li>
<li>所谓静态指的是训练好之后每个单词的表达就固定住了</li>
</ul>
</li>
</ul>
<h4 id="从Word-Embedding到ELMo"><a href="#从Word-Embedding到ELMo" class="headerlink" title="从Word Embedding到ELMo"></a>从Word Embedding到ELMo</h4><p><em>ELMo, Embedding from Language Models</em><br><em>根据当前上下文对Word Embedding动态调整的思路</em></p>
<ul>
<li>ELMo 论文原文: <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">NAACL 2018 Best Paper, Deep contextualized word representations</a></li>
<li>ELMo的本质思想：<ul>
<li>事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分</li>
<li>实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示</li>
<li>这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了</li>
</ul>
</li>
<li>ELMo是典型的两阶段训练过程: 预训练 + 特征融合?<ul>
<li>第一个阶段是利用语言模型进行预训练</li>
<li>第二阶段通过基于特征融合的方式训练</li>
</ul>
</li>
<li>ELMo预训练过程示意图<img src="/Notes/DL/DL——BERT/ELMo_PreTraining.jpg">
<img src="/Notes/DL/DL——BERT/ELMo_PreTraining_2.jpg"></li>
<li>ELMo预训练后如何处理下游任务?<img src="/Notes/DL/DL——BERT/ELMo_Usage.jpg">
<ul>
<li>预训练训练完成后, 模型训练时使用在线特征抽取,和特征集成的方式对词向量在不同的上下文中进行不同的修正,从而区分多义词</li>
</ul>
</li>
</ul>
<h5 id="补充知识-下游任务"><a href="#补充知识-下游任务" class="headerlink" title="补充知识: 下游任务"></a>补充知识: 下游任务</h5><p><em>下游任务包括很多, 整体上可以分为四大类</em></p>
<h6 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h6><ul>
<li>分词</li>
<li>POS Tag</li>
<li>NER</li>
<li>语义标注</li>
<li>…</li>
</ul>
<h6 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h6><ul>
<li>文本分类</li>
<li>情感计算</li>
<li>…</li>
</ul>
<h6 id="句子关系判断"><a href="#句子关系判断" class="headerlink" title="句子关系判断"></a>句子关系判断</h6><ul>
<li>Entailment</li>
<li>QA</li>
<li>自然语言推理</li>
<li>…</li>
</ul>
<h6 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h6><ul>
<li>机器翻译</li>
<li>文本摘要</li>
<li>…</li>
</ul>
<h5 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h5><ul>
<li>预训练模型是什么?<ul>
<li><strong>预训练模型是指在训练结束是结果比较好的一组权重值,研究人员分享出来供他人使用,基于这些预训练好的权重可以提升我们的模型训练速度和精度</strong></li>
<li>预训练模型能够成功的本质是我们假设预训练模型足够好, 能学到句子的所有信息(包括序列信息等)</li>
</ul>
</li>
<li>两阶段预训练模型如何处理下游任务?<ul>
<li>预训练与下游任务无关<ul>
<li>预训练阶段是预训练模型自己选择相应的NLP任务,然后让模型在学习处理这些任务的途中实现参数的训练</li>
<li>比如BERT选择的就是MLM(屏蔽语言模型)和NSP(Next Sentence Predition, 下一个句子预测)两个任务来做预训练</li>
</ul>
</li>
<li>不同的下游任务往往需要修改第二阶段中的模型结构等</li>
<li>为适应不同的下游任务, 第二阶段可能使用不同结构, 比如添加Softmax层等方式</li>
</ul>
</li>
</ul>
<h5 id="ELMo的优缺点"><a href="#ELMo的优缺点" class="headerlink" title="ELMo的优缺点"></a>ELMo的优缺点</h5><ul>
<li>优点:<ul>
<li>很好的解决了多义词问题,而且效果非常好</li>
<li>采用上下文来训练词(从上下文预测单词, 上文称为Context-before, 下文称为Context-after)</li>
</ul>
</li>
<li>缺点:<ul>
<li>特征提取器没有使用新贵Transformer, 而是传统的LSTM, 特征抽取能力不足</li>
</ul>
</li>
</ul>
<h4 id="从Word-Embedding到GPT"><a href="#从Word-Embedding到GPT" class="headerlink" title="从Word Embedding到GPT"></a>从Word Embedding到GPT</h4><p><em>GPT, Generative Pre-Training</em></p>
<ul>
<li>ELMo的训练方法和图像领域的预训练方法对比,模式不同, ELMo使用的是<strong>基于特征融合的预训练方法</strong></li>
<li>GPT使用的预训练方法则是在NLP领域开创了和图像领域一致的预训练方法<strong>基于Fine-tuning的模式</strong><img src="/Notes/DL/DL——BERT/GPT_Overview.jpg"></li>
<li>GPT也采用两阶段过程: 预训练 + Fine-tuning<ul>
<li>第一个阶段是利用语言模型进行预训练</li>
<li>第二阶段通过Fine-tuning的模式训练</li>
</ul>
</li>
<li>GPT预训练后如何处理下游任务?<img src="/Notes/DL/DL——BERT/GPT_Usage.jpg"></li>
<li>一些下游任务的Fine-tuning结构<img src="/Notes/DL/DL——BERT/GPT_Usage_2.png">

</li>
</ul>
<h4 id="GPT的优缺点"><a href="#GPT的优缺点" class="headerlink" title="GPT的优缺点"></a>GPT的优缺点</h4><ul>
<li>优点:<ul>
<li>特征提取器是Transformer,不是RNN, 所以特征提取效果好</li>
</ul>
</li>
<li>缺点<ul>
<li>GPT使用的是<strong>单向语言模型</strong>: 也就是说只用到了上文来预测词</li>
<li>词嵌入时没有单词的下文, 失去了很多信息</li>
</ul>
</li>
</ul>
<hr>
<h3 id="BERT结构和原理"><a href="#BERT结构和原理" class="headerlink" title="BERT结构和原理"></a>BERT结构和原理</h3><p><em>下面的讲解都将按照原论文的思路讲解</em></p>
<ul>
<li>BERT(<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers), 原文 <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li>下图是BERT与GPT和ELMo对比的结构图<img src="/Notes/DL/DL——BERT/BERT_GPT_ELMo_PreTraining.png">
<ul>
<li>图中的每个 <code>Trm</code> 组件就是一个 Transformer 的Encoder部分,也就是下图中的左半部分<img src="/Notes/DL/DL——BERT/Transfomer_Architecture.png">


</li>
</ul>
</li>
</ul>
<h4 id="BERT的特点"><a href="#BERT的特点" class="headerlink" title="BERT的特点"></a>BERT的特点</h4><ul>
<li>一体化特征融合的双向(Bidirectional)语言模型<ul>
<li>利用语言的双向信息</li>
<li>GPT是单向语言模型, 只能利用一个方向的信息</li>
<li>ELMo也是双向语言模型,但是 ELMo实际上是两个方向相反的单向语言模型的拼接, 融合特征的能力比BERT那种一体化的融合特征方式弱</li>
</ul>
</li>
<li>特征提取器:<ul>
<li>使用Transformer(实际上使用的是Transformer的Encoder部分, 图中每个)</li>
</ul>
</li>
<li>预训练任务: <ul>
<li><strong>屏蔽语言模型</strong>(MLM, Masked Language Model) + <strong>相邻句子判断</strong>(NSP, Next Sentence Prediction)两个任务的多任务训练目标</li>
</ul>
</li>
<li>训练数据量:<ul>
<li>超大规模的数据训练使得BERT结果达到了全新的高度</li>
<li>可以使用BERT作为词嵌入(Word2Vec)的转换矩阵, 实现比其他词嵌入模型更好的结果</li>
</ul>
</li>
</ul>
<h4 id="输入表示"><a href="#输入表示" class="headerlink" title="输入表示"></a>输入表示</h4><ul>
<li>输入结构图<img src="/Notes/DL/DL——BERT/BERT_Input_Representation.png"></li>
<li>BERT输入是一个512维的编码向量, 是三个嵌入特征的单位和</li>
</ul>
<h5 id="WordPiece嵌入"><a href="#WordPiece嵌入" class="headerlink" title="WordPiece嵌入"></a>WordPiece嵌入</h5><ul>
<li>对应图中的Token Embedding</li>
<li>WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡</li>
<li>举例: 原文中 “playing” 被拆分成了 “play” 和 “##ing” 两部分</li>
</ul>
<h5 id="Segment-Embedding"><a href="#Segment-Embedding" class="headerlink" title="Segment Embedding"></a>Segment Embedding</h5><p><em>分割嵌入</em></p>
<ul>
<li>对应图中的Segment Embedding</li>
<li>用于区分两个句子，例如B是否是A的下文(对话场景，问答场景等)</li>
<li>对于句子对，第一个句子的特征值是0，第二个句子的特征值是1, 从而模型可以表达出词出现在前一个句子还是后一个句子</li>
</ul>
<h5 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h5><p><em>位置嵌入</em></p>
<ul>
<li>对应图中的 Position Embedding</li>
<li>位置嵌入是指将单词的位置信息编码成特征向量</li>
<li>这是继承自论文<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a>中, 文章中的Transformer架构没有使用RNN,不能编码位置信息,就是在进入Attention前使用了 词嵌入 + 位置嵌入 的方式让模型能够表达位置信息的</li>
</ul>
<h4 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h4><ul>
<li>通常预训练是指在训练阶段让模型去解决自然语言任务, 从而训练完成后得到可移植到其他模型(或者当前模型)使用的参数(包括词向量等)</li>
<li>BERT的预训练使用了两个NLP任务: MLM + NSP</li>
<li>BERT预训练和使用概览:<img src="/Notes/DL/DL——BERT/BERT_PreTraining_and_FineTraining_Overview.png">
<ul>
<li>从上图可以看出, BERT的预训练包含了两方面的任务, NSP和 MLM</li>
<li>实验证明, MLM 优于标准的 LTR(left-to-right)语言模型(OpenAI GPT 使用的就是这个)</li>
</ul>
</li>
</ul>
<h5 id="屏蔽语言模型-MLM"><a href="#屏蔽语言模型-MLM" class="headerlink" title="屏蔽语言模型(MLM)"></a>屏蔽语言模型(MLM)</h5><p><em>Masked Language Model</em></p>
<ul>
<li>Masked Language Model(MLM)核心思想取自Wilson Taylor在1953年发表的一篇论文<a href="https://journals.sagepub.com/doi/abs/10.1177/107769905303000401" target="_blank" rel="noopener">“Cloze Procedure”: A New Tool for Measuring Readability</a></li>
<li>在训练的时候随机从输入预料上屏蔽(Mask)掉一些单词，然后通过的上下文预测该单词(“完形填空”)</li>
<li>传统的语言模型是Left-to-Right(LTR)或者是Right-to-Left(RTL)的, 和 RNN 结构匹配</li>
<li>MLM 的性质 和 Transformer 的结构匹配</li>
<li>BERT实验中, 有15%的WordPiece Token句子会被屏蔽掉, 在屏蔽的时候,又有不同的概率</li>
<li>如果已经选中(15%概率)要屏蔽 <code>my dog is hairy</code> 中的 <code>hairy</code>, 那么我们的处理方式是:<ul>
<li>80%： <code>my dog is hairy -&gt; my dog is [MASK]</code></li>
<li>10%： <code>my dog is hairy -&gt; my dog is apple</code></li>
<li>10%： <code>my dog is hairy -&gt; my dog is hairy</code></li>
<li>防止句子中的某个Token 100%都会被mask掉，那么在Fine-tuning的时候模型就会有一些没有见过的单词</li>
<li>加入随机Token的原因是因为Transformer要保持对每个输入Token的分布式表征，否则模型就会记住这个[MASK]是 Token “hairy”</li>
<li>错误单词带来的负面影响: 一个单词被随机替换掉的概率只有\(15% \times 10% = 1.5%\) 这个负面影响其实是可以忽略不计的</li>
</ul>
</li>
<li>另外: 文章指出每次只预测15%的单词，因此模型收敛的比较慢</li>
</ul>
<h6 id="为什么使用MLM"><a href="#为什么使用MLM" class="headerlink" title="为什么使用MLM"></a>为什么使用MLM</h6><ul>
<li>因为效果好,解释就是MLM更符合Transformer的结构</li>
<li>论文中的实验结果:<img src="/Notes/DL/DL——BERT/BERT_MLM_and_LTR_in_MNLI.png">
<ul>
<li>MNLI(Multi-Genre Natural Language Inference)是多类型自然语言推理任务, 是一个大规模的众包蕴含分类任务, 给定一个句子,目标是预测第二句相对与第一句是一个蕴含语句, 矛盾语句, 还是中性语句</li>
<li>从图中可以看出,在MNLI任务中, MLM预训练 + MNLI Fine-tuning 的效果明显优于 LTR预训练 + MNLI Fine-tuning 的效果</li>
</ul>
</li>
</ul>
<h5 id="相邻句子预测-NSP"><a href="#相邻句子预测-NSP" class="headerlink" title="相邻句子预测(NSP)"></a>相邻句子预测(NSP)</h5><p><em>Next Sentence Prediction</em></p>
<ul>
<li>NSP 的任务是判断句子B是否是句子A的下文</li>
<li>图中的[CLS]符号就是用于分类的, 如果是的话输出’IsNext‘，否则输出’NotNext‘</li>
<li>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的</li>
<li>举例来说: <ul>
<li><strong>Input</strong> = <code>[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</code></li>
<li><strong>Label</strong> = <code>IsNext</code></li>
<li><strong>Input</strong> = <code>[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</code></li>
<li><strong>Label</strong> = <code>NotNext</code></li>
</ul>
</li>
</ul>
<h4 id="Fine-tuning-处理下游任务"><a href="#Fine-tuning-处理下游任务" class="headerlink" title="Fine-tuning 处理下游任务"></a>Fine-tuning 处理下游任务</h4><p><em>Fine-tining, 中文也称为<strong>微调</strong></em></p>
<ul>
<li><p>下图是BERT在不同任务的的微调方法</p>
<img src="/Notes/DL/DL——BERT/BERT_FineTraing_in_Different_Task.png">
</li>
<li><p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同</p>
<ul>
<li>句子类关系任务: 和GPT一样,增加起始和终结符号,输出部分Transformer最后一层每个单词对应部分都进行分类即可</li>
<li>除了生成任务外, 其他任务Bert都涉及到了</li>
</ul>
</li>
</ul>
<hr>
<h3 id="BERT的使用"><a href="#BERT的使用" class="headerlink" title="BERT的使用"></a>BERT的使用</h3><ul>
<li>Google公开了两个不同规模的 BERT模型:<ul>
<li><strong>\(BERT_{BASE}\)</strong> : 110M模型参数</li>
<li><strong>\(BERT_{LARGE}\)</strong>: 340M模型参数</li>
</ul>
</li>
<li>同时公开了两个模型在大规模数据预训练后的参数集合, 供开发者下载和使用</li>
</ul>
<hr>
<h3 id="基于BERT的新秀"><a href="#基于BERT的新秀" class="headerlink" title="基于BERT的新秀"></a>基于BERT的新秀</h3><ul>
<li>Token仍然使用词, 但是MLM屏蔽时选择屏蔽短语或者实体</li>
</ul>
<h4 id="ERNIE-from-Baidu"><a href="#ERNIE-from-Baidu" class="headerlink" title="ERNIE from Baidu"></a>ERNIE from Baidu</h4><ul>
<li>参考文章: [ERNIE: Enhanced Representation through Knowledge Integration]</li>
<li>核心思想: <ul>
<li>用先验知识来加强预训练模型(考虑实体,短语等级别的屏蔽)</li>
<li>在BERT的预训练阶段, MLM模型中屏蔽一个实体(Entity)或者短语(Phrase)而不是屏蔽一个字(Word)<img src="/Notes/DL/DL——BERT/Different_Masking_ERNIE_from_Baidu_and_BERT.png"></li>
</ul>
</li>
<li>文中提出三种级别的屏蔽方式<img src="/Notes/DL/DL——BERT/Different_Masking_Level_of_a_Sentence.png">
<ul>
<li>基本级别(Basic-level)</li>
<li>实体级别(Entity-level)</li>
<li>短语级别(Phrase-level)</li>
</ul>
</li>
</ul>
<h5 id="实验对比"><a href="#实验对比" class="headerlink" title="实验对比"></a>实验对比</h5><img src="/Notes/DL/DL——BERT/ERNIE_from_baidu_vs_BERT.png">

<h4 id="ERINE-from-THU"><a href="#ERINE-from-THU" class="headerlink" title="ERINE from THU"></a>ERINE from THU</h4><ul>
<li>参考文章: <a href="https://arxiv.org/pdf/1905.07129.pdf" target="_blank" rel="noopener">ERNIE: Enhanced Language Representation with Informative Entities</a></li>
<li>核心思想:<ul>
<li>利用先验知识来加强预训练模型(引入知识图谱)</li>
<li>提出了将知识纳入语言表达模型的方法</li>
<li>使用知识聚合器(Knowledgeable aggregator)和预训练任务 dEA, 更好的融合来自文本和知识图谱的异构信息</li>
</ul>
</li>
<li>知识信息<img src="/Notes/DL/DL——BERT/Knowledge_inofrmation_Overview.png"></li>
<li>模型架构<img src="/Notes/DL/DL——BERT/ERNIE_from_THU_Overview.png"></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——负样本采样修正</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E8%B4%9F%E6%A0%B7%E6%9C%AC%E9%87%87%E6%A0%B7%E4%BF%AE%E6%AD%A3.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<hr>
<h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><ul>
<li>在CTR预估模型中，通常需要对负样本进行采样，以提升训练效率</li>
<li>负样本采样会改变样本分布，从而导致预估均值出现偏差</li>
</ul>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>假设对负样本进行采样，此时数据分布（正负样本比例）会发生改变，为了保证预估均值，需要对模型进行修正</li>
</ul>
<h3 id="修正方式"><a href="#修正方式" class="headerlink" title="修正方式"></a>修正方式</h3><ul>
<li><em>修正方式一般有两种</em></li>
</ul>
<h4 id="训练时修正"><a href="#训练时修正" class="headerlink" title="训练时修正"></a>训练时修正</h4><ul>
<li>灵活，可以在训练时考虑到修正，保证训练后的模型可以直接使用<h4 id="训练后修正"><a href="#训练后修正" class="headerlink" title="训练后修正"></a>训练后修正</h4></li>
<li>不常用，需要记录训练时的采样率等，容易出现问题</li>
</ul>
<h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><h4 id="训练时修正推导"><a href="#训练时修正推导" class="headerlink" title="训练时修正推导"></a>训练时修正推导</h4><ul>
<li><p>假设原始样本数，正样本数，负样本数分别为\(N,N_p,N_n\)</p>
</li>
<li><p>采样前：</p>
<ul>
<li>令正样本概率为\(p = \frac{N_p}{N}\)，则有\(1-p = \frac{N_n}{N}\)</li>
<li>进一步有：\(\frac{p}{1-p} = \frac{N_p}{N_n}\)</li>
</ul>
</li>
<li><p>按照比例\(r\)对负样本进行采样后剩下负样本数为\(r\cdot N_n\)：</p>
<ul>
<li>同理有：\(\frac{p’}{1-p’} = \frac{N_p}{N_n/r} = r\cdot\frac{N_p}{N_n}\)</li>
</ul>
</li>
<li><p>假定CTR的输出经过sigmoid激活函数输出概率值</p>
</li>
<li><p>为了保证预估时可以使用如下公式：<br>  $$ p = \frac{1}{1+e^{-wx}} $$</p>
</li>
<li><p>需要满足的训练公式为：<br>  $$ p‘ = f(x) $$</p>
</li>
<li><p>求解\(f\)的流程如下：</p>
<ul>
<li>由sigmoid函数(\(p = \frac{1}{1+e^{-wx}}\))的定义可得：<br>  $$ wx = ln \frac{p}{1-p} $$</li>
<li>又因为：<br>  $$<br>  \frac{p}{1-p} = \frac{N_p}{N_n} \\<br>  \frac{p’}{1-p’} = \frac{1}{r}\cdot\frac{N_p}{N_n} \\<br>  ln(\frac{p’}{1-p’}) = ln(\frac{1}{r}\cdot\frac{p}{1-p}) = ln(\frac{1}{r}) + ln(\frac{p}{1-p}) \\<br>  $$</li>
<li>所以有:<br>  $$ ln(\frac{p’}{1-p’}) = ln(\frac{1}{r}) + ln(\frac{p}{1-p}) = ln(\frac{1}{r}) + wx $$</li>
<li>于是有预估时的公式为：<br>  $$ p’ = \frac{1}{1+e^{-(wx+ln(\frac{1}{r}))}} $$</li>
</ul>
</li>
<li><p>综上所述，<strong>训练时</strong>下面的式子训练：<br>  $$ p’ = \frac{1}{1+e^{-(wx+ln(\frac{1}{r}))}} $$</p>
</li>
<li><p><strong>预估时</strong>可以按照下面的式子预估：<br>  $$ p = \frac{1}{1+e^{-wx}} $$</p>
</li>
</ul>
<h4 id="训练后修正的推导"><a href="#训练后修正的推导" class="headerlink" title="训练后修正的推导"></a>训练后修正的推导</h4><ul>
<li>由两者的定义可以推导如下：<br>  $$<br>  \frac{p}{1-p} = \frac{N_p}{N_n} \\<br>  \frac{p’}{1-p’} = \frac{1}{r}\cdot\frac{N_p}{N_n} \\<br>  $$</li>
<li>进一步有：<br>  $$<br>  \frac{p’}{1-p’} = \frac{1}{r}\cdot\frac{p}{1-p} \\<br>  \frac{p}{1-p} = r \cdot \frac{p’}{1-p’} = \frac{r \cdot p’}{1-p’} \\<br>  $$</li>
<li>调整位置得到：<br>  $$<br>  \frac{1-p}{p} = \frac{1-p’}{r \cdot p’} \\<br>  $$</li>
<li>即：<br>  $$<br>  \frac{1}{p} - 1 = \frac{1-p’}{r \cdot p’} \\<br>  $$</li>
<li>进而有：<br>  $$<br>  \frac{1}{p} =  \frac{r \cdot p’+1-p’}{r \cdot p’} \\<br>  $$</li>
<li>最终得到：<br>  $$<br>  p =  \frac{r \cdot p’}{r \cdot p’+1-p’}  \\<br>  p = \frac{ p’}{p’+\frac{1-p’}{r}}\\<br>  $$</li>
<li>综上所述，<strong>训练时</strong>下面的式子训练：<br>  $$ p’ = \frac{1}{1+e^{-wx}} $$</li>
<li><strong>预估时</strong>可以按照下面的式子预估：<br>  $$ p = \frac{ p’}{p’+\frac{1-p’}{r}} $$</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——模型压缩技术</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF.html</url>
    <content><![CDATA[<h3 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h3><ul>
<li>模型压缩（Model Compression）技术包含模型量化、蒸馏、剪枝等</li>
<li>有时候模型量化和蒸馏会同时使用</li>
<li>剪枝不常见</li>
</ul>
<h3 id="模型量化"><a href="#模型量化" class="headerlink" title="模型量化"></a>模型量化</h3><ul>
<li>模型量化（Model Quantization），是一种将模型中的权重和激活从浮点数（高位宽）转换为低精度（低位宽）的表示的方法，如8位整数（INT8）</li>
<li>数值上来看，量化是将连续值离散化的过程</li>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/693034534" target="_blank" rel="noopener">TinyML —— 模型量化（quantization）</a></li>
<li>B站北大学生汇报：<a href="https://www.bilibili.com/video/BV1xf4y1f7wn/" target="_blank" rel="noopener">模型量化加速</a></li>
<li><a href="https://juejin.cn/post/7259275893795782717" target="_blank" rel="noopener">QLoRA: 训练更大的GPT</a></li>
</ul>
</li>
</ul>
<h4 id="线性量化与非线性量化"><a href="#线性量化与非线性量化" class="headerlink" title="线性量化与非线性量化"></a>线性量化与非线性量化</h4><ul>
<li>线性量化(linear quantization)，也叫仿射量化(affine quantization)或者均匀量化<ul>
<li>我们很容易给出量化公式：<br>其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。</li>
</ul>
</li>
<li>非线性量化（nonlinear quantization），也叫作非均匀量化</li>
</ul>
<h4 id="对称量化与非对称量化"><a href="#对称量化与非对称量化" class="headerlink" title="对称量化与非对称量化"></a>对称量化与非对称量化</h4><ul>
<li>对称量化(symmetric quantization)：映射前后0点相同</li>
<li>非对称量化(asymmetric quantization)：映射前后0点不相同</li>
</ul>
<h4 id="量化粒度"><a href="#量化粒度" class="headerlink" title="量化粒度"></a>量化粒度</h4><ul>
<li>Per-Tensor Quantization（逐张量量化）：<ul>
<li>这是最简单的量化方式，对整个张量（即模型中的一个参数矩阵或输入数据）使用相同的量化参数（比如最小值和最大值，或者量化因子）。</li>
<li>由于所有值共享相同的量化参数，因此这种方法的精度较低，但计算简单，存储和传输效率高。</li>
</ul>
</li>
<li>Per-Channel Quantization（逐通道量化）：<ul>
<li>在这种量化方式中，每个通道（对于卷积神经网络中的滤波器来说，通道通常指的是滤波器输出的不同颜色或特征）使用不同的量化参数。</li>
<li>这种方法比逐张量量化更精细，因为不同的通道可能具有不同的值范围，因此可以独立地进行量化，以保持每个通道的精度。</li>
<li>因为有研究发现不同Channel的参数量级差距较大</li>
</ul>
</li>
<li>Per-Layer Quantization（逐层量化）：<ul>
<li>逐层量化意味着网络中的每一层都有自己的量化参数集。</li>
<li>这种方法允许每一层根据其激活值的范围独立地进行量化，这可能比逐张量量化提供更好的精度，但比逐通道量化的计算成本要低。</li>
</ul>
</li>
<li>Per-Axis Quantization（逐轴量化）：<ul>
<li>这种量化方式通常用于多维张量，比如二维的权重矩阵。在这种情况下，”axis”可以指特定的维度，比如行或列。</li>
<li>逐轴量化意味着沿着张量的一个或多个维度，量化参数是不同的。例如，在二维张量中，可以对每一行或每一列使用不同的量化参数。</li>
</ul>
</li>
</ul>
<h4 id="量化方式（PTQ-vs-QAT"><a href="#量化方式（PTQ-vs-QAT" class="headerlink" title="量化方式（PTQ vs QAT)"></a>量化方式（PTQ vs QAT)</h4><ul>
<li>PTQ(Post training quantization)，后训练量化，训完的模型直接量化，然后进行推理</li>
<li>QAT(Quantization aware training)，量化感知训练，训练完的模型加载到内存，进行微调后再用于推理<ul>
<li>LLM常用的思路就是float16训练base模型并存储，SFT时使用INT8量化并使用LoRA微调模型，然后存储LoRA参数，推理时加载base模型(INT8量化加载)和LoRA参数一起推理</li>
</ul>
</li>
</ul>
<h4 id="量化位宽"><a href="#量化位宽" class="headerlink" title="量化位宽"></a>量化位宽</h4><ul>
<li>统一位宽</li>
<li>混合精度</li>
</ul>
<h4 id="量化模型训练梯度"><a href="#量化模型训练梯度" class="headerlink" title="量化模型训练梯度"></a>量化模型训练梯度</h4><ul>
<li>梯度回传时量化是离散的，梯度为0，实际上可以设置为1，因为量化一般是阶梯函数，类似于线性的</li>
</ul>
<h4 id="最新的量化模型"><a href="#最新的量化模型" class="headerlink" title="最新的量化模型"></a>最新的量化模型</h4><h5 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h5><ul>
<li>QLoRA是一种高效的微调方法，它允许在保持完整的16位微调任务性能的同时，将内存使用量降低到足以在单个48GB GPU上微调650亿参数模型。QLoRA通过冻结的4位量化预训练语言模型向低秩适配器（Low Rank Adapters，简称LoRA）反向传播梯度。这种方法的主要贡献包括：<ul>
<li>4-bit NormalFloat (NF4)：一种新的数据类型，理论上对正态分布的权重是最优的。</li>
<li>Double Quantization：通过量化量化常数来减少平均内存占用。</li>
<li>Paged Optimizers：使用NVIDIA统一内存特性，自动在CPU和GPU之间进行页面转换，以避免梯度检查点操作时内存不足。</li>
</ul>
</li>
<li>QLoRA主要用于微调训练阶段，使得在单个GPU上进行大型模型的微调成为可能，这对于资源有限的研究者和开发者来说是一个重大突破。</li>
</ul>
<h5 id="GPTQ"><a href="#GPTQ" class="headerlink" title="GPTQ"></a>GPTQ</h5><ul>
<li>GPTQ（Generative Pre-trained Transformer Quantization）是一种针对生成预训练Transformer模型的量化技术。GPTQ旨在解决大型GPT模型的高计算和存储成本问题。这些模型由于其庞大的规模，即使在高性能GPU上进行推理也需要大量的计算资源。GPTQ通过以下方式来提高效率：<ul>
<li>一次性权重量化：基于近似二阶信息的方法，可以在不需要重新训练的情况下压缩模型。</li>
<li>高压缩率：能够将模型量化到每个权重3或4位，同时几乎不降低准确度。</li>
<li>高效执行：允许在单个GPU上执行大型参数模型的生成推理，显著减少了所需的硬件资源。</li>
</ul>
</li>
<li>GPTQ通过用于部署推理阶段，用于减少模型的大小和内存占用，使得这些大型模型更易于部署和使用。</li>
</ul>
<hr>
<h3 id="模型蒸馏"><a href="#模型蒸馏" class="headerlink" title="模型蒸馏"></a>模型蒸馏</h3><ul>
<li>模型蒸馏（Model Distillation），主要包含Logits蒸馏和特征蒸馏两大类</li>
<li>以模型压缩为目标的模型蒸馏通常也叫作知识蒸馏，知识蒸馏主要目的是将一个大型、复杂、训练良好的模型（教师模型）的知识转移到一个小型、简单、更易于部署的模型（学生模型）中。<img src="/Notes/DL/DL——模型压缩技术/Model-Distillation-Logits-vs-Features.png"></li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/583273832" target="_blank" rel="noopener">知识蒸馏算法汇总</a><blockquote>
<p>知识蒸馏有两大类：一类是logits蒸馏，另一类是特征蒸馏。logits蒸馏指的是在softmax时使用较高的温度系数，提升负标签的信息，然后使用Student和Teacher在高温softmax下logits的KL散度作为loss。中间特征蒸馏就是强迫Student去学习Teacher某些中间层的特征，直接匹配中间的特征或学习特征之间的转换关系。例如，在特征No.1和No.2中间，知识可以表示为如何模做两者中间的转化，可以用一个矩阵让学习者产生这个矩阵，学习者和转化之间的学习关系。 这篇文章汇总了常用的知识蒸馏的论文和代码，方便后续的学习和研究。</p>
</blockquote>
</li>
</ul>
<h4 id="Logits蒸馏"><a href="#Logits蒸馏" class="headerlink" title="Logits蒸馏"></a>Logits蒸馏</h4><ul>
<li>关注输出层</li>
<li>也叫作基于反馈的知识蒸馏</li>
<li>学生模型被训练以模仿教师模型的输出概率分布。通过最小化两个模型输出概率分布之间的KL散度（或其他相似性度量），学生模型学习教师模型的“软目标”，即对每个类别的概率预测，而不是单一的预测标签。</li>
</ul>
<h4 id="特征蒸馏"><a href="#特征蒸馏" class="headerlink" title="特征蒸馏"></a>特征蒸馏</h4><ul>
<li>关注中间层</li>
<li>也叫作基于特征的知识蒸馏</li>
<li>学生模型被训练以模仿教师模型在中间层的激活或特征图。这通常通过最小化两个模型对应层的特征表示之间的距离（如L2距离或cosine相似度）来实现。</li>
</ul>
<hr>
<h3 id="模型剪枝"><a href="#模型剪枝" class="headerlink" title="模型剪枝"></a>模型剪枝</h3><ul>
<li>Model Pruning</li>
<li>通过删减网络结构然后调整分布实现模型压缩，剪枝方法不常用</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Git——管理HuggingFace项目</title>
    <url>/Notes/Git/Git%E2%80%94%E2%80%94%E7%AE%A1%E7%90%86HuggingFace%E9%A1%B9%E7%9B%AE.html</url>
    <content><![CDATA[<hr>
<h3 id="LFS环境安装"><a href="#LFS环境安装" class="headerlink" title="LFS环境安装"></a>LFS环境安装</h3><ul>
<li>配置HuggingFace环境需要安装<code>git lfs</code></li>
<li>LFS全称Large File Storge，即大文件存储，可以帮助我们管理比较大的文件</li>
<li>Ubuntu安装<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash</span><br><span class="line">sudo apt-get install git-lfs</span><br><span class="line">git lfs install</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h3 id="拉取HuggingFace文件"><a href="#拉取HuggingFace文件" class="headerlink" title="拉取HuggingFace文件"></a>拉取HuggingFace文件</h3><ul>
<li><p>执行拉取操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https://huggingface.co/xxx/xxx</span><br></pre></td></tr></table></figure>
</li>
<li><p>该命令可在HuggingFace网站项目首页查找</p>
</li>
<li><p><code>git clone</code>命令可以将整个项目下载下来，但是下载大的文件时可能会特别慢，最佳实践是：</p>
<ul>
<li>使用<code>git clone xxx</code>下载项目文件，待除大文件外的其他文件下载结束时结束命令<ul>
<li>一些未完成的项目会出现在<code>.git/lfs/incomplete</code>文件夹下，结束命令后可以删除该文件夹，该文件夹可能会存储较大的未完成文件，浪费存储</li>
</ul>
</li>
<li>使用网页或其他方式下载模型到指定目录下</li>
</ul>
</li>
</ul>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>如果直接使用transformers包下载<code>Bert</code>等预训练模型时，默认安装在<code>~/.cache/huggingface/hub</code>文件夹下</li>
</ul>
]]></content>
      <tags>
        <tag>Git - HuggingFace</tag>
      </tags>
  </entry>
  <entry>
    <title>Git——rebase</title>
    <url>/Notes/Git/Git%E2%80%94%E2%80%94rebase.html</url>
    <content><![CDATA[<hr>
<h3 id="Git-rebase命令的使用"><a href="#Git-rebase命令的使用" class="headerlink" title="Git rebase命令的使用"></a>Git rebase命令的使用</h3><ul>
<li>rebase可以将某个目标分支上的提交作为基础，并将当前分支与目标分支不同的提交合并后放到后面</li>
<li>合并完成后，commit的名称不变，但hash值发生了变化</li>
<li>可视化描述：<ul>
<li>master分支开始创建两个分支A，B</li>
<li>分支A: master-&gt;A1-&gt;A2-A3</li>
<li>分支B: master-&gt;B1-&gt;B2</li>
<li>在分支A上rebase目标分支B执行<code>git rebase B</code></li>
<li>解决冲突【缺陷是这里的分支冲突好像无法使用IDEA的工具检查，只能自己搜索查看】<ul>
<li>rebase时<code>git rebase target_branch</code>会一个个文件出现冲突</li>
<li>使用<code>git rebase --skip</code>可以跳过当前冲突对应的文件的当前分支的修改，保留别人<code>target_branch</code>的修改，慎用</li>
</ul>
</li>
<li>解决冲突的文件使用<code>git add file</code>或者<code>git rm file</code>标记为已解决</li>
<li>所有文件都解决以后使用<code>git rebase --continue</code>完成rebase操作【注意，这个过程不可逆，不像merge一样可以回退】<ul>
<li>解决冲突并<code>git rebase --continue</code>后会出现下一个冲突，直到没有冲突</li>
</ul>
</li>
<li>此时的分支情况<ul>
<li>分支A: master-&gt;B1-&gt;B2-&gt;A1’-&gt;A2’-A3’<ul>
<li>A1’和A1 commit的名称相同，但是hash值不同，已经不是同一个提交了，是融合了B1,B2的提交</li>
<li>A2’和A2 以及 A3’和A3 commit的情况相似</li>
</ul>
</li>
<li>分支B: master-&gt;B1-&gt;B2</li>
</ul>
</li>
<li>中途取消rebase操作可以使用<code>git rebase --abort</code>回退到原始分支的【但一旦<code>git rebase --continue</code>提交成功后无法回退】 </li>
</ul>
</li>
</ul>
<h4 id="rebase和merge的区别"><a href="#rebase和merge的区别" class="headerlink" title="rebase和merge的区别"></a>rebase和merge的区别</h4><ul>
<li>rebase<ul>
<li>操作后当前分支的commit【从相同commit开始往后的】都被修改了，所以无法回退到当前分支和目标分支的交叉之间的commit</li>
<li>在version control工具上看不出来是哪些线合并的，只保留一条线，看起来就像是从未有过分支一样</li>
<li>rebase时<code>git rebase target_branch</code>会一个个文件出现冲突，解决冲突并<code>git rebase --continue</code>后会出现下一个，直到全部完成，而merge时<code>git merge target_branch</code>是所有文件的冲突一起出现的</li>
</ul>
</li>
<li>merge<ul>
<li>操作后是保留了所有分支的commit，新创建了一个commit用于合并分支，还能从当前分支回退到之前的版本</li>
<li>在version control工具上看起来就是两条线合并到了一起</li>
</ul>
</li>
<li>如果想使用IDEA进行冲突解决，需要从IDEA上提交rebase或merge请求</li>
</ul>
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——函数重载</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E9%87%8D%E8%BD%BD.html</url>
    <content><![CDATA[<p><em>Python不支持函数重载,但是Python3提供了一个装饰器<code>@singledispatch</code>,用于定义一个泛型函数</em></p>
<ul>
<li>参考博客: <a href="https://www.cnblogs.com/sunlong88/articles/singledispatch.html" target="_blank" rel="noopener">https://www.cnblogs.com/sunlong88/articles/singledispatch.html</a></li>
</ul>
<h3 id="singledispath装饰器"><a href="#singledispath装饰器" class="headerlink" title="@singledispath装饰器"></a><code>@singledispath</code>装饰器</h3><h4 id="普通函数中使用"><a href="#普通函数中使用" class="headerlink" title="普通函数中使用"></a>普通函数中使用</h4><ul>
<li>示例代码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import singledispatch</span><br><span class="line">from collections import  abc</span><br><span class="line">@singledispatch</span><br><span class="line">def show(obj):</span><br><span class="line">    print (obj, type(obj), &quot;obj&quot;)</span><br><span class="line"> </span><br><span class="line">#参数字符串</span><br><span class="line">@show.register(str)</span><br><span class="line">def _(text):</span><br><span class="line">    print (text, type(text), &quot;str&quot;)</span><br><span class="line"> </span><br><span class="line">#参数int</span><br><span class="line">@show.register(int)</span><br><span class="line">def _(n):</span><br><span class="line">    print (n, type(n), &quot;int&quot;)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">#参数元祖或者字典均可</span><br><span class="line">@show.register(tuple)</span><br><span class="line">@show.register(dict)</span><br><span class="line">def _(tup_dic):</span><br><span class="line">    print (tup_dic, type(tup_dic), &quot;tuple or dict&quot;)</span><br><span class="line"> </span><br><span class="line">show(1)</span><br><span class="line">show(&quot;xx&quot;)</span><br><span class="line">show([1])</span><br><span class="line">show((1,2,3))</span><br><span class="line">show(&#123;&quot;a&quot;:&quot;b&quot;&#125;)</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">1 &lt;class &apos;int&apos;&gt; int</span><br><span class="line">xx &lt;class &apos;str&apos;&gt; str</span><br><span class="line">[1] &lt;class &apos;list&apos;&gt; obj</span><br><span class="line">(1, 2, 3) &lt;class &apos;tuple&apos;&gt; tuple or dict</span><br><span class="line">&#123;&apos;a&apos;: &apos;b&apos;&#125; &lt;class &apos;dict&apos;&gt; tuple or dict</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="类中使用"><a href="#类中使用" class="headerlink" title="类中使用"></a>类中使用</h4><ul>
<li>示例代码:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import singledispatch</span><br><span class="line">class abs:</span><br><span class="line">    def type(self,args):</span><br><span class="line">        &quot;&quot;</span><br><span class="line"></span><br><span class="line">class Person(abs):</span><br><span class="line"></span><br><span class="line">    @singledispatch</span><br><span class="line">    def type(self,args):</span><br><span class="line">        super().type(&quot;&quot;,args)</span><br><span class="line">        print(&quot;我可以接受%s类型的参数%s&quot;%(type(args),args))</span><br><span class="line"></span><br><span class="line">    @type.register(str)</span><br><span class="line">    def _(text):</span><br><span class="line">        print(&quot;str&quot;,text)</span><br><span class="line"></span><br><span class="line">    @type.register(tuple)</span><br><span class="line">    def _(text):</span><br><span class="line">        print(&quot;tuple&quot;, text)</span><br><span class="line"></span><br><span class="line">    @type.register(list)</span><br><span class="line">    @type.register(dict)</span><br><span class="line">    def _(text):</span><br><span class="line">        print(&quot;list or dict&quot;, text)</span><br><span class="line"></span><br><span class="line">Person.type(&quot;safly&quot;)</span><br><span class="line">Person.type((1,2,3))</span><br><span class="line">Person.type([1,2,3])</span><br><span class="line">Person.type(&#123;&quot;a&quot;:1&#125;)</span><br><span class="line">Person.type(Person,True)</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">str safly</span><br><span class="line">tuple (1, 2, 3)</span><br><span class="line">list or dict [1, 2, 3]</span><br><span class="line">list or dict &#123;&apos;a&apos;: 1&#125;</span><br><span class="line">我可以接受&lt;class &apos;bool&apos;&gt;类型的参数True</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——pickle</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94pickle.html</url>
    <content><![CDATA[<p><em>Python pickle</em></p>
<hr>
<h3 id="关于pickle模块"><a href="#关于pickle模块" class="headerlink" title="关于pickle模块"></a>关于pickle模块</h3><ul>
<li>Python的一个序列化与反序列化模块,支持Python基本数据类型</li>
<li>可以处理自定义的类对象,方法等</li>
</ul>
<h4 id="内存中使用"><a href="#内存中使用" class="headerlink" title="内存中使用"></a>内存中使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">origin = [1, 2, 3, [4, 5, 6]]</span><br><span class="line">print &quot;origin: %s&quot; % origin</span><br><span class="line">temp = pickle.dumps(origin)</span><br><span class="line">print &quot;temp: %s&quot; % temp</span><br><span class="line">new = pickle.loads(temp)</span><br><span class="line">print &quot;new: %s&quot; % new</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">origin: [1, 2, 3, [4, 5, 6]]</span><br><span class="line">temp: (lp0</span><br><span class="line">I1</span><br><span class="line">aI2</span><br><span class="line">aI3</span><br><span class="line">a(lp1</span><br><span class="line">I4</span><br><span class="line">aI5</span><br><span class="line">aI6</span><br><span class="line">aa.</span><br><span class="line">new: [1, 2, 3, [4, 5, 6]]</span><br></pre></td></tr></table></figure>

<h4 id="硬盘中使用"><a href="#硬盘中使用" class="headerlink" title="硬盘中使用"></a>硬盘中使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pickle</span><br><span class="line"></span><br><span class="line">origin = [1, 2, 3, [4, 5, 6]]</span><br><span class="line">print &quot;origin: %s&quot; % origin</span><br><span class="line"># open a binary and write the result</span><br><span class="line">pickle.dump(origin, open(&apos;temp&apos;, &apos;wb&apos;))</span><br><span class="line"># open a binary and read the original object</span><br><span class="line">new = pickle.load(open(&apos;temp&apos;, &apos;rb&apos;))</span><br><span class="line">print &quot;new: %s&quot; % new</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">origin: [1, 2, 3, [4, 5, 6]]</span><br><span class="line">new: [1, 2, 3, [4, 5, 6]]</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy——random模块</title>
    <url>/Notes/Python/Numpy/Numpy%E2%80%94%E2%80%94random%E6%A8%A1%E5%9D%97.html</url>
    <content><![CDATA[<p><em>库名: np.random</em></p>
<hr>
<h3 id="RandomState"><a href="#RandomState" class="headerlink" title="RandomState()"></a>RandomState()</h3><ul>
<li><p>np.random.RandomState(seed)</p>
<ul>
<li><p>seed 相同时两个不同的RandomState对象会产生相同的随机数据序列</p>
</li>
<li><p>seed 默认值为None,此时不同的RandomState对象产生不同的随机数据序列,此时RandomState将从/dev/urandom 或者从clock otherwise读取seed值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print np.random.RandomState(1).randint(1, 100010)</span><br><span class="line">print np.random.RandomState(1).randint(1, 100000)</span><br><span class="line">print np.random.RandomState(1).randint(1, 100000)</span><br><span class="line">print np.random.RandomState().randint(1, 100000)</span><br><span class="line">print np.random.RandomState().randint(1, 100000)</span><br><span class="line">print np.random.RandomState().randint(1, 100000)</span><br><span class="line">print np.random.RandomState(1) is np.random.RandomState(1)</span><br><span class="line">print np.random.RandomState() is np.random.RandomState()</span><br></pre></td></tr></table></figure>
</li>
<li><p>输出如下:</p>
<blockquote>
<p>98540<br>98540<br>98540<br>38317<br>42305<br>70464<br>False<br>False</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="关于初始化向量的维度"><a href="#关于初始化向量的维度" class="headerlink" title="关于初始化向量的维度"></a>关于初始化向量的维度</h3><ul>
<li><p>不是行向量也不是列向量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.randn(5)</span><br><span class="line"># [1,2,3,4,5]</span><br></pre></td></tr></table></figure>

<ul>
<li>shape为(5,)</li>
<li>是一个特殊的数据结构</li>
<li>是一个一维向量，不是矩阵，不是行向量，也不是列向量</li>
</ul>
</li>
<li><p>列向量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.randn(5,1)</span><br><span class="line"># [[1]</span><br><span class="line">   [2]</span><br><span class="line">   [3]</span><br><span class="line">   [4]</span><br><span class="line">   [5]]</span><br></pre></td></tr></table></figure>

<ul>
<li>shape为(5,1)</li>
<li>是一个矩阵</li>
</ul>
</li>
<li><p>行向量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.randn(1,5)</span><br><span class="line"># [[1,2,3,4,5]]</span><br></pre></td></tr></table></figure>

<ul>
<li>shape为(1,5)</li>
<li>是一个矩阵</li>
</ul>
</li>
<li><p>一个好的习惯是使用向量时用Assert语句确保维度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assert(a.shape == (3,4))</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy——一些说明</title>
    <url>/Notes/Python/Numpy/Numpy%E2%80%94%E2%80%94%E4%B8%80%E4%BA%9B%E8%AF%B4%E6%98%8E.html</url>
    <content><![CDATA[<h3 id="关于效率"><a href="#关于效率" class="headerlink" title="关于效率"></a>关于效率</h3><p><em>Numpy包含很多高效的函数，能够替换普通的循环，实现非常快</em></p>
<h4 id="累加变成向量运算"><a href="#累加变成向量运算" class="headerlink" title="累加变成向量运算"></a>累加变成向量运算</h4><h4 id="对向量每个元素进行某个运算生成另一个向量"><a href="#对向量每个元素进行某个运算生成另一个向量" class="headerlink" title="对向量每个元素进行某个运算生成另一个向量"></a>对向量每个元素进行某个运算生成另一个向量</h4><ul>
<li><p>普通用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.zeros((n, 1))</span><br><span class="line">b = np.zeros((n, 1))</span><br><span class="line"></span><br><span class="line">for i in range(n):</span><br><span class="line">	b[i] = math.exp(a[i])</span><br></pre></td></tr></table></figure>
</li>
<li><p>高效用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.zeros((n, 1))</span><br><span class="line">b = np.exp(a)</span><br></pre></td></tr></table></figure>
</li>
<li><p>相似的还有<code>log</code>,<code>abs</code>等函数</p>
</li>
</ul>
<h4 id="值得关注的广播机制-broadcasting"><a href="#值得关注的广播机制-broadcasting" class="headerlink" title="值得关注的广播机制(broadcasting)"></a>值得关注的广播机制(broadcasting)</h4><ul>
<li>当两个向量（<code>numpy</code>的对象）的维度不同时，Python会将维度小的一个拓展（复制）成与维度大的相同，以便于计算</li>
<li>举例<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = np.zeros((n, 1))</span><br><span class="line">b = a + 10</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="广播规则"><a href="#广播规则" class="headerlink" title="广播规则"></a>广播规则</h5><ul>
<li><p>形式1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(m,n) [+-*/] (m,1) </span><br><span class="line">&lt;===&gt;</span><br><span class="line">(m,n) [+-*/] (m,n) # 按列复制第二个n次</span><br></pre></td></tr></table></figure>
</li>
<li><p>形式2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(m,n) [+-*/] (1,n) </span><br><span class="line">&lt;===&gt;</span><br><span class="line">(m,n) [+-*/] (m,n) # 按行复制第二个m次</span><br></pre></td></tr></table></figure>
</li>
<li><p>形式3</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(m,n) [+-*/] r # r为实数，维度为1</span><br><span class="line">&lt;===&gt;</span><br><span class="line">(m,n) [+-*/] (m,n) # 复制r m*n 次</span><br></pre></td></tr></table></figure>
</li>
<li><p>形式4</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(m,1) [+-*/] (1,n) </span><br><span class="line">&lt;===&gt;</span><br><span class="line">(m,n) [+-*/] (m,n) # 按行复制第二个m次,并按列复制第一个n次</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="需要注意"><a href="#需要注意" class="headerlink" title="需要注意"></a>需要注意</h5><ul>
<li>广播机制使得书写更加美观，代码更加简洁</li>
<li>但广播机制往往会出现用户意想不到的微妙bug, 需要开发者注意</li>
</ul>
<h4 id="关于矩阵运算的维度"><a href="#关于矩阵运算的维度" class="headerlink" title="关于矩阵运算的维度"></a>关于矩阵运算的维度</h4><ul>
<li><code>axis=i</code>表示第<code>i</code>维计算后将会消失（该维度的size变成1）</li>
</ul>
<h4 id="多使用reshape函数"><a href="#多使用reshape函数" class="headerlink" title="多使用reshape函数"></a>多使用reshape函数</h4><ul>
<li>reshape函数复杂度是常数的(O(1))</li>
<li>reshape函数可确保我们的程序正确，不用随意猜测矩阵的维度</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——Transformer</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94Transformer.html</url>
    <content><![CDATA[<p><em>本文主要介绍Transformer和Attention相关内容</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>由于LaTex中矩阵的黑体表示过于复杂，在不会引起混淆的情况下，本文中有些地方会被简写为非黑体</li>
</ul>
<hr>
<h3 id="相关论文介绍"><a href="#相关论文介绍" class="headerlink" title="相关论文介绍"></a>相关论文介绍</h3><ul>
<li>Transformer原始文章: <ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a></li>
<li>文章中介绍了一种应用Attention机制的新型特征提取器,命名为Transformer, 实验证明Transformer优于RNN(LSTM),CNN等常规的特征提取器</li>
</ul>
</li>
<li>Transformer的使用: <ul>
<li>GPT: <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li>
<li>BERT: <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li>以上两个工作都使用了Transformer作为特征提取器, 使用两阶段训练的方式实现迁移学习(Pre-Training and Fine-Training)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="相关博客介绍"><a href="#相关博客介绍" class="headerlink" title="相关博客介绍"></a>相关博客介绍</h3><ul>
<li>强烈推荐看看jalammar的博客: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">illustrated-transformer</a></li>
<li>另一篇不错的Attention和Transformer讲解<a href="https://www.sohu.com/a/226596189_500659" target="_blank" rel="noopener">自然语言处理中的自注意力机制(Self-Attention Mechanism)</a></li>
<li>一篇很多个人理解的博客 <a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读</a></li>
</ul>
<hr>
<h3 id="Transformer讲解"><a href="#Transformer讲解" class="headerlink" title="Transformer讲解"></a>Transformer讲解</h3><ul>
<li>最直观的动态图理解<img src="/Notes/DL/DL——Transformer/transform_dynamic.gif"></li>
<li>本文讲解主要按照<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Google Brain, NIPS 2017: Attention Is All You Need</a>的思路走,该论文的亮点在于:<ul>
<li>不同于以往主流机器翻译使用基于 RNN 的 Seq2Seq 模型框架，该论文用 <strong>Attention 机制代替了 RNN</strong> 搭建了整个模型框架, 这是一个从换自行车零件到把自行车换成汽车的突破</li>
<li>提出了<strong>多头注意力</strong>(Multi-Head Attention)机制方法，在编码器和解码器中大量的使用了多头自注意力机制(Multi-Head self-attention)</li>
<li>在WMT2014语料库的英德和英法语言翻译任务上取得了先进结果</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Transformer是什么"><a href="#Transformer是什么" class="headerlink" title="Transformer是什么?"></a>Transformer是什么?</h3><ul>
<li>本质上是个序列转换器 <img src="/Notes/DL/DL——Transformer/the_transformer_high_level.png"></li>
<li>进一步讲,是个 Encoder-Decoder 模型的序列转换器<img src="/Notes/DL/DL——Transformer/The_transformer_encoders_decoders.png"></li>
<li>更进一步的讲,是个 6层Encoder + 6层Decoder 结构的序列转换器<img src="/Notes/DL/DL——Transformer/The_transformer_encoder_decoder_stack.png"></li>
<li>上面的图中,每个 Encoder 是<img src="/Notes/DL/DL——Transformer/Transformer_encoder.png"></li>
<li>详细的讲, 每个Encoder是<img src="/Notes/DL/DL——Transformer/encoder_with_tensors.png"></li>
<li>展开看里面 Encoder 中的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm.png"></li>
<li>更进一步的展开看 Encoder 中的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm_2.png"></li>
<li>两层 Encoder + 两层Decoder (其中一个Decoder没有完全画出来) 的数据流向<img src="/Notes/DL/DL——Transformer/transformer_resideual_layer_norm_3.png"></li>
<li>带细节动图查看数据流向<img src="/Notes/DL/DL——Transformer/transformer_decoding_1.gif">
</li>
<li>最后,我们给出Transformer的结构图(来自原文中)<img src="/Notes/DL/DL——Transformer/Transfomer_Architecture.png">


</li>
</ul>
<hr>
<h3 id="Transformer中的Attention"><a href="#Transformer中的Attention" class="headerlink" title="Transformer中的Attention"></a>Transformer中的Attention</h3><p><em>Transformer中使用了 Multi-Head Attention, 同时也是一种 Self Attention</em></p>
<ul>
<li>由于Transformer的Multi_Head Attention中 <strong>Query == Key == Query</strong>, 所以也是一种 <strong>Self Attention</strong><ul>
<li>即$$\boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
</li>
<li>更多关于广义Attention的理解请参考: <a href="/Notes/DL/DL%E2%80%94%E2%80%94Attention.html">DL——Attention</a></li>
</ul>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul>
<li>Muti-Head Attention，也称为多头Attention，由 \(h\) 个 Scaled Dot-Product Attention和其他线性层和Concat操作等组成<img src="/Notes/DL/DL——Transformer/Scaled_dot_product_attention_and_Multi_head_attention.png">
<ul>
<li>Scaled Dot Product Attention中Mask操作是可选的</li>
<li>Scaled Dot Product Attention数学定义为(没有Mask操作)<br>$$<br>\begin{align}<br>Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}<br>\end{align}<br>$$<ul>
<li>Softmax前除以\(\sqrt{d_k}\)的原因是防止梯度消失问题，基本思想是（原始论文脚注中有提到）：假设\(\boldsymbol{Q},\boldsymbol{K}\)中每个元素是服从均值为0，方差为1的正太分布（\(\sim N(0,1)\)），那么他们任意取两个列向量\(\boldsymbol{q}_i,\boldsymbol{k}_i\)的内积服从均值为0，方差为\(d_k\)的正太分布（\(\sim N(0,d_k)\)），具体证明可参考<a href="https://zhuanlan.zhihu.com/p/584569220" target="_blank" rel="noopener">没有比这更详细的推导 attention为什么除以根号dk——深入理解Bert系列文章</a>，过大的方差会导致softmax后梯度消失</li>
</ul>
</li>
<li>Multi-Head Attention的某个输出的数学定义为<br>$$<br>\begin{align}<br>MultiHead(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) &amp;= Concat(head_1,\dots,head_h)\boldsymbol{W}^{O} \\<br>where \quad head_i &amp;= Attention(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)<br>\end{align}<br>$$</li>
<li>注意，在一般的Attention中，没有\(\boldsymbol{W}^{O}\)这个参数，这个是用于多头Attention中，将多头的输出Concat后映射一下再输出</li>
<li>一般来说，\(head_i\)的维度是\(\frac{d_{model}}{N_{head}}=\frac{d_{model}}{h}=d_v = d_k\)，所以Multi-Head Attention的参数数量与head的数量无关，且无论多少个头，其的输出结果还是\(d_{model} = d_v * h\)维</li>
<li>原始论文中常用\(d_{model} = h * d_k = h * d_v\)，且base模型的参数设置为\(512 = 8 * 64\)</li>
</ul>
</li>
</ul>
<h5 id="有关Multi-Head-Attention的理解"><a href="#有关Multi-Head-Attention的理解" class="headerlink" title="有关Multi-Head Attention的理解"></a>有关Multi-Head Attention的理解</h5><ul>
<li>原论文的描述:</li>
</ul>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, </p>
</blockquote>
<ul>
<li>理解:<ul>
<li>所谓多头,就是多做几次(\(h\)次)同样的事情(参数\((W_i^Q, W_i^K, W_i^V)\)不共享, 即当 \(i \neq j \) 时, \((W_i^Q, W_i^K, W_i^V) \neq (W_j^Q, W_j^K, W_j^V)\)),然后把结果拼接</li>
<li>Multi-Head Attention中, 每个头(Scaled Dot-Product Attention)负责不同的子空间(subspaces at differect positions)</li>
<li>每个头权重不同, 所以他们的关注点也会不同,注意, 初始化时他们的参数不能相同, 否则会造成他们的参数永远相同, 因为他们是同构的</li>
<li>个人理解: 多头的作用可以类比于CNN中的卷积层, 负责从不同的角度提取原始数据的特征</li>
</ul>
</li>
</ul>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul>
<li>Self Attention是只 Key和Query相同的 Attention, 这里因为 Key 和 Value 也相同,所以有 <strong>Query == Key == Query</strong></li>
<li>即$$ \boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
<h4 id="Transformer中的Attention-1"><a href="#Transformer中的Attention-1" class="headerlink" title="Transformer中的Attention"></a>Transformer中的Attention</h4><ul>
<li>既是Multi-Head Attention, 也是 Self Attention</li>
<li>所以有$$\boldsymbol{Y_{AttentionOutput}} = MultiHead(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
<h4 id="Masked-Multi-Head-Attetion"><a href="#Masked-Multi-Head-Attetion" class="headerlink" title="Masked Multi-Head Attetion"></a>Masked Multi-Head Attetion</h4><ul>
<li>MaskedMHA，掩码多头Attention，用于Decoder中防止前面的token看到后面的token，Encoder中不需要MaskedMHA</li>
<li>一般性的，Masked Self-Attention是更一般的实现，不一定非要和Multi-Head绑定</li>
<li>代码实现时，主要是在计算Softmax前，按照掩码将看不到的token对应的q,k内积替换为一个大负数，比如\(-1e9\)</li>
</ul>
<h4 id="Cross-Multi-Head-Attention"><a href="#Cross-Multi-Head-Attention" class="headerlink" title="Cross Multi-Head Attention"></a>Cross Multi-Head Attention</h4><ul>
<li>CrossMHA不是Self-Attention，CrossMHA的Q,K是Encoder的输出，V来自Decoder</li>
</ul>
<hr>
<h3 id="Transformer-输入层"><a href="#Transformer-输入层" class="headerlink" title="Transformer 输入层"></a>Transformer 输入层</h3><ul>
<li>Transformer的输入层使用了 Word Embedding + Position Embedding</li>
<li>由于Transformer去除RNN的Attention机制完全不考虑词的顺序, 也就是说, 随机打乱句子中词的顺序 (也就是将键值对\((\boldsymbol{K}, \boldsymbol{V})\)对随机打乱), Transformer中Attention的结果不变</li>
<li>实际上, <strong>目前为止, Transformer中的Attention模型顶多是个非常精妙的”词袋模型”</strong> (这句话来自博客:<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">https://kexue.fm/archives/4765</a>)</li>
</ul>
<h4 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h4><ul>
<li>和之前的词嵌入一样, 将One-Hot值映射成词向量嵌入模型中</li>
</ul>
<h4 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h4><p><em>FaceBook的《Convolutional Sequence to Sequence Learning》中曾经用过Position Embedding</em></p>
<ul>
<li>在不使用RNN的情况下建模词的顺序, 弥补”词袋模型”的不足</li>
<li>用 Position Embedding来为每个位置一个向量化表示<ul>
<li>将每个位置编号，然后每个编号对应一个向量</li>
<li>通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了</li>
</ul>
</li>
<li>原始论文中, 作者提出了一种周期性位置编码的表示, 数学公式如下:<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin(pos/10000^{2i/d_{\text{model}}}) \\<br>  PE(pos, 2i+1) &amp;= cos(pos/10000^{2i/d_{\text{model}}})<br>  \end{align}<br>  $$</li>
<li>我觉得上述公式太丑了,转换一下写法可能更容易理解<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\<br>  PE(pos, 2i+1) &amp;= cos\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)<br>  \end{align}<br>  $$<ul>
<li>\(pos\) 是位置编号</li>
<li>\(i\) 表示位置向量的第 \(i\) 维</li>
<li>从公式来看，为什么选择\(10000^{\frac{2i}{d_{\text{model}}}}\)? <ul>
<li>\(i\)表示频率随模型embedding维度变动（模型embedding不同维度频率不同，低维度高频，高维度低频）</li>
<li>\(pos\) 表示周期，随着位置变化，每个维度的值呈现周期变化，但是不同维度的变化周期（频率）不同</li>
<li>10000是一个放缩因子，理论上可以换，在transformer原始论文实现中用了这个，且效果不错</li>
</ul>
</li>
<li>选择正弦函数的原因是假设这将允许模型学到相对位置信息<ul>
<li>因为对于固定的 \(k\), \(PE_{pos+k} = LinearFuction(PE_{pos})\), 所以这给模型提供了表达相对位置的可能性</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="与之前的Position-Embedding的区别"><a href="#与之前的Position-Embedding的区别" class="headerlink" title="与之前的Position Embedding的区别"></a>与之前的Position Embedding的区别</h5><ul>
<li>Position Embedding对模型的意义不同:<ul>
<li>以前在RNN、CNN模型中Position Embedding是锦上添花的辅助手段，也就是“有它会更好、没它也就差一点点”的情况，因为RNN、CNN本身就能捕捉到位置信息</li>
<li>在Transformer这个纯Attention模型中，Position Embedding是位置信息的唯一来源，因此它是模型的核心成分之一，并非仅仅是简单的辅助手段</li>
</ul>
</li>
<li>Position Embedding的向量构造方式不同<ul>
<li>在以往的Position Embedding中，基本都是根据任务训练出来的向量</li>
<li>而Google直接给出了一个构造Position Embedding的公式:<br>  $$<br>  \begin{align}<br>  PE(pos,2i) &amp;= sin\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\<br>  PE(pos, 2i+1) &amp;= cos\left (\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)<br>  \end{align}<br>  $$</li>
<li>Google经过实验, 学到的位置嵌入和这种计算得到的位置嵌入结果很相近</li>
<li>Google选用这种嵌入方式的原因是这种方式允许模型以后可以<strong>扩展到比训练时遇到的序列长度更长的句子</strong></li>
</ul>
</li>
</ul>
<h4 id="输入层的输出-Attention的输入"><a href="#输入层的输出-Attention的输入" class="headerlink" title="输入层的输出(Attention的输入)"></a>输入层的输出(Attention的输入)</h4><ul>
<li>综合词嵌入和位置嵌入信息,我们可以得到下面的公式<br>$$<br>\begin{align}<br>\boldsymbol{x} = \boldsymbol{x}_{WE} + \boldsymbol{x}_{PE}<br>\end{align}<br>$$<ul>
<li>\(\boldsymbol{x}\) 为输入层经过词嵌入和位置嵌入后的 输出, 也就是Attention的输入</li>
<li>\(\boldsymbol{x}_{WE}\) 指词嵌入的结果</li>
<li>\(\boldsymbol{x}_{PE}\) 指位置嵌入的结果</li>
</ul>
</li>
</ul>
<h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><ul>
<li>FFN，Feed Forward Network，前馈网络层<br>$$<br>FFN(\mathbf{X}) = ReLU(\mathbf{X}\mathbf{W}^U + \mathbf{b}_1)\mathbf{W}^D + \mathbf{b}_2<br>$$</li>
<li>原始Transformer使用的是ReLU作为激活函数，现在很多时候也会选用sigmoid</li>
<li>可以看到前馈神经网络包含了两层 </li>
</ul>
<h3 id="Layer-Normaliztion"><a href="#Layer-Normaliztion" class="headerlink" title="Layer Normaliztion"></a>Layer Normaliztion</h3><ul>
<li>层归一化，是Transformer特有的一种归一化方法</li>
<li>Batch Normalization(BN)不适用与Transformer中，至少有以下原因：<ul>
<li>Transformer训练样本通常（特别是模型很大时）可能会比较小，在Batch较小时BN不再适用</li>
<li>BN是按照token维度（特征维度）来归一化的，不利于处理变长输入序列</li>
</ul>
</li>
</ul>
<p>$$<br>LayerNorm(\mathbf{x}) = \frac{\mathbf{x}-\mathbf{\mu}}{\mathbf{\sigma}}\cdot \mathbf{\gamma} + \mathbf{\beta} \\<br>\mathbf{\mu} = \frac{1}{H}\sum_{i=1}^H x_i, \quad \mathbf{\sigma} = \sqrt{\frac{1}{H}\sum_{i=1}^H(x_i-\mathbf{\mu})^2} \\<br>$$</p>
<ul>
<li>代码实现是会在分母的更号内增加一个极小量 \(\epsilon\)，防止出现除0的情况</li>
</ul>
<h4 id="LN是token维度的"><a href="#LN是token维度的" class="headerlink" title="LN是token维度的"></a>LN是token维度的</h4><ul>
<li><p>按照Transformer源码实现来看，LayerNorm是Token维度的，不是Seq维度，也就是说，token向量LayerNorm的结果只与token向量自身相关，与所在序列的其他token无关</p>
<ul>
<li>这一点是Decoder可以增量解码的关键，这一点保证了Decoder的前序词不会受到后续词的影响</li>
<li>增量解码是指：Decoder中输出下一个词时，可以使用前序词的缓存结果，由于前面的词看不到后面的词，所以增加词前后Transformer-Decoder中前序每个词的输出在每一层都不会受到影响</li>
</ul>
</li>
<li><p>一个LayerNorm的示例如下，Transformer源码中实现与这个类似</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 假设d_model=4，d_model在有些实现中为hidden_size</span><br><span class="line">layer_norm = nn.LayerNorm(4) # 这里使用LayerNorm，也可以使用RMSNorm，两者作用维度相同，只是公式不同</span><br><span class="line"># test case 1:</span><br><span class="line">input_tensor = torch.Tensor([[[1, 2, 3, 4],</span><br><span class="line">                              [2, 3, 4, 5]]])</span><br><span class="line">output_tensor = layer_norm(input_tensor)</span><br><span class="line">print(output_tensor)</span><br><span class="line"># output:</span><br><span class="line"># tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],</span><br><span class="line">#          [-1.3416, -0.4472,  0.4472,  1.3416]]],</span><br><span class="line">#        grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br><span class="line"></span><br><span class="line"># test case 2:</span><br><span class="line">input_tensor = torch.Tensor([[[1, 2, 3, 4],</span><br><span class="line">                              [200, 3, 4, 5]]])</span><br><span class="line">output_tensor = layer_norm(input_tensor)</span><br><span class="line">print(output_tensor)</span><br><span class="line"></span><br><span class="line"># tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],</span><br><span class="line">#          [ 1.7320, -0.5891, -0.5773, -0.5655]]],</span><br><span class="line">#        grad_fn=&lt;NativeLayerNormBackward0&gt;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>从示例中可以看出：</p>
<ul>
<li>修改第二个token的某个元素值，只影响第二个token的LN输出，不影响第一个token</li>
</ul>
</li>
</ul>
<h3 id="Transformer改进-LN"><a href="#Transformer改进-LN" class="headerlink" title="Transformer改进-LN"></a>Transformer改进-LN</h3><p><em>原始LN参见本文之前的内容</em></p>
<h4 id="LN的改进——RMSNorm"><a href="#LN的改进——RMSNorm" class="headerlink" title="LN的改进——RMSNorm"></a>LN的改进——RMSNorm</h4><p>$$<br>\begin{align}<br>RMSNorm(\mathbf{x}) &amp;= \frac{\mathbf{x}-\mathbf{\mu}}{RMS(\mathbf{x})}\cdot \mathbf{\gamma} \\<br>  RMS(\mathbf{x}) &amp;= \sqrt{\frac{1}{H}\sum_{i=1}^H x_i^2} \\<br>\end{align}<br>$$</p>
<ul>
<li>代码实现是会在分母的更号内增加一个极小量 \(\epsilon\)，防止出现除0的情况</li>
</ul>
<h4 id="LN的改进——DeepNorm"><a href="#LN的改进——DeepNorm" class="headerlink" title="LN的改进——DeepNorm"></a>LN的改进——DeepNorm</h4><p>$$<br>DeepNorm(\mathbf{x}) = LayerNorm(\alpha\cdot \mathbf{x} + Sublayer(\mathbf{x})) \\<br>$$</p>
<ul>
<li>这里的\(Sublayer(\mathbf{x})\)是指Transformer中的前馈神经网络层或自注意力模块（两者都会作为LN的输入）</li>
<li>实际上，原始的Transformer中，每次LN的内容都是加上残差的，这里根据归一化位置的不同还有会有不同的实现</li>
<li>原始的Transformer中，相当于\(\alpha=1\)的DeepNorm</li>
<li>这里叫做<strong>DeepNorm</strong>的原因是因为缩放残差\(\mathbf{x}\)可以扩展Transformer的深度，有论文提到利用该方法可将深度提升到1000层（<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231" target="_blank" rel="noopener">DeepNet: Scaling Transformers to 1,000 Layers</a>）</li>
</ul>
<h4 id="归一化的位置"><a href="#归一化的位置" class="headerlink" title="归一化的位置"></a>归一化的位置</h4><ul>
<li>归一化的位置包括Post-Norm、Pre-Norm和Sandwich-Norm等</li>
<li>Post-Norm<ul>
<li>原始Transformer使用的方法</li>
<li>将归一化模块使用到加法（需要把残差加到FFN/MHA的输出上）之后，详细公式是：<br>$$<br>\text{Post-Norm}(\mathbf{x}) = Norm(\mathbf{x} + Sublayer(\mathbf{x}))<br>$$</li>
</ul>
</li>
<li>Pre-Norm<ul>
<li>归一化模块放到FFN/MHA之前，详细公式是：<br>$$<br>\text{Pre-Norm}(\mathbf{x}) = \mathbf{x} + Sublayer(Norm(\mathbf{x}))<br>$$</li>
</ul>
</li>
<li>Sandwich_Norm<ul>
<li>三明治归一化，从字面意思可以知道，是两个Norm将某个层夹起来，实际上，该层是前馈神经网络层或自注意力模块<br>$$<br>\text{Sandwish-Norm}(\mathbf{x}) = \mathbf{x} + Norm(Sublayer(Norm(\mathbf{x})))<br>$$</li>
</ul>
</li>
</ul>
<h4 id="归一化位置的比较"><a href="#归一化位置的比较" class="headerlink" title="归一化位置的比较"></a>归一化位置的比较</h4><p><em><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231" target="_blank" rel="noopener">DeepNet: Scaling Transformers to 1,000 Layers</a>中也有有关Pre-Norm和Post-Norm的探讨</em></p>
<ul>
<li>一般来说，使用Post-Norm比较多，效果也更好</li>
<li>Pre-Norm在深层Transformer中容易训练（容易训练不代表效果好，Pre-Norm的拟合能力一般不如Post-Norm）<ul>
<li>所以有些模型还是会使用Pre-Norm，因为它更稳定</li>
</ul>
</li>
<li>浅层中建议使用Post-Norm</li>
<li>详情参考苏神的回答<a href="https://zhuanlan.zhihu.com/p/494661681" target="_blank" rel="noopener">为什么Pre Norm的效果不如Post Norm？</a></li>
</ul>
<h3 id="Transformer改进-激活函数"><a href="#Transformer改进-激活函数" class="headerlink" title="Transformer改进-激活函数"></a>Transformer改进-激活函数</h3><p><em>原始激活函数是ReLU(Rectified Linear Unit)</em></p>
<h4 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h4><ul>
<li>Swish, Sigmoid-weighted Linear Unit<ul>
<li><em>Swish 的全称为 “Scaled Exponential Linear Unit with Squishing Hyperbolic Tangent”,直译为“带有压缩的缩放指数线性单元”</em><br>$$<br>\text{Swish}_\beta(x) = x \cdot sigmoid(\beta x)<br>$$</li>
</ul>
</li>
<li>许多实现中常常设置\(\beta=1\)</li>
</ul>
<h4 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h4><ul>
<li>GELU, Gaussion Error Linear Unit，有时候也写作GeLU<br>$$<br>\text{GELU}(x) = 0.5x \cdot [1+erf(\frac{x}{\sqrt{2}})], \quad erf(x) = \frac{2}{\sqrt{\pi}}\int_1^x e^{-t^2} dt<br>$$</li>
<li>从公式可以看出GELU的本质是对一个正太分布的概率密度函数进行积分，实际上就是累积分布函数</li>
<li>GELU和ReLU的比较如下（图片来自<a href="https://zhuanlan.zhihu.com/p/662042707" target="_blank" rel="noopener">简单理解GELU 激活函数</a>）：<img src="/Notes/DL/DL——Transformer/GELU-ReLU.webp">

</li>
</ul>
<h4 id="补充：GLU及其变换"><a href="#补充：GLU及其变换" class="headerlink" title="补充：GLU及其变换"></a>补充：GLU及其变换</h4><ul>
<li>GLU，Gated Linear Units，是一种利用门的思想实现的激活函数，该激活函数可以理解为对输入进行门控选择，一些维度的值可以通过门，一些则不可以，门一般是一个基础的非线性激活函数</li>
<li>原始GLU形式如下：<br>$$<br>GLU = \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \odot (\mathbf{W}_2\mathbf{x} + \mathbf{b}_2)<br>$$</li>
<li>\(\sigma\)可以替换成其他非线性激活函数<ul>
<li>注意整个公式中始终只有一个非线性激活函数，其他部分都是线性映射（线性激活函数）</li>
</ul>
</li>
<li>\(\odot\)表示矩阵按照元素相乘，\(W_1,W_2,b_1,b_2\)是可学习的参数</li>
<li>该激活函数非常特殊，首先使用两个权重矩阵对输入数据进行线性变换，然后通过sigmoid激活函数进行非线性变换。这种设计使得GLU在前馈传播过程中能够更好地捕捉输入数据的非线性特征，从而提高模型的表达能力和泛化能力</li>
<li>原始论文<a href="https://arxiv.org/pdf/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer</a>中也写作下面的形式(其中\(W,V,b,c\)是可学习的参数)：<br>$$<br>GLU(\mathbf{x,W,V,b,c}) = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b}) \odot (\mathbf{V}\mathbf{x} + \mathbf{c})<br>$$</li>
<li>去掉激活函数的版本也叫作Bilinear，写作<br>$$<br>Bilinear(\mathbf{x,W,V,b,c}) = (\mathbf{W}\mathbf{x} + \mathbf{b}) \odot (\mathbf{V}\mathbf{x} + \mathbf{c})<br>$$</li>
<li>其他相关形式<br>$$<br>ReGLU(x, W, V, b, c) = max(0, xW + b) \odot (xV + c) \\<br>GEGLU(x, W, V, b, c) = GELU(xW + b) \odot (xV + c) \\<br>SwiGLU(x, W, V, b, c, \beta) = Swish_\beta(xW + b) \odot (xV + c) \\<br>$$</li>
</ul>
<h4 id="补充：FFN激活函数形式"><a href="#补充：FFN激活函数形式" class="headerlink" title="补充：FFN激活函数形式"></a>补充：FFN激活函数形式</h4><ul>
<li>FFN的ReLU激活函数形式<br>$$<br>FFN(x, W_1, W_2, b_1, b_2) = max(0, xW_1 + b_1)W_2 + b_2<br>$$</li>
<li>为了表示方便，也因为在一些文章中使用了简化，后续该形式会被简化成没有偏置项(bias)的形式：<br>$$<br>FFNReLU(x, W_1, W_2) = max(xW_1, 0)W_2<br>$$</li>
</ul>
<h4 id="FFN各种激活函数形式"><a href="#FFN各种激活函数形式" class="headerlink" title="FFN各种激活函数形式"></a>FFN各种激活函数形式</h4><ul>
<li>常用FFN的激活函数改进有，GLU,Bilinear,ReGLU,GEGLU(GeGLU),SwiGLU等<br>$$<br>\begin{align}<br>FFN_{GLU}(x, W, V, W_2) &amp;= (\sigma(xW) \odot xV )W_2 \\<br>FFN_{Bilinear}(x, W, V, W_2) &amp;= (xW \odot xV )W_2 \\<br>FFN_{ReGLU}(x, W, V, W_2) &amp;= (max(0, xW) \odot xV )W_2 \\<br>FFN_{GEGLU}(x, W, V, W_2) &amp;= (GELU(xW) \odot xV )W_2 \\<br>FFN_{SwiGLU}(x, W, V, W_2) &amp;= (Swish_1(xW) \odot xV )W_2 \\<br>\end{align}<br>$$</li>
<li>可以理解为\(\mathbf{W}^G,\mathbf{W}^U\)中包含了偏置项\(\mathbf{b}\)，有些文章/模型中则会将偏置项\(\mathbf{b}\)去掉</li>
<li>最常用的是SwiGLU</li>
<li>从形式上看，可以知道相对原始FFN激活函数形式，SwiGLU等改进增加了一个参数矩阵，为了保证原始参数数量不变，原始论文<a href="https://arxiv.org/pdf/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer</a>中提出了一种方法，通过将矩阵设置为如下的大小来保证参数数量相等<ul>
<li>原始FFN层参数为(下面\(d = d_{model}\)是模型的隐藏层大小，注意，同一层的不同token是共享FFN的):<br>$$<br>W_1^{d\times d} + W_2^{d\times d}<br>$$</li>
<li>使用SwiGLU且对齐参数数量后<br>$$<br>W^{r\times d} + V^{d\times r} + W_{2}^{r\times d}<br>$$</li>
<li>显然，当 \(r=\frac{2}{3}d\) 时，使用 SwiGLU 前后FFN层参数数量相同，都等于 \(2d^2\)</li>
</ul>
</li>
<li>一个疑问：原始的SwiGLU函数会引入两个参数矩阵 \(W,V\)，原始的FFN包含两个参数矩阵 \(W_1, W_2\)，为什么两者结合以后只剩下 \(FFN_{SwiGLU}\) 只剩三个参数\(W,V,W_2\)呢？<ul>
<li>回答：因为两个线性矩阵相乘，可以合并为 \(W = WV\)，虽然还叫做\(W\)，但实际上是多了一个矩阵乘进去的，线上训练时也只需要训练这一个矩阵即可</li>
</ul>
</li>
</ul>
<h3 id="Transformer总结"><a href="#Transformer总结" class="headerlink" title="Transformer总结"></a>Transformer总结</h3><ul>
<li>Transformer是一个特征提取能力非常强(超越LSTM)的特征提取器</li>
<li>一些讨论<ul>
<li>Transformer与CNN没关系,但是Transformer中使用多个 Scaled Dot-Product Attention 来最后拼接的方法(Multi-Head Attention), 就是CNN的多个卷积核的思想</li>
<li>Transformer论文原文中提到的残差结构也来源于CNN</li>
<li>无法对位置信息进行很好地建模，这是硬伤。尽管可以引入Position Embedding，但我认为这只是一个缓解方案，并没有根本解决问题。举个例子，用这种纯Attention机制训练一个文本分类模型或者是机器翻译模型，效果应该都还不错，但是用来训练一个序列标注模型（分词、实体识别等），效果就不怎么好了。那为什么在机器翻译任务上好？我觉得原因是机器翻译这个任务并不特别强调语序，因此Position Embedding 所带来的位置信息已经足够了，此外翻译任务的评测指标BLEU也并不特别强调语序</li>
<li>Attention如果作为一个和CNN,RNN平级的组件来使用,可能会集成到各自的优点, 而不是”口气”很大的 “Attention is All You Need”</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——VAE</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94VAE.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>




<hr>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><ul>
<li>网络输出对数方差，事实使用\(e^\sigma\)，能保证方差非负<img src="/Notes/DL/DL——VAE/A-VAE.png">


</li>
</ul>
<hr>
<h3 id="李宏毅公式推导"><a href="#李宏毅公式推导" class="headerlink" title="李宏毅公式推导"></a>李宏毅公式推导</h3><ul>
<li>目标是让似然函数最大化，也就是最大化\(\sum_x \log P(x)\)，推导可得相当于最大化变分下界(Evidence Lower Bound，\(ELBO(q)\))<img src="/Notes/DL/DL——VAE/B-VAE-formula-lihongyi.png"></li>
<li>为什么要通过求解\(q\)来实现似然函数最大化/ELBO最大化呢？因为优化\(q\)时，与\(P(x)\)无关，相当于最小化KL散度<img src="/Notes/DL/DL——VAE/C-VAE-formula-lihongyi-why-q.png"></li>
<li>进一步拆解变分下界<img src="/Notes/DL/DL——VAE/D-VAE-formula-lihongyi.png"></li>
<li>变分下界的两个部分分别可用在网络中建模，两个损失函数同时优化就是VAE<img src="/Notes/DL/DL——VAE/E-VAE-formula-lihongyi.png">
<ul>
<li>期望部分：通过带采样的Auto-Encoder实现，损失函数为Auto-Encoder的损失函数</li>
<li>KL散度部分【TODO：有时间手动推导一下】：使用一个关于均值和方差的损失函数可以实现，详情看原始论文可推导<a href="https://arxiv.org/pdf/1312.6114.pdf?source=post_page---------------------------" target="_blank" rel="noopener">Auto-Encoding Variational Bayes</a><img src="/Notes/DL/DL——VAE/original-paper-VAE-loss1-proof.png">
<ul>
<li>假设了\(p_\theta(z)\)是均值为0方差为1的标准正太分布\(N(0,I)\)，所以这里本质是尽量保证分布\(q(z|x)\)尽可能接近标准正太分布</li>
</ul>
</li>
<li>KL散度部分的<a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">其他证明</a>：<img src="/Notes/DL/DL——VAE/spaces-VAE-formula.png">
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="其他公式推导"><a href="#其他公式推导" class="headerlink" title="其他公式推导"></a>其他公式推导</h3><img src="/Notes/DL/DL——VAE/other-VAE-formula.png">

<hr>
<h3 id="AE-VAE-CVAE"><a href="#AE-VAE-CVAE" class="headerlink" title="AE-VAE-CVAE"></a>AE-VAE-CVAE</h3><img src="/Notes/DL/DL——VAE/other-AE-VAE-CVAE.png">

]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——Ubuntu和Centos服务器管理</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94Ubuntu%E5%92%8CCentos%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AE%A1%E7%90%86.html</url>
    <content><![CDATA[<hr>
<h3 id="Centos"><a href="#Centos" class="headerlink" title="Centos"></a>Centos</h3><h4 id="配置静态IP"><a href="#配置静态IP" class="headerlink" title="配置静态IP"></a>配置静态IP</h4><ul>
<li><p>查看网卡名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure>

<ul>
<li>查看网卡信息,每行的第一项为网卡名称</li>
<li>一般服务器可能有多个网卡,按照需求选择一个即可</li>
<li>网卡信息包含当前网卡的ipv4和ipv6地址,MAC地址等信息</li>
</ul>
</li>
<li><p>根据网卡名字打开相应网卡的配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 假设网卡名称为eth0</span><br><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改相关配置内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IPADDR=12.12.12.12</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=12.12.12.1</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line">DNS2=8.8.8.8</span><br></pre></td></tr></table></figure>

<ul>
<li>等号后面是具体的ip地址名称</li>
<li>DNS可以有多个,用数字标识即可</li>
</ul>
</li>
<li><p>重新启动网络服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="管理用户"><a href="#管理用户" class="headerlink" title="管理用户"></a>管理用户</h4><h5 id="修改root密码"><a href="#修改root密码" class="headerlink" title="修改root密码"></a>修改root密码</h5><p><em>默认可远程登录root用户,只需知道密码即可(SSH默认已经安装)</em></p>
<ul>
<li><p>修改root密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>
</li>
<li><p>若需要找回root密码,参考博客 <a href="https://blog.csdn.net/shanvlang/article/details/80385913" target="_blank" rel="noopener">https://blog.csdn.net/shanvlang/article/details/80385913</a></p>
</li>
</ul>
<h5 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h5><p><em>假设添加用户名为test的用户</em></p>
<ul>
<li><p>添加新用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd test</span><br></pre></td></tr></table></figure>

<ul>
<li>centos中,以上命令将自动为test用户分配一个属于test用户的/home/test文件夹</li>
</ul>
</li>
<li><p>为新用户修改密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">passwd test</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加用户权限</p>
<ul>
<li><p>打开配置文件</p>
</li>
<li><p>如果root用户没有该文件的写权限的话需要用<code>chmod u+w /etc/sudoers</code>, 修改完成再改回到<code>chmod u-w /etc/sudoers</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/sudoers</span><br></pre></td></tr></table></figure>
</li>
<li><p>编辑以下内容,注意空格类型,最好复制一行修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root   ALL=(ALL)    ALL </span><br><span class="line">test   ALL=(ALL)    ALL</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>删除用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">userdel test</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><h4 id="配置静态IP-1"><a href="#配置静态IP-1" class="headerlink" title="配置静态IP"></a>配置静态IP</h4><ul>
<li><p>查看网卡名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure>

<ul>
<li>查看网卡信息,每行的第一项为网卡名称</li>
<li>一般服务器可能有多个网卡,按照需求选择一个即可</li>
<li>网卡信息包含当前网卡的ipv4和ipv6地址,MAC地址等信息</li>
</ul>
</li>
<li><p>修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/network/interfaces</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改相关配置内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 假设网卡名称为eth0</span><br><span class="line"></span><br><span class="line">auto eth0</span><br><span class="line">iface eth0 inet static</span><br><span class="line">address 12.12.12.12</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line">gateway 12.12.12.1</span><br><span class="line">dns-nameserver 114.114.114.114 8.8.8.8</span><br></pre></td></tr></table></figure>

<ul>
<li>dns可以有多个,以空格间隔开</li>
</ul>
</li>
<li><p>重新启动网络服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo /etc/init.d/networking restart</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="管理用户-1"><a href="#管理用户-1" class="headerlink" title="管理用户"></a>管理用户</h4><h5 id="修改root密码-1"><a href="#修改root密码-1" class="headerlink" title="修改root密码"></a>修改root密码</h5><p><em>Ubuntu默认禁用root用户登录,需要配置root账户才能以root身份登录(SSH默认已经安装)</em></p>
<ul>
<li><p>修改root密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>
</li>
<li><p>若需要找回root密码,参考博客 <a href="https://blog.csdn.net/shanvlang/article/details/80385913" target="_blank" rel="noopener">https://blog.csdn.net/shanvlang/article/details/80385913</a></p>
</li>
</ul>
<h5 id="添加用户-1"><a href="#添加用户-1" class="headerlink" title="添加用户"></a>添加用户</h5><p><em>假设添加用户名为test的用户</em></p>
<ul>
<li><p>添加新用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd test</span><br></pre></td></tr></table></figure>

<ul>
<li>Ubuntu中以上命令并不会直接为当前用户分配自己的文件夹,所以需要我们为其手动添加一个并使用<code>chown test /home/test</code>命令将文件家分配给test用户,这样使用test用户登录时将会自动转到/home/test文件夹下面工作</li>
</ul>
</li>
<li><p>为新用户修改密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">passwd test</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加用户权限</p>
<ul>
<li><p>打开配置文件</p>
</li>
<li><p>如果root用户没有该文件的写权限的话需要用<code>chmod u+w /etc/sudoers</code>, 修改完成再改回到<code>chmod u-w /etc/sudoers</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/sudoers</span><br></pre></td></tr></table></figure>
</li>
<li><p>编辑以下内容,注意空格类型,最好复制一行修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># User privilege specification</span><br><span class="line">root    ALL=(ALL:ALL) ALL</span><br><span class="line">test    ALL=(ALL:ALL) ALL</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>删除用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">userdel test</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Centos</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Git——问题记录</title>
    <url>/Notes/Git/Git%E2%80%94%E2%80%94%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95.html</url>
    <content><![CDATA[<hr>
<h3 id="Ubuntu中文乱码问题"><a href="#Ubuntu中文乱码问题" class="headerlink" title="Ubuntu中文乱码问题"></a>Ubuntu中文乱码问题</h3><ul>
<li>解决方案<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global core.quotepath false</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Windows拉取项目后无法check"><a href="#Windows拉取项目后无法check" class="headerlink" title="Windows拉取项目后无法check"></a>Windows拉取项目后无法check</h3><ul>
<li><p>表现：拉取远程分支后无法进行check操作，具体错误表现为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fatal：unable to checkout working tree</span><br></pre></td></tr></table></figure>
</li>
<li><p>原因：一般是因为文件名命名包含非法字符导致，比如亲测文件名包含<code>|</code>就会出错（通常是有些不适用Windows且命名不规范的作者容易出现该错误）</p>
</li>
<li><p>解决方案：修改文件名称，将非法字符去掉</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——Python3新特性f-string</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94Python3%E6%96%B0%E7%89%B9%E6%80%A7f-string.html</url>
    <content><![CDATA[<p><em>Formatted string literals</em></p>
<hr>
<h3 id="f-string"><a href="#f-string" class="headerlink" title="f-string"></a>f-string</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul>
<li>格式化的字符串文字以“f”为前缀</li>
<li>类似于str.format()接受的格式字符串</li>
<li>它们包含由花括号包围的替换字段</li>
<li>替换字段是表达式，在运行时进行评估，然后使用format()协议进行格式化</li>
</ul>
<h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ul>
<li>从字符串中提取的表达式在f字符串出现的上下文中计算</li>
<li>这意味着表达式可以完全访问本地和全局变量</li>
<li>可以使用任何有效的Python表达式，包括函数和方法调用</li>
</ul>
<h4 id="与之前的表达式对比"><a href="#与之前的表达式对比" class="headerlink" title="与之前的表达式对比"></a>与之前的表达式对比</h4><ul>
<li><p>之前</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(&quot;%s%s%s&quot; % (a, b, c+d))</span><br></pre></td></tr></table></figure>
</li>
<li><p>f-string</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(f&apos;&#123;a&#125;&#123;b&#125;&#123;c+d&#125;&apos;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——正则表达式</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    <content><![CDATA[<hr>
<h3 id="将文本分行"><a href="#将文本分行" class="headerlink" title="将文本分行"></a>将文本分行</h3><ul>
<li><code>text.split(&quot;\n&quot;)</code><ul>
<li>适用于普通文本,这种文本的特点是写到文件或者print输出后看不到<code>\n</code>字符串,自动转义成换行符,显示出来就是分行的</li>
</ul>
</li>
<li><code>text.split(&quot;\\n&quot;)</code><ul>
<li>适用于被编码后的文本,该文本的特点是经过编码,所以写到文件或者输出时还能看到<code>\n</code>字符串,本质上需要二次转义才能显示为换行符号</li>
</ul>
</li>
<li><code>text.split(r&quot;\n&quot;)</code><ul>
<li>同上,等价与告诉别人不需要转义<code>r&quot;\n&quot;</code>本身代表着<code>&quot;\n&quot;</code>是原始文本,无需转义,体现在分</li>
</ul>
</li>
<li><code>text.decode(&quot;unicode-escape&quot;).split(&quot;\n&quot;)</code><ul>
<li>效果同上</li>
</ul>
</li>
<li><strong>第一种文本是一次转义就能按行显示的文本,比如一次print和write操作都会转义</strong></li>
<li><strong>后三种文本需要两次转义才能按行显示,中间两种分割方式等价,最后一种是先转义再分割</strong></li>
</ul>
<hr>
<h3 id="正则表达式匹配完整字符串"><a href="#正则表达式匹配完整字符串" class="headerlink" title="正则表达式匹配完整字符串"></a>正则表达式匹配完整字符串</h3><ul>
<li>必须使用^和$, 否则部分匹配也会返回结果<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line">def totally_match(pattern, string):</span><br><span class="line">	if re.match(pattern, string) is not None:</span><br><span class="line">		return True</span><br><span class="line">totally_match(r&quot;^cat$&quot;, &quot;cat&quot;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Regex</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP——关于英文单词的处理总结</title>
    <url>/Notes/NLP/NLP%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E8%8B%B1%E6%96%87%E5%8D%95%E8%AF%8D%E7%9A%84%E5%A4%84%E7%90%86%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<hr>
<h3 id="保留词根"><a href="#保留词根" class="headerlink" title="保留词根"></a>保留词根</h3><ul>
<li><p>安装相关库:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install pattern</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入和使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pattern.text.en import lemma</span><br><span class="line">lemma(&quot;describing&quot;)</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># describe</span><br></pre></td></tr></table></figure>

<ul>
<li>需要nltk中的几个语料库包, 如果没有以下包,导入时会报出zip文件相关的错,按装这几个语料库包直接使用nltk.download(“wordent”)等语句就行<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;wordnet&quot;, &quot;wordnet_ic&quot;, &quot;sentiwordnet&quot;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="保留词干"><a href="#保留词干" class="headerlink" title="保留词干"></a>保留词干</h3><ul>
<li><p>安装相关库:</p>
<ul>
<li>安装nltk即可</li>
</ul>
</li>
<li><p>导入和使用:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.stem.porter import PorterStemmer</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">stemmer.stem(&quot;describing&quot;)</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># describ</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP——困惑度-Perplexity</title>
    <url>/Notes/NLP/NLP%E2%80%94%E2%80%94%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity.html</url>
    <content><![CDATA[<p><em>本文主要介绍困惑度在语言模型评估中的作用</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="如何评价一个语言模型的好坏"><a href="#如何评价一个语言模型的好坏" class="headerlink" title="如何评价一个语言模型的好坏"></a>如何评价一个语言模型的好坏</h3><h4 id="直接评估法"><a href="#直接评估法" class="headerlink" title="直接评估法"></a>直接评估法</h4><ul>
<li>将语言模型应用到实际的具体问题中去,比如机器翻译,词性标注,拼写矫正等</li>
<li>评估语言模型在这些实际问题中的具体表现</li>
</ul>
<h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>准确</li>
<li>能精确评估在具体应用场景中的效果</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>费时</li>
<li>难以操作</li>
</ul>
<h4 id="困惑度-Perplexity"><a href="#困惑度-Perplexity" class="headerlink" title="困惑度(Perplexity)"></a>困惑度(Perplexity)</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><ul>
<li>给<strong>测试集</strong>的句子赋予较高概率值的语言模型较好</li>
<li>当一个语言模型训练完成后,测试集中的句子(正常的自然语言句子)出现概率越高越好</li>
</ul>
<h5 id="困惑度的定义"><a href="#困惑度的定义" class="headerlink" title="困惑度的定义"></a>困惑度的定义</h5><ul>
<li>如果存在测试文本\(d = (w_{1}, w_{2},,,w_{N})\),那么该文本在模型Model中的困惑度为:<br>$$<br>\begin{align}<br>Perplexity(d|Model) &amp;= P(d|Model)^{-\frac{1}{N}} \\<br>&amp;= P(w_{1}, w_{2},,,w_{N}|Model)^{-\frac{1}{N}} \\<br>&amp;= \sqrt[N]{\frac{1}{P(w_{1}, w_{2},,,w_{N}|Model)}}<br>\end{align}<br>$$</li>
<li>两边取log有<br>$$<br>\begin{align}<br>Log(Perplexity(d|Model)) &amp;= -\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model) \\<br>\end{align}<br>$$</li>
<li>一般来说计算时使用公式<br>$$<br>\begin{align}<br>Perplexity(d|Model) &amp;= e^{-\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model)} \\<br>&amp;= exp\left (-\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model)\right ) \\<br>\end{align}<br>$$</li>
<li>如果在已知模型Model参数时,文档\(d\)中的词独立,即\(w_{1},w_{2},,,w_{N}\)互相独立,则有:<br>$$<br>\begin{align}<br>P(w_{1}, w_{2},,,w_{N}|Model) = \prod_{n=1}^{N}P(w_{n}|Model) \\<br>\end{align}<br>$$</li>
<li>进一步有<br>$$<br>\begin{align}<br>Perplexity(d|Model) &amp;= exp\left (-\frac{1}{N}logP(w_{1}, w_{2},,,w_{N}|Model)\right ) \\<br>&amp;= exp\left (-\frac{1}{N}log\prod_{n=1}^{N}P(w_{n}|Model)\right ) \\<br>&amp;= exp\left (-\frac{1}{N}\sum_{n=1}^{N}logP(w_{n}|Model)\right ) \\<br>\end{align}<br>$$</li>
<li>以上是一个文档的表述,对于多个文档\(D = (d_{1}, d_{2},,,d_{M})\)<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= P(D|Model)^{-\frac{1}{\sum_{m=1}^{M}N_{m}}} \\<br>&amp;= \prod_{m=1}^{M} P(d_{m}|Model)^{-\frac{1}{\sum_{m=1}^{M}N_{m}}} \\<br>&amp;= \sqrt[(\sum_{m=1}^{M}N_{m})]{\frac{1}{\prod_{m=1}^{M}P(d_{m}|Model)}}<br>\end{align}<br>$$</li>
<li>两边取log<br>$$<br>\begin{align}<br>Log(Perplexity(D|Model)) &amp;= -\frac{1}{\sum_{m=1}^{M}N_{m}}log(\prod_{m=1}^{M}P(d_{m}|Model)) \\<br>&amp;= -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(d_{m}|Model) \\<br>&amp;= -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \\<br>\end{align}<br>$$</li>
<li>一般来说计算公式<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= e^{-\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model)} \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \right )<br>\end{align}<br>$$</li>
<li>如果在已知Model参数的情况下,每个文档中的词都相互独立,即任取文档\(d_{m}\)有\(w_{1}^{m},w_{2}^{m},,,w_{N_{m}}^{m}\)互相独立,则有<br>$$<br>\begin{align}<br>P(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) = \prod_{n=1}^{N_{m}}P(w_{n}^{m}|Model) \\<br>\end{align}<br>$$</li>
<li>进一步可得<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}P(w_{n}^{m}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}\sum_{n=1}^{N_{m}}logP(w_{n}^{m}|Model) \right ) \\<br>\end{align}<br>$$</li>
<li>注意,多个文档的困惑度<strong>不等于</strong>所有文档困惑度的<strong>积</strong>,而是等于<strong>把所有文档合并成一个大文档,大文档的困惑度则是最终所有文档的困惑度</strong></li>
</ul>
<ul>
<li>在给定模型中,<strong>测试句子</strong>出现的概率越大,对应的困惑度越小,模型越好</li>
</ul>
<h5 id="LDA的困惑度"><a href="#LDA的困惑度" class="headerlink" title="LDA的困惑度"></a>LDA的困惑度</h5><ul>
<li>LDA中\(w_{1},w_{2},,,w_{n}\)在参数已知的情况下是互相独立的,则有<br>$$<br>\begin{align}<br>Perplexity(D|Model) &amp;= e^{-\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model)} \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}logP(w_{1}^{m}, w_{2}^{m},,,w_{N_{m}}^{m}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}P(w_{n}|Model) \right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}\sum_{k=1}^{K}P(w_{n}=t|z_{n}=k;Model)P(z_{n}=k|d=d_{m};Model)\right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}\sum_{k=1}^{K}\theta_{m,k}\phi_{k,t}\right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}log\prod_{n=1}^{N_{m}}\theta_{m,:}\phi_{:,t}\right ) \\<br>&amp;= exp \left ( -\frac{1}{\sum_{m=1}^{M}N_{m}}\sum_{m=1}^{M}\sum_{n=1}^{N_{m}}log\theta_{m,:}\phi_{:,t}\right ) \\<br>\end{align}<br>$$</li>
<li>其中\(\phi_{k,t}\)表示单词t在主题k中出现的概率,\(\theta_{m,k}\)表示主题k在文档m中出现的概率</li>
<li>\(\sum_{k=1}^{K}\theta_{m,k}\phi_{k,t} = (\theta_{m,:}\phi_{:,t})\)就是单词t出现在文档m中的概率(对隐变量主题k积分)</li>
<li>上面式子中\((\theta_{m,:}\phi_{:,t})\)就是两个向量的内积,在这里:\(\theta_{m,:}\)代表行向量,表示当前文档\(d_{m}\)的主题分布,\(\phi_{:,t}\)代表列向量,表示当前每个主题生成词\(w_{t}\)的概率</li>
<li>计算公式的代码可参考L-LDA模型的实现<a href="https://github.com/JoeZJH/Labeled-LDA-Python/blob/master/model/labeled_lda.py" target="_blank" rel="noopener">GitHub仓库: Labeled-LDA-Python</a> 中的<code>perplexity</code>函数和<code>log_perplexity</code>函数</li>
</ul>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP——LLDA的Gibbs采样实现</title>
    <url>/Notes/NLP/NLP%E2%80%94%E2%80%94LLDA%E7%9A%84Gibbs%E9%87%87%E6%A0%B7%E5%AE%9E%E7%8E%B0.html</url>
    <content><![CDATA[<p><em>本文介绍基于Python的有标签隐式狄利克雷分布(Labeled Latent Dirichlet Allocation, L-LDA)的Gibbs Sampling实现</em><br><em>本文回答了项目实现中的一些问题,作为个人LLDA实现过程中遇到的问题和论文中的思考,写得比较多,比较杂</em></p>
<ul>
<li>项目地址: <a href="https://github.com/JoeZJH/Labeled-LDA-Python" target="_blank" rel="noopener">https://github.com/JoeZJH/Labeled-LDA-Python</a><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


</li>
</ul>
<hr>
<h3 id="为什么选择吉布斯采样"><a href="#为什么选择吉布斯采样" class="headerlink" title="为什么选择吉布斯采样"></a>为什么选择吉布斯采样</h3><h4 id="问"><a href="#问" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>为什么使用Gibbs Sampling而不是变分推断实现?</strong></li>
</ul>
<h4 id="答"><a href="#答" class="headerlink" title="答"></a>答</h4><p><em>参考文献:Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</em><br>    * <strong>变分推断不能使用(在L-LDA原始文章中只找到Gibbs采样的实现介绍)</strong></p>
<ul>
<li>首先对于LDA模型而言:erbub    * 变分推断的优点(Gibbs采样的缺点):<pre><code>  * 速度收敛速度快(慢)
  * 可以用于分布式(不能用于分布式)</code></pre>
<ul>
<li>变分推断的缺点(Gibbs采样的优点):<ul>
<li>不够精确,只是近似模拟(也是近似模拟,但是更精确)</li>
<li>变分推断收敛后容易陷入局部最优(得到的往往是局部最优的近似后验分布[已知的简单分布]来估计真实后验分布),效果不如Gibbs采样</li>
</ul>
</li>
</ul>
</li>
<li>其次对于L-LDA而言(<strong>即使可以使用变分推断实现L-LDA</strong>):<ul>
<li>Gibbs采样虽然速度慢一些,但是在我们的场景中,由于需要生成新的训练数据(半监督学习的模型),所以需要确保模型的精确度,而不是时间效率</li>
<li>在半监督训练完成后,实际的应用场景中,为了节约时间,此时我们可以使用已经生成的大量数据集,用变分推断的方法实现,提升用户体验</li>
</ul>
</li>
</ul>
<hr>
<h3 id="收敛性的判断"><a href="#收敛性的判断" class="headerlink" title="收敛性的判断"></a>收敛性的判断</h3><h4 id="问-1"><a href="#问-1" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>收敛性的判断使用参数变化量还是使用<a href="/Notes/NLP/NLP%E2%80%94%E2%80%94%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity.html">困惑度</a>?</strong></li>
</ul>
<h4 id="答-1"><a href="#答-1" class="headerlink" title="答"></a>答</h4><h5 id="收敛的判断方式"><a href="#收敛的判断方式" class="headerlink" title="收敛的判断方式"></a>收敛的判断方式</h5><ul>
<li><p><strong>采样一定次数后停止</strong></p>
<ul>
<li>网上太多实现使用的是采样循环(对所有词的主题的完全采样一次算一个循环)一定次数后停止</li>
<li>优点:<ul>
<li>每次循环结束无需任何额外的计算和判断,只需把迭代次数加一即可</li>
<li>无需存储参数的中间信息</li>
</ul>
</li>
<li>缺点:<ul>
<li>难以确定采样循环次数,次数太多浪费时间,次数太少容易造成不收敛</li>
</ul>
</li>
</ul>
</li>
<li><p>使用<strong>参数变化量</strong>(Gibbs 采样收敛性的判断使用这个感觉更靠谱)</p>
<ul>
<li>基本原理是: 采样收敛以后概率不变,模型参数也会收敛,两次迭代之间参数变化量会很小</li>
<li>优点:<ul>
<li>可以通过参数变化来精确知道模型是否收敛</li>
</ul>
</li>
<li>缺点:<ul>
<li>每次采样循环结束后需要计算参数变化量</li>
<li>需要存储之前的参数的中间结果结果(参数是所有概率矩阵,往往并不小)</li>
</ul>
</li>
</ul>
</li>
<li><p>一种可能可行的方法,使用一个评估指标<strong>困惑度</strong>的变化量</p>
<ul>
<li>在采样的过程中,模型的困惑度一定是递减的(偶尔可能有微弱的增加,属于正常现象)</li>
<li>当困惑度小到一定程度后困惑度应该趋近于收敛,几乎不变,如果困惑度不再有大的变化,我们认为模型收敛</li>
<li>优点:<ul>
<li>无需存储参数,仅仅存储困惑度的值即可(浮点数)</li>
</ul>
</li>
<li>缺点:<ul>
<li>每次评估困惑度时计算量大</li>
<li>当前尚未看到有人使用这个指标作为语言模型收敛度的判断</li>
</ul>
</li>
<li>在我们的场景中，因为我们想知道每一轮的困惑度，所以就把困惑度的变化量当做收敛性判断了，实际上在训练时我们为了收敛保证，选择的是采样次数在一定范围内(不能太小，也不能太大)，同时参数变化量变得很小的双重判断标准(这个判断标准与困惑度不同，不同场景中往往差值很大，我们在GitHub项目中有所实现,允许用户自定义参数)</li>
</ul>
</li>
</ul>
<h5 id="我们的实现"><a href="#我们的实现" class="headerlink" title="我们的实现"></a>我们的实现</h5><ul>
<li>实现了困惑度评估收敛性的方法: 相邻10轮采样(每一轮针对所有词),模型困惑度都不变化,那么认为该模型收敛<ul>
<li>实际上在不同的实际应用中收敛性的判断应该使用不同的方式，我们这里使用困惑度是因为反正都要计算困惑度的，为了充分利用困惑度的计算结果，顺便将其作为收敛性判断</li>
</ul>
</li>
<li>实际论文和工程实验中为了保险使用的是迭代次数的方式</li>
<li>测试说明:<ul>
<li>经测试发现,模型的困惑度10次采样前后变化不大后,参数的确收敛了,而且模型的预测效果也收敛,继续采样50次循环对模型的效果无明显提升</li>
</ul>
</li>
</ul>
<hr>
<h3 id="增量更新训练数据"><a href="#增量更新训练数据" class="headerlink" title="增量更新训练数据"></a>增量更新训练数据</h3><h4 id="问-2"><a href="#问-2" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>在向原始训练数据集(TDS)中添加新的训练数据(NewTDS)时,如何利用原始训练数据上一次的训练结果</strong></li>
</ul>
<h4 id="答-2"><a href="#答-2" class="headerlink" title="答"></a>答</h4><ul>
<li>我们实现了一种可以增量更新模型的方式: 将上一次模型(TDS训练得到)已经训练好的参数作为新模型的初始参数,提升收敛速度</li>
<li>由于Gibbs采样的收敛状态与初始状态无关,所以我们可以从数学理论上证明该方法的正确性</li>
<li>测试说明:<ul>
<li>经测试发现,本方法可以很大程度提升新模型的收敛速度</li>
<li>经测试发现,本方法训练的结果与随机初始化参数的训练结果,模型效果相同<ul>
<li>两个模型的困惑度收敛到近似相等</li>
<li>实际场景中测试说明两个模型的精度也相同</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="推断-预测"><a href="#推断-预测" class="headerlink" title="推断(预测)"></a>推断(预测)</h3><h4 id="问-3"><a href="#问-3" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>模型训练收敛后,对新来的文档,如何给出主题预测?</strong></li>
</ul>
<h4 id="答-3"><a href="#答-3" class="headerlink" title="答"></a>答</h4><p><em>参考文献: LDA数学八卦, L-LDA</em></p>
<ul>
<li>在训练阶段得到模型的主题-词矩阵\(\beta\)后,可以继续进行采样</li>
<li>不用存储文档-主题\(\theta\)矩阵,采样时不需要这个参数,训练后这个参数无需存储</li>
<li>流程如下:<ul>
<li>随机初始化, 对文档中的没个词随机赋一个主题</li>
<li>重新扫描当前文档,按照吉布斯采样公式<ul>
<li>公式为:<br>$$<br>\begin{align}<br>P(z_{i}=k|\vec{z}_{\not{i}}, \vec{w}) &amp;\propto E(\theta_{m,k}) E(\phi_{k,t}) \\<br>&amp;= \hat{\theta}_{m,k} \hat{\phi}_{k,t}<br>\end{align}<br>$$</li>
<li>\(\vec{z}_{\not{i}}\)为除了\(z_{i}\)的所有当前主题\(z_{i}\)是词\(w_{i}\)的主题</li>
<li>\(\hat{\theta}_{m,k} \hat{\phi}_{k,t}\)是样本均值,这里是用当前均值估计期望</li>
<li>显然对每个词,重新采样它的主题(每次采样时实际上<strong>当前词的主题</strong>只与<strong>当前文档其他词的主题</strong>和<strong>主题-词矩阵</strong>相关, 这里推断和训练期间都一样)</li>
<li>实际实现时, 我们并没有存储参数 \(\hat{\phi}_{k,t}\)的值,而是存储一个主题-词[数量]矩阵,方便计算和采样,这种实现推断期间 \(\hat{\phi}_{k,t}\)的值还会继续变化,实际上是更符合实际(精确)的做法,这种做法是使得当前推断和训练的采样方法一模一样(连参数\(\hat{\phi}_{k,t}\)的计算都一样,论文推荐的是在推断期间\(\hat{\phi}_{k,t}\)值不变,与训练步骤不同)<ul>
<li>这里也可以把 \(\hat{\phi}_{k,t}\)的参数(整个矩阵)都存储下来,然后推断期间都不变,这种做法是论文中推荐的,这种做法采样速度快,不需要每次都计算一下当前的\(\hat{\phi}_{k,t}\)值,但是实际上并不精确(当然:当训练数据非常大时,这里近似于精确的,我们的应用场景中考虑到可能训练数据一开始并不多,所以确保精确,我们实现的是前面那种更精确的做法)</li>
</ul>
</li>
</ul>
</li>
<li>重复扫描采样直至收敛</li>
<li>统计文档中的主题分布,得到新文档的文档主题分布\(\vec{\theta}_{new}\)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="实际使用时主题数的确定"><a href="#实际使用时主题数的确定" class="headerlink" title="实际使用时主题数的确定"></a>实际使用时主题数的确定</h3><h4 id="问-4"><a href="#问-4" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>实际使用中主题数如何确定的?</strong></li>
</ul>
<h4 id="答-4"><a href="#答-4" class="headerlink" title="答"></a>答</h4><ul>
<li>一般情况下可以使用困惑度,不同的主题数对应模型的困惑度不一样,k从小到大,对应模型的困惑度应该是先减小后增加,选择困惑度最小的模型对应的主题数即可</li>
<li>在我们论文实验中,由于主题数与标签数量一致,直接设定即可</li>
</ul>
<hr>
<h3 id="验证集问题"><a href="#验证集问题" class="headerlink" title="验证集问题"></a>验证集问题</h3><h4 id="问-5"><a href="#问-5" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>实际在SemiTagRec实现的时候，只提到训练集和测试集，那网格搜索的时候验证集是什么数据呢？</strong></li>
</ul>
<h4 id="答-5"><a href="#答-5" class="headerlink" title="答"></a>答</h4><ul>
<li><p>我们的算法中，如果考虑详细情况的话，网格搜索超参数其实需要每次都调一遍</p>
</li>
<li><p>但是幸亏我们测试发现Integrator的超参数很容易调</p>
</li>
<li><p>实际实验时，我们使用80%和90%样本作为训练集，然后只用随机采样50个左右的样本作为验证集基本就收敛了(随机采样多个都是这样，能在0.91到0.09周围得到最优值)，而且后面调试几乎不变(基本上就在0.89-0.93和0.11-0.07之间且精度几乎没变化，所以我们实际上用所有测试样本测试通过得到了最优的值为0.91和0.09[取平均值得到的])</p>
</li>
<li><p>最终方法：在我们的算法中，首先经过多次训练和测试(实际上就是每次训练完，然后使用网格搜索法完成了Integrator的超参数设定，基本都是使用0.91和0.09最好)，然后接下来的模型迭代训练中我们没有再修改这个超参数了,所以也不用验证集了，这可以为我们之后的训练增加训练和测试数据，对我们来说是个好消息</p>
</li>
</ul>
<hr>
<h3 id="训练集和测试集的划分"><a href="#训练集和测试集的划分" class="headerlink" title="训练集和测试集的划分"></a>训练集和测试集的划分</h3><h4 id="问-6"><a href="#问-6" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>为什么使用90%这么多的样本作为训练集？测试集使用10%足够了吗？</strong></li>
</ul>
<h4 id="答-6"><a href="#答-6" class="headerlink" title="答"></a>答</h4><ul>
<li>实际上我们的算法中，我们一开始的分割方式是60%训练，20%验证，20%测试，这样训练的到的结果不理想(LLDA的训练结果测试就非常差)，分析原因其实是3000+的训练集太少了，对我们的训练模型LLDA来说，远远不够</li>
<li>由于可用的训练样本数量太少，为了保证训练质量，我们选择90%(5000个左右)用于训练后，10%(550个左右)用于测试，550个测试样本总数有2000+个正确的标签，实际上够用了(多次测试证明，随机选取300个测试样本以后基本上增加测试样本模型的精度几乎没变化)</li>
</ul>
<hr>
<h3 id="为什么不使用十折交叉验证法"><a href="#为什么不使用十折交叉验证法" class="headerlink" title="为什么不使用十折交叉验证法"></a>为什么不使用十折交叉验证法</h3><h4 id="问-7"><a href="#问-7" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>既然分配给测试的数据太少，为了增加训练集的同时保证模型评估的精确性，是不是应该使用十折交叉验证法</strong></li>
</ul>
<h4 id="答-7"><a href="#答-7" class="headerlink" title="答"></a>答</h4><ul>
<li>我们的模型使用的训练时间太长了，采样花的时间比较多，考虑到时间因素，没有使用十折交叉验证</li>
<li>待更新：这个回答真的好吗？</li>
</ul>
<hr>
<h3 id="为什么使用多处理器？"><a href="#为什么使用多处理器？" class="headerlink" title="为什么使用多处理器？"></a>为什么使用多处理器？</h3><ul>
<li>使用多处理器的初始想法(这是一个错误的想法)<ul>
<li>预测时文本的预测结果与初始值(当前文本每个词的主题初始分配)有很大关系[错误，这里实际上是采样还没收敛]，不同初始值会收敛到不同的结果，为了防止初始值不同带来的误差，我们采用多次初始化并且多次收敛的方法，最终对结果求均值，得到最终的预测结果</li>
<li>实验证明，这种多处理器的方法对模型预测能力有很大提升？</li>
</ul>
</li>
<li>问题：吉布斯采样应该是能够收敛到目标分布的，为什么预测时这里不会收敛到目标分布？</li>
<li>回答：会收敛的，每次迭代次数不要太少，迭代一定次数后开始丢弃之前的一定数量的采样，去后面的采样平均值，会得到收敛的结果，核心是采样次数一定要够</li>
<li>进一步实验证明：采样次数不足时多个不同初始值采样的结果取平均的确是有帮助的，但是采样次数非常多以后就不需要了，采样结果收敛后一定是到那个目标分布的！</li>
</ul>
<h4 id="多处理器依然存在的意义"><a href="#多处理器依然存在的意义" class="headerlink" title="多处理器依然存在的意义"></a>多处理器依然存在的意义</h4><ul>
<li><p>用于对相同文档同时采样，收敛后每个处理器返回自己的一个平稳分布的样本(可以为一个，可以为多个)，然后所有样本构成最终平稳分布的代表样本</p>
<ul>
<li>实际上收敛后这些样本都是服从目标分布产生的</li>
</ul>
</li>
<li><p>实现上没有问题，但是这里浪费了很多时间采样到收敛的过程(多条不同的采样过程分别采样到收敛，很浪费时间)</p>
</li>
<li><p>从单个处理器来看，不论怎样都需要采样到平稳分布的，在多处理器同时工作时，从平稳状态中采样平稳分布的样本需要的采样次数实际上是被均分到多个处理器上的，从这里来看不考虑内存占用等方面的问题，多处理器工作的确是能节约我们的整体时间的</p>
</li>
<li><p>一般为了避免随机变量统计量(如期望等)估计的偏差,需要产生独立同分布的样本,我们这里就需要: 同时使用多条马尔科夫链可以得到独立同分布的样本,否则同一条链上的样本往往<br>不是独立的,因为同一条链上的后一个样本由前一个样本通过某种特定的状态转移概率得到. </p>
<ul>
<li>实践中,在同一条马尔科夫链上每隔若干个样本才选取一个可以得到近似独立的样本</li>
<li>如果仅仅是采样,不需要样本间相互独立,我们一般就直接使用一条链产生多个样本即可(举例:[待更新])</li>
</ul>
</li>
</ul>
<hr>
<h3 id="为什么训练的时候不是采样多个样本来预测分布"><a href="#为什么训练的时候不是采样多个样本来预测分布" class="headerlink" title="为什么训练的时候不是采样多个样本来预测分布"></a>为什么训练的时候不是采样多个样本来预测分布</h3><h4 id="问-8"><a href="#问-8" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>为什么训练的时候不使用采样多个样本(每个样本代表当前所有文档的所有词的主题矩阵)？</strong></li>
</ul>
<h4 id="答-8"><a href="#答-8" class="headerlink" title="答"></a>答</h4><ul>
<li>由于训练样本非常多，所以单个样本足以代表整体主题-词分布<ul>
<li>因为我们不评估每个文档的主题分布：由于对每个文档来说，不取多个采样值，无法代表文档本身的词分布</li>
<li>而是关注每个主题的词分布：实际上对每个主题来说，词的数量已经非常多了，完全可以代表当前主题-词分布了</li>
<li>经过测试证明的，也有论文支持</li>
</ul>
</li>
</ul>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ul>
<li>训练收敛后取后面m个样本作为训练结果计算主题-词分布与收敛后取最后一个样本得到的结果(主题-词分布矩阵)相同</li>
<li>所以为了节约内存和减少计算量，我们只保留了最后一个样本，丢弃了前面的样本</li>
<li>论文原文引用说明</li>
<li>Parameter estimation for text analysis， Gregor Heinrich*</li>
</ul>
<blockquote>
<p>To obtain the resulting model parameters from a Gibbs sampler, several approaches exist. One is to just use only one read out, another is to average a number of samples, and often it is desirable to leave an interval of L iteration between subsequent read-outs to obtain decorrelated states of the Markov chain. This interval is often called “thinning interval” or sampling lag.</p>
</blockquote>
<ul>
<li><p>说明：上面这段话的是针对LDA的Gibbs采样方法而言的(虽然上面这段话没提到LDA)</p>
</li>
<li><p>上面的引用说明选取LDA收敛后的训练样本选择有两种方式：</p>
<ul>
<li><p>选取一个作为样本(这里只有在LDA训练时可用这种方法，其他的采样模型还要视情况而定的)</p>
<ul>
<li>在LDA中，由于训练样本非常多，所以单个样本足以代表整体主题-词分布(由于LDA同时还关注着文档-主题分布，所以在LDA中私以为还是采样多个样本保险一些，特别是对于词数比较少的文档)</li>
<li>但是，在我们的应用场景中(在LLDA中), <ul>
<li>只关注每个主题的词分布：实际上对每个主题来说，词的数量已经非常多了，完全可以代表当前主题-词分布了，</li>
<li>训练时我们不评估每个文档的主题分布</li>
<li>预测时：由于对每个文档来说，不取多个采样值，无法代表文档本身的词分布(特别是当文挡中的词比较少时)，所以后面的预测过程中对单个文本的预测问题我们需要采样多个收敛后的样本计算均值</li>
</ul>
</li>
</ul>
</li>
<li><p>选取多个样本的平均值作为样本</p>
<ul>
<li>注意：选取多个样本时，为了得到马尔科夫模型不相关的状态，需要间隔L次迭代进行间隔采样</li>
<li>一般来说，在Markov chain收敛后开始从1计数，Gibbs采样(这里不针对LDA)选取一次完整迭代后的结果作为平稳分布的样本即可<ul>
<li>也就是\([(x_{1}^{t},x_{2}^{t},,,x_{n}^{t}), (x_{1}^{t+1},x_{2}^{t+1},,,x_{n}^{t+1}),,, (x_{1}^{t+s},x_{2}^{t+s},,,x_{n}^{t+s})]\)<ul>
<li>其中需要的平稳分布的样本数是(s+1)</li>
<li>注意\([(x_{1}^{t+1},x_{2}^{t},,,x_{n}^{t}), (x_{1}^{t+1},x_{2}^{t+1},,,x_{n}^{t})]\)这些不完整迭代的结果都不能成为平稳分布的样本，因为这些样本之间相关度太高，不够独立，不能用来代表最终的平稳分布，容易造成局部偏差</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="论文中为什么选择LLDA而不使用深度学习"><a href="#论文中为什么选择LLDA而不使用深度学习" class="headerlink" title="论文中为什么选择LLDA而不使用深度学习"></a>论文中为什么选择LLDA而不使用深度学习</h3><h4 id="问-9"><a href="#问-9" class="headerlink" title="问"></a>问</h4><ul>
<li><strong>为什么使用LLDA模型,没有考虑过使用深度学习模型吗?</strong></li>
</ul>
<h4 id="答-9"><a href="#答-9" class="headerlink" title="答"></a>答</h4><ul>
<li>数据量不够</li>
<li>写论文需要可解释性,深度学习模型的解释性远远不如LLDA模型的解释性</li>
<li>初始训练时本人测试时也没有拿出能优于LLDA的模型<ul>
<li>模型较简单,简单的对每个词OneHot编码,然后+固定分类类别数量为1000,所以输出为1000维度</li>
<li>模型是对NNLM的一种改进</li>
<li>损失函数使用的是binary_crossentropy</li>
<li></li>
</ul>
</li>
<li>深度学习在我们的场景中效果不好的原因可能包括以下方面:<ul>
<li>训练数据量不够,太稀疏</li>
<li>没找到合适的神经网络模型,比如可以考虑使用一些权重共享的思想,降低由于数据太少引起的过拟合</li>
<li>在未来的想法: 如果可以增加数据量, 或者能够使用一些新的有效模型或者词嵌入的数据集, 可以重新尝试使用神经网络模型</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——进程相关操作</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E8%BF%9B%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C.html</url>
    <content><![CDATA[<hr>
<h3 id="批量杀死进程"><a href="#批量杀死进程" class="headerlink" title="批量杀死进程"></a>批量杀死进程</h3><ul>
<li>杀死某个用户的所有进程<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pkill -u jiahong</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="top查看进程信息"><a href="#top查看进程信息" class="headerlink" title="top查看进程信息"></a>top查看进程信息</h3><ul>
<li>M: 根据驻留内存大小进行排序</li>
<li>P: 根据CPU使用百分比大小进行排序</li>
<li>f: 每列的参数说明</li>
</ul>
<hr>
<h3 id="查看进程的堆栈"><a href="#查看进程的堆栈" class="headerlink" title="查看进程的堆栈"></a>查看进程的堆栈</h3><ul>
<li>pstack</li>
<li>gstack</li>
</ul>
<hr>
<h3 id="杀死特定进程"><a href="#杀死特定进程" class="headerlink" title="杀死特定进程"></a>杀死特定进程</h3><h4 id="已知进程pid"><a href="#已知进程pid" class="headerlink" title="已知进程pid"></a>已知进程pid</h4><ul>
<li><p>杀死进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kill [pid]</span><br></pre></td></tr></table></figure>
</li>
<li><p>强制杀死进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kill -9 [pid]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="未知进程pid"><a href="#未知进程pid" class="headerlink" title="未知进程pid"></a>未知进程pid</h4><ul>
<li><p>如果不知道进程pid可以在top命令里面找<code>COMMAND</code>列中相关的进程</p>
</li>
<li><p>有时候通过top的信息不方便找到(比如想通过启动命令查找,比如想进程太多,找不过来)</p>
<ul>
<li><p>此时通过下面的命令查找</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -aux | grep &quot;command&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>更多<code>ps -aux</code>的解析可以参考博客<a href="https://www.cnblogs.com/dion-90/articles/9048627.html" target="_blank" rel="noopener">https://www.cnblogs.com/dion-90/articles/9048627.html</a></p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="htop查看进程"><a href="#htop查看进程" class="headerlink" title="htop查看进程"></a>htop查看进程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum intall htop</span><br><span class="line">htop</span><br></pre></td></tr></table></figure>

<ul>
<li>比top更优秀的进程查看工具</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统——同步vs异步-阻塞vs非阻塞</title>
    <url>/Notes/Others/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94%E5%90%8C%E6%AD%A5vs%E5%BC%82%E6%AD%A5-%E9%98%BB%E5%A1%9Evs%E9%9D%9E%E9%98%BB%E5%A1%9E.html</url>
    <content><![CDATA[<ul>
<li>参考链接<ul>
<li><a href="https://www.zhihu.com/question/19732473/answer/20851256" target="_blank" rel="noopener">https://www.zhihu.com/question/19732473/answer/20851256</a></li>
<li><a href="https://www.zhihu.com/question/26393784/answer/513257548" target="_blank" rel="noopener">https://www.zhihu.com/question/26393784/answer/513257548</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="同步与异步"><a href="#同步与异步" class="headerlink" title="同步与异步"></a>同步与异步</h3><ul>
<li>同步与异步关注的是<strong>消息通信机制</strong>：<strong>被调用者是否有回传功能在任务结束时通知调用者</strong><h4 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h4></li>
<li><strong>同步</strong>：就是在发出一个<em>调用</em>时，在没有得到结果之前，该<em>调用</em>就不返回。但是一旦调用返回，就得到返回值了<ul>
<li>比如写一个函数调用另一个函数，必须等待返回结果才继续下一步骤，因为同步调用是被调用者没有回调通知的功能，所以必须等</li>
</ul>
</li>
</ul>
<h4 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h4><ul>
<li><strong>异步</strong>：<em>调用</em>在发出之后，这个调用就直接返回了，所以没有返回结果</li>
<li>调用者不会立刻得到结果，被调用者完成工作后会通知调用者【通过回调函数等方式】</li>
<li>比如在Future和Callable配合使用时<ul>
<li>调用者可以启动Callable对应的线程执行任务，然后马上返回，获取到一个Future对象</li>
<li>Callable完成工作后，会将返回值存放到Future【相当于一个回调】</li>
<li>调用者可以通过检查Future对象,调用其.get()方法得到返回值<ul>
<li>【如果没有返回的话，会被阻塞，但是启动任务和调用get()期间，调用者是可以做自己的事情的，所以是异步非阻塞调用，然后get()阻塞式接受结果】</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="区别分析"><a href="#区别分析" class="headerlink" title="区别分析"></a>区别分析</h4><ul>
<li>同步与异步的重点区别在于是否有返回通知的功能:<a href="https://www.zhihu.com/question/26393784/answer/513257548" target="_blank" rel="noopener">https://www.zhihu.com/question/26393784/answer/513257548</a></li>
<li>是否马上返回感觉只是一个附属的结论，因为异步能通知，所以才能马上离开</li>
</ul>
<h3 id="阻塞与非阻塞"><a href="#阻塞与非阻塞" class="headerlink" title="阻塞与非阻塞"></a>阻塞与非阻塞</h3><ul>
<li>阻塞和非阻塞关注的是程序在<strong>等待调用结果（消息，返回值）时的状态</strong>：<strong>被调用者是否被阻塞</strong><h4 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h4></li>
<li><strong>阻塞</strong>：阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。<ul>
<li>如果被调用者没有完成工作，就一直等待</li>
</ul>
</li>
</ul>
<h4 id="非阻塞"><a href="#非阻塞" class="headerlink" title="非阻塞"></a>非阻塞</h4><ul>
<li><strong>非阻塞</strong>：非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。<ul>
<li>如果被调用者还没有完成工作，就返回默认值</li>
</ul>
</li>
</ul>
<h4 id="区别分析-1"><a href="#区别分析-1" class="headerlink" title="区别分析"></a>区别分析</h4><ul>
<li>同步与异步的区别在于<strong>等待结果时的状态</strong>：<a href="https://www.zhihu.com/question/19732473/answer/20851256" target="_blank" rel="noopener">https://www.zhihu.com/question/19732473/answer/20851256</a></li>
</ul>
<h3 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h3><ul>
<li>同步一般伴随着阻塞，因为同步时不等待的话拿不到结果<ul>
<li>没见过同步的非阻塞是调用，除非是不需要结果的调用，只是为了发送一个通知给被调用者</li>
</ul>
</li>
<li>异步一般伴随着非阻塞，不然就浪费了异步的功能<ul>
<li>异步调用后可以选择两种调用拿到结果<ul>
<li>阻塞式get + 轮训</li>
<li>阻塞式get</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——ProcessPoolExecutor和ThreadPoolExecutor</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94ProcessPoolExecutor%E5%92%8CThreadPoolExecutor.html</url>
    <content><![CDATA[<p><em>Python 中如何使用CPU的多个核</em></p>
<hr>
<h3 id="全局解释器锁-GIL"><a href="#全局解释器锁-GIL" class="headerlink" title="全局解释器锁(GIL)"></a>全局解释器锁(GIL)</h3><ul>
<li>由于CPython解释器本身就不是线程安全的，所以需要一个全局解释器锁，以保证同一时刻仅有一个线程在执行Python的字节码</li>
<li>由于GIL的存在，造成了Python多线程不是不能真正并行，尽管有多个CPU核心也不能全都用上</li>
<li>由于标准库中执行所有阻塞型IO操作的函数，在等待操作系统返回结果时都会释放GIL，这意味着Python在这个层次上可以使用多线程，所以对于IO密集型任务来说，多线程是有作用的</li>
</ul>
<hr>
<h3 id="ThreadPoolExecutor"><a href="#ThreadPoolExecutor" class="headerlink" title="ThreadPoolExecutor"></a>ThreadPoolExecutor</h3><ul>
<li>使用多线程，适用于IO密集型的任务<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="ProcessPoolExecutor"><a href="#ProcessPoolExecutor" class="headerlink" title="ProcessPoolExecutor"></a>ProcessPoolExecutor</h3><ul>
<li>使用多进程，突破GIL的限制，绕开GIL，成功使用多个CPU核<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker——Dockerfile命令总结</title>
    <url>/Notes/Docker/Docker%E2%80%94%E2%80%94Dockerfile%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<hr>
<h3 id="Dockerfile简介"><a href="#Dockerfile简介" class="headerlink" title="Dockerfile简介"></a>Dockerfile简介</h3><hr>
<h3 id="Dockerfile命令总结"><a href="#Dockerfile命令总结" class="headerlink" title="Dockerfile命令总结"></a>Dockerfile命令总结</h3><h4 id="ARG"><a href="#ARG" class="headerlink" title="ARG"></a>ARG</h4><ul>
<li><p>实现在<code>docker build</code>过程中传入值,用法</p>
<ul>
<li><p>Dockerfile:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ARG &lt;name&gt;[=&lt;default value&gt;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>编译时</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build --build-arg &lt;varname&gt;=&lt;value&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>可以使用默认值，默认值在<code>docker build</code>不传入参数时生效</p>
</li>
<li><p>该命令在<code>docker build</code>过程和<code>CMD</code>,<code>ENTRY</code>中有效, 在docker镜像创建并启动container后无效</p>
</li>
<li></li>
</ul>
<h4 id="LABEL"><a href="#LABEL" class="headerlink" title="LABEL"></a>LABEL</h4><ul>
<li><h4 id="ENV"><a href="#ENV" class="headerlink" title="ENV"></a>ENV</h4></li>
</ul>
<h4 id="CMD"><a href="#CMD" class="headerlink" title="CMD"></a>CMD</h4><img src="/Notes/Docker/Docker——Dockerfile命令总结/CMD_Overview.png">

<h4 id="ENTRY"><a href="#ENTRY" class="headerlink" title="ENTRY"></a>ENTRY</h4><hr>
<h3 id="Dockerfile命令相关说明"><a href="#Dockerfile命令相关说明" class="headerlink" title="Dockerfile命令相关说明"></a>Dockerfile命令相关说明</h3><h4 id="比较CMD和ENTRY的区别"><a href="#比较CMD和ENTRY的区别" class="headerlink" title="比较CMD和ENTRY的区别"></a>比较CMD和ENTRY的区别</h4><h4 id="比较ENV和ARG的区别"><a href="#比较ENV和ARG的区别" class="headerlink" title="比较ENV和ARG的区别"></a>比较ENV和ARG的区别</h4><ul>
<li>ARG可以在<code>docker build</code>过程中传值，同时也可以设置默认值</li>
<li>ENV只能定义到Dockerfile中，不能在<code>docker build</code>过程中传入</li>
</ul>
]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker——Docker使用笔记</title>
    <url>/Notes/Docker/Docker%E2%80%94%E2%80%94Docker%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>本文用于记录一些Docker使用过程中的经验</em></p>
<h3 id="清空没用的镜像以节约空间"><a href="#清空没用的镜像以节约空间" class="headerlink" title="清空没用的镜像以节约空间"></a>清空没用的镜像以节约空间</h3><ul>
<li>清空所有悬空（dangling）镜像<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker image prune</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Docker服务相关操作"><a href="#Docker服务相关操作" class="headerlink" title="Docker服务相关操作"></a>Docker服务相关操作</h3><ul>
<li><p>查看服务是否启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Docker服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl start docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止Docker服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl stop docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>重启Docker服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置开机启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl enable docker</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>拍卖机制——智能出价与激励兼容</title>
    <url>/Notes/Others/%E6%8B%8D%E5%8D%96%E6%9C%BA%E5%88%B6%E2%80%94%E2%80%94%E6%99%BA%E8%83%BD%E5%87%BA%E4%BB%B7%E4%B8%8E%E6%BF%80%E5%8A%B1%E5%85%BC%E5%AE%B9.html</url>
    <content><![CDATA[<p><em>以下思考仅为笔者与其他同事讨论时的一些想法，仅供参考</em></p>
<hr>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>疑问：拍卖机制的激励兼容是指机制能否让商家说真话时从系统获得最大收益，但在智能出价时，还支持激励兼容吗？</li>
<li>激励兼容（IC）的定义：讲真话是利益相关者在这个机制下的最优选择</li>
<li>占优策略激励兼容的定义：竞拍者的真实出价是一个占优策略，且效用非负<ul>
<li>竞拍者的占优策略是指，无论别人怎么出价，这样出价对该竞拍者来说都是最好的</li>
</ul>
</li>
</ul>
<hr>
<h3 id="回答"><a href="#回答" class="headerlink" title="回答"></a>回答</h3><ul>
<li>从计费方式上看，由于系统为了保ROI会计费到满足商家出价ROI，本质上算是ROI上的一价计费，从这里看不能算是激励兼容了</li>
<li>另外，激励兼容有个等价表达是，同时满足以下两个条件：<ul>
<li>第一，分配规则：随着出价提升，分配到的资源是单调不减</li>
<li>第二，计费规则：净胜者需要支付的数额等于将使他赢得拍卖的最小值</li>
</ul>
</li>
<li>在oCPC中（假设商家投放期间固定目标ROI），可以算是激励兼容的，但是比较弱：<ul>
<li>分配规则：流量可以看做是连续的，所以流量随着商家出价是单调递增的，分配规则满足激励兼容</li>
<li>计费规则：由于流量是随着出价单调递增的， 所以实际上，商家计费对应的流量就是他拿到这些流量的最小计费</li>
<li>但是，从商家视角考虑，随着出价增多，边际效益递减（流量会越来越贵），再增加支付成本拿到的流量并不多，所以其实容易出现计费高了，但是流量涨幅不多（甚至相当于没涨），从这个角度来讲，好像上述两条激励兼容的满足又有点弱。</li>
</ul>
</li>
</ul>
<h3 id="其他思考（非严格证明）"><a href="#其他思考（非严格证明）" class="headerlink" title="其他思考（非严格证明）"></a>其他思考（非严格证明）</h3><ul>
<li>如果把智能出价下的系统分成两层<ul>
<li>第一层：对每个投放周期内，商家出价，给出指定目标（目标在每个投放周期内固定），比如ROI约束下最大化点击的产品，商家出价则是目标ROI值</li>
<li>第二层：对每个请求，智能出价在商家约束和目标下，智能体给出满足商家约束且能最大化商家效果的单次出价</li>
</ul>
</li>
<li>在第一层中，激励兼容可以理解为：每个投放周期内，商家都会说真话，选择一个自己能接受且能最大化自己效果的目标值，比这个目标值小或大都不符合商家利益<ul>
<li>此时每个投放周期内，对一个商家来说，系统都在进行一次拍卖，是对一个投放周期内整体流量进行打包拍卖，商家出一次价即可购买一定量的效果，这种拍卖不是0-1拍卖，不是简单的拍卖成功或拍卖失败，而是给出一个连续的出价目标，对应一个连续成本和一个连续的收益，成本和收益之间一般是正相关关系</li>
</ul>
</li>
<li>在第二层中，激励兼容可以理解为：每个请求下，智能体给出的出价都是达成商家目标所能给出的最优出价，即能让自己收益最大化的最优出价<ul>
<li>当然，此时商家出价不再仅仅与这次拍卖相关，而是与整个投放周期内的成本和效果相关</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>服务器——内存扩展</title>
    <url>/Notes/Others/%E6%9C%8D%E5%8A%A1%E5%99%A8%E2%80%94%E2%80%94%E5%86%85%E5%AD%98%E6%89%A9%E5%B1%95.html</url>
    <content><![CDATA[<hr>
<h3 id="服务器内存通道说明"><a href="#服务器内存通道说明" class="headerlink" title="服务器内存通道说明"></a>服务器内存通道说明</h3><ul>
<li>一般来说两个槽一个通道，由黑白不同的两个插槽组成</li>
<li>某些特殊的产品也会有单个插槽就是一个通道的情况</li>
<li>大部分CPU一般有4个通道，有些会有6个通道</li>
</ul>
<hr>
<h3 id="一般服务器内存扩展原则"><a href="#一般服务器内存扩展原则" class="headerlink" title="一般服务器内存扩展原则"></a>一般服务器内存扩展原则</h3><ul>
<li>先插满白色口，再插黑色口</li>
<li>也就是充分利用通道，每个通道插一个是速度最快的最优选择</li>
</ul>
<hr>
<h3 id="对于特定产品"><a href="#对于特定产品" class="headerlink" title="对于特定产品"></a>对于特定产品</h3><h4 id="DELL-Power-Edge"><a href="#DELL-Power-Edge" class="headerlink" title="DELL Power Edge"></a>DELL Power Edge</h4><ul>
<li><p>第一个CPU通道数：6个</p>
<ul>
<li>[A3, A5]分别为一个一个单独的通道</li>
<li>其余的每双槽构成一个通道</li>
<li>一共10个插槽</li>
</ul>
</li>
<li><p>第二个CPU通道数：3个</p>
<ul>
<li>每个通道两个插槽</li>
<li>6个插槽组成3通道</li>
</ul>
</li>
<li><p>对于一个CPU而言，插内存的组合方式</p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">内存条数量</th>
<th align="center">插槽对应位置</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">A1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">A1, A2</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">A1, A2, A4, A5</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">A1, A2, A3, A4, A5, A6</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">A1, A2, A3, A4, A5, A6, A7, A8</td>
</tr>
<tr>
<td align="center">10</td>
<td align="center">A1, A2, A3, A4, A5, A6, A7, A8, A9, A10</td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>浏览器——设置网页为可编辑模式</title>
    <url>/Notes/Others/%E6%B5%8F%E8%A7%88%E5%99%A8%E2%80%94%E2%80%94%E8%AE%BE%E7%BD%AE%E7%BD%91%E9%A1%B5%E4%B8%BA%E5%8F%AF%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F.html</url>
    <content><![CDATA[<p><em>用一行代码将网页设置成可修改模式，方便修改</em></p>
<hr>
<h3 id="修改网页为可编辑模式"><a href="#修改网页为可编辑模式" class="headerlink" title="修改网页为可编辑模式"></a>修改网页为可编辑模式</h3><ul>
<li>打开console</li>
<li>执行以下代码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">document.body.contentEditable = &apos;true&apos;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——修改图片背景颜色</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E4%BF%AE%E6%94%B9%E5%9B%BE%E7%89%87%E8%83%8C%E6%99%AF%E9%A2%9C%E8%89%B2.html</url>
    <content><![CDATA[<p><em>利用Python的skimage包实现修改图片背景色等</em></p>
<hr>
<h3 id="修改签名背景，按照灰度图像处理"><a href="#修改签名背景，按照灰度图像处理" class="headerlink" title="修改签名背景，按照灰度图像处理"></a>修改签名背景，按照灰度图像处理</h3><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from skimage import io, color</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def change_grey_to_white(origin, new):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Change background color for image, such as signature</span><br><span class="line">    :param origin: the path of original image</span><br><span class="line">    :param new: the path of new image</span><br><span class="line">    :return: None</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # img = io.imread(&apos;./origin.jpeg&apos;)</span><br><span class="line">    img = io.imread(origin)</span><br><span class="line">    io.imshow(img)</span><br><span class="line">    plt.show()</span><br><span class="line">    img_gray = color.rgb2gray(img)</span><br><span class="line">    rows, cols = img_gray.shape</span><br><span class="line">    for i in range(rows):</span><br><span class="line">        for j in range(cols):</span><br><span class="line">            if img_gray[i, j] &lt;= 0.5:</span><br><span class="line">                img_gray[i, j] = 0</span><br><span class="line">            else:</span><br><span class="line">                img_gray[i, j] = 1</span><br><span class="line">    io.imshow(img_gray)</span><br><span class="line">    plt.show()</span><br><span class="line">    io.imsave(new, img_gray)</span><br><span class="line"></span><br><span class="line"># example</span><br><span class="line">change_grey_to_white(&apos;./origin.jpeg&apos;, &apos;new.jpeg&apos;)</span><br></pre></td></tr></table></figure>

<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul>
<li>这里可接受彩色图片，比如拍照片得到的原始图片</li>
<li>函数会将其转换为灰度图片然后处理，最终输出也是灰度图片</li>
<li>如果需要处理指定灰度，可通过修改判断语句中0.5这个值从而实现（比如修改为区间等）</li>
</ul>
<hr>
<h3 id="修改证件照背景，按彩色图像处理"><a href="#修改证件照背景，按彩色图像处理" class="headerlink" title="修改证件照背景，按彩色图像处理"></a>修改证件照背景，按彩色图像处理</h3><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from skimage import io</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def change_pixel_color(pixel, old_pixel, new_pixel=None, error=60):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    change color for pixel if pixel in range [old_pixel-error, old_pixel+error]</span><br><span class="line">    :param pixel:</span><br><span class="line">    :param old_pixel:</span><br><span class="line">    :param new_pixel:</span><br><span class="line">    :param error:</span><br><span class="line">    :return: the new pixel</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if new_pixel is None:</span><br><span class="line">        new_pixel = [255, 255, 255]</span><br><span class="line">    similar = True</span><br><span class="line">    for i in abs(pixel - old_pixel):</span><br><span class="line">        if i &gt; error:</span><br><span class="line">            similar = False</span><br><span class="line">            break</span><br><span class="line">    if similar:</span><br><span class="line">        return new_pixel</span><br><span class="line">    else:</span><br><span class="line">        return pixel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def change_background(origin_path, new_path, new_color, error=60):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    change background, auto detect background</span><br><span class="line">    :param origin_path:</span><br><span class="line">    :param new_path:</span><br><span class="line">    :param new_color: target background color</span><br><span class="line">    :param error: the error for change color</span><br><span class="line">    :return: None</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    img = io.imread(origin_path)</span><br><span class="line">    bg = img[10:30, 10:30, :]</span><br><span class="line">    bg_pixel = [1.0 * np.sum(bg[:, :, channel])/bg[:, :, channel].size for channel in range(3)]</span><br><span class="line">    print(&quot;background color: %s&quot; % bg_pixel)</span><br><span class="line">    new_img = np.array([[change_pixel_color(pixel, [0, 160, 234], new_color, error) for pixel in row] for row in img])</span><br><span class="line">    io.imshow(new_img)</span><br><span class="line">    # import matplotlib.pyplot as plt</span><br><span class="line">    # plt.show()</span><br><span class="line">    io.imsave(new_path, new_img)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># example</span><br><span class="line">change_background(origin_path=&quot;origin.jpeg&quot;,</span><br><span class="line">                  new_path=&apos;new.jpeg&apos;,</span><br><span class="line">                  new_color=[0, 255, 255])</span><br></pre></td></tr></table></figure>

<h4 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h4><ul>
<li>自动读取图片的左上角部分像素的平均值作为背景颜色</li>
<li>允许差范围在合适的范围内，可通过<code>error</code>参数调节，该参数不宜过大也不宜过小，可测试多次选择比较合适的</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——缺失值处理</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86.html</url>
    <content><![CDATA[<p><em>missing值处理</em></p>
<hr>
<h3 id="对于数值类型的特征"><a href="#对于数值类型的特征" class="headerlink" title="对于数值类型的特征"></a>对于数值类型的特征</h3><h4 id="中位数填充"><a href="#中位数填充" class="headerlink" title="中位数填充"></a>中位数填充</h4><ul>
<li>用当前特征所有未缺失的值的中位数(median)填充当前特征的缺失值</li>
</ul>
<h4 id="均值填充"><a href="#均值填充" class="headerlink" title="均值填充"></a>均值填充</h4><ul>
<li>用当前特征所有未缺失的值的均值填充当前特征的缺失值</li>
</ul>
<h4 id="加权填充"><a href="#加权填充" class="headerlink" title="加权填充"></a>加权填充</h4><ul>
<li>引入相似性矩阵,评估缺失样本与其他未缺失样本的相似性,按照相似性分配权重</li>
<li>效果更好,但是需要更多时间</li>
</ul>
<hr>
<h3 id="对于Category类型"><a href="#对于Category类型" class="headerlink" title="对于Category类型"></a>对于Category类型</h3><h4 id="特殊字符填充"><a href="#特殊字符填充" class="headerlink" title="特殊字符填充"></a>特殊字符填充</h4><ul>
<li>使用某种未出现过的特殊字符填充</li>
<li>等价于将缺失值看成是个特殊类别</li>
</ul>
<h4 id="填充众数"><a href="#填充众数" class="headerlink" title="填充众数"></a>填充众数</h4><ul>
<li>寻找所有未缺失数据中样本最多的类别,然后将缺失值填充为该众数类别</li>
</ul>
<hr>
<h3 id="什么情况下不用填充"><a href="#什么情况下不用填充" class="headerlink" title="什么情况下不用填充"></a>什么情况下不用填充</h3><h4 id="树模型"><a href="#树模型" class="headerlink" title="树模型"></a>树模型</h4><h5 id="普通树模型"><a href="#普通树模型" class="headerlink" title="普通树模型"></a>普通树模型</h5><ul>
<li>ID3不支持缺失值处理(也可能可以,但是我没看到具体介绍)</li>
<li>C4.5和CART都使用下面的方法进行缺失值处理</li>
<li>结点分裂时:<ul>
<li><em>参考自周志华机器学习书籍中</em></li>
<li>先按照无缺失的数据正常划分(缺失数据不参与计算)</li>
<li>对于无缺失值的样本,正常分配到对应叶子节点</li>
<li>对于缺失值的样本,每个样本以不同的概率分配到各个叶子节点, 概率值为: <strong>子节点中无缺失样本的数量 / 无缺失样本的总数</strong></li>
</ul>
</li>
<li>问题: 如果是训练时没有缺失,预测时有缺失怎么办?<ul>
<li>一种可能的方式是直接放到某个结点中</li>
<li>[待更新]</li>
</ul>
</li>
</ul>
<h5 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h5><ul>
<li>如果使用的是树模型(CART)作为基分类器是否可以直接借用树模型对缺失值的处理方法?</li>
<li>如果使用的是逻辑回归模型作为基分类器,需要自己对缺失值进行处理</li>
</ul>
<h5 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h5><ul>
<li>寻找分裂点时(split point):<ul>
<li>不遍历缺失值对应的样本,只使用无缺失的样本确定分裂点(节省开销)</li>
<li>尝试将缺失值分配到左叶子结点或者右叶子结点,分别计算增益(保证完备性)</li>
<li>选择增益大的点即可</li>
</ul>
</li>
<li>如果训练时没有缺失值,预测时有缺失值:<ul>
<li>将缺失值自动放到右子树中即可</li>
</ul>
</li>
</ul>
<hr>
<h3 id="不同模型对缺失值的敏感度总结"><a href="#不同模型对缺失值的敏感度总结" class="headerlink" title="不同模型对缺失值的敏感度总结"></a>不同模型对缺失值的敏感度总结</h3><h4 id="不敏感模型"><a href="#不敏感模型" class="headerlink" title="不敏感模型"></a>不敏感模型</h4><ul>
<li>树模型</li>
<li>神经网络的鲁棒性强, 数据量够的话不敏感? <ul>
<li>神经网络的输入必须是没有缺失值的,需要我们使用特征工程的方法填充缺失值</li>
<li>其实我觉得自己需要填充数据的其实都有点敏感吧, 所以神经网络在使用时有时候感觉并不敏感,因为缺失值被我们填充后总能得到不错的效果</li>
</ul>
</li>
</ul>
<h4 id="敏感模型"><a href="#敏感模型" class="headerlink" title="敏感模型"></a>敏感模型</h4><ul>
<li>距离度量模型: KNN, SVM等<ul>
<li>在尽量保证缺失值是随机的前提下使用基于统计分布的填充方法可能降低缺失值造成的负面影响</li>
<li>但是SVM这样的模型对缺失值的抗性非常差,不恰当的非随机缺失值可能导致模型出现意外</li>
</ul>
</li>
<li>线性模型的损失函数往往也涉及到距离的计算?</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Chrome——常用插件总结</title>
    <url>/Notes/Others/Chrome%E2%80%94%E2%80%94%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>Chrome是最好用的浏览器没有之一，本文将总结Chrome中功能强大的插件</em></p>
<hr>
<h3 id="Markdown-Viewer"><a href="#Markdown-Viewer" class="headerlink" title="Markdown Viewer"></a>Markdown Viewer</h3><ul>
<li>用于浏览markdown文件内容</li>
<li>自动为markdown文件生成HTML源码</li>
<li>安装后记得设置允许该插件访问文件URL</li>
</ul>
<hr>
<h3 id="油猴插件-Tampermonkey"><a href="#油猴插件-Tampermonkey" class="headerlink" title="油猴插件(Tampermonkey)"></a>油猴插件(Tampermonkey)</h3><ul>
<li>脚本引擎，用于管理其他脚本</li>
<li>功能强大，里面有形形色色黑科技油猴脚本</li>
</ul>
<hr>
<h3 id="Postman"><a href="#Postman" class="headerlink" title="Postman"></a>Postman</h3><ul>
<li>程序员必备，测试API首选</li>
<li>各种http接口（如GET, POST）和数据格式（如表格）</li>
</ul>
<hr>
<h3 id="AdBlock"><a href="#AdBlock" class="headerlink" title="AdBlock"></a>AdBlock</h3><ul>
<li>能屏蔽大部分的广告</li>
</ul>
<hr>
<h3 id="SwitchyOmega"><a href="#SwitchyOmega" class="headerlink" title="SwitchyOmega"></a>SwitchyOmega</h3><ul>
<li>shadowsocks的伴侣</li>
<li>特别是Ubuntu必备</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Conference——各种会议知识总结</title>
    <url>/Notes/Others/Conference%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E4%BC%9A%E8%AE%AE%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<h3 id="NIPS和Advances-in-Neural-Information-Processing-Systems的关系"><a href="#NIPS和Advances-in-Neural-Information-Processing-Systems的关系" class="headerlink" title="NIPS和Advances in Neural Information Processing Systems的关系"></a>NIPS和Advances in Neural Information Processing Systems的关系</h3><ul>
<li>NIPS会议全称为：Annual Conference on Neural Information Processing Systems，年度神经信息处理系统大会</li>
<li>NIPS收录的会议论文会以“Advances in Neural Information Processing Systems”为名出版成书，一般由mit和Morgan Kaufmann出版社出版</li>
<li>引用时一般会以“Advances in Neural Information Processing Systems”命名</li>
</ul>
<h3 id="ICLR不在CCF列表中"><a href="#ICLR不在CCF列表中" class="headerlink" title="ICLR不在CCF列表中"></a>ICLR不在CCF列表中</h3><ul>
<li>ICLR(International Conference on Learning Representations)是近年来的会议，也是很好的会议，甚至可以比拟NIPS，但是因为国内发的不多，所以没有列到CCF列表中</li>
<li>ICLR在清华的列表中是A类</li>
</ul>
<h3 id="SIGKDD-vs-KDD"><a href="#SIGKDD-vs-KDD" class="headerlink" title="SIGKDD vs KDD"></a>SIGKDD vs KDD</h3><ul>
<li>这两者指的是同一个会议，一般正式叫做SIGKDD</li>
<li>会议全称：ACM Knowledge Discovery and Data Mining</li>
</ul>
<h3 id="AI顶会-amp-顶刊排名"><a href="#AI顶会-amp-顶刊排名" class="headerlink" title="AI顶会&amp;顶刊排名"></a>AI顶会&amp;顶刊排名</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/375822515" target="_blank" rel="noopener">AI顶会&amp;顶刊</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25153492" target="_blank" rel="noopener">人工智能方面顶级会议（转）</a></li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>FS——文件系统总结</title>
    <url>/Notes/Others/FS%E2%80%94%E2%80%94%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<hr>
<h3 id="FAT32"><a href="#FAT32" class="headerlink" title="FAT32"></a>FAT32</h3><p><em>常用于闪存</em></p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>通用格式,任何USB都会预装FAT32,任何操作系统平台上都能读写</li>
<li>节约空间</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>单个文件的大小限制为4GB</li>
<li>无读写日志,不能记录磁盘上文件的修改记录?</li>
</ul>
<hr>
<h3 id="ExFAT"><a href="#ExFAT" class="headerlink" title="ExFAT"></a>ExFAT</h3><p><em>微软自家创建的用来取代FAT32的新型文件格式类型</em></p>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul>
<li>跨平台</li>
<li>能支持4GB以上的单个文件</li>
</ul>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>无读写日志,不能记录磁盘上文件的修改记录</li>
</ul>
<hr>
<h3 id="NTFS"><a href="#NTFS" class="headerlink" title="NTFS"></a>NTFS</h3><p><em>New Technology File System</em><br><em>微软为硬盘和固态硬盘创建的默认新型文件系统</em><br><em>几乎集成了所有文件系统的优点</em></p>
<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul>
<li>日志功能</li>
<li>无文件大小限制</li>
<li>支持文件压缩和长文件名</li>
<li>服务器文件管理权限</li>
</ul>
<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>MacOS不能写,只能读(严格来说并不能算是NTFS的问题,是苹果自己不适配吧)</li>
</ul>
<hr>
<h3 id="MacOS日志式"><a href="#MacOS日志式" class="headerlink" title="MacOS日志式"></a>MacOS日志式</h3><h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><ul>
<li>没啥缺点也是一种优点</li>
</ul>
<h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>如果要在MacOS上访问使用就用ExFAT,否则一律使用NTFS</li>
<li>对于移动硬盘可以考虑部分分区格式化为ExFAT,部分分区格式化为NTFS</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>IDEA——一些常见的问题解决方案</title>
    <url>/Notes/Others/IDEA%E2%80%94%E2%80%94%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</url>
    <content><![CDATA[<p><em>本文总结一些IDEA使用过程中遇到的问题和解决方案</em></p>
<hr>
<h3 id="模块导入问题"><a href="#模块导入问题" class="headerlink" title="模块导入问题"></a>模块导入问题</h3><ul>
<li>同一目录下Python模块导入有红色波浪线,但是可以运行<ul>
<li>解决方案:<ul>
<li>将当前文件夹标记成Source Root</li>
<li>如果是使用的<code>from</code>,可使用当前文档标记符来解决问题<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from .test_module import test_function</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>机器码——源码-补码-反码</title>
    <url>/Notes/Others/%E6%9C%BA%E5%99%A8%E7%A0%81%E2%80%94%E2%80%94%E6%BA%90%E7%A0%81-%E8%A1%A5%E7%A0%81-%E5%8F%8D%E7%A0%81.html</url>
    <content><![CDATA[<p>参考博客：<a href="https://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html</a></p>
<hr>
<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><h4 id="机器数"><a href="#机器数" class="headerlink" title="机器数"></a>机器数</h4><p>一个数在计算机中的二进制表示形式,  叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1.</p>
<p>比如，十进制中的数 +3 ，计算机字长为8位，转换成二进制就是00000011。如果是 -3 ，就是 10000011 。</p>
<p>那么，这里的 00000011 和 10000011 就是机器数。</p>
<h4 id="真值"><a href="#真值" class="headerlink" title="真值"></a>真值</h4><p>因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位1代表负，其真正数值是 -3 而不是形式值131（10000011转换成十进制等于131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。</p>
<p>例：0000 0001的真值 = +000 0001 = +1，1000 0001的真值 = –000 0001 = –1</p>
<hr>
<h3 id="原码-反码-补码"><a href="#原码-反码-补码" class="headerlink" title="原码, 反码, 补码"></a>原码, 反码, 补码</h3><ul>
<li>计算机要使用一定的编码方式进行存储. 原码, 反码, 补码是机器存储一个具体数字的编码方式.</li>
</ul>
<h4 id="原码"><a href="#原码" class="headerlink" title="原码"></a>原码</h4><ul>
<li>原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:</li>
</ul>
<blockquote>
<p>[+1]原 = 0000 0001<br>[-1]原 = 1000 0001</p>
</blockquote>
<ul>
<li>第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是:</li>
</ul>
<blockquote>
<p>[1111 1111 , 0111 1111]<br>[-127 , 127]</p>
</blockquote>
<ul>
<li>原码是人脑最容易理解和计算的表示方式.</li>
</ul>
<h4 id="反码"><a href="#反码" class="headerlink" title="反码"></a>反码</h4><ul>
<li>反码的表示方法是:<ul>
<li>正数的反码是其本身</li>
<li>负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.</li>
</ul>
</li>
</ul>
<blockquote>
<p>[+1] = [00000001]原 = [00000001]反<br>[-1] = [10000001]原 = [11111110]反</p>
</blockquote>
<ul>
<li>可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算.</li>
</ul>
<h4 id="补码"><a href="#补码" class="headerlink" title="补码"></a>补码</h4><ul>
<li>补码的表示方法是:<ul>
<li>正数的补码就是其本身</li>
<li>负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)</li>
</ul>
</li>
</ul>
<blockquote>
<p>[+1] = [00000001]原 = [00000001]反 = [00000001]补<br>[-1] = [10000001]原 = [11111110]反 = [11111111]补</p>
</blockquote>
<ul>
<li>对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值.</li>
</ul>
<hr>
<h3 id="Python中特殊加法运算负数非负的问题"><a href="#Python中特殊加法运算负数非负的问题" class="headerlink" title="Python中特殊加法运算负数非负的问题"></a>Python中特殊加法运算负数非负的问题</h3><h4 id="原始题目"><a href="#原始题目" class="headerlink" title="原始题目"></a>原始题目</h4><ul>
<li>剑指Offer：不用加减乘除对两个整数做加法<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def Add(num1, num2):</span><br><span class="line">	mask = 0xFFFFFFFF</span><br><span class="line">	max_value = 0x7FFFFFFF</span><br><span class="line">	while num2:</span><br><span class="line">		num1, num2 = (num1^num2)&amp;mask, (num1&amp;num2)&lt;&lt;1</span><br><span class="line">	return num1 if num1 &lt;= max_value else ~(num1^mask)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="num1-mask-的理解"><a href="#num1-mask-的理解" class="headerlink" title="~(num1^mask)的理解"></a>~(num1^mask)的理解</h5><ul>
<li>此处实际上是假设num1是一个32位的整型数(在C++和Java中的定义为int，也就是默认32位整型数)，Python中由于无法定义整型位数，也就无法识别正常的负数<ul>
<li>num1为0x8FFFFFFF在C++和Java中为32位整型数时，实际上是个负数</li>
<li>Python无法识别到这个负数，因为Python的数值存储长度很长，强制转换也不行</li>
<li>如果我们可以<strong>将num1的后32位不变，前面的所有位置取反(由0变成1)</strong>，那么就可以得到Python中的负数值</li>
<li>此时如果先用<code>num1^mask</code>即可得到一个后面32位取反，前面其他位置全部保留正常(实际上都还是0)<ul>
<li>原因是mask后32位是1，所以num1每一位为0时异或结果为1，为1时异或结果为0，整体上就等价于对后32位取反</li>
</ul>
</li>
<li>在后面的位置取反后再对所有位置全部取反，那么就可以实现对num1的后32位不变，前面所有位置取反的操作得到Python中的负数的补码</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构——堆排序</title>
    <url>/Notes/Others/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E5%A0%86%E6%8E%92%E5%BA%8F.html</url>
    <content><![CDATA[<p><em>本文介绍堆排序的思想和代码</em></p>
<ul>
<li>参考链接：<a href="https://blog.csdn.net/qq_65518575/article/details/136925513" target="_blank" rel="noopener">堆排序【超详细+代码】</a></li>
</ul>
<hr>
<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><ul>
<li>生成最大堆(heapify)：从倒数第一个非叶节点开始，按照下虑操作找到该节点在最大堆中的位置，逐步遍历每一个非叶结点执行上述操作，生成最大堆</li>
<li>交换最大值到末尾：交换根节点与最后一个元素的值，树节点减少1（刚才根节点是当前树的最大值，更换以后就是有序的元素了，不再参与最大堆调整）</li>
<li>重新生成最大堆（调整根节点）：调整新换上来的根节点（下虑操作），找到其应该在的地方，重新生成最大堆</li>
<li>循环交换根节点和调整根节点这两个步骤，直到最大堆只剩下一个元素</li>
</ul>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 堆排序实现，对长度为n的数组a进行排序</span><br><span class="line">void HeapSort(int* a, int n) </span><br><span class="line">&#123;</span><br><span class="line">    // 生成堆 heapify</span><br><span class="line">	for (int i = n/2-1; i &gt;= 0; --i) // 从n/2-1开始，找到最大的非叶节点</span><br><span class="line">	&#123;</span><br><span class="line">		AdjustDown(a, n, i); // 将第i个节点按照下虑操作放置到它在最大堆中应该在的地方，注意本函数执行完后，当前节点i的所有子节点都小于等于节点i，也就是说，在最终生成的最大堆中，节点i的位置不会再更靠后了，详情见后面函数的实现</span><br><span class="line">	&#125;</span><br><span class="line">	int end = n - 1; // 用于表示数组尾部结点的下标 数值表示此前数字个数</span><br><span class="line">	while (end &gt; 0)</span><br><span class="line">	&#123;</span><br><span class="line">		Swap(&amp;a[0], &amp;a[end]); // 将根部数据排在最后 达到升序的效果</span><br><span class="line">		AdjustDown(a, end, 0);</span><br><span class="line">		--end; // 将从根部换下的值不当作堆中数据 对堆重复以上操作</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 最大堆的下虑操作实现：将 数组a的前size个数 所表达的最大堆的 parent节点 通过下虑操作找到其应在的位置</span><br><span class="line">void AdjustDown(int* a, int size, int parent)//向下调整（O(logn)）</span><br><span class="line">&#123;</span><br><span class="line">	// 假设法，假设最大孩子节点是左节点</span><br><span class="line">	int child = parent * 2 + 1;</span><br><span class="line">	while (child &lt; size)</span><br><span class="line">	&#123;</span><br><span class="line">		// 大堆中 向下调整 找大的孩子节点</span><br><span class="line">		if (child + 1 &lt; size &amp;&amp; a[child] &lt; a[child + 1]) // 假设错误，右孩子大于左孩子</span><br><span class="line">		&#123;</span><br><span class="line">			child++; // 更新为右孩子</span><br><span class="line">		&#125;</span><br><span class="line">		if (a[child] &gt; a[parent]) // 如果不符合最大堆规则，将父节点与最大的孩子节点进行交换</span><br><span class="line">		&#123;</span><br><span class="line">			Swap(&amp;a[child], &amp;a[parent]); // 交换完成</span><br><span class="line">			// 更新父节点和子节点</span><br><span class="line">			parent = child; // 此时的父节点变为孩子节点所在位置（还是入参parent对应的那个数字），需要继续进行下一轮最大堆验证</span><br><span class="line">			child = parent * 2 + 1; // 依然假设法，假设最大孩子节点是左节点</span><br><span class="line">		&#125;</span><br><span class="line">		else // 如果满足最大堆规则，终止</span><br><span class="line">		&#123;</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>U盘——做系统盘后恢复空间</title>
    <url>/Notes/Others/U%E7%9B%98%E2%80%94%E2%80%94%E5%81%9A%E7%B3%BB%E7%BB%9F%E7%9B%98%E5%90%8E%E6%81%A2%E5%A4%8D%E7%A9%BA%E9%97%B4.html</url>
    <content><![CDATA[<p><em>U盘做系统盘后空间会减少，本文给出恢复U盘空间的方法</em><br><strong>U盘做系统时会自动隐藏部分空间，用于放置启动文件，这些被隐藏的空间用普通的格式化方法无法恢复</strong></p>
<hr>
<h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><ul>
<li><p>清除U盘： cmd输入<code>diskpart</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># show all disks</span><br><span class="line">list disk</span><br><span class="line"># select target disk(our usb flash disk)</span><br><span class="line">select disk=n </span><br><span class="line"># show all disks again</span><br><span class="line"># make sure we have chosen the right disk</span><br><span class="line"># there will be a star in choosed disk</span><br><span class="line">list disk</span><br><span class="line"># clean the disk</span><br><span class="line">clean</span><br></pre></td></tr></table></figure>
</li>
<li><p>用任意方式格式化U盘</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>系统安装——Win10-U盘启动盘制作</title>
    <url>/Notes/Others/%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E2%80%94%E2%80%94Win10-U%E7%9B%98%E5%90%AF%E5%8A%A8%E7%9B%98%E5%88%B6%E4%BD%9C.html</url>
    <content><![CDATA[<p><em>由于Windows10过大，install.win文件超过4GB，常规的U盘启动盘制作都是面向FAT分区方式的，该方式无法存储超过4GB的文件，本文给出一种解决方案</em></p>
<hr>
<h3 id="Windows10-U盘启动盘制作方法："><a href="#Windows10-U盘启动盘制作方法：" class="headerlink" title="Windows10 U盘启动盘制作方法："></a>Windows10 U盘启动盘制作方法：</h3><ul>
<li>打开UltraISO制作镜像（注意不勾选create boot partition选项，否则U盘中会隐藏分区，看不到文件）</li>
<li>使用convert H:/fs:ntfs（H为U盘盘符，ntfs可支持4GB以上的文件）</li>
<li>解压ISO文件并替换U盘中的./sources/install.win文件（因为该文件大于4GB，FAT分区格式无法写入）</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy——reshape函数</title>
    <url>/Notes/Python/Numpy/Numpy%E2%80%94%E2%80%94reshape%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<hr>
<h3 id="维度确定"><a href="#维度确定" class="headerlink" title="维度确定"></a>维度确定</h3><p><strong><em>规则为从前到后，参数由外到内，由行到列</em></strong></p>
<ul>
<li>values.reshape(2,3) 表示二行三列(2,3)</li>
<li>values.reshape(4,2,3) 表示四个(2,3),四个两行三列</li>
<li>values.reshape(5,4,2,3) 表示五个(4,2,3)</li>
</ul>
<hr>
<h3 id="reshape本质理解"><a href="#reshape本质理解" class="headerlink" title="reshape本质理解"></a>reshape本质理解</h3><p><strong><em>实际上numpy多维数组变化维度时，内存数据没有变化，只是有一个shape参数指明维度即可</em></strong></p>
<ul>
<li>每次变化维度的过程可以理解为先将维度还原到(参考：<strong><em>维度确定</em></strong>)一维数组，然后再转化成制定的目标维度</li>
<li>返回的是新对象，但是新老对象数据共享，只有shape等内部属性不一样，所以reshape操作不浪费内存，也不耗费时间，但是需要注意数据共享可能造成的误操作<ul>
<li>测试说明： 使用DataFrame.values.reshape()生成的新对象内存数据不共享</li>
</ul>
</li>
</ul>
<hr>
<h3 id="特殊参数-“-1“"><a href="#特殊参数-“-1“" class="headerlink" title="特殊参数 “-1“"></a>特殊参数 “<code>-1</code>“</h3><p><strong><em>用于智能补齐某一维度，只能有一个参数为<code>-1</code></em></strong></p>
<ul>
<li>除了<code>-1</code>参数外，其余的部分一定要能够被整除</li>
<li><code>-1</code>本身占用一个维度</li>
<li>假设一维数组维度为12<ul>
<li>(-1, 2) &lt;==&gt; (6, 2) &lt;==&gt; (6, -1)</li>
<li>(-1) &lt;==&gt; (12)</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy——transpose函数</title>
    <url>/Notes/Python/Numpy/Numpy%E2%80%94%E2%80%94transpose%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<p><em>用于矩阵维度的转换</em></p>
<hr>
<h3 id="transpose函数的使用"><a href="#transpose函数的使用" class="headerlink" title="transpose函数的使用"></a>transpose函数的使用</h3><ul>
<li><p>用法说明</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nums.transpose(dims)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>nums</code>为<code>np.ndarray</code>类型的对象</li>
<li><code>dims</code>为表示维度排列的<code>tuple</code><ul>
<li>对于二维矩阵,只有<code>0, 1</code>两个维度,所以只能是<code>(0,1), (1,0)</code></li>
<li>三维矩阵,有<code>0,1,2</code>三个维度,所以可以是是<code>(0,1,2)</code>的所有全排列,共6个</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="简单易懂的解释"><a href="#简单易懂的解释" class="headerlink" title="简单易懂的解释"></a>简单易懂的解释</h4><ul>
<li>只改变维度, 每一维度对应的数据不会变, 比如图片的<code>shape</code>为: <code>(28, 28, 3)</code>,那么无论怎么变化维度, 变换后的数据中维度大小为<code>3</code>的那个维度都是表示通道, <code>(28,28)</code>总是表示图片的每个通道的数据</li>
<li>原始<code>numpy.ndarray</code>,<code>shape</code>为<code>(2,3,4)</code><ul>
<li><code>.transpose((0,1,2))</code>: <code>shape</code>为<code>(2,3,4)</code>, 不变</li>
<li><code>.transpose((1,0,2))</code>: <code>shape</code>为<code>(3,2,4)</code></li>
<li><code>.transpose((2,1,0))</code>: <code>shape</code>为<code>(4,3,2)</code></li>
</ul>
</li>
<li>测试代码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">origin = np.random.random_integers(1, 100, (2,3,4))</span><br><span class="line">print(&quot;-&quot;*20 + &quot;origin&quot;)</span><br><span class="line">print(origin)</span><br><span class="line">print(&quot;-&quot;*20 + &quot;(0,1,2)&quot;)</span><br><span class="line">print(origin.transpose((0,1,2)))</span><br><span class="line">print(&quot;-&quot;*20 + &quot;(1,0,2)&quot;)</span><br><span class="line">print(origin.transpose((1,0,2)))</span><br><span class="line">print(&quot;-&quot;*20 + &quot;(2,1,0)&quot;)</span><br><span class="line">print(origin.transpose((2,1,0)))</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">--------------------origin</span><br><span class="line">[[[50 36 80 53]</span><br><span class="line">  [45 45 94 91]</span><br><span class="line">  [49 29 69 53]]</span><br><span class="line"></span><br><span class="line"> [[85 83 61 18]</span><br><span class="line">  [16 89 80 60]</span><br><span class="line">  [99 13 36 40]]]</span><br><span class="line">--------------------(0,1,2)</span><br><span class="line">[[[50 36 80 53]</span><br><span class="line">  [45 45 94 91]</span><br><span class="line">  [49 29 69 53]]</span><br><span class="line"></span><br><span class="line"> [[85 83 61 18]</span><br><span class="line">  [16 89 80 60]</span><br><span class="line">  [99 13 36 40]]]</span><br><span class="line">--------------------(1,0,2)</span><br><span class="line">[[[50 36 80 53]</span><br><span class="line">  [85 83 61 18]]</span><br><span class="line"></span><br><span class="line"> [[45 45 94 91]</span><br><span class="line">  [16 89 80 60]]</span><br><span class="line"></span><br><span class="line"> [[49 29 69 53]</span><br><span class="line">  [99 13 36 40]]]</span><br><span class="line">--------------------(2,1,0)</span><br><span class="line">[[[50 85]</span><br><span class="line">  [45 16]</span><br><span class="line">  [49 99]]</span><br><span class="line"></span><br><span class="line"> [[36 83]</span><br><span class="line">  [45 89]</span><br><span class="line">  [29 13]]</span><br><span class="line"></span><br><span class="line"> [[80 61]</span><br><span class="line">  [94 80]</span><br><span class="line">  [69 36]]</span><br><span class="line"></span><br><span class="line"> [[53 18]</span><br><span class="line">  [91 60]</span><br><span class="line">  [53 40]]]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="与reshape的区别"><a href="#与reshape的区别" class="headerlink" title="与reshape的区别"></a>与reshape的区别</h4><h5 id="reshape的用法"><a href="#reshape的用法" class="headerlink" title="reshape的用法:"></a><code>reshape</code>的用法:</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nums.reshape(shape)</span><br></pre></td></tr></table></figure>

<pre><code>* `nums`为`np.ndarray`类型的对象
* `shape`为表示维度排列的`tuple`
    * `shape`必须与原始`nums.shape`兼容,即每个`shape`元素相乘的结果相等</code></pre>
<ul>
<li>从<code>numpy</code>中矩阵的存储开始讲起,<code>numpy</code>存储数据是C++的方式存储为一行的,然后分为不同<code>shape</code>以不同方式进行索引</li>
<li><code>reshape</code>相当于是把原始数据先还原成一行(实际上没操作),然后再转变成指定<code>shape</code>的数据,本质上数据存储没变化,只是<code>shape</code>变了,以后索引方式也就变了</li>
</ul>
<h5 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h5><ul>
<li><code>reshape</code>修改的只是<code>shape</code>,每个维度的意义变了</li>
<li><code>transpose</code>修改的只是维度,同时<code>shape</code>跟着变化而已,每个维度的顺序变了,但是意义不会变</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>Sklearn——CountVectorizer使用介绍</title>
    <url>/Notes/ML/Sklearn/Futures/Sklearn%E2%80%94%E2%80%94CountVectorizer%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D.html</url>
    <content><![CDATA[<hr>
<h3 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 新建一个CountVectorizer对象，以下简称CV对象</span><br><span class="line">vectoerizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words, token_pattern=r&quot;(?u)\b[\w-][\w-]+\b&quot;)</span><br><span class="line"></span><br><span class="line"># 使用文本集合(一个文本的列表)训练当前CountVectorizer对象</span><br><span class="line"># texts = [&quot;this is a text&quot;, &quot;this is another text&quot;]</span><br><span class="line">vectoerizer.fit(texts)</span><br><span class="line"></span><br><span class="line"># 训练后可以读取当前CV对象信息</span><br><span class="line">bag_of_words = vectoerizer.get_feature_names()</span><br><span class="line"></span><br><span class="line"># 使用CV对象生成任意文本集合的信息，返回对象是文档到字典的数组统计(统计单词数量)</span><br><span class="line"># 这里的texts不必要是之前使用过的，可以是新的文本集合，但是词典已经确定，不能再拓展了，不存在字典中的词直接忽略</span><br><span class="line">X = vectoerizer.transform(texts)</span><br><span class="line"></span><br><span class="line"># 使用X对象，toarray()将返回 term/token counts 矩阵，可直接用于TfidfTransformer等对象的训练</span><br><span class="line">X.toarray()</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="重要参数说明"><a href="#重要参数说明" class="headerlink" title="重要参数说明"></a>重要参数说明</h3><ul>
<li><p><code>min_df</code>: 用于排除出现次数太少的terms</p>
<ul>
<li><code>min_df = 0.01</code> 意味着将忽略出现在少于%1的文档中的词</li>
<li><code>min_df = 5</code> 意味着将忽略只出现在5篇以下文档中的词，不包括5篇</li>
</ul>
</li>
<li><p><code>max_df</code>:</p>
<ul>
<li><code>max_df = 0.50</code> 意味着将忽略出现在多于%50的文档中的词</li>
<li><code>max_df = 25</code> 意味着将忽略出现在25篇以上文档中的词，不包括25篇</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——并发模型</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%B9%B6%E5%8F%91%E6%A8%A1%E5%9E%8B.html</url>
    <content><![CDATA[<p><em>本文介绍Python中的并发机制，并给出一种最简洁的Python并发库</em></p>
<hr>
<h3 id="使用协程-yield语句"><a href="#使用协程-yield语句" class="headerlink" title="使用协程(yield语句)"></a>使用协程(yield语句)</h3><ul>
<li>实现随时暂停和开始</li>
<li>完全串行的操作，无法实现时间上的并行，这里指的是不能同时进行某个操作</li>
<li>与Go语言的协程不同，Python的协程更像是一个“生成器”</li>
</ul>
<hr>
<h3 id="使用线程"><a href="#使用线程" class="headerlink" title="使用线程"></a>使用线程</h3><ul>
<li>参考threading模块实现自己的线程</li>
</ul>
<hr>
<h3 id="使用concurrent-futures"><a href="#使用concurrent-futures" class="headerlink" title="使用concurrent.futures"></a>使用concurrent.futures</h3><h4 id="实现线程池模型"><a href="#实现线程池模型" class="headerlink" title="实现线程池模型"></a>实现线程池模型</h4><ul>
<li><p>实现一般的线程池模型，代码如下，关键代码仅仅两行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import time</span><br><span class="line">from concurrent import futures</span><br><span class="line"></span><br><span class="line">def sleep_one_second(key):</span><br><span class="line">    time.sleep(1)</span><br><span class="line">    return &quot;[%s]Done&quot; % key</span><br><span class="line"></span><br><span class="line">ml = &quot;ABCDEFGHIJ&quot;</span><br><span class="line">with futures.ThreadPoolExecutor(10) as executor:</span><br><span class="line">    res = executor.map(sleep_one_second, ml)</span><br><span class="line"></span><br><span class="line">print([r for r in res])</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面的代码可以在一秒内执行完成，因为共有10个线程并发</p>
</li>
<li><p><strong>在实现爬虫程序时，如果需要爬取的某些数据是相对独立的，那么我们完全可以用线程池实现，而不用使用复杂的线程模块*</strong></p>
</li>
</ul>
<h4 id="实现进程池模型"><a href="#实现进程池模型" class="headerlink" title="实现进程池模型"></a>实现进程池模型</h4><ul>
<li>仅仅需要修改futures.ThreadPoolExecutor为futures.ProcessPoolExecutor即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import time</span><br><span class="line">from concurrent import futures</span><br><span class="line"></span><br><span class="line">def sleep_one_second(key):</span><br><span class="line">    time.sleep(1)</span><br><span class="line">    return &quot;[%s]Done&quot; % key</span><br><span class="line"></span><br><span class="line">ml = &quot;ABCDEFGHIJ&quot;</span><br><span class="line">with futures.ProcessPoolExecutor(10) as executor:</span><br><span class="line">    res = executor.map(sleep_one_second, ml)</span><br><span class="line"></span><br><span class="line">print([r for r in res])</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="进程与线程内存区别"><a href="#进程与线程内存区别" class="headerlink" title="进程与线程内存区别"></a>进程与线程内存区别</h4><h5 id="对全局变量的访问对比"><a href="#对全局变量的访问对比" class="headerlink" title="对全局变量的访问对比"></a>对全局变量的访问对比</h5><ul>
<li><p>线程:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from concurrent import futures</span><br><span class="line">global_list = []</span><br><span class="line"></span><br><span class="line">def test_futures(range_num):</span><br><span class="line">    global_list.append(range_num)</span><br><span class="line">    print global_list</span><br><span class="line">    return range_num</span><br><span class="line"></span><br><span class="line">with futures.ThreadPoolExecutor(8) as executor:</span><br><span class="line">    res = executor.map(test_futures, range(10))</span><br><span class="line"></span><br><span class="line">print &quot;the final global_list: %s&quot; % global_list</span><br></pre></td></tr></table></figure>
</li>
<li><ul>
<li>上面的代码输出如下:<blockquote>
<p>[0]<br>[0, [10, 2]<br>,  1[0, 1, 2, , 32]<br>,  3, [04, 1], 2, 3, 4<br>[0, , 5]<br>1, [0, 21, 2, [3, , 3, 044, , 51, , 6, , 75]<br>2, [0, 1, 63, 7, , 2, 3[40, , 8, , 4, 9, 155, , 6, 2]6, , 7<br>, 8, 7, 9, ]3<br>8, 4, , 59, 6, ]<br>7, 8, 9]<br>the final global_list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]<br>the results: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p>
</blockquote>
</li>
</ul>
</li>
<li><p>进程: </p>
</li>
</ul>
<pre><code>from concurrent import futures
global_list = []

def test_futures(range_num):
    global_list.append(range_num)
    print global_list
    return range_num

with futures.ProcessPoolExecutor(8) as executor:
    res = executor.map(test_futures, range(10))

print &quot;the final global_list: %s&quot; % global_list</code></pre>
<ul>
<li><ul>
<li>上面的代码输出如下:<blockquote>
<p>[0]<br>[1]<br>[2]<br>[3]<br>[0, 4]<br>[5]<br>[6]<br>[7]<br>[1, 8]<br>[2, 9]<br>the final global_list: []<br>the results: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p>
</blockquote>
</li>
</ul>
</li>
<li><p>原因分析</p>
<ul>
<li>线程之间共享地址空间,所以所有的线程线程访问同一个全局共享变量</li>
<li>进程之间不共享地址空间,所以不同进程访问不同共享变量</li>
<li>在程序中Process之间不共享地址空间,但是futures.ProcessPoolExecutor(max_workers)任务分配时受限与参数max_workers的影响,所以可以预估本地机器最多开启max_workers个进程,同一进程中地址空间共享,所以会有部分任务被分配给同一进程的不同线程,从而出现部分共享变量被不同任务访问到</li>
<li>总结:<ul>
<li>futures.ThreadPoolExecutor单进程多线程中全局变量共享</li>
<li>futures.ProcessPoolExecutor多进程多线程中每个进程内部的线程全局变量共享</li>
<li>不同进程之间即使时全局变量也不能共享</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Python中进程-VS-线程"><a href="#Python中进程-VS-线程" class="headerlink" title="Python中进程 VS 线程"></a>Python中进程 VS 线程</h3><ul>
<li>Python中由于全局解释器锁（GIL）的存在,同一进程中的所有线程使用同一个解释器对象,所以它们无法真正实现并行</li>
<li>所以在想要充分利用多核的时候,需要选择使用多进程</li>
<li>更多信息参考<a href="/Notes/Python/Python%E2%80%94%E2%80%94ProcessPoolExecutor%E5%92%8CThreadPoolExecutor.html" title="/Notes/Python/Python——ProcessPoolExecutor和ThreadPoolExecutor.html">Process和Thread分析</a></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Matplotlib——各种参数的一般含义总结</title>
    <url>/Notes/Python/Matplotlib/Matplotlib%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%80%E8%88%AC%E5%90%AB%E4%B9%89%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>Matplotlib中有很多画图函数，画图时又有很多参数，本文从一般性的角度说明每个参数的一般含义</em></p>
<hr>
<h3 id="loc"><a href="#loc" class="headerlink" title="loc"></a>loc</h3><ul>
<li>这个参数一般是指明图标的位置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL——数据库导入导出</title>
    <url>/Notes/MySQL/MySQL%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA.html</url>
    <content><![CDATA[<p><em>参考博客: <a href="https://blog.csdn.net/u013626215/article/details/88548342" target="_blank" rel="noopener">https://blog.csdn.net/u013626215/article/details/88548342</a></em></p>
<hr>
<h3 id="从MySQL中导出数据库信息"><a href="#从MySQL中导出数据库信息" class="headerlink" title="从MySQL中导出数据库信息"></a>从MySQL中导出数据库信息</h3><ul>
<li><p>导出所有表结构</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqldump -h [host_ip] -u [user_name] -p -d [db_name] &gt; [file_name]</span><br></pre></td></tr></table></figure>
</li>
<li><p>导出所有表结构和数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqldump -h [host_ip] -u [user_name] -p [db_name] &gt; [file_name]</span><br></pre></td></tr></table></figure>
</li>
<li><p>导出某一张表结构</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqldump -h [host_ip] -u [user_name] -p -d [db_name] [table_name] &gt; [file_name]</span><br></pre></td></tr></table></figure>
</li>
<li><p>导出某一张表结构和数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqldump -h [host_ip] -u [user_name] -p [db_name] [table_name] &gt; [file_name]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="导入数据库到MySQL"><a href="#导入数据库到MySQL" class="headerlink" title="导入数据库到MySQL"></a>导入数据库到MySQL</h3><ul>
<li><p>登录数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h [host_ip] -u [user_name] -p</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行导入命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source [file_name]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><em>注意:数据导入需要提前创建数据库</em></p>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——抽象类和继承与Java的不同点</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E6%8A%BD%E8%B1%A1%E7%B1%BB%E5%92%8C%E7%BB%A7%E6%89%BF%E4%B8%8EJava%E7%9A%84%E4%B8%8D%E5%90%8C%E7%82%B9.html</url>
    <content><![CDATA[<p><em>Python也开始支持抽象类和多态了，但是Python的继承与Java有很多不同的地方</em></p>
<hr>
<h3 id="Python只有私有属性"><a href="#Python只有私有属性" class="headerlink" title="Python只有私有属性"></a>Python只有私有属性</h3><p><em>(双下划线<code>__</code>属性)和公有属性(默认属性)，没有protected属性</em></p>
<ul>
<li>解决方案是Python提出命名时单下划线的属性为protected属性<ul>
<li>但是这只是一种口头约定，Python解释器并不做强制处理</li>
</ul>
</li>
<li>模块中的函数如果是单下划线开头的，那么该函数属于当前模块保护的<ul>
<li>此时某些IDE，比如Idea能够给出提示，但是用户仍可访问</li>
<li>编程时建议将模块中不被调用的文件定义为_开头的，这样可以提示自己和别人当前函数不会在模块外被调用</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Python类属性定义"><a href="#Python类属性定义" class="headerlink" title="Python类属性定义"></a>Python类属性定义</h3><p><em>属性定义在<code>__init__</code>函数中才能在对象中通过<code>__dict__</code>属性读取到</em></p>
<hr>
<h3 id="Python支持虚拟继承"><a href="#Python支持虚拟继承" class="headerlink" title="Python支持虚拟继承"></a>Python支持虚拟继承</h3><p><em>可以欺骗编译器，不用真正实现虚拟基类的所有接口</em></p>
<hr>
<h3 id="都是强类型语言"><a href="#都是强类型语言" class="headerlink" title="都是强类型语言"></a>都是强类型语言</h3><p><strong><em>这是他们的相同点，但我觉得写在这里是必要的</em></strong></p>
<hr>
<h3 id="方法类型不同"><a href="#方法类型不同" class="headerlink" title="方法类型不同"></a>方法类型不同</h3><ul>
<li><strong>Java</strong>中有<strong>一般方法</strong>(实例方法)，<strong>静态方法</strong>(<code>static</code>类名和对象均可直接调用)两种<ul>
<li>Java中这两种方法也都可以被<strong>继承</strong>和<strong>隐藏</strong>，而<strong>不能被重写</strong>(隐藏的意思时换成对应的父类指针还能访问到父类的静态属性和方法)</li>
</ul>
</li>
<li><strong>Python</strong>中有<strong>一般方法</strong>(实例方法)，<strong>静态方法</strong>(<code>@staticmethod</code>类名和对象均可直接调用)和<strong>类方法</strong><code>@classmethod</code><ul>
<li>对<code>@staticmethod</code>方法的理解是，该方法与类密切相关，但是不需要访问这个类</li>
<li>Python的<code>@staticmethod</code>和<code>@classmethod</code>方法调用方式一样，唯一的区别在于<code>@classmethod</code>方法第一个参数必须是当前类(一般命名为<code>cls</code>)</li>
<li>Python中的这<strong>三种方法都可以被继承和重写</strong></li>
</ul>
</li>
</ul>
<hr>
<h3 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h3><ul>
<li>三种函数, 普通函数(实例函数), 类函数(classmethod), 静态函数(staticmethod)</li>
</ul>
<h4 id="普通函数"><a href="#普通函数" class="headerlink" title="普通函数"></a>普通函数</h4><ul>
<li>无需任何装饰器</li>
<li>无论如何定义,函数的第一个参数都会被当成实例对象本身(一般用self)</li>
</ul>
<h4 id="类函数"><a href="#类函数" class="headerlink" title="类函数"></a>类函数</h4><ul>
<li>装饰器 @classmethod</li>
<li>无论如何定义,函数的第一个参数都会被当成实例对象本身(一般用cls)</li>
</ul>
<h4 id="静态函数"><a href="#静态函数" class="headerlink" title="静态函数"></a>静态函数</h4><ul>
<li>装饰器 @staticmethod</li>
<li>所有参数都是函数所有,不被占用</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——Copy-and-Deepcopy</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94Copy-and-Deepcopy.html</url>
    <content><![CDATA[<p><em>deepcopy与copy的本质区别——是否为可变对象创建新的对象(内存空间)</em><br><strong>以下图片截取自<a href="http://www.pythontutor.com/" title="http://www.pythontutor.com" target="_blank" rel="noopener">Python Tutorial</a></strong></p>
<hr>
<h3 id="代码片段一"><a href="#代码片段一" class="headerlink" title="代码片段一"></a>代码片段一</h3><ul>
<li>可变对象list中包含着可变list和不可变对象tuple, 且tuple中不包含可变对象<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line">l1 = [3, [66, 55, 44], (7, 8, 9)]</span><br><span class="line">l2 = list(l1) # &lt;==&gt; l2 = copy.copy(l1)</span><br><span class="line">			  #	&lt;==&gt; l2 = l1[:]</span><br><span class="line">l3 = copy.deepcopy(l1)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<img src="/Notes/Python/Python——Copy-and-Deepcopy/copy-and-deepcopy_normal.png" title="copy-and-deepcopy_normal">

<hr>
<h3 id="代码片段二"><a href="#代码片段二" class="headerlink" title="代码片段二"></a>代码片段二</h3><ul>
<li>可变对象list中包含着可变list和不可变对象tuple, 且tuple中包含可变对象list<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line">l1 = [3, [66, 55, 44], (7, 8, [1, 2])]</span><br><span class="line">l2 = list(l1) # &lt;==&gt; l2 = copy.copy(l1) </span><br><span class="line">			  #	&lt;==&gt; l2 = l1[:]</span><br><span class="line">l3 = copy.deepcopy(l1)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<img src="/Notes/Python/Python——Copy-and-Deepcopy/copy-and-deepcopy_list-tuple-list.png" title="copy-and-deepcopy_list-tuple-list">

<hr>
<h3 id="代码片段三"><a href="#代码片段三" class="headerlink" title="代码片段三"></a>代码片段三</h3><ul>
<li>不可变对象中包含可变对象</li>
<li><strong>值得强调的是，copy复制tuple时，不会像list那样直接创建新对象，无论tuple中是否有可变对象*</strong></li>
<li><strong>除非包含不可变对象，否则deepcopy复制tuple时也不会创建新对象*</strong><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line">t1 = (3, [66, 55, 44], (7, 8, [1, 2]))</span><br><span class="line">t2 = t1</span><br><span class="line">t3 = tuple(t1) # &lt;==&gt; t4 = t1[:]</span><br><span class="line">			   # t5 = copy.copy(t1)</span><br><span class="line">t6 = copy.deepcopy(t1)</span><br><span class="line">t1[1].append(100)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<img src="/Notes/Python/Python——Copy-and-Deepcopy/copy-and-deepcopy_tuple-list.png" title="copy-and-deepcopy_tuple-list">

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>对于不可变对象</p>
<ul>
<li>deepcopy与copy操作一样，都不为创建新对象，而是直接引用</li>
<li>在后面如果有修改该不可变对象的操作时再创建新对象，此时两个版本的不可变对象地址变得不同</li>
<li>这里是Python的常态，比如<code>tuple(tuple1)</code>将返回一个tuple1对象的引用而不是副本，当修改tuple1时才会创建新对象</li>
<li>因为无论如何，修改不可变对象的不可变部分都不会修改原始对象，所以为了节约内存，Python解释器完全可以将创建新对象保留到修改内容时</li>
</ul>
</li>
<li><p>对于可变对象</p>
<ul>
<li>deepcopy将创建新对象</li>
<li>copy只复制对象地址</li>
<li>后面修改该可变对象时地址不变，而是直接修改内容</li>
</ul>
</li>
</ul>
<p><strong><em>总之</em></strong><br><strong><em>deepcopy的目的就是确保，无论如何修改，原始对象和副本之间都完全独立</em></strong><br><strong><em>所以</em></strong><br><strong><em>只要是可能被修改的对象都要复制！！！</em></strong><br><strong><em>不可变对象在复制时会自动新建</em></strong><br><strong><em>故而deepcopy时不可变对象不用创建新对象</em></strong></p>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——Python中没有字符类型</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94Python%E4%B8%AD%E6%B2%A1%E6%9C%89%E5%AD%97%E7%AC%A6%E7%B1%BB%E5%9E%8B.html</url>
    <content><![CDATA[<p><em>不同于C++和Java等语言,Python中没有字符char类型,只有字符串类型</em></p>
<hr>
<h3 id="Python中的数据类型"><a href="#Python中的数据类型" class="headerlink" title="Python中的数据类型"></a>Python中的数据类型</h3><h4 id="标准数据类型"><a href="#标准数据类型" class="headerlink" title="标准数据类型"></a>标准数据类型</h4><h5 id="Numbers（数字）"><a href="#Numbers（数字）" class="headerlink" title="Numbers（数字）"></a>Numbers（数字）</h5><h5 id="String（字符串）"><a href="#String（字符串）" class="headerlink" title="String（字符串）"></a>String（字符串）</h5><h5 id="List（列表）"><a href="#List（列表）" class="headerlink" title="List（列表）"></a>List（列表）</h5><h5 id="Tuple（元组）"><a href="#Tuple（元组）" class="headerlink" title="Tuple（元组）"></a>Tuple（元组）</h5><h5 id="Dictionary（字典）"><a href="#Dictionary（字典）" class="headerlink" title="Dictionary（字典）"></a>Dictionary（字典）</h5><h4 id="关于字符类型的操作"><a href="#关于字符类型的操作" class="headerlink" title="关于字符类型的操作"></a>关于字符类型的操作</h4><ul>
<li>把长度为1的字符串当成字符来操作,比如函数ord(s)中只要s的长度为1(len(s) == 1)即可,否则ord函数抛出异常<ul>
<li>长度为1的字符串本质上还是一个字符串类型<code>&lt;type str&gt;</code></li>
</ul>
</li>
<li>判断字符串某一位置的字符时直接比较即可,如<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s = &quot;12345&quot;</span><br><span class="line">if s[2] == &apos;2&apos;:</span><br><span class="line">	print s[3]</span><br><span class="line"># 等价于</span><br><span class="line">if s[2] == &quot;2&quot;:</span><br><span class="line">	print s[3]</span><br><span class="line"># &quot;2&quot;和&apos;2&apos;都是&lt;type str&gt;类型的</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——判断未知源的编码类型</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%88%A4%E6%96%AD%E6%9C%AA%E7%9F%A5%E6%BA%90%E7%9A%84%E7%BC%96%E7%A0%81%E7%B1%BB%E5%9E%8B.html</url>
    <content><![CDATA[<p><em>有时候遇到一个文件，而我们并不知道它是什么编码方式编码的,本文给出了一些判断未知文件编码方式的方法</em></p>
<hr>
<h3 id="使用chardet包"><a href="#使用chardet包" class="headerlink" title="使用chardet包"></a>使用<a href="https://pypi.org/project/chardet/" title="https://pypi.org/project/chardet/" target="_blank" rel="noopener">chardet包</a></h3><hr>
<h3 id="在程序中判断"><a href="#在程序中判断" class="headerlink" title="在程序中判断"></a>在程序中判断</h3><ul>
<li><p>安装Chardet包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install chardet</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用Chardet包做判断</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import urllib</span><br><span class="line">rawdata = urllib.urlopen(&apos;http://yahoo.co.jp/&apos;).read()</span><br><span class="line">import chardet</span><br><span class="line">print chardet.detect(rawdata)</span><br><span class="line"># Output:</span><br><span class="line">&#123;&apos;confidence&apos;: 0.99, &apos;language&apos;: &apos;&apos;, &apos;encoding&apos;: &apos;utf-8&apos;&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>更多高级使用方法可参考<a href="https://chardet.readthedocs.io/en/latest/usage.html#basic-usage" title="https://chardet.readthedocs.io/en/latest/usage.html#basic-usage" target="_blank" rel="noopener">chardet文档</a></strong></p>
<hr>
<h3 id="直接使用命令判断"><a href="#直接使用命令判断" class="headerlink" title="直接使用命令判断"></a>直接使用命令判断</h3><ul>
<li><p>安装chardetect工具</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install chardet</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用chardetect命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 检测test.txt文件的编码方式</span><br><span class="line">chardetect test.txt</span><br><span class="line"># Output：</span><br><span class="line">test-chardetect.txt: ascii with confidence 1.0</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu——ElasticSearch安装与配置(logstash)</title>
    <url>/Notes/Linux/Ubuntu%E2%80%94%E2%80%94ElasticSearch%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html</url>
    <content><![CDATA[<p><em>ElasticSearch的安装与基本配置</em></p>
<hr>
<h3 id="安装ElasticSearch"><a href="#安装ElasticSearch" class="headerlink" title="安装ElasticSearch"></a>安装ElasticSearch</h3><ul>
<li>下载<a href="https://www.elastic.co/downloads/elasticsearch" title="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="noopener">ElasticSearch</a> deb安装包</li>
<li>安装ElasticSearch<ul>
<li>Ubuntu中默认安装路径为<code>/usr/share/elasticsearch/</code></li>
</ul>
</li>
<li>配置ES为一个服务<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo /bin/systemctl daemon-reload</span><br><span class="line">sudo /bin/systemctl enable elasticsearch.service</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="启动ElasticSearch"><a href="#启动ElasticSearch" class="headerlink" title="启动ElasticSearch"></a>启动ElasticSearch</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl start elasticsearch.service</span><br></pre></td></tr></table></figure>

<h4 id="启动相关问题"><a href="#启动相关问题" class="headerlink" title="启动相关问题"></a>启动相关问题</h4><ul>
<li>设置远程访问：<ul>
<li>修改<code>config/elasticsearch.yml</code>中<code>network.host : 0.0.0.0</code></li>
</ul>
</li>
<li>可能遇到的问题1：</li>
</ul>
<blockquote>
<p>[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] </p>
</blockquote>
<ul>
<li>解决方案：<a href="https://www.cnblogs.com/chenjiangbin/p/12060899.html" target="_blank" rel="noopener">https://www.cnblogs.com/chenjiangbin/p/12060899.html</a></li>
</ul>
<ul>
<li>可能遇到的问题2：</li>
</ul>
<blockquote>
<p>[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]</p>
</blockquote>
<ul>
<li>解决方案：<a href="https://blog.csdn.net/python36/article/details/84257343" target="_blank" rel="noopener">https://blog.csdn.net/python36/article/details/84257343</a></li>
</ul>
<ul>
<li>可能遇到的问题3：</li>
</ul>
<blockquote>
<p>[1]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured</p>
</blockquote>
<ul>
<li>解决方案：<a href="https://www.cnblogs.com/hellxz/p/11057234.html" target="_blank" rel="noopener">https://www.cnblogs.com/hellxz/p/11057234.html</a></li>
</ul>
<hr>
<h3 id="关闭ElasticSearch"><a href="#关闭ElasticSearch" class="headerlink" title="关闭ElasticSearch"></a>关闭ElasticSearch</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl stop elasticsearch.service</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="ElasticSearch的使用"><a href="#ElasticSearch的使用" class="headerlink" title="ElasticSearch的使用"></a>ElasticSearch的使用</h3><p><strong><em>ES的使用可从<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_cluster_health.html" title="https://www.elastic.co/guide/en/elasticsearch/reference/current/_cluster_health.html" target="_blank" rel="noopener">官网</a>上查看，点击下一步接着可看完整个流程</em></strong><br><strong>*操作ES接口时，建议使用postman *</strong></p>
<h4 id="查看索引"><a href="#查看索引" class="headerlink" title="查看索引"></a>查看索引</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /_cat/indices?v</span><br></pre></td></tr></table></figure>

<blockquote>
<p>health status index uuid pri rep docs.count docs.deleted store.size pri.store.size</p>
</blockquote>
<h4 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个名字为customer的Index</span><br><span class="line">PUT /customer?pretty</span><br></pre></td></tr></table></figure>

<h4 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h4><p><em>给Index添加Document数据</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向customer Index中添加id为1的Document</span><br><span class="line"># 内容为&#123;&quot;name&quot;: &quot;John Doe&quot;&#125;</span><br><span class="line">PUT /customer/_doc/1?pretty</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;John Doe&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><em>如果是使用Postman，这里选择json类型即可</em></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;customer&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; : 1,</span><br><span class="line">  &quot;result&quot; : &quot;created&quot;,</span><br><span class="line">  &quot;_shards&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 2,</span><br><span class="line">    &quot;successful&quot; : 1,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_seq_no&quot; : 0,</span><br><span class="line">  &quot;_primary_term&quot; : 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 从customer中查询id为1的数据</span><br><span class="line">GET /customer/_doc/1?pretty</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;customer&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; : 1,</span><br><span class="line">  &quot;found&quot; : true,</span><br><span class="line">  &quot;_source&quot; : &#123; &quot;name&quot;: &quot;John Doe&quot; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除名为customer的索引</span><br><span class="line">DELETE /customer?pretty</span><br></pre></td></tr></table></figure>

<h4 id="修改数据"><a href="#修改数据" class="headerlink" title="修改数据"></a>修改数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 向customer Index中添加id为1的Document</span><br><span class="line"># 如果目标Document已经存在，则修改目标Document为指定的数据</span><br><span class="line"># 内容为&#123;&quot;name&quot;: &quot;John Doe&quot;&#125;</span><br><span class="line">PUT /customer/_doc/1?pretty</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Joe Doe&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="使用logstash同步数据"><a href="#使用logstash同步数据" class="headerlink" title="使用logstash同步数据"></a>使用logstash同步数据</h3><p><em>6.2.4同步mysql数据到ES</em></p>
<h4 id="安装logstash"><a href="#安装logstash" class="headerlink" title="安装logstash"></a>安装logstash</h4><ul>
<li>下载<a href="https://www.elastic.co/downloads/past-releases/logstash-6-2-4" title="https://www.elastic.co/downloads/past-releases/logstash-6-2-4" target="_blank" rel="noopener">logstash 6.2.4</a><ul>
<li>为了方便配置管理，建议下载zip或者tar.gz版本</li>
</ul>
</li>
<li>解压到指定文件夹，建议在相关项目下创建<code>ElasticSearch</code>文件夹，并存储以下数据<ul>
<li>logstash 解压文件夹<code>logstash-6-2-4</code></li>
<li>新建文件: logstash更新 mysql 数据库索引到 ES 时的配置文件，一般命名为mysql.config <em>[后面会给出详细内容]</em></li>
<li>下载jdbc库： <a href="https://downloads.mysql.com/archives/c-j/" title="https://downloads.mysql.com/archives/c-j/" target="_blank" rel="noopener">mysql-connector-java.jar</a> <em>[用于连接数据库]</em><ul>
<li>这里下载Platform Independent的压缩包版本解压即可找到需要的jar包</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="配置文件及jdbc连接库"><a href="#配置文件及jdbc连接库" class="headerlink" title="配置文件及jdbc连接库"></a>配置文件及jdbc连接库</h4><ul>
<li><p>新建一个名为mysql.config的文件，内容为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># logstash-6.2.2\bin\logstash.bat -f mysql.config</span><br><span class="line"># logstash-6.2.4/bin/logstash -f mysql.config</span><br><span class="line">input &#123;</span><br><span class="line">    jdbc &#123;</span><br><span class="line">      # mysql jdbc connection string to our backup databse</span><br><span class="line">      jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/docker_manager?serverTimezone=UTC&quot;</span><br><span class="line">      # the user we wish to excute our statement as</span><br><span class="line">      jdbc_user =&gt; &quot;docker&quot;</span><br><span class="line">      jdbc_password =&gt; &quot;123456&quot;</span><br><span class="line">      # the path to our downloaded jdbc driver</span><br><span class="line">      jdbc_driver_library =&gt; &quot;/home/jiahong/Workspace/IdeaProjects/DockerManagerSystem/elastic-search/mysql-connector-java-8.0.11.jar&quot;</span><br><span class="line">      # the name of the driver class for mysql</span><br><span class="line">      jdbc_driver_class =&gt; &quot;Java::com.mysql.jdbc.Driver&quot;</span><br><span class="line">      # jdbc_paging_enabled =&gt; &quot;true&quot;</span><br><span class="line">      # jdbc_page_size =&gt; &quot;50000&quot;</span><br><span class="line">      # statement_filepath =&gt; &quot;jdbc.sql&quot;</span><br><span class="line">      schedule =&gt; &quot;* * * * *&quot;</span><br><span class="line">      # type =&gt; &quot;jdbc&quot;</span><br><span class="line">	  statement =&gt; &quot;SELECT * FROM docker_manager.DockerManager_docker WHERE id &gt; :sql_last_value&quot;</span><br><span class="line">	  use_column_value =&gt; true</span><br><span class="line">	  tracking_column =&gt; &quot;id&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">        hosts =&gt; &quot;127.0.0.1:9200&quot;</span><br><span class="line">        index =&gt; &quot;docker&quot;</span><br><span class="line">        document_id =&gt; &quot;%&#123;id&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>关于mysql.config，一般需要配置的地方为：    <ul>
<li>jdbc_connection_string: 注意docker_manager为数据库名，后面的时区参数有时也需要修改，关于时区的问题可我的博客<br><a href="/Notes/Linux/Linux%E2%80%94%E2%80%94Logstash%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98.html">Linux——Logstash时区问题</a> </li>
<li>jdbc_driver_library: 这里需指定到对应的jdbc连接库</li>
<li>statement: 数据库查询语句</li>
<li>jdbc_user: 数据库用户名l</li>
<li>jdbc_password: MySQL用户名对应的密码</li>
<li>hosts: 输出到ES地址</li>
<li>index: ES服务器的Index(相当于MySQL中的数据库)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="利用logstash同步mysql数据库数据到ES"><a href="#利用logstash同步mysql数据库数据到ES" class="headerlink" title="利用logstash同步mysql数据库数据到ES"></a>利用logstash同步mysql数据库数据到ES</h4><ul>
<li>启动同步操作<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 进入之前创建用于解压logstash源文件的目录下</span><br><span class="line">cd ElasticSearch</span><br><span class="line"># 启动同步</span><br><span class="line">./logstash-6.2.4/bin/logstash -f mysql.config</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong><em>一旦同步开始，如果不关闭进程，那么将一直自动同步，建议同步完成(当输出不再变化)后关闭进程</em></strong></p>
<ul>
<li>查看ES中相应的Index是否已经被更新</li>
</ul>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu——屏幕亮度调节</title>
    <url>/Notes/Linux/Ubuntu%E2%80%94%E2%80%94%E5%B1%8F%E5%B9%95%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82.html</url>
    <content><![CDATA[<p><em>Ubuntu屏幕亮度调节</em></p>
<hr>
<h3 id="Flux"><a href="#Flux" class="headerlink" title="Flux"></a>Flux</h3><ul>
<li>Flux是一款非常好用的屏幕调整软件，不仅可以调整屏幕亮度，还能调整色调等</li>
<li>但是，许多Ubuntu版本下，Flux都不能使用，还会影响<code>apt-get update</code>命令，一般情况下，都会将其从表中删除<ul>
<li>如果Flux可以使用，建议优先使用Flux</li>
</ul>
</li>
</ul>
<h3 id="xrandr调整屏幕亮度"><a href="#xrandr调整屏幕亮度" class="headerlink" title="xrandr调整屏幕亮度"></a>xrandr调整屏幕亮度</h3><ul>
<li><p>展示已有屏幕</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xrandr -q</span><br></pre></td></tr></table></figure>

<ul>
<li>或者可以使用更简洁的版本<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xrandr -q | grep &quot; connected&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>调整指定屏幕的亮度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xrandr --output HDMI-0 --brightness 0.5</span><br></pre></td></tr></table></figure>

<ul>
<li>其中<code>HDMI-0</code>是屏幕名称，<code>0.5</code>是亮度参数，大于1时屏幕会发白，注意不要调整的太亮</li>
</ul>
</li>
<li><p>说明：<code>xrandr</code>修改的屏幕亮度在重启电脑后不会保存，如果想要每次开机自动调整，可以在开机启动添加脚本</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL——Ubuntu18.04安装及配置</title>
    <url>/Notes/MySQL/MySQL%E2%80%94%E2%80%94Ubuntu18.04%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE.html</url>
    <content><![CDATA[<p><em>解决Ubuntu18.04安装Ubuntu后普通用户没有权限登录问题</em></p>
<hr>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装mysql服务</span><br><span class="line">sudo apt-get install mysql-server</span><br><span class="line"># 安装客户端</span><br><span class="line">sudo apt install mysql-client</span><br><span class="line"># 安装依赖</span><br><span class="line">sudo apt install libmysqlclient-dev</span><br><span class="line"># 检查状态</span><br><span class="line">sudo netstat -tap | grep mysql</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="远程链接设置"><a href="#远程链接设置" class="headerlink" title="远程链接设置"></a>远程链接设置</h3><h4 id="默认远程链接问题"><a href="#默认远程链接问题" class="headerlink" title="默认远程链接问题"></a>默认远程链接问题</h4><p><em>Ubuntu18.04 安装mysql后，普通用户没有连接mysql数据库权限(local或者remote均没有权限)</em></p>
<ul>
<li><code>mysql -u root</code>或者直接用使用MySQL Workbench连接均失败，显示如下错误<blockquote>
<p>ERROR 1045: Access denied for user: ‘root@localhost’ (Using password: YES)</p>
</blockquote>
</li>
<li>使用<code>sudo mysql -u root</code>能正确连接</li>
</ul>
<h4 id="打开远程链接数据"><a href="#打开远程链接数据" class="headerlink" title="打开远程链接数据"></a>打开远程链接数据</h4><p><em>(删除之前的root并重新创建账户)</em></p>
<ul>
<li><p>登录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo mysql -u root</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后查看当前用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select User,Host from mysql.user;</span><br><span class="line"># Output:</span><br><span class="line">+------------------+-----------+</span><br><span class="line">| User       | Host   |</span><br><span class="line">+------------------+-----------+</span><br><span class="line">| admin      | localhost |</span><br><span class="line">| debian-sys-maint | localhost |</span><br><span class="line">| magento_user   | localhost |</span><br><span class="line">| mysql.sys    | localhost |</span><br><span class="line">| root       | localhost |</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除root账号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">drop user &apos;root&apos;@&apos;localhost&apos;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建新的root账号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create user &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>为新账号授权</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
</li>
<li><p>退出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exit;</span><br></pre></td></tr></table></figure>
</li>
<li><p>以普通用户身份登录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——反编译(Disassemble)与字节码(Bytecode)</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%8F%8D%E7%BC%96%E8%AF%91%E4%B8%8E%E5%AD%97%E8%8A%82%E7%A0%81.html</url>
    <content><![CDATA[<p><em>为了知道Python代码底层都做了哪些操作，我们常常需要反编译Python代码以获得Python的字节码</em><br><strong>我们可以获得: classes, methods, functions, or code 的字节码</strong></p>
<hr>
<h3 id="获取字节码的方法"><a href="#获取字节码的方法" class="headerlink" title="获取字节码的方法"></a>获取字节码的方法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 比较`[]`和`list()`两者的不同</span><br><span class="line">from dis import dis</span><br><span class="line"></span><br><span class="line"># test case 1</span><br><span class="line">dis(&quot;[]&quot;)</span><br><span class="line"># Output:</span><br><span class="line">  1           0 BUILD_LIST               0</span><br><span class="line">              2 RETURN_VALUE</span><br><span class="line"></span><br><span class="line"># test case 2</span><br><span class="line">dis(&quot;list()&quot;)</span><br><span class="line"># Output:</span><br><span class="line">  1           0 LOAD_NAME                0 (list)</span><br><span class="line">              2 CALL_FUNCTION            0</span><br><span class="line">              4 RETURN_VALUE</span><br><span class="line"></span><br><span class="line">**由上述输出可知，`list()` 比 `[]` 会多执行一行字节码`LODA_NAME`**</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——命名规范</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83.html</url>
    <content><![CDATA[<p><em>Python命名规范(Name Convention)</em><br>Reference: <a href="https://blog.csdn.net/real_myth/article/details/68927665" target="_blank" rel="noopener">https://blog.csdn.net/real_myth/article/details/68927665</a></p>
<hr>
<h3 id="命名："><a href="#命名：" class="headerlink" title="命名："></a>命名：</h3><p>module_name, package_name, ClassName, method_name, ExceptionName, function_name, GLOBAL_VAR_NAME, instance_var_name, function_parameter_name, local_var_name.</p>
<hr>
<h3 id="应该避免的名称"><a href="#应该避免的名称" class="headerlink" title="应该避免的名称"></a>应该避免的名称</h3><blockquote>
<ul>
<li>单字符名称, 除了计数器和迭代器.</li>
</ul>
</blockquote>
<ul>
<li>包/模块名中的连字符(-)</li>
<li>双下划线开头并结尾的名称(Python保留, 例如__init__)</li>
</ul>
<hr>
<h3 id="命名约定"><a href="#命名约定" class="headerlink" title="命名约定"></a>命名约定</h3><blockquote>
<ul>
<li>所谓”内部(Internal)”表示仅模块内可用, 或者, 在类内是保护或私有的.</li>
</ul>
</blockquote>
<ul>
<li>用单下划线(_)开头表示模块变量或函数是protected的(使用import * from时不会包含).</li>
<li>用双下划线(__)开头的实例变量或方法表示类内私有.</li>
<li>将相关的类和顶级函数放在同一个模块里. 不像Java, 没必要限制一个类一个模块.</li>
<li>对类名使用大写字母开头的单词(如CapWords, 即Pascal风格), 但是模块名应该用小写加下划线的方式(如lower_with_under.py). 尽管已经有很多现存的模块使用类似于CapWords.py这样的命名, 但现在已经不鼓励这样做, 因为如果模块名碰巧和类名一致, 这会让人困扰.</li>
</ul>
<hr>
<h3 id="引号"><a href="#引号" class="headerlink" title="引号"></a>引号</h3><blockquote>
<ul>
<li>自然语言使用双引号（想表达人为意思的，比如log，错误，提示等）</li>
</ul>
</blockquote>
<ul>
<li>机器标识使用单引号（比如dict的key等）</li>
<li>正则表达式使用原生的双引号 r”…”</li>
<li>docstring使用三双引号</li>
</ul>
<hr>
<h3 id="Python之父Guido推荐的规范"><a href="#Python之父Guido推荐的规范" class="headerlink" title="Python之父Guido推荐的规范"></a>Python之父Guido推荐的规范</h3><table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Public</th>
<th align="left">Internal</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Modules</td>
<td align="left">lower_with_under</td>
<td align="left">_lower_with_under</td>
</tr>
<tr>
<td align="left">Packages</td>
<td align="left">lower_with_under</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Classes</td>
<td align="left">CapWords</td>
<td align="left">_CapWords</td>
</tr>
<tr>
<td align="left">Exceptions</td>
<td align="left">CapWords</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Functions</td>
<td align="left">lower_with_under()</td>
<td align="left">_lower_with_under()</td>
</tr>
<tr>
<td align="left">Global/Class Constants</td>
<td align="left">CAPS_WITH_UNDER</td>
<td align="left">_CAPS_WITH_UNDER</td>
</tr>
<tr>
<td align="left">gobal/Class Variables</td>
<td align="left">lower_with_under</td>
<td align="left">_lower_with_under</td>
</tr>
<tr>
<td align="left">Instance Variables</td>
<td align="left">lower_with_under</td>
<td align="left">_lower_with_under (protected) or __lower_with_under (private)</td>
</tr>
<tr>
<td align="left">Method Names</td>
<td align="left">lower_with_under()</td>
<td align="left">_lower_with_under() (protected) or __lower_with_under() (private)</td>
</tr>
<tr>
<td align="left">Function/Method Parameters</td>
<td align="left">lower_with_under</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Local Variables</td>
<td align="left">lower_with_under</td>
<td align="left"></td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——np.nan, None的判断和比较</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94np.nan,%20None%E7%9A%84%E5%88%A4%E6%96%AD%E5%92%8C%E6%AF%94%E8%BE%83.html</url>
    <content><![CDATA[<p><em>Python值的判断与比较： np.nan, None</em></p>
<hr>
<h3 id="None"><a href="#None" class="headerlink" title="None"></a>None</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type(None)</span><br><span class="line"># Output: &lt;type &apos;NoneType&apos;&gt;</span><br><span class="line"></span><br><span class="line">None is None</span><br><span class="line"># Output: True</span><br><span class="line"></span><br><span class="line">None == None</span><br><span class="line"># Output: True</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="np-nan"><a href="#np-nan" class="headerlink" title="np.nan"></a>np.nan</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type(np.nan)</span><br><span class="line"># Output: &lt;type &apos;float&apos;&gt;</span><br><span class="line"></span><br><span class="line">np.nan == np.nan</span><br><span class="line"># Output: False </span><br><span class="line"></span><br><span class="line">np.nan is np.nan</span><br><span class="line"># Output: True </span><br><span class="line"></span><br><span class="line">np.nan != np.nan</span><br><span class="line"># Output: True </span><br><span class="line"></span><br><span class="line">np.nan &gt; np.nan</span><br><span class="line"># Output: False</span><br><span class="line"></span><br><span class="line">np.nan &lt; np.nan</span><br><span class="line"># Output: False</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="np-nan-与-None"><a href="#np-nan-与-None" class="headerlink" title="np.nan 与 None"></a>np.nan 与 None</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">None == np.nan</span><br><span class="line"># Output: False </span><br><span class="line"></span><br><span class="line">None != np.nan</span><br><span class="line"># Output: True</span><br><span class="line"></span><br><span class="line">None is np.nan</span><br><span class="line"># Output: False</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="与数字的比较"><a href="#与数字的比较" class="headerlink" title="与数字的比较"></a>与数字的比较</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.nan &gt; 10</span><br><span class="line"># Output: False </span><br><span class="line"></span><br><span class="line">np.nan &lt; 10</span><br><span class="line"># Output: False </span><br><span class="line"></span><br><span class="line">np.nan == 10</span><br><span class="line"># Output: False</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>【Python2和Python3表现相同】<br>  <strong><em>np.nan</em></strong> 只有在<code>np.nan != np.nan</code>或者<code>np.nan is np.nan</code>时为<strong><em>True</em></strong>， 其他情况下和数字比较(<strong><em>包括和自身</em></strong>)都为<strong><em>False</em></strong></li>
<li>【Python2和Python3表现相异】<br>  Python3与Python2在直接使用<strong><em>np.nan</em></strong>时表现正常,但是当涉及到<strong><em>DataFrame</em></strong>的<strong><em>NaN</em></strong>时表现不同</li>
</ul>
<hr>
<h3 id="特殊情况"><a href="#特殊情况" class="headerlink" title="特殊情况"></a>特殊情况</h3><p><strong><em><code>DataFrame</code>中的<code>NaN</code>与数字比较时会出现有时候为<code>True</code>有时候为<code>False</code>的情况</em></strong></p>
<ul>
<li>这种情况出现在Python3中，当<strong><em>NaN</em></strong> 与数字比较时<ul>
<li>此时对于列属性类型为数值型，那么返回<strong><em>False</em></strong></li>
<li>否则返回<strong><em>True</em></strong></li>
</ul>
</li>
<li>Python2中<strong><em>NaN</em></strong>和数字的就是<strong><em>np.nan</em></strong>和数字比较的结果，都为<strong><em>False</em></strong></li>
</ul>
<h4 id="Python2与Python3比较"><a href="#Python2与Python3比较" class="headerlink" title="Python2与Python3比较"></a>Python2与Python3比较</h4><ul>
<li>代码示例：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Python3：</span><br><span class="line">import pandas as pd</span><br><span class="line">df = pd.DataFrame([[&apos;a&apos;,2,3], [&apos;a&apos;,3,4], [&apos;a&apos;,8,9]], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</span><br><span class="line">df = df.reindex([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])</span><br><span class="line">print(df)</span><br><span class="line"># Output: </span><br><span class="line">	     0    1    2</span><br><span class="line">	a    a  2.0  3.0</span><br><span class="line">	b    a  3.0  4.0</span><br><span class="line">	c    a  8.0  9.0</span><br><span class="line">	d  NaN  NaN  NaN</span><br><span class="line">	e  NaN  NaN  NaN</span><br><span class="line">	f  NaN  NaN  NaN</span><br><span class="line"></span><br><span class="line">print(df &gt; 5)</span><br><span class="line"># Output: </span><br><span class="line">	      0      1      2</span><br><span class="line">	a  True  False  False</span><br><span class="line">	b  True  False  False</span><br><span class="line">	c  True   True   True</span><br><span class="line">	d  True  False  False</span><br><span class="line">	e  True  False  False</span><br><span class="line">	f  True  False  False</span><br><span class="line"></span><br><span class="line">print(df.values)</span><br><span class="line"># Output:</span><br><span class="line">	[[&apos;a&apos; 2.0 3.0]</span><br><span class="line">	 [&apos;a&apos; 3.0 4.0]</span><br><span class="line">	 [&apos;a&apos; 8.0 9.0]</span><br><span class="line">	 [nan nan nan]</span><br><span class="line">	 [nan nan nan]</span><br><span class="line">	 [nan nan nan]]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Python2：</span><br><span class="line">import pandas as pd</span><br><span class="line">df = pd.DataFrame([[&apos;a&apos;,2,3], [&apos;a&apos;,3,4], [&apos;a&apos;,8,9]], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</span><br><span class="line">df = df.reindex([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])</span><br><span class="line">print(df)</span><br><span class="line"># Output: </span><br><span class="line">	     0    1    2</span><br><span class="line">	a    a  2.0  3.0</span><br><span class="line">	b    a  3.0  4.0</span><br><span class="line">	c    a  8.0  9.0</span><br><span class="line">	d  NaN  NaN  NaN</span><br><span class="line">	e  NaN  NaN  NaN</span><br><span class="line">	f  NaN  NaN  NaN</span><br><span class="line"></span><br><span class="line">print(df &gt; 5)</span><br><span class="line"># Output: </span><br><span class="line">	       0      1      2</span><br><span class="line">	a   True  False  False</span><br><span class="line">	b   True  False  False</span><br><span class="line">	c   True   True   True</span><br><span class="line">	d  False  False  False</span><br><span class="line">	e  False  False  False</span><br><span class="line">	f  False  False  False</span><br><span class="line"></span><br><span class="line">print(df.values)</span><br><span class="line"># Output: </span><br><span class="line">	[[&apos;a&apos; 2.0 3.0]</span><br><span class="line">	 [&apos;a&apos; 3.0 4.0]</span><br><span class="line">	 [&apos;a&apos; 8.0 9.0]</span><br><span class="line">	 [nan nan nan]</span><br><span class="line">	 [nan nan nan]</span><br><span class="line">	 [nan nan nan]]</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——位运算与逻辑运算和C++有什么不同</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E4%BD%8D%E8%BF%90%E7%AE%97%E4%B8%8E%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E5%92%8CC++%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C.html</url>
    <content><![CDATA[<p>参考链接: <a href="https://blog.csdn.net/weixin_39129504/article/details/85958295" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39129504/article/details/85958295</a></p>
<hr>
<h3 id="位运算-与C-相同"><a href="#位运算-与C-相同" class="headerlink" title="位运算(与C++相同)"></a>位运算(与C++相同)</h3><h4 id="按位与-amp"><a href="#按位与-amp" class="headerlink" title="按位与 &amp;"></a>按位与 <code>&amp;</code></h4><h4 id="按位或"><a href="#按位或" class="headerlink" title="按位或 |"></a>按位或 <code>|</code></h4><h4 id="按位异或"><a href="#按位异或" class="headerlink" title="按位异或 ^"></a>按位异或 <code>^</code></h4><h4 id="按位取反"><a href="#按位取反" class="headerlink" title="按位取反 ~"></a>按位取反 <code>~</code></h4><h4 id="移位运算-gt-gt-lt-lt-lt-lt-和-gt-gt"><a href="#移位运算-gt-gt-lt-lt-lt-lt-和-gt-gt" class="headerlink" title="移位运算 &gt;&gt;,&lt;&lt;,&lt;&lt;=和&gt;&gt;="></a>移位运算 <code>&gt;&gt;</code>,<code>&lt;&lt;</code>,<code>&lt;&lt;=</code>和<code>&gt;&gt;=</code></h4><hr>
<h3 id="逻辑运算-与C-不同"><a href="#逻辑运算-与C-不同" class="headerlink" title="逻辑运算(与C++不同)"></a>逻辑运算(与C++不同)</h3><hr>
<h3 id="逻辑与"><a href="#逻辑与" class="headerlink" title="逻辑与"></a>逻辑与</h3><ul>
<li>Python: <code>and</code></li>
<li>C++: <code>&amp;&amp;</code></li>
</ul>
<hr>
<h3 id="逻辑或"><a href="#逻辑或" class="headerlink" title="逻辑或"></a>逻辑或</h3><ul>
<li>Python: <code>or</code></li>
<li>C++: <code>||</code></li>
</ul>
<hr>
<h3 id="逻辑非"><a href="#逻辑非" class="headerlink" title="逻辑非"></a>逻辑非</h3><ul>
<li>Python: <code>not</code></li>
<li>C++: <code>!</code></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——函数返回值是copy还是引用</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E8%BF%94%E5%9B%9E%E5%80%BC%E6%98%AFcopy%E8%BF%98%E6%98%AF%E5%BC%95%E7%94%A8.html</url>
    <content><![CDATA[<p><em>Python中有些函数是直接操作当前对象的，有些函数是操作副本的</em></p>
<hr>
<h3 id="操作当前对象的"><a href="#操作当前对象的" class="headerlink" title="操作当前对象的"></a>操作当前对象的</h3><ul>
<li>list.sort()， 返回空</li>
<li>random.shuffle(my_list)， 返回空</li>
<li>func(inplace=True), 这里Pandas库中的其他方法几乎均适用</li>
</ul>
<hr>
<h3 id="返回copy的"><a href="#返回copy的" class="headerlink" title="返回copy的"></a>返回copy的</h3><ul>
<li>除了numpy的reshape()外目前默认都为</li>
<li>sorted(), 内置函数，返回新对象，不论接受什么参数返回的都是列表</li>
<li>np.ndarray.reshape() <strong>返回新对象，但是新对象除了shape属性外，数据属性是和原始对象共享的</strong><ul>
<li>np.ndarray存储着数据和一个shape属性</li>
<li>我们可通过修改shape属性而不是创建新对象来修改当前对象的shape <code>object.shape = 3,4</code></li>
<li>使用reshape时可以理解为创建了一个新对象，但是共享了数据，两个ndarray对象有相同的数据引用</li>
<li><strong><em>numpy包没有array类，只有ndarray类，array是一个函数，用于构造ndarray,也可以用ndarray函数构建，但是不推荐，测试ndarray函数发现用法很奇怪</em></strong></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——标准输入input函数</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E6%A0%87%E5%87%86%E8%BE%93%E5%85%A5input%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<p><em>本文简单讲解input和raw_input函数的用法,两者都是内置函数,无需用户自己导入模块</em></p>
<hr>
<h3 id="input函数"><a href="#input函数" class="headerlink" title="input函数"></a>input函数</h3><ul>
<li><p>用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input[[prompt]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>实例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    a = input(&quot;input: &quot;)</span><br><span class="line">    print type(a), a</span><br></pre></td></tr></table></figure>
</li>
<li><p>交互结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input: 123</span><br><span class="line">&lt;type &apos;int&apos;&gt; 123</span><br><span class="line">input: &quot;abc&quot;</span><br><span class="line">&lt;type &apos;str&apos;&gt; abc</span><br><span class="line">input: 1.0</span><br><span class="line">&lt;type &apos;float&apos;&gt; 1.0</span><br><span class="line">input: abc</span><br><span class="line">SyntaxError: unexpected EOF while parsing</span><br></pre></td></tr></table></figure>
</li>
<li><p>总结:</p>
<ul>
<li>当输入为整数时,识别为int和long类型</li>
<li>当输入为小数时,识别为float等类型</li>
<li>当输入为string(两边需要添加<code>&quot;</code>)时,识别为str类型</li>
<li>当输入为未知,像是string但是没被<code>&quot;</code>引用起来时,抛出语法错误异常</li>
</ul>
</li>
</ul>
<hr>
<h3 id="raw-input函数"><a href="#raw-input函数" class="headerlink" title="raw_input函数"></a>raw_input函数</h3><ul>
<li><p>用法与inputx完全相同</p>
</li>
<li><p>把所有的输入都当做字符串,注意,如果输入的字符串带有<code>&quot;</code>,那么<code>&quot;</code>会被保留在字符串内部</p>
</li>
<li><p>用法实例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while True:</span><br><span class="line">    a = raw_input(&quot;input: &quot;)</span><br><span class="line">    print type(a), a</span><br></pre></td></tr></table></figure>
</li>
<li><p>交互结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input: 123</span><br><span class="line">&lt;type &apos;str&apos;&gt; 123</span><br><span class="line">input: &quot;abc&quot;</span><br><span class="line">&lt;type &apos;str&apos;&gt; &quot;abc&quot;</span><br><span class="line">input: 1.0</span><br><span class="line">&lt;type &apos;str&apos;&gt; 1.0</span><br><span class="line">input: abc</span><br><span class="line">&lt;type &apos;str&apos;&gt; abc</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——编程笔记，各种易忘点总结</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%8C%E5%90%84%E7%A7%8D%E6%98%93%E5%BF%98%E7%82%B9%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>Python编程笔记，各种易忘点总结</em><br><strong><em>持续更新</em></strong></p>
<hr>
<h3 id="快速排序和归并排序参数不可使用list子列表"><a href="#快速排序和归并排序参数不可使用list子列表" class="headerlink" title="快速排序和归并排序参数不可使用list子列表"></a>快速排序和归并排序参数不可使用list子列表</h3><ul>
<li><strong>注意使用子列表时是一个新对象,操作子列表与原始list无关</strong></li>
<li>在快速排序和归并排序中不可将子列表传入,以期待可以从函数中修改原始列表的值</li>
</ul>
<hr>
<h3 id="list初始化"><a href="#list初始化" class="headerlink" title="list初始化"></a>list初始化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [0] * 10</span><br><span class="line">l = [0 for _ in range(10)]</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="list-count函数的应用"><a href="#list-count函数的应用" class="headerlink" title="list.count函数的应用"></a>list.count函数的应用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1, 3, 2, 3, 3, 3]</span><br><span class="line">print l.count(3)</span><br><span class="line"># output:</span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="str是一个不可变对象"><a href="#str是一个不可变对象" class="headerlink" title="str是一个不可变对象"></a>str是一个不可变对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s = &quot;12345&quot;</span><br><span class="line"># OK</span><br><span class="line">print s[1]</span><br><span class="line">print s[2:4]</span><br><span class="line"># Error</span><br><span class="line">s[1] = 10</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Random的应用"><a href="#Random的应用" class="headerlink" title="Random的应用"></a>Random的应用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line">print random.randint(start, end)</span><br></pre></td></tr></table></figure>

<ul>
<li>输出一个[start, end]之间(包括start和end)的随机数</li>
</ul>
<hr>
<h3 id="sorted函数不修改原始数组"><a href="#sorted函数不修改原始数组" class="headerlink" title="sorted函数不修改原始数组"></a>sorted函数不修改原始数组</h3><ul>
<li>sorted函数不修改原始数组</li>
<li>a.sort()会修改原始数组<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1,3,4,2]</span><br><span class="line">l1 = sorted(l)</span><br><span class="line">print l, l1</span><br><span class="line">l.sort()</span><br><span class="line">print l</span><br><span class="line"># output</span><br><span class="line">[1,3,4,2] [1,2,3,4]</span><br><span class="line">[1,2,3,4]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="sorted参数cmp和key比较"><a href="#sorted参数cmp和key比较" class="headerlink" title="sorted参数cmp和key比较"></a>sorted参数cmp和key比较</h3><ul>
<li>key是个单参数函数,返回值为一个可用于比较的值即可</li>
<li>cmp是个双参数函数,返回值为<strong>-1, 1, 0,分别表示小于,大于,等于</strong><ul>
<li>特别注意<strong>不是返回True和False</strong></li>
</ul>
</li>
<li>二者均可作为排序的比较函数</li>
</ul>
<hr>
<h3 id="Python数值类型自动转换"><a href="#Python数值类型自动转换" class="headerlink" title="Python数值类型自动转换"></a>Python数值类型自动转换</h3><ul>
<li>强制类型转换: int(a)</li>
<li>隐式转换<ul>
<li>boolean 型转int型: True =1 False =0</li>
<li>自动类型提升: int型转float型</li>
<li>注意: <strong>两个int型的除法不会保留小数</strong>,这点与C++一致</li>
</ul>
</li>
</ul>
<hr>
<h3 id="关于bool"><a href="#关于bool" class="headerlink" title="关于bool"></a>关于<strong>bool</strong></h3><ul>
<li><strong>if</strong>判断语句中，实际上时调用<code>bool(object)</code></li>
<li><code>bool(object)</code>调用的时<code>object.__bool__()</code></li>
<li>如果一个对象没有实现<code>__bool__</code>方法，那么会尝试调用<code>__len__</code>方法<ul>
<li>返回为0时表示<strong>False</strong></li>
<li>否则返回<strong>True</strong></li>
</ul>
</li>
</ul>
<hr>
<h3 id="对象ID"><a href="#对象ID" class="headerlink" title="对象ID"></a>对象ID</h3><ul>
<li>Python中对象的<strong>ID</strong>类似于其他语言中对象的地址</li>
<li>调用方法为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">id(object)</span><br><span class="line"># Output: 4332312578</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="运算符号的内部实现"><a href="#运算符号的内部实现" class="headerlink" title="运算符号的内部实现"></a>运算符号的内部实现</h3><ul>
<li>+ 和 +=<ul>
<li><strong>+</strong>: <strong>add</strong>()</li>
<li><strong>+=</strong>: <strong>iadd</strong>()<ul>
<li>当没有<strong>iadd</strong>()时Python解释器会调用<strong>add</strong>()</li>
</ul>
</li>
</ul>
</li>
<li>* 和 *=<ul>
<li><strong>*</strong>: <strong>mul</strong>()</li>
<li><strong>*=</strong>: <strong>imul</strong>()<ul>
<li>当没有<strong>imul</strong>()时Python解释器会调用和<strong>mul</strong>()</li>
</ul>
</li>
</ul>
</li>
<li><strong>不可变变量，比如tuple也可以调用<code>*=</code>和<code>+=</code>，表现也是一样的，只是对象id会改变，等价于调用了<code>__add__()</code>然后又赋值给当前变量*</strong><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tu1 = (1,2,3)</span><br><span class="line">tu2 = (2,3,4)</span><br><span class="line">tu1 += tu2 # &lt;==&gt; tu1 = tu1 + tu2, id of tu1 will change</span><br><span class="line"># Output: (1,2,3,2,3,4)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="尽量避免使用最小整数"><a href="#尽量避免使用最小整数" class="headerlink" title="尽量避免使用最小整数"></a>尽量避免使用最小整数</h3><ul>
<li>需要初始化一个最小值,然后方便求得某个序列的最大值,此时可以初始化为某个可能的值,从而避免寻找最小整数的尴尬,可能会找错,初始化错的话很容易造成后面结果都错</li>
</ul>
<hr>
<h3 id="参数”key“"><a href="#参数”key“" class="headerlink" title="参数”key“"></a>参数”<code>key</code>“</h3><p><em>一些需要比较功能的函数都会有此参数</em></p>
<ul>
<li>key参数是一个函数，这个函数接受一个唯一的对象，然后返回用于比较的值，外层函数比较时会使用key函数返回值进行比较<ul>
<li>比如可用与字符窜长度<code>key = len</code>排序，忽略大小写排序<code>key = str.lower</code>比较等功能</li>
</ul>
</li>
<li>可用于<strong>list.sort(), sorted(), min(), max()</strong>等函数</li>
<li>另外一些其他标准库也会接受这个参数，用法相似</li>
</ul>
<hr>
<h3 id="Foreach局部变量"><a href="#Foreach局部变量" class="headerlink" title="Foreach局部变量"></a>Foreach局部变量</h3><ul>
<li>Python for循环语句中的“局部”变量与Java中的不同<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Python</span><br><span class="line">for i in range(0, 10):</span><br><span class="line">	pass</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// Java</span><br><span class="line">for(int i=1; i &lt; 10; i++)</span><br></pre></td></tr></table></figure>

<ul>
<li>上面的代码执行完之后i的值为多少？<ul>
<li>Java中<code>i</code>是局部变量，所以在代码执行完成后变量<code>i</code>是不能访问的</li>
<li>Python中<code>i</code>是全局变量，所以i的值为最后一次迭代的值<code>9</code></li>
<li>Java中要实现与Python相同的效果，可以使用全局变量(将i的定义放到for循环外面即可)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="函数内部定义函数时注意"><a href="#函数内部定义函数时注意" class="headerlink" title="函数内部定义函数时注意"></a>函数内部定义函数时注意</h3><ul>
<li>注意内部函数是否访问到Inner外的变量</li>
<li>如果某个函数Otter只被访问一次且另一个函数Inner只被Otter访问,那么Inner一般定义在Otter内部比较合适</li>
</ul>
<hr>
<h3 id="正则表达式匹配完整字符串"><a href="#正则表达式匹配完整字符串" class="headerlink" title="正则表达式匹配完整字符串"></a>正则表达式匹配完整字符串</h3><ul>
<li>必须使用^和$, 否则部分匹配也会返回结果<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line">def totally_match(pattern, string):</span><br><span class="line">	if re.match(pattern, string) is not None:</span><br><span class="line">		return True</span><br><span class="line">totally_match(r&quot;^cat$&quot;, &quot;cat&quot;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="函数定义后再定义全局变量"><a href="#函数定义后再定义全局变量" class="headerlink" title="函数定义后再定义全局变量"></a>函数定义后再定义全局变量</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def visit():</span><br><span class="line">	print global_variable</span><br><span class="line">global_variable = &quot;testing&quot;</span><br><span class="line">print visit()</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Python无穷大的数"><a href="#Python无穷大的数" class="headerlink" title="Python无穷大的数"></a>Python无穷大的数</h3><h4 id="常用的是无穷大的实数"><a href="#常用的是无穷大的实数" class="headerlink" title="常用的是无穷大的实数:"></a>常用的是无穷大的实数:</h4><ul>
<li>正无穷: float(‘inf’)</li>
<li>负无穷: float(‘-inf’)</li>
</ul>
<h4 id="运算"><a href="#运算" class="headerlink" title="运算:"></a>运算:</h4><ul>
<li><p>Python里面的无穷大与C++不同,C++里面是定义一个最大的整数实现,Python里面可以视为一个无穷大的对象</p>
</li>
<li><p>和数学分析里面一样,我们可以和无穷大做计算,加上无穷大还是无穷大</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x = float(&apos;inf&apos;)</span><br><span class="line">print x</span><br><span class="line">print x - 1</span><br><span class="line">print x + 1</span><br><span class="line">print x + x</span><br><span class="line">print x - x</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">inf</span><br><span class="line">inf</span><br><span class="line">inf</span><br><span class="line">inf</span><br><span class="line">nan</span><br></pre></td></tr></table></figure>

<ul>
<li>无穷大减去无穷大为一个未知结果nan</li>
<li>判断一个数是否为nan,<code>nan == nan</code>返回<code>False</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="复制一个普通列表时不要用copy模块"><a href="#复制一个普通列表时不要用copy模块" class="headerlink" title="复制一个普通列表时不要用copy模块"></a>复制一个普通列表时不要用copy模块</h3><ul>
<li><p>copy.deepcopy支持对可变对象的深度复制,直到解析到不可变对象为止</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line">list1 = [1,3,4,[5,6]]</span><br><span class="line">list2 = copy.deepcopy(list1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果list对象元素都是不可变对象,那么可以有简便实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list1 = [1,3,4,5,6]</span><br><span class="line">list2 = list1[:]</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line">list1 = [1, 3, 4, [5, 6]]</span><br><span class="line">l1 = list1[:]</span><br><span class="line">l2 = copy.deepcopy(list1)</span><br><span class="line">l3 = copy.copy(list1)</span><br><span class="line">l4 = list1</span><br><span class="line">list1.append(10)</span><br><span class="line">list1[3].append(7)</span><br><span class="line">print &quot;l[:]&quot;, l1</span><br><span class="line">print &quot;deepcopy&quot;, l2</span><br><span class="line">print &quot;copy&quot;, l3</span><br><span class="line">print &quot;l&quot;, l4</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">l[:] [1, 3, 4, [5, 6, 7]]</span><br><span class="line">deepcopy [1, 3, 4, [5, 6]]</span><br><span class="line">copy [1, 3, 4, [5, 6, 7]]</span><br><span class="line">l [1, 3, 4, [5, 6, 7], 10]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="not-quot-quot-返回的是False"><a href="#not-quot-quot-返回的是False" class="headerlink" title="not &quot; &quot;返回的是False"></a><code>not &quot; &quot;</code>返回的是<code>False</code></h3><ul>
<li>在编程时容易错误的以为空白就是没有,所以容易认为<code>not &quot; &quot;</code>是<code>True</code><ul>
<li><code>&quot; &quot;</code>不是什么都没有,而是有个space字符</li>
</ul>
</li>
<li>实际上只有空字符串,空列表和<code>None</code>等是空的,<code>not None</code>, <code>not []</code>, <code>not &#39;&#39;</code>等均为<code>True</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print not &quot; &quot;</span><br><span class="line">print not &quot;&quot;</span><br><span class="line">print not []</span><br><span class="line">print not None</span><br><span class="line">print not 0</span><br><span class="line">print not -1</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">False</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Python中32位最小和最大整形数"><a href="#Python中32位最小和最大整形数" class="headerlink" title="Python中32位最小和最大整形数"></a>Python中32位最小和最大整形数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># max</span><br><span class="line">max_int = 0x7FFFFFFF</span><br><span class="line"># min</span><br><span class="line">min_int = -0x80000000</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="使用abs-n-求n的绝对值"><a href="#使用abs-n-求n的绝对值" class="headerlink" title="使用abs(n)求n的绝对值"></a>使用abs(n)求n的绝对值</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print abs(-11)</span><br><span class="line"># output</span><br><span class="line">11</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="使用整除符号"><a href="#使用整除符号" class="headerlink" title="使用整除符号//"></a>使用整除符号//</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print 3.5 // 2</span><br><span class="line">print 3.5 / 2</span><br><span class="line">print 3 // 2</span><br><span class="line">print 3 / 2</span><br></pre></td></tr></table></figure>

<ul>
<li><code>//</code>是整除符号，只保留整数部分，但是结果的类型可能为整数，也可能为浮点数，具体取决于除法两边是否含有浮点数</li>
</ul>
<hr>
<h3 id="list中的子列表"><a href="#list中的子列表" class="headerlink" title="list中的子列表"></a>list中的子列表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1,2,3,4,5]</span><br><span class="line">print l[2:-1]</span><br><span class="line">print l[-1]</span><br><span class="line"># output</span><br><span class="line">[3, 4]</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<ul>
<li>list<code>l</code>中使用<code>-1</code>可以理解为<code>len(l)-1</code>,不管是字列表还是元素的索引操作</li>
</ul>
<hr>
<h3 id="sorted的返回值总是list"><a href="#sorted的返回值总是list" class="headerlink" title="sorted的返回值总是list"></a>sorted的返回值总是list</h3><ul>
<li>即使传入的是一个string,返回值也是list,需要牢记<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print sorted(&quot;1523&quot;)</span><br><span class="line"># output:</span><br><span class="line">[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;5&apos;]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Python的for循环语句结束时i的值与Java不同"><a href="#Python的for循环语句结束时i的值与Java不同" class="headerlink" title="Python的for循环语句结束时i的值与Java不同"></a>Python的for循环语句结束时i的值与Java不同</h3><ul>
<li><p>Python</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in range(0, 5):</span><br><span class="line">	print i</span><br><span class="line">print &quot;final:&quot;, i</span><br><span class="line"># output</span><br><span class="line">0 1 2 3 4</span><br><span class="line">final: 4</span><br></pre></td></tr></table></figure>
</li>
<li><p>Java/C++</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int i;</span><br><span class="line">for(i = 0; i &lt; 5; i++)&#123;</span><br><span class="line">	System.out.print(i);</span><br><span class="line">&#125;</span><br><span class="line">System.out.print(&quot;final:&quot;)</span><br><span class="line">System.out.print(i)</span><br><span class="line"># output</span><br><span class="line">0 1 2 3 4</span><br><span class="line">final: 4</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果想得到Java/C++的效果,可以使用while语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">i = 0</span><br><span class="line">while i &lt; 5:</span><br><span class="line">	print i</span><br><span class="line">	i+= 1</span><br><span class="line">print &quot;final:&quot;, i</span><br><span class="line"># output</span><br><span class="line">0 1 2 3 4</span><br><span class="line">final: 4</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="join函数调用的条件"><a href="#join函数调用的条件" class="headerlink" title="join函数调用的条件"></a>join函数调用的条件</h3><ul>
<li><p>特别注意: <strong>join函数的参数只能是字符串,不能是数字</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1,2,3,4]</span><br><span class="line">print &apos;&apos;.join(l)</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">TypeError: sequence item 0: expected string, int found</span><br></pre></td></tr></table></figure>

<ul>
<li>使用非string元素的列表时抛出TypeError的错误</li>
</ul>
</li>
</ul>
<hr>
<h3 id="dict-get"><a href="#dict-get" class="headerlink" title="dict.get()"></a>dict.get()</h3><ul>
<li><p>原始定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dict.get(key[,default=None])</span><br></pre></td></tr></table></figure>

<ul>
<li><code>default</code>参数可以指定默认值,当<code>key</code>值不存在时可以返回默认值,如果不指定,则默认<code>key</code>值不存在时返回<code>None</code></li>
</ul>
</li>
<li><p>与<code>dict[key]</code>对比</p>
<ul>
<li>使用<code>dict[key]</code>时要确保<code>key</code>在<code>dict</code>中,否则会报异常</li>
</ul>
</li>
</ul>
<hr>
<h3 id="一行太长的代码需要分多行"><a href="#一行太长的代码需要分多行" class="headerlink" title="一行太长的代码需要分多行"></a>一行太长的代码需要分多行</h3><ul>
<li><p>必须在每个子行行尾部使用<code>\</code></p>
</li>
<li><p>子行内部不用对齐,因为解析时Python解释器会将所有子行合并成一行</p>
</li>
<li><p>示例:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 9 &lt; 10 and 11 &lt; 12 and 13 &lt; 14:</span><br><span class="line">   print &quot;works&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>等价于</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 9 &lt; 10 and \</span><br><span class="line">	11 &lt; 12 and \</span><br><span class="line">    13 &lt; 14:</span><br><span class="line">    print &quot;works&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>等价于</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 9 &lt; 10 and \</span><br><span class="line">		11 &lt; 12 and \</span><br><span class="line"> 13 &lt; 14:</span><br><span class="line">    print &quot;works&quot;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="全局变量只要在函数调用前声明就行了"><a href="#全局变量只要在函数调用前声明就行了" class="headerlink" title="全局变量只要在函数调用前声明就行了"></a>全局变量只要在函数调用前声明就行了</h3><ul>
<li>我们定义函数时,函数里面的变量可以没有定义</li>
<li>调用函数的时候,默认这个函数中没定义过的变量都是全局变量,函数会主动寻找相关的全局变量,找不在再报错</li>
<li>核心: 定义函数时函数中没定义的变量被使用了(如<code>x=10</code>这样的赋值算是变量的定义,不是使用),那么默认函数认为他是全局变量,当函数被调用的时候,才会寻找全局变量是否在当前Python运行环境中</li>
<li>所以,可以先定义函数,再初始化(定义全局变量),最后调用函数<ul>
<li>只要初始化全局变量在调用函数之前即可</li>
<li>但是需要注意函数中不能给全局变量赋值,被赋值的变量将被函数认为是局部变量<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sum_x(y):</span><br><span class="line">    return x+y</span><br><span class="line"></span><br><span class="line">x = 100</span><br><span class="line">print sum_x(10)</span><br><span class="line"></span><br><span class="line"># output </span><br><span class="line">110</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="使用列表切片修改列表"><a href="#使用列表切片修改列表" class="headerlink" title="使用列表切片修改列表"></a>使用列表切片修改列表</h3><ul>
<li><p>测试代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = [1,2,3,4,5,6]</span><br><span class="line">b = [1,2,3,4,5,6]</span><br><span class="line">c = [0,0,0,0,0,0]</span><br><span class="line"></span><br><span class="line">a[:2] = c[:2]</span><br><span class="line">print(a)</span><br><span class="line">d = b[:2]</span><br><span class="line">d[:] = c[:2]</span><br><span class="line">print(d)</span><br><span class="line">print(b)</span><br><span class="line">b[:2] = [0]</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">[0, 0, 3, 4, 5, 6]</span><br><span class="line">[0, 0]</span><br><span class="line">[1, 2, 3, 4, 5, 6]</span><br><span class="line">[0, 3, 4, 5, 6]</span><br></pre></td></tr></table></figure>

<ul>
<li>列表切片在左边时,可以修改数组内部数据,甚至是长度都可以修改(最后两行代码)</li>
<li>列表切片在右边时,表现为复制一份列表返回给变量<code>d</code>,所以修改<code>d</code>的值将不影响原始的列表<code>b</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="使用del删除列表或字典中的元素"><a href="#使用del删除列表或字典中的元素" class="headerlink" title="使用del删除列表或字典中的元素"></a>使用<code>del</code>删除列表或字典中的元素</h3><ul>
<li><p>删除列表或字典中的元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1, 2, 3]</span><br><span class="line">d = &#123;&quot;m&quot;: 10, &quot;x&quot;: 11&#125;</span><br><span class="line">print(l)</span><br><span class="line">print(d)</span><br><span class="line">del l[0]</span><br><span class="line">print(l)</span><br><span class="line">del l[0]</span><br><span class="line">print(l)</span><br><span class="line">del d[&quot;m&quot;]</span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">[1, 2, 3]</span><br><span class="line">&#123;&apos;m&apos;: 10, &apos;x&apos;: 11&#125;</span><br><span class="line">[2, 3]</span><br><span class="line">[3]</span><br><span class="line">&#123;&apos;x&apos;: 11&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意不能删除元组中的元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t = (1,2,3)</span><br><span class="line">del t[1]</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/home/jiahong/JupyterWorkspace/test2.py&quot;, line 13, in &lt;module&gt;</span><br><span class="line">    del t[1]</span><br><span class="line">TypeError: &apos;tuple&apos; object doesn&apos;t support item deletion</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="dict-keys-返回的是列表吗"><a href="#dict-keys-返回的是列表吗" class="headerlink" title="dict.keys()返回的是列表吗?"></a>dict.keys()返回的是列表吗?</h3><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">d = &#123;&quot;m&quot;: 10, &quot;x&quot;: 11&#125;</span><br><span class="line">print(d.keys())</span><br><span class="line">print(type(d.keys()))</span><br></pre></td></tr></table></figure>

<h4 id="Python-2-7中输出"><a href="#Python-2-7中输出" class="headerlink" title="Python 2.7中输出"></a>Python 2.7中输出</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&apos;x&apos;, &apos;m&apos;]</span><br><span class="line">&lt;type &apos;list&apos;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="Python-3-6中输出"><a href="#Python-3-6中输出" class="headerlink" title="Python 3.6中输出"></a>Python 3.6中输出</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dict_keys([&apos;m&apos;, &apos;x&apos;])</span><br><span class="line">&lt;class &apos;dict_keys&apos;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>Python2.7中输出是列表,丢失了<code>set</code>信息,占用空间小,但是会造成使用<code>x in d.keys()</code>时变成线性搜索时间 O(n)</li>
<li>Python3.6中输出是<code>dict_keys</code>类型的对象,保留了<code>set</code>信息,占用空间也大了,便于使用<code>x in d.keys()</code>时变成常数搜索时间 O(1)</li>
</ul>
<hr>
<h3 id="列表切片的详细说明-1"><a href="#列表切片的详细说明-1" class="headerlink" title="列表切片的详细说明[::-1]"></a>列表切片的详细说明<code>[::-1]</code></h3><ul>
<li><p>切片完整用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">li[start:end:step]</span><br></pre></td></tr></table></figure>

<ul>
<li><code>start</code>: 开始索引,包含<code>li[start]</code>,默认为<code>0</code></li>
<li><code>end</code>: 结束索引, 不包含<code>li[end]</code>,默认为<code>len(li)</code></li>
<li><code>step</code>: 跳着取元素,<code>step</code>为间隔,默认为<code>1</code><ul>
<li><code>step</code>可以设置为负, 此时若<code>start &gt; end</code>则能得到,从<code>[start,end]</code>结束的序列,包含<code>li[start]</code>, 不包含<code>li[end]</code>, 由于此时<code>start &gt; end</code>, 所以得到的是逆序列</li>
<li>注意: 若<code>step</code>参数省略的话第二个<code>:</code>也能省略</li>
</ul>
</li>
</ul>
</li>
<li><p>代码示例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A = [0,1,2,3,4,5,6,7,8,9]</span><br><span class="line">print(A)</span><br><span class="line">print(A[::-1])</span><br><span class="line">print(A[::-2])</span><br><span class="line">print(A[::1])</span><br><span class="line">print(A[::2])</span><br><span class="line">print(A[1:5:])</span><br><span class="line">print(A[1:5:])</span><br><span class="line">print(A[5:1:-1])</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]</span><br><span class="line">[9, 7, 5, 3, 1]</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[0, 2, 4, 6, 8]</span><br><span class="line">[1, 2, 3, 4]</span><br><span class="line">[1, 2, 3, 4]</span><br><span class="line">[5, 4, 3, 2]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="vars函数的使用"><a href="#vars函数的使用" class="headerlink" title="vars函数的使用"></a><code>vars</code>函数的使用</h3><ul>
<li><p>定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def vars(p_object=None): # real signature unknown; restored from __doc__</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    vars([object]) -&gt; dictionary</span><br><span class="line">    </span><br><span class="line">    Without arguments, equivalent to locals().</span><br><span class="line">    With an argument, equivalent to object.__dict__.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return &#123;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>vars(object)</code>返回对象的字典</li>
</ul>
</li>
<li><p>代码示例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class A:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.a = 10</span><br><span class="line">        self.b = &quot;abc&quot;</span><br><span class="line"></span><br><span class="line">    def getA(self):</span><br><span class="line">        self.c = &quot;100&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = A()</span><br><span class="line">print(A.__dict__)</span><br><span class="line">print(vars(A))</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">&#123;&apos;__module__&apos;: &apos;__main__&apos;, &apos;__init__&apos;: &lt;function A.__init__ at 0x7f87b2be01e0&gt;, &apos;getA&apos;: &lt;function A.getA at 0x7f87994c2e18&gt;, &apos;__dict__&apos;: &lt;attribute &apos;__dict__&apos; of &apos;A&apos; objects&gt;, &apos;__weakref__&apos;: &lt;attribute &apos;__weakref__&apos; of &apos;A&apos; objects&gt;, &apos;__doc__&apos;: None&#125;</span><br><span class="line">&#123;&apos;__module__&apos;: &apos;__main__&apos;, &apos;__init__&apos;: &lt;function A.__init__ at 0x7f87b2be01e0&gt;, &apos;getA&apos;: &lt;function A.getA at 0x7f87994c2e18&gt;, &apos;__dict__&apos;: &lt;attribute &apos;__dict__&apos; of &apos;A&apos; objects&gt;, &apos;__weakref__&apos;: &lt;attribute &apos;__weakref__&apos; of &apos;A&apos; objects&gt;, &apos;__doc__&apos;: None&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意: <code>object.__dict__</code>一般在序列化的时候访问,平时不会访问</p>
</li>
</ul>
<hr>
<h3 id="整数除法-负整数"><a href="#整数除法-负整数" class="headerlink" title="整数除法(负整数)"></a>整数除法(负整数)</h3><ul>
<li><strong>C++ 和 Java</strong>中整数除法是<strong>向0取整</strong></li>
<li><strong>Python</strong>中整数除法是<strong>向下取整</strong>(向负无穷取整)</li>
<li><strong>正整数</strong>除法他们的<strong>商和余数</strong>都<strong>相同</strong></li>
<li><strong>负整数</strong>除法<strong>商和余数</strong>都<strong>不同</strong>,需要注意,不要用错</li>
</ul>
<hr>
<h3 id="如何获取一个正数的小数部分"><a href="#如何获取一个正数的小数部分" class="headerlink" title="如何获取一个正数的小数部分?"></a>如何获取一个正数的小数部分?</h3><ul>
<li><p>方法1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = 10.234</span><br><span class="line">decimal = a - int(a)</span><br></pre></td></tr></table></figure>
</li>
<li><p>方法2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = 10.234</span><br><span class="line">decimal = a % 1</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="for循环中使用lambda的坑"><a href="#for循环中使用lambda的坑" class="headerlink" title="for循环中使用lambda的坑"></a>for循环中使用lambda的坑</h3><ul>
<li><p>问题代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">funs = []</span><br><span class="line">for i in range(3):</span><br><span class="line">	funs.append(lambda: i)</span><br><span class="line">print funs[0](), funs[1](), funs[2]()</span><br><span class="line"></span><br><span class="line">## output: 2 2 2</span><br></pre></td></tr></table></figure>

<ul>
<li>问题原因：由于lambda引用对象不会被lambda定义时复制，lambda定义后x还可以在外面被修改，最终结果是所有函数都持有相同的<code>i</code>作为引用对象</li>
</ul>
</li>
<li><p>解决方案：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">funs = []</span><br><span class="line">for i in range(3):</span><br><span class="line">    funs.append(lambda x=i: x)</span><br><span class="line">print funs[0](), funs[1](), funs[2]()</span><br><span class="line"></span><br><span class="line">## output: 0 1 2</span><br></pre></td></tr></table></figure>

<ul>
<li>核心思想：在定义lambda时将<code>i</code>的值复制给默认参数，这一步实现了值的复制</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——编程规范</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83.html</url>
    <content><![CDATA[<p><em>Python 编程中经验型的一些规范</em><br><strong><em>持续更新</em></strong></p>
<hr>
<h3 id="使用li-代替copy-copy-li"><a href="#使用li-代替copy-copy-li" class="headerlink" title="使用li[:]代替copy.copy(li)"></a>使用li[:]代替copy.copy(li)</h3><ul>
<li><code>li[:]</code>等价于<code>copy.copy(li)</code></li>
<li><code>li[:]</code>不等价于<code>copy.deepcopy(li)</code></li>
<li><code>li[:]</code>可以简化代码</li>
</ul>
<hr>
<h3 id="从后开始访问列表"><a href="#从后开始访问列表" class="headerlink" title="从后开始访问列表"></a>从后开始访问列表</h3><ul>
<li>list作为stack用时访问栈顶元素list[-1]<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list1 = [1, 5, 2, 3]</span><br><span class="line">print list1[-1]</span><br><span class="line">print list1[-2]</span><br><span class="line">print list1[1:-1]</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">[5, 2]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="函数内函数"><a href="#函数内函数" class="headerlink" title="函数内函数"></a>函数内函数</h3><ul>
<li>Python可以在函数内部定义函数,但是要注意不能与外部变量混淆<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sum_all(alist):</span><br><span class="line"></span><br><span class="line">    def sum_two(a, b):</span><br><span class="line">        return a+b</span><br><span class="line"></span><br><span class="line">    result = 0</span><br><span class="line">    for i in alist:</span><br><span class="line">        result = sum_two(result, i)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print sum_all([1, 2, 3, 4, 5])</span><br><span class="line"># output:</span><br><span class="line">15</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="不要随意调用特殊方法"><a href="#不要随意调用特殊方法" class="headerlink" title="不要随意调用特殊方法"></a>不要随意调用特殊方法</h3><p><em>(比如：__len__)</em></p>
<ul>
<li>特殊方法不应该被开发者调用，而应该被开发者定义新类时实现</li>
<li>一般使用<code>len(object)</code>即可</li>
<li>原因： 内置的方法(比如：len())可能会直接返回对象中的<code>ob_size</code>属性，而不用调用<code>__len__()</code>(这个函数往往会用迭代或者其他比较复杂的方法实现)</li>
</ul>
<hr>
<h3 id="多使用列表推导"><a href="#多使用列表推导" class="headerlink" title="多使用列表推导"></a>多使用列表推导</h3><p><em>(list comprehension, 简写listcomps)和生成器表达式(generator expression, 简写genexps)</em></p>
<ul>
<li><p>listcomps</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[str(i) for i in range(1,10)]</span><br></pre></td></tr></table></figure>
</li>
<li><p>genexps</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(str(i) for i in range(1,10))</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="考虑使用reduce而不是循环语句"><a href="#考虑使用reduce而不是循环语句" class="headerlink" title="考虑使用reduce而不是循环语句"></a>考虑使用reduce而不是循环语句</h3><ul>
<li>reduce<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def add(x, y):</span><br><span class="line">	return x + y</span><br><span class="line">reduce(add, [1,2,3,4,5])</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from operator import mul</span><br><span class="line">reduce(mul, range(1, 10))</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="多使用pprint"><a href="#多使用pprint" class="headerlink" title="多使用pprint"></a>多使用pprint</h3><ul>
<li>(Pretty Print)而不是print<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pprint</span><br><span class="line">l1 = (1, &#123;2: 3&#125;, &quot;first&quot;, (&quot;second&quot;, 5, 6, [7, 8, 9]), [1, 3, 5], &quot;this is a pprint&quot;)</span><br><span class="line">pprint.pprint(l1)</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">(1,</span><br><span class="line"> &#123;2: 3&#125;,</span><br><span class="line"> &apos;first&apos;,</span><br><span class="line"> (&apos;second&apos;, 5, 6, [7, 8, 9]),</span><br><span class="line"> [1, 3, 5],</span><br><span class="line"> &apos;this is a pprint&apos;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="多使用三目运算符"><a href="#多使用三目运算符" class="headerlink" title="多使用三目运算符"></a>多使用三目运算符</h3><ul>
<li><p>Python不像Java和C++一样,有x?y:z这样的三目运算符号,但是可以有自己的特殊使用方法,等价于三目运算符且更容易理解</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># same as x?y:z</span><br><span class="line">result = y if x is True else z</span><br><span class="line">return y if x is True else z</span><br></pre></td></tr></table></figure>

<ul>
<li>注意: 在使用return语句中的三目运算时必须有else语句,否则编译不通过,因为返回值可能会缺失</li>
</ul>
</li>
</ul>
<hr>
<h3 id="匿名变量的使用"><a href="#匿名变量的使用" class="headerlink" title="匿名变量的使用"></a>匿名变量的使用</h3><ul>
<li>初始化一个列表时<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list1 = [_ for _ in list2 if _.val &gt; 10]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Python异常处理"><a href="#Python异常处理" class="headerlink" title="Python异常处理"></a>Python异常处理</h3><ul>
<li><p>异常处理的正确姿势, 注意如果不是必要的话不要使用Exception, 可以考虑列出来需要捕获的所有异常, 然后在函数内部判断异常类型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">	read some thing</span><br><span class="line">except IOError, ValueError, e:</span><br><span class="line">	exception_type = type(e)</span><br><span class="line">	print(&quot;%s&quot; % e)</span><br></pre></td></tr></table></figure>
</li>
<li><p>当然, 我们一般为了方便也会直接使用下面的方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">	read some thing</span><br><span class="line">except Exception, e:</span><br><span class="line">	exception_type = type(e)</span><br><span class="line">	print(&quot;%s&quot; % e)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><ul>
<li>在Python中, 位运算与C++中有所不同</li>
<li>参见<a href="/Notes/Python/Python%E2%80%94%E2%80%94%E4%BD%8D%E8%BF%90%E7%AE%97%E4%B8%8E%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E5%92%8CC++%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C.html" title="/Notes/Python/Python——位运算与逻辑运算和C++有什么不同.html">Python——位运算与逻辑运算和C++有什么不同</a></li>
</ul>
<hr>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul>
<li>内部函数访问全局变量时使用<code>global</code>关键字声明(与外部函数一样)</li>
<li>内部函数访问外部函数的变量使用<code>nonlocal</code>(仅限Python 3)关键字声明</li>
<li>如果没有声明<ul>
<li>变量变为只读的<ul>
<li>可以写出<code>a.append()</code>这样的语句</li>
<li>但不可以写出<code>a = b</code>这样的语句</li>
</ul>
</li>
</ul>
</li>
<li>函数内部变量与函数外同名时:<ul>
<li>若写出赋值操作,则认为当前变量为局部变量</li>
<li>否则认为是函数外的全局变量</li>
<li>若先访问变量(一般访问,不是对变量赋值,此时视为全局变量),然后对变量赋值(此时视为局部变量),则产生矛盾,Python解释器报错</li>
</ul>
</li>
</ul>
<hr>
<h3 id="三元表达式的使用"><a href="#三元表达式的使用" class="headerlink" title="三元表达式的使用"></a>三元表达式的使用</h3><ul>
<li><p>如果在三元表达式使用在加法中,需要加上括号,不然整体意思会变成错误的</p>
</li>
<li><p>比如两个结点的加法操作,带有进位<code>carry</code>,我们可以简单的写一行:</p>
<ul>
<li><p>下面是错误示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val = l1.val if l1 else 0 + l2.val if l2 else 0 + carry</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面的表达式可以理解为如果<code>l1</code>不为空,返回<code>l1.val</code>,否则返回<code>0 + l2.val if l2 else 0 + carry</code></p>
</li>
<li><p>当<code>l1</code>为空时又可以理解为如果<code>l2</code>不为空,返回<code>0+l2.val</code>否则返回<code>0+carry</code></p>
</li>
</ul>
</li>
<li><p>正确的写法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val = (l1.val if l1 else 0) + (l2.val if l2 else 0) + carry</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="迭代dict的键时用dict-iterkeys"><a href="#迭代dict的键时用dict-iterkeys" class="headerlink" title="迭代dict的键时用dict.iterkeys()"></a>迭代dict的键时用dict.iterkeys()</h3><ul>
<li><p>迭代键值时使用<code>iterkeys()</code>而不是<code>keys()</code></p>
</li>
<li><p><code>iterkeys()</code>返回一个迭代器而不是所有键值列表</p>
</li>
<li><p><code>keys()</code>返回所有键值列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 迭代效率高</span><br><span class="line">for _ in dict.iterkeys()</span><br><span class="line"># 迭代效率低</span><br><span class="line">for _ in dict.keys()</span><br></pre></td></tr></table></figure>
</li>
<li><p>迭代值时也同理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 迭代效率高</span><br><span class="line">for _ in dict.itervalues()</span><br><span class="line"># 迭代效率低</span><br><span class="line">for _ in dict.values()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="zip实现二维列表的行列变换"><a href="#zip实现二维列表的行列变换" class="headerlink" title="zip实现二维列表的行列变换"></a>zip实现二维列表的行列变换</h3><p><em>只适用于Python3，因为涉及到<code>*</code>操作</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = [[1,2,3],</span><br><span class="line">	 [4,5,6]]</span><br><span class="line"></span><br><span class="line">print [e for e in zip(*a)]</span><br><span class="line"># Output</span><br><span class="line">[(1,4), (2,5), (3,6)]</span><br></pre></td></tr></table></figure>

<ul>
<li>Python3中才能使用<code>*</code>作为操作列表解压符</li>
<li>Python3中<code>zip</code>函数返回的是一个生成器而不是列表，所以需要迭代成列表</li>
</ul>
<hr>
<h3 id="Python中逻辑运算符的巧用"><a href="#Python中逻辑运算符的巧用" class="headerlink" title="Python中逻辑运算符的巧用"></a>Python中逻辑运算符的巧用</h3><p><em>不建议使用</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print 0 or 1</span><br><span class="line">print 10 or 1</span><br><span class="line">print None or 1</span><br><span class="line">print 0 and 1</span><br><span class="line">print 1 and 10</span><br><span class="line">print None and 10</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">1</span><br><span class="line">10</span><br><span class="line">1</span><br><span class="line">0</span><br><span class="line">10</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<ul>
<li><p>理解</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print a or b </span><br><span class="line">&lt;==&gt;</span><br><span class="line">print a if a else b</span><br><span class="line"></span><br><span class="line">print a and b</span><br><span class="line">&lt;==&gt;</span><br><span class="line">print b if a else a</span><br></pre></td></tr></table></figure>
</li>
<li><p>记忆: </p>
<ul>
<li>把<code>a</code>, <code>b</code>当做逻辑表达式,如果访问到<code>b</code>则返回<code>b</code>,否则返回<code>a</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="reversed函数的使用"><a href="#reversed函数的使用" class="headerlink" title="reversed函数的使用"></a>reversed函数的使用</h3><ul>
<li><p>接受参数为可迭代对象,返回一个反向访问迭代对象的迭代器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in reversed(range(n)):</span><br><span class="line"></span><br><span class="line">&lt;==&gt;</span><br><span class="line"></span><br><span class="line">for i in range(n-1, -1, -1):</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>reversed</code>的使用似乎更优雅,也更容易理解</p>
</li>
<li><p>容易遗忘的点,需要注意: <strong>reversed的参数必须是可迭代的对象,而不是两个数字</strong></p>
</li>
</ul>
<hr>
<h3 id="将简单的句子优雅的写到一行"><a href="#将简单的句子优雅的写到一行" class="headerlink" title="将简单的句子优雅的写到一行"></a>将简单的句子优雅的写到一行</h3><ul>
<li><p>返回值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if i &lt; 0: return Flase</span><br><span class="line">&lt;==&gt;</span><br><span class="line">if i &lt; 0:</span><br><span class="line">	return False</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他简单的执行语句直接合并</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">i = 10; j = 10</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断语句之后使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if bool: i = 10;</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断语句后使用多条</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if bool: i = 10; j = 20</span><br></pre></td></tr></table></figure>
</li>
<li><p>while语句后使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while(bool): print 10; print 20</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>当句子较长时不建议使用</strong></p>
</li>
<li><p><strong>一般为了美观,平时的项目也不建议使用</strong></p>
</li>
<li><p><strong>刷题时为了让代码看起来简便,是可以使用的,但是这样会使得代码不易调试</strong></p>
</li>
<li><p><strong>总之:慎用</strong></p>
</li>
</ul>
<hr>
<h3 id="用同一个值初始化两个变量"><a href="#用同一个值初始化两个变量" class="headerlink" title="用同一个值初始化两个变量"></a>用同一个值初始化两个变量</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x = y = value</span><br></pre></td></tr></table></figure>

<ul>
<li>在初始化链表头部时最常用<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head, curr = ListNode(None)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="使用collections-Counter"><a href="#使用collections-Counter" class="headerlink" title="使用collections.Counter"></a>使用<code>collections.Counter</code></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import collections</span><br><span class="line"></span><br><span class="line">counter = collections.Counter([1, 2, 1, 3, 4, &quot;a&quot;, &quot;a&quot;, &quot;c&quot;])</span><br><span class="line"></span><br><span class="line">print counter</span><br><span class="line">print counter[&quot;a&quot;]</span><br><span class="line">print counter[1]</span><br><span class="line">print counter[-1]</span><br><span class="line">print counter.get(&quot;a&quot;)</span><br><span class="line">print counter.get(-1, 0)</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">Counter(&#123;&apos;a&apos;: 2, 1: 2, 2: 1, 3: 1, 4: 1, &apos;c&apos;: 1&#125;)</span><br><span class="line">2</span><br><span class="line">2</span><br><span class="line">0</span><br><span class="line">2</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="递归函数中定义函数会影响递归函数的运行效率吗-待更新"><a href="#递归函数中定义函数会影响递归函数的运行效率吗-待更新" class="headerlink" title="递归函数中定义函数会影响递归函数的运行效率吗?(待更新)"></a>递归函数中定义函数会影响递归函数的运行效率吗?(待更新)</h3>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas——apply, applymap和map函数</title>
    <url>/Notes/Python/Pandas/Pandas%E2%80%94%E2%80%94apply,%20applymap%E5%92%8Cmap%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<p><em>Pandas库中apply, applymap和map函数的使用</em></p>
<hr>
<h3 id="所属类"><a href="#所属类" class="headerlink" title="所属类"></a>所属类</h3><ul>
<li><strong><em>apply</em></strong>: 属于<code>DataFrame</code>类和<code>Series</code>类</li>
<li><strong><em>applymap</em></strong>: 属于<code>DataFrame</code>类</li>
<li><strong><em>map</em></strong>: 属于<code>Series</code>类</li>
</ul>
<hr>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><ul>
<li><strong><em>map</em></strong> 和 <strong><em>applymap</em></strong>都是对每个元素分别操作的：</li>
<li><strong><em>apply</em></strong><ul>
<li>在<code>DataFrame</code>中是对列或者行操作，每一列或者行都是一个<code>Series</code>(列： <code>axis=0</code>[默认值]，行： <code>axis=1</code>)</li>
<li>在<code>Series</code>中是对每个元素进行操作(其实换个角度理解为对<code>Series</code>的每一列操作也行，此时的每一列就是一个元素，值得注意的是此时的每个元素是数值类型而不是<code>Series</code>类型，所以不能对其调用<code>sum</code>等函数)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><pre><code># a simple example for apply(), applymap() and map()
func_series = lambda x: x.sum()
func_element = lambda x: &quot;%.2f&quot; % x

df.apply(func_series)
df.applymap(func_element)

ser.apply(func_element)
ser.map(func_element)</code></pre>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>概率——共轭分布</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83.html</url>
    <content><![CDATA[<p><em>待更新,共轭分布</em><br><em>参考:LDA数学八卦</em></p>
<hr>
<h3 id="扩展——关于共轭分布"><a href="#扩展——关于共轭分布" class="headerlink" title="扩展——关于共轭分布"></a>扩展——关于共轭分布</h3><ul>
<li>高斯分布和高斯分布<ul>
<li>Gaussion-Gaussion共轭</li>
</ul>
</li>
<li>Beta分布和二项分布<ul>
<li>Beta-Binomial共轭</li>
</ul>
</li>
<li>Dirichlet分布和多项分布<ul>
<li>Dirichlet-Multinomial共轭</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>PDF——无法打印等权限问题.md</title>
    <url>/Notes/Others/PDF%E2%80%94%E2%80%94%E6%97%A0%E6%B3%95%E6%89%93%E5%8D%B0%E7%AD%89%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p><em>解决PDF的权限问题</em><br><strong>很多PDF文件为保护版权，会通过加密来限制打印和修改权限。随着Acrobat DC版本的发布，PDF文件的加密强度已经达到AES 256-bit</strong></p>
<hr>
<h3 id="在线解密"><a href="#在线解密" class="headerlink" title="在线解密"></a>在线解密</h3><ul>
<li>将文档添加到附件并发送到邮件地址： <a href="mailto:&#111;&#99;&#114;&#x61;&#x6c;&#108;&#x40;&#118;&#105;&#x70;&#x2e;&#49;&#54;&#x33;&#46;&#99;&#111;&#x6d;" target="_blank" rel="noopener">&#111;&#99;&#114;&#x61;&#x6c;&#108;&#x40;&#118;&#105;&#x70;&#x2e;&#49;&#54;&#x33;&#46;&#99;&#111;&#x6d;</a></li>
</ul>
<hr>
<h3 id="离线解密"><a href="#离线解密" class="headerlink" title="离线解密"></a>离线解密</h3><ul>
<li>使用一些离线工具，比如<strong><em>eo pdf decrypter</em></strong>等</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>预估——CTR预估中的样本采样</title>
    <url>/Notes/Others/%E9%A2%84%E4%BC%B0%E2%80%94%E2%80%94CTR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E6%A0%B7%E6%9C%AC%E9%87%87%E6%A0%B7.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><em>待更新</em></p>
<ul>
<li>参考文献：<a href="https://zhuanlan.zhihu.com/p/83982030" target="_blank" rel="noopener">ctr 校准的方式</a></li>
<li>论文：<a href="http://quinonero.net/Publications/predicting-clicks-facebook.pdf" target="_blank" rel="noopener">《Practical Lessons from Predicting Clicks on Ads at Facebook》</a></li>
</ul>
<hr>
<h3 id="CTR预估的样本采样"><a href="#CTR预估的样本采样" class="headerlink" title="CTR预估的样本采样"></a>CTR预估的样本采样</h3><ul>
<li>CTR预估任务中，正负样本可能偏差很多，此时需要做样本采样，以提升训练速度和效果<ul>
<li>“负采样可以提升训练速度和模型效果”来自论文《Practical Lessons from Predicting Clicks on Ads at<br>Facebook》</li>
</ul>
</li>
</ul>
<h3 id="CTR预估样本采样后的校准"><a href="#CTR预估样本采样后的校准" class="headerlink" title="CTR预估样本采样后的校准"></a>CTR预估样本采样后的校准</h3><ul>
<li>方法一<ul>
<li>参见<a href="https://zhuanlan.zhihu.com/p/83982030" target="_blank" rel="noopener">ctr 校准的方式</a>中的推导</li>
</ul>
</li>
<li>方法二【常用方式】<ul>
<li>假设负采样比例为\(w\)，原始预估值为\(p\)，则校准后的预估值\(q\)为：<br>$$<br>\begin{align}<br>q = \frac{p}{p+(1-p)/w}<br>\end{align}<br>$$</li>
<li>具体实现如下：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">logits = ctr_mlp_layer(input)</span><br><span class="line">pctr = tf.nn.sigmoid(logits)</span><br><span class="line">final_pctr = pctr / (pctr + (1 - pctr) / negative_downsampling_rate)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——LdaModel在gensim的使用</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94LdaModel%E5%9C%A8gensim%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
    <content><![CDATA[<p><em>LDA在Python库</em>gensim<em>中的模型和参数介绍</em></p>
<hr>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class LdaModel(interfaces.TransformationABC, basemodel.BaseTopicModel):</span><br><span class="line">	def __init__(self, corpus=None, num_topics=100, id2word=None,</span><br><span class="line">	             distributed=False, chunksize=2000, passes=1, update_every=1,</span><br><span class="line">	             alpha=&apos;symmetric&apos;, eta=None, decay=0.5, offset=1.0, eval_every=10,</span><br><span class="line">	             iterations=50, gamma_threshold=0.001, minimum_probability=0.01,</span><br><span class="line">	             random_state=None, ns_conf=None, minimum_phi_value=0.01,</span><br><span class="line">	             per_word_topics=False, callbacks=None, dtype=np.float32)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a simple example</span><br><span class="line">import gensim</span><br><span class="line">gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><p>主要参数：</p>
<ul>
<li><p>corpus: 语料库，类似于</p>
<blockquote>
<p>[ [(1, 1),(4, 1)], [(2, 1),(3, 2)] ]</p>
</blockquote>
<ul>
<li>gensim库中一般默认corpus参数是经过字典编码统计的，类似于上面的形式，而texts是文本的列表的形式</li>
</ul>
</li>
<li><p>num_topics: 主题数量，超参数</p>
</li>
<li><p>id2word: dict of (int, str), :class:<code>gensim.corpora.dictionary.Dictionary</code></p>
<ul>
<li>用于将corpus中的数字与词进行对应，这里应该为把texts转成corpus的那个字典</li>
</ul>
</li>
<li><p>passes: 训练时的迭代次数</p>
</li>
<li><p>iterations: 推断时的迭代次数</p>
</li>
<li><p>alpha: 主题的先验概率</p>
<ul>
<li>一个num_topics大小的数组表明每个主题的概率</li>
<li>也可以是str类型的值<ul>
<li>“asymmetric”: 固定初始化为1.0/num_topics</li>
</ul>
</li>
</ul>
</li>
<li><p>decay: (0.5, 1]之间的浮点数，前一个lambda值被遗忘的百分比？【待确认参数】        </p>
</li>
</ul>
</li>
<li><p>其他参数：</p>
<ul>
<li>distributed: 是否使用分布式计算</li>
</ul>
</li>
</ul>
<hr>
<h3 id="相关类介绍"><a href="#相关类介绍" class="headerlink" title="相关类介绍"></a>相关类介绍</h3><ul>
<li>gensim.corpora.dictionary.Dictionary<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Dictionary(utils.SaveLoad, Mapping):</span><br><span class="line">	def__init__(self, documents=None, prune_at=2000000)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a simple example</span><br><span class="line">from gensim.corpora import Dictionary</span><br><span class="line">texts = [[&apos;human&apos;, &apos;interface&apos;, &apos;computer&apos;]]</span><br><span class="line">dct = Dictionary(texts)  # initialize a Dictionary</span><br><span class="line">dct.add_documents([[&quot;cat&quot;, &quot;say&quot;, &quot;meow&quot;], [&quot;dog&quot;]])  # add more document (extend the vocabulary)</span><br><span class="line">dct.doc2bow([&quot;dog&quot;, &quot;computer&quot;, &quot;non_existent_word&quot;])</span><br><span class="line"># output: [(0, 1), (6, 1)]</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="完整代码示例"><a href="#完整代码示例" class="headerlink" title="完整代码示例"></a>完整代码示例</h3>]]></content>
      <tags>
        <tag>Python</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——在root用户下也无法删除文件</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E5%9C%A8root%E7%94%A8%E6%88%B7%E4%B8%8B%E4%B9%9F%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6.html</url>
    <content><![CDATA[<p><em>本文介绍Linux中即使是root用户,某些特殊文件也无法删除的问题</em></p>
<ul>
<li>相关指令:<ul>
<li>chattr</li>
<li>lsattr</li>
</ul>
</li>
</ul>
<hr>
<h3 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h3><ul>
<li>chattr命令给予用户权限去修改文件属性已提高系统的安全性<ul>
<li>注意chattr命令不能保护<code>/</code>, <code>/dev</code>, <code>tmp</code>, <code>var</code>目录(本身)</li>
</ul>
</li>
<li>其中一个属性<code>i</code>可以使得文件无法被删除,重命名,设置链接,同时不能修改内容等<ul>
<li>添加<code>i</code>属性指令: <code>chattr +i [filename]</code></li>
<li>移除<code>i</code>属性指令: <code>chattr -i [filename]</code></li>
</ul>
</li>
<li>问题特征表现为,在root用户权限下,某些文件无法删除,报错为:<blockquote>
<p>rm: cannot remove ‘[filename]’: Operation not permitted</p>
</blockquote>
</li>
</ul>
<hr>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul>
<li><p>查看文件属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lsattr [filename]</span><br></pre></td></tr></table></figure>

<ul>
<li>此时文件属性中会多一个<code>i</code>属性</li>
</ul>
</li>
<li><p>删除文件<code>i</code>属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chattr -i [filename]</span><br></pre></td></tr></table></figure>

<ul>
<li>文件<code>i</code>属性删除后可以正常修改或者删除文件了</li>
</ul>
</li>
</ul>
<hr>
<h3 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h3><ul>
<li>参考博客链接为: <a href="http://www.ha97.com/5172.html" target="_blank" rel="noopener">http://www.ha97.com/5172.html</a><blockquote>
<p>+ ：在原有参数设定基础上，追加参数。<br>- ：在原有参数设定基础上，移除参数。<br>= ：更新为指定参数设定。<br>A：文件或目录的 atime (access time)不可被修改(modified), 可以有效预防例如手提电脑磁盘I/O错误的发生。<br>S：硬盘I/O同步选项，功能类似sync。<br>a：即append，设定该参数后，只能向文件中添加数据，而不能删除，多用于服务器日志文件安全，只有root才能设定这个属性。<br>c：即compresse，设定文件是否经压缩后再存储。读取时需要经过自动解压操作。<br>d：即no dump，设定文件不能成为dump程序的备份目标。<br>i：设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容。i参数对于文件 系统的安全设置有很大帮助。<br>j：即journal，设定此参数使得当通过mount参数：data=ordered 或者 data=writeback 挂 载的文件系统，文件在写入时会先被记录(在journal中)。如果filesystem被设定参数为 data=journal，则该参数自动失效。<br>s：保密性地删除文件或目录，即硬盘空间被全部收回。<br>u：与s相反，当设定为u时，数据内容其实还存在磁盘中，可以用于undeletion。<br>各参数选项中常用到的是a和i。a选项强制只可添加不可删除，多用于日志系统的安全设定。而i是更为严格的安全设定，只有superuser (root) 或具有CAP_LINUX_IMMUTABLE处理能力（标识）的进程能够施加该选项。</p>
</blockquote>
</li>
</ul>
<hr>
<h3 id="评论补充说明"><a href="#评论补充说明" class="headerlink" title="评论补充说明"></a>评论补充说明</h3><ul>
<li>参考地址: <a href="http://www.ha97.com/5172.html" target="_blank" rel="noopener">http://www.ha97.com/5172.html</a> 的评论部分<blockquote>
<p>i: 将无法对文件进行修改,若对目录设置后则仅能修改子文件而不能新建或删除。<br>a: 仅允许补充（追加）内容.无法覆盖/删除(Append Only)。<br>S: 文件内容变更后立即同步到硬盘(sync)。<br>s: 彻底从硬盘中删除，不可恢复(用0填充原文件所在硬盘区域)。<br>A: 不再修改这个文件的最后访问时间(atime)。<br>b: 不再修改文件或目录的存取时间。<br>D: 检查压缩文件中的错误。<br>d: 当使用dump命令备份时忽略本文件/目录。<br>c: 默认将文件或目录进行压缩。<br>u: 当删除此文件后依然保留其在硬盘中的数据，方便日后恢复。<br>t: 让文件系统支持尾部合并（tail-merging）。<br>X: 可以直接访问压缩文件的内容。</p>
</blockquote>
</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——各种数据分析图介绍.md</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9B%BE%E4%BB%8B%E7%BB%8D.html</url>
    <content><![CDATA[<p><em>[日常编辑]</em><br><em>Python数据分析时可能用到各种各样的图，本文将简单介绍数据分析中常用的几种基本图</em></p>
<hr>
<h3 id="直方图-Hist-plot"><a href="#直方图-Hist-plot" class="headerlink" title="直方图(Hist-plot)"></a>直方图(Hist-plot)</h3><hr>
<h3 id="柱形图-Dist-plot"><a href="#柱形图-Dist-plot" class="headerlink" title="柱形图(Dist-plot)"></a>柱形图(Dist-plot)</h3><p><strong>seaborn.distplot</strong></p>
<ul>
<li>集合了matplotlib的hist()与核函数估计kdeplot的功能</li>
</ul>
<hr>
<h3 id="核密度估计图-KDE-plot"><a href="#核密度估计图-KDE-plot" class="headerlink" title="核密度估计图(KDE-plot)"></a>核密度估计图(KDE-plot)</h3><p><strong>seaborn.kdeplot</strong></p>
<hr>
<h3 id="箱式图-Box-plot"><a href="#箱式图-Box-plot" class="headerlink" title="箱式图(Box-plot)"></a>箱式图(Box-plot)</h3><hr>
<h3 id="小提琴图-Violin-plot"><a href="#小提琴图-Violin-plot" class="headerlink" title="小提琴图(Violin-plot)"></a>小提琴图(Violin-plot)</h3><ul>
<li>用于显示数据分布及其概率密度</li>
<li>竖向显示类似于箱式图，横向表征密度分布，越宽的地方密度分布越高</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——机器学习中的编码方式</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F.html</url>
    <content><![CDATA[<p><em>One-hot encoding与Dummy-encoding易混淆点区分</em><br><em>Label Encoding标签编码</em></p>
<hr>
<h3 id="独热码-One-Hot-code"><a href="#独热码-One-Hot-code" class="headerlink" title="独热码(One-Hot code)"></a>独热码(One-Hot code)</h3><ul>
<li>又称独热编码、一位有效编码,直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。</li>
<li>其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。</li>
</ul>
<hr>
<h3 id="哑变量编码-Dummy-encoding"><a href="#哑变量编码-Dummy-encoding" class="headerlink" title=" 哑变量编码(Dummy encoding)"></a> 哑变量编码(Dummy encoding)</h3><ul>
<li>哑变量编码直观的解释就是在One-Hot编码的基础上任意的将一个状态位去除<ul>
<li>比热独码少一维即可编码</li>
</ul>
</li>
<li>可以理解为多个状态位之间是相关的，已知n-1个那么可以推出剩下的那个<ul>
<li>比如已知前n-1个状态位为0，那么最后一位一定为1 </li>
<li>一种做法是: 全0算是一维(理解: 由于全0可以默认最后一位为1, 其他非全0的可以默认最后一维为0,所以能够区分不同样本)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="哑变量-Dummy-variable"><a href="#哑变量-Dummy-variable" class="headerlink" title="哑变量(Dummy variable)"></a>哑变量(Dummy variable)</h3><p><em>亦称指示变量(Indicator variable)</em></p>
<ul>
<li>以上两种编码得到的变量都称为指示变量或者哑变量</li>
</ul>
<hr>
<h3 id="为什么需要One-Hot编码"><a href="#为什么需要One-Hot编码" class="headerlink" title="为什么需要One-Hot编码?"></a>为什么需要One-Hot编码?</h3><ul>
<li>大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的, 使用one-hot编码， 将离散特征的取值扩展到了欧式空间， 离散特征的某个取值就对应欧式空间的某个点, 将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理</li>
<li>将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间</li>
</ul>
<hr>
<h3 id="独热编码优缺点"><a href="#独热编码优缺点" class="headerlink" title="独热编码优缺点"></a>独热编码优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用</li>
<li>它的值只有0和1，不同的类型存储在垂直的空间</li>
<li>数据天然归一化了, 非常优秀</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>当类别的数量很多时，特征空间会变得非常大<ul>
<li>一般可以用PCA来减少维度</li>
<li>One-Hot encoding + PCA 这种组合在实际中也非常有用</li>
</ul>
</li>
</ul>
<hr>
<h3 id="什么时候不用独热编码"><a href="#什么时候不用独热编码" class="headerlink" title="什么时候不用独热编码"></a>什么时候不用独热编码</h3><ul>
<li>有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码, 典型的代表如XGBoost, LightGBM等</li>
<li>存在偏序关系的特征,不能用独热编码, 独热编码会使得特征失去原来的偏序关系</li>
</ul>
<hr>
<h3 id="标签编码"><a href="#标签编码" class="headerlink" title="标签编码"></a>标签编码</h3><p><em>Label Encoding</em></p>
<ul>
<li>将类别编码为连续的数值类型(0,1,2,3…)</li>
<li>举例<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line">le = LabelEncoder()</span><br><span class="line">le.fit([1,8,9,67,5,8,6])</span><br><span class="line">print(le.transform([1,1,8,9,67,5,5]))</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">[0 0 3 4 5 1 1]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="附录-机器学习过程"><a href="#附录-机器学习过程" class="headerlink" title="附录: 机器学习过程"></a>附录: 机器学习过程</h3><img src="/Notes/ML/ML——机器学习中的编码方式/ML_Steps_Overview.png">]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——箱式图基本概念介绍</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E7%AE%B1%E5%BC%8F%E5%9B%BE%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D.html</url>
    <content><![CDATA[<p><em>箱式图，是指—种描述数据分布的统计图</em></p>
<ul>
<li>是表述最小值、第一四分位数、中位数、第三四分位数与最大值的一种图形方法。</li>
<li>可以粗略地看出数据是否具有对称性，分布的分散程度等信息</li>
</ul>
<hr>
<h3 id="异常值"><a href="#异常值" class="headerlink" title="异常值"></a>异常值</h3><p><strong>又称离群值(Outlier), 指不在区间[Q1-1.5<em>IQR, Q3+1.5</em>IQR]中的值</strong></p>
<ul>
<li>Q1为25%中位数，也称为下四分位数</li>
<li>Q3为75%中位数，也称为上四分位数</li>
<li>IQR为Q3-Q1，四分数间距</li>
</ul>
<hr>
<h3 id="绘图方法"><a href="#绘图方法" class="headerlink" title="绘图方法"></a>绘图方法</h3><p><strong>去除离群值之后的部分计入图中，分别标记五个特征值</strong></p>
<ul>
<li><strong>特征值</strong> 最小值，Q1，中位数，Q3，最大值</li>
<li>将五个数值描绘在一个图上，五个特征值在一个直线上</li>
<li>最小值和Q1连接起来，Q1、中位数、Q3分别作平行等长线段</li>
<li>连接两个四分位数构成箱子</li>
<li>连接两个极值点与箱子，形成箱式图</li>
<li>最后点上离群值</li>
</ul>
<hr>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3>]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——误差棒简单介绍</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E8%AF%AF%E5%B7%AE%E6%A3%92%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D.html</url>
    <content><![CDATA[<p>Reference[1]: <a href="https://en.wikipedia.org/wiki/Error_bar" title="https://en.wikipedia.org/wiki/Error_bar" target="_blank" rel="noopener">维基百科</a><br>Reference[2]: <a href="https://www.techwalla.com/13713431/how-to-make-an-app" title="https://www.techwalla.com/13713431/how-to-make-an-app" target="_blank" rel="noopener">How to Calculate Error Bars?</a></p>
<hr>
<h3 id="误差棒"><a href="#误差棒" class="headerlink" title="误差棒"></a>误差棒</h3><p><em>(Error bar, 也称为误差线)</em></p>
<p><strong>显示潜在的误差或相对于系列中每个数据标志的不确定程度</strong></p>
<ul>
<li>误差线可以用标准差(standard deviation)或者标准误差(standard error)，一般用标准差</li>
</ul>
<hr>
<h3 id="标准差与标准误差"><a href="#标准差与标准误差" class="headerlink" title="标准差与标准误差"></a>标准差与标准误差</h3><ul>
<li>标准差是离均差平方和平均后的方根</li>
<li>标准误是标准误差，定义为各测量值误差的平方和的平均值的平方根</li>
<li>标准差与均数结合估计参考值范围，计算变异系数，计算标准误等。标准误用于估计参数的可信区间，进行假设检验等</li>
<li>当样本含量 n 足够大时，标准差趋向稳定；而标准误随n的增大而减小，甚至趋于0</li>
</ul>
<hr>
<h3 id="计算误差棒"><a href="#计算误差棒" class="headerlink" title="计算误差棒"></a>计算误差棒</h3><ul>
<li>Step1: 计算均值 E</li>
<li>Step2: 计算标准差 D</li>
<li>Step3: 计算误差棒的两端值<ul>
<li>barBegin = E-D</li>
<li>barEnd = E+D</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Git——子目录中有Git项目时如何管理</title>
    <url>/Notes/Git/Git%E2%80%94%E2%80%94%E5%AD%90%E7%9B%AE%E5%BD%95%E4%B8%AD%E6%9C%89Git%E5%AD%90%E9%A1%B9%E7%9B%AE%E6%97%B6%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86.html</url>
    <content><![CDATA[<p><em>Git——submodule</em></p>
<p><em>如果Git项目下面有个子项目也是Git下来(往往是git clone命令下载的)的，我们往往不能正常的提交和管理项目，本文给出了一些解决方案</em><br><strong>如果子项目是<code>git clone</code>别人的项目，我们选择将子项目提交到整个大项目中</strong><br><strong>如果直接添加项目到Git往往提示：<code>modified:xxx(modified content, untracked content)</code></strong><br><strong>此时如果直接提交，那么远程仓库里面子项目将是空的</strong></p>
<hr>
<h3 id="融合子项目"><a href="#融合子项目" class="headerlink" title="融合子项目"></a>融合子项目</h3><h4 id="删除-git-文件夹"><a href="#删除-git-文件夹" class="headerlink" title="删除.git/文件夹"></a>删除<code>.git/</code>文件夹</h4><ul>
<li>删除子项目下的<code>.git/</code>文件夹<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rf xxx/.git/</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="删除缓存"><a href="#删除缓存" class="headerlink" title="删除缓存"></a>删除缓存</h4><ul>
<li>删除之前提交过的子项目cache<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除子项目缓存 </span><br><span class="line">git rm -r --cached xxx/</span><br><span class="line"># 如果提示error: the following file has staged content different from both the file and the HEAD: xxx</span><br><span class="line"># 那么按照提示-f参数即可</span><br><span class="line">git rm -rf --cached xxx/</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><em>Note: 没有第二步的话直接进行第三步将和之前一样，子目录还是空的</em></p>
<h4 id="重新添加"><a href="#重新添加" class="headerlink" title="重新添加"></a>重新添加</h4><ul>
<li>重新添加子项目文件夹<br>  git add xxx</li>
</ul>
<hr>
<h3 id="保持子项目为一个独立的项目"><a href="#保持子项目为一个独立的项目" class="headerlink" title="保持子项目为一个独立的项目"></a>保持子项目为一个独立的项目</h3><h4 id="创建子项目"><a href="#创建子项目" class="headerlink" title="创建子项目"></a>创建子项目</h4><ul>
<li><p>像正常创建项目一样在Github上创建项目</p>
</li>
<li><p>添加一个项目为子项目</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git submodule add git@github.com:JoeZJH/Submodule.git submodule_name</span><br></pre></td></tr></table></figure>

<ul>
<li><p>这个操作将会带来三方面的效果</p>
<ul>
<li>在当前文件夹和<code>.git/modules/</code>文件夹下以<code>submodule_name</code>为文件夹名创建新的文件夹</li>
<li>将<code>git@github.com:JoeZJH/Submodule.git</code>链接和文件夹名<code>submodule_name</code>添加到当前文件夹的<code>.gitmodules</code>文件(若没有该文件则会自动新建该文件)和<code>.git/config</code>文件夹中</li>
<li>将<code>git@github.com:JoeZJH/Submodule.git</code>项目中的文件下载到<code>submodule_name</code>中,并在<code>submodule_name</code>文件夹下生成<code>.git</code>文件夹</li>
</ul>
</li>
<li><p>需要注意的是:</p>
<ul>
<li>这里项目里面应该有内容(项目内容不能为空,否则添加会失败)</li>
<li>理解:项目为空时被添加为子项目会在本地和<code>.git/modules/</code>中生成文件夹,但是不会执行后面的步骤</li>
</ul>
</li>
</ul>
</li>
<li><p>查看添加项目结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>

<ul>
<li>这里可以看到至少<ul>
<li><code>.gitmodules</code>被修改</li>
<li><code>submodule_name</code>被添加</li>
</ul>
</li>
</ul>
</li>
<li><p>提交子项目添加结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;comments&quot;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="修改子项目"><a href="#修改子项目" class="headerlink" title="修改子项目"></a>修改子项目</h4><ul>
<li>直接切换到子项目目录下</li>
<li>然后按照正常Git项目操作,提交</li>
</ul>
<h4 id="在外层项目上提交子项目修改"><a href="#在外层项目上提交子项目修改" class="headerlink" title="在外层项目上提交子项目修改"></a>在外层项目上提交子项目修改</h4><ul>
<li>提交子项目修改直接把子项目的文件夹当成一个整体的文件</li>
<li>子项目必须在全部修改内容被commit后才能被外层项目提交修改,否则在外层项目中执行<code>git add .</code>操作将无发添加子项目修改内容</li>
</ul>
<h4 id="移除子项目"><a href="#移除子项目" class="headerlink" title="移除子项目"></a>移除子项目</h4><ul>
<li>递归删除<code>./</code>和<code>.git/modules/</code>下的子项目(模块)目录</li>
<li>删除<code>.gitmodules</code>,<code>.git/config</code>中相关的模块条目</li>
</ul>
<h4 id="初始化带有子项目的项目"><a href="#初始化带有子项目的项目" class="headerlink" title="初始化带有子项目的项目"></a>初始化带有子项目的项目</h4><ul>
<li><p>正常拉取外层项目</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull origin master:master</span><br></pre></td></tr></table></figure>

<ul>
<li>此时关于子模块的信息除了<code>./.gitmodules</code>文件包含外,其他的文件都不包含,包括<code>./.git</code>中</li>
</ul>
</li>
<li><p>初始化子模块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git submodule init</span><br></pre></td></tr></table></figure>

<ul>
<li>将<code>.gitmodules</code>中的所有子模块注册到外层项目中</li>
<li>注册方式: 添加子模块信息(文件夹路径和子模块项目地址)到<code>.git/config</code>文件中并指明子模块对应的<code>active = true</code></li>
</ul>
</li>
<li><p>更新子模块(如果子模块之前存在于<code>.git/config</code>中,且<code>active = false</code>,这个初始化操作会修改为<code>active = true</code>)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git submodule update</span><br></pre></td></tr></table></figure>

<ul>
<li>将<code>.git/config</code>和<code>.gitmodules</code>中的所有子模块的链接地址项目下载到<code>.git/modules/</code>中,并自动同步(自动<code>checkout</code>操作)到子项目文件夹中</li>
<li>注意<code>git submodule init</code>后,<code>.git/config</code>和<code>.gitmodules</code>应该是一致的</li>
<li><code>.git/config</code>和<code>.gitmodules</code>中都有,且在<code>.git/config</code>中<code>active = true</code>的子项目才能被<code>update</code>操作下载</li>
</ul>
</li>
</ul>
<hr>
<h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><ul>
<li>子项目自己知道自己被当做子项目<ul>
<li>一个项目被作为子项目后,他的<code>./submodule_name/.git</code>将不再是一个文件夹,而是一个指明<code>.git/</code>文件夹路径的配置文件<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat ./submodule_name/.git</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<blockquote>
<p>gitdir: ../.git/modules/submodule_name</p>
</blockquote>
<pre><code>* `.git/`文件夹可以在`./.git/modules/submodule_name/.git/`中找到</code></pre>
<ul>
<li><p>子项目相关信息都在外层项目中显示出来</p>
</li>
<li><p>在子项目文件夹<code>./submodule_name/</code>下,子项目的更新,提交等操作正常按照一般项目进行即可</p>
<ul>
<li>这里操作时虽然仓库在外层项目的<code>./.git/modules/submodule_name/.git/</code>中,但是在子项目的目录下我们可以正常访问子项目的仓库</li>
<li>也就是说在子项目文件夹下的<code>git</code>操作(<code>add</code>, <code>commit</code>)实际上不修改当前文件夹下的任何文件,修改都在外层项目的<code>./.git/modules/submodule_name/.git/</code>仓库中</li>
</ul>
</li>
<li><p>外层项目只存储</p>
<ul>
<li>子项目文件夹</li>
<li>在<code>./.gitmodules</code>中存储子项目相关信息(文件夹路径与子项目远程地址)</li>
<li>在GitHub中,直接用网页打开项目可以看到子项目会被自动解析远程地址和最近提交的ID信息,点击子项目对应的文件夹链接即可跳转到子项目远程仓库地址中</li>
</ul>
</li>
</ul>
<h4 id="递归子项目"><a href="#递归子项目" class="headerlink" title="递归子项目"></a>递归子项目</h4><ul>
<li>递归时记住项目的库都在副项目的库中即可<ul>
<li>这句话等价于所有项目的库都在根项目的<code>.git/</code>中</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu——远程登录Ubuntu后只有一个$符号</title>
    <url>/Notes/Linux/Ubuntu%E2%80%94%E2%80%94%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95Ubuntu%E5%90%8E%E5%8F%AA%E6%9C%89%E4%B8%80%E4%B8%AA$%E7%AC%A6%E5%8F%B7.html</url>
    <content><![CDATA[<p><em>对新添加的用户,我们远程登录Ubuntu后有时候只有一个<code>$</code>符号</em></p>
<hr>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>对新添加的用户,远程登录Ubuntu后有时候只有一个<code>$</code>符号</li>
</ul>
<hr>
<h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><ul>
<li>Ubuntu为新用户默认启动的是<code>sh</code>而不是<code>bash</code></li>
<li><code>sh</code>的命令比较少,只有一些<code>ls</code>, <code>pwd</code>这样的命令</li>
</ul>
<hr>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="不修改源文件"><a href="#不修改源文件" class="headerlink" title="不修改源文件"></a>不修改源文件</h4><ul>
<li>每次登录时先使用<code>bash</code>命令运行<code>bash</code>程序</li>
<li>之后的指令都会是<code>bash</code>解释执行</li>
<li>在<code>bash</code>中使用<code>sh</code>可会退到<code>sh</code>命令行</li>
<li>该方法的缺陷在于多次执行<code>bash</code>和<code>sh</code>后,会造成进程嵌套多次,这样exit指令退出时需要多次</li>
</ul>
<h4 id="修改源文件"><a href="#修改源文件" class="headerlink" title="修改源文件"></a>修改源文件</h4><ul>
<li>标记文件/etc/passwd</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vi /etc/passwd</span><br></pre></td></tr></table></figure>

<ul>
<li>找到用户对应的启动命令并将<code>sh</code>(可能没有<code>sh</code>,直接是空白)修改为<code>bash</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改前</span><br><span class="line">jiahong:x:1001:1001::/home/jiahong:</span><br><span class="line"># 修改后</span><br><span class="line">jiahong:x:1001:1001::/home/jiahong:/bin/bash</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu——安装Ubuntu后要首先要做的事情</title>
    <url>/Notes/Linux/Ubuntu%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85Ubuntu%E5%90%8E%E8%A6%81%E9%A6%96%E5%85%88%E8%A6%81%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85.html</url>
    <content><![CDATA[<p><em>原生的Ubuntu有许多没用的软件和包，也有很多我们需要但是没装的软件和包，本文将介绍安装Ubuntu后一般来说需要对系统做哪些自定义的修改，可按照需要选择适合的软件</em></p>
<hr>
<h3 id="删除几乎不用的软件"><a href="#删除几乎不用的软件" class="headerlink" title="删除几乎不用的软件"></a>删除几乎不用的软件</h3><pre><code>sudo apt-get remove libreoffice-common

sudo apt-get remove unity-webapps-common

sudo apt-get remove thunderbird totem rhythmbox empathy brasero simple-scan gnome-mahjongg aisleriot gnome-mines cheese transmission-common gnome-orca webbrowser-app gnome-sudoku  landscape-client-ui-install

sudo apt-get remove onboard deja-dup</code></pre>
<p>删除的软件参考自其他博客<br>reference: <a href="https://blog.csdn.net/skykingf/article/details/45267517" target="_blank" rel="noopener">https://blog.csdn.net/skykingf/article/details/45267517</a></p>
<hr>
<h3 id="安装Vim"><a href="#安装Vim" class="headerlink" title="安装Vim"></a>安装Vim</h3><pre><code>sudo apt-get install vim</code></pre>
<hr>
<h3 id="安装Google-Chrome"><a href="#安装Google-Chrome" class="headerlink" title="安装Google Chrome"></a>安装Google Chrome</h3><pre><code>cd ~/Downloads
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt-get install libappindicator1 libindicator7
sudo dpkg -i google-chrome-stable_current_amd64.deb 
sudo apt-get -f install

# 运行chrome
google-chrome</code></pre>
<hr>
<h3 id="安装Oracle-Java"><a href="#安装Oracle-Java" class="headerlink" title="安装Oracle Java"></a>安装Oracle Java</h3><pre><code>sudo add-apt-repository ppa:webupd8team/java  
sudo apt-get update  
sudo apt-get install oracle-java8-installer 

# 删除残留
sudo rm /usr/share/upstart/sessions/jayatana.conf

# 测试安装成果
java -version
# 如果第一行是版本号则成功</code></pre>
<hr>
<h3 id="安装经典菜单指示器"><a href="#安装经典菜单指示器" class="headerlink" title="安装经典菜单指示器"></a>安装经典菜单指示器</h3><ul>
<li>ClassicMenu Indicator, 用于代替Ubuntu自带的应用检索器<br>  sudo add-apt-repository ppa:diesch/testing<br>  sudo apt-get update<br>  sudo apt-get install classicmenu-indicator</li>
</ul>
<hr>
<h3 id="安装系统指示器SysPeek"><a href="#安装系统指示器SysPeek" class="headerlink" title="安装系统指示器SysPeek"></a>安装系统指示器SysPeek</h3><ul>
<li>查看系统管理，包括内存和CPU消耗等<br>  sudo add-apt-repository ppa:nilarimogard/webupd8<br>  sudo apt-get update<br>  sudo apt-get install syspeek  </li>
</ul>
<hr>
<h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><pre><code>sudo apt-get install git</code></pre>
<hr>
<h3 id="安装unrar"><a href="#安装unrar" class="headerlink" title="安装unrar"></a>安装unrar</h3><pre><code>sudo apt-get install unrar

# 使用命令
unrar x test.rar</code></pre>
<hr>
<h3 id="安装flux的gui版本fluxgui"><a href="#安装flux的gui版本fluxgui" class="headerlink" title="安装flux的gui版本fluxgui"></a>安装flux的gui版本fluxgui</h3><pre><code>sudo add-apt-repository ppa:nathan-renniewaldock/flux
sudo apt-get update
sudo apt-get install fluxgui</code></pre>
<hr>
<h3 id="安装网易云音乐"><a href="#安装网易云音乐" class="headerlink" title="安装网易云音乐"></a>安装网易云音乐</h3><p><strong>Note: 新版本1.1.0有问题，所以选择1.0.0版本</strong><br><em><a href="https://pan.baidu.com/s/16eWS9PBXBugALHgcAOu1og" target="_blank" rel="noopener">网易云1.0.0版本下载地址</a></em></p>
<hr>
<h3 id="安装Remmina"><a href="#安装Remmina" class="headerlink" title="安装Remmina"></a>安装Remmina</h3><ul>
<li><p>一款开源的优雅的远程桌面GUI客户端</p>
<p>  sudo apt-add-repository ppa:remmina-ppa-team/remmina-next<br>  sudo apt-get update<br>  sudo apt-get install remmina remmina-plugin-rdp libfreerdp-plugins-standard</p>
</li>
<li><p>如有问题参考<a href="https://remmina.org/how-to-install-remmina/" title="https://remmina.org/how-to-install-remmina/" target="_blank" rel="noopener">Remmina安装链接</a>*</p>
</li>
</ul>
<hr>
<h3 id="安装词典GoldenDict"><a href="#安装词典GoldenDict" class="headerlink" title="安装词典GoldenDict"></a>安装词典GoldenDict</h3><ul>
<li>一款崇尚自定义的词典</li>
</ul>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装词典</span><br><span class="line">sudo apt-get install goldendict</span><br><span class="line"># 安装En-En在线词库【可选】 </span><br><span class="line">sudo apt-get install goldendict-wordnet</span><br></pre></td></tr></table></figure>

<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><ul>
<li>配置在线有道词典<ul>
<li>Edit-&gt;Dictionaries-&gt;Websites-&gt;Add</li>
<li>Name: Youdao</li>
<li>Address: <a href="http://dict.youdao.com/search?q=%25GDWORD%25&amp;ue=utf8" target="_blank" rel="noopener">http://dict.youdao.com/search?q=%GDWORD%&amp;ue=utf8</a></li>
<li>Enabled: Checked</li>
<li>-&gt;Apply-&gt;OK</li>
</ul>
</li>
<li>配置Bing在线词典<ul>
<li>参考在线有道词典的配置</li>
<li>Name: Bing</li>
<li>Address： <a href="https://cn.bing.com/dict/search?q=%25GDWORD%25" target="_blank" rel="noopener">https://cn.bing.com/dict/search?q=%GDWORD%</a></li>
</ul>
</li>
<li>配置离线词典<ul>
<li>下载需要的词典： <a href="http://download.huzheng.org/zh_CN/" target="_blank" rel="noopener">http://download.huzheng.org/zh_CN/</a></li>
<li>解压</li>
<li>将解压后的文件目录添加到字典中： <ul>
<li>Edit-&gt;Dictionaries-&gt;Files-&gt;Add</li>
<li>选中刚才的文件目录</li>
<li>-&gt;Apply-&gt;OK</li>
</ul>
</li>
</ul>
</li>
<li>开启划词翻译<ul>
<li>在GoldenDict主页点击小笔即可</li>
</ul>
</li>
</ul>
<hr>
<h3 id="安装Shadowsocks"><a href="#安装Shadowsocks" class="headerlink" title="安装Shadowsocks"></a>安装Shadowsocks</h3><p><em>安装Shadowsocksy有两种方式，分别为命令行和Gui版本， 两种方式安装后都需要设置浏览器才能使用VPN</em></p>
<h4 id="命令行版本安装"><a href="#命令行版本安装" class="headerlink" title="命令行版本安装"></a>命令行版本安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install python-pip</span><br><span class="line"># 下面这行有时候可选</span><br><span class="line">sudo apt-get install python-setuptools m2crypto</span><br><span class="line">pip install shadowsocks</span><br><span class="line">sudo gedit /etc/shadowsocks.json</span><br><span class="line"></span><br><span class="line"># 配置文件</span><br><span class="line">&#123;</span><br><span class="line">    &quot;server&quot;:&quot;xx.xx.xx.xx&quot;,</span><br><span class="line">    &quot;server_port&quot;:xxxx,</span><br><span class="line">    &quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;local_port&quot;:1080,</span><br><span class="line">    &quot;password&quot;:&quot;xxxxxxxx&quot;,</span><br><span class="line">    &quot;timeout&quot;:600,</span><br><span class="line">    &quot;method&quot;:&quot;xxx&quot;,</span><br><span class="line">    &quot;fast_open&quot;: true,</span><br><span class="line">    &quot;workers&quot;: 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 运行</span><br><span class="line">sslocal -c /etc/shadowsocks.json</span><br></pre></td></tr></table></figure>

<h4 id="Shadowsocks-Gui版本安装"><a href="#Shadowsocks-Gui版本安装" class="headerlink" title="Shadowsocks Gui版本安装"></a>Shadowsocks Gui版本安装</h4><ul>
<li><p>从网站下载release版本： <a href="https://github.com/shadowsocks/shadowsocks-qt5" target="_blank" rel="noopener">shadowsocks-qt5 GitHub</a></p>
</li>
<li><p>添加运行权限：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x xxx</span><br></pre></td></tr></table></figure>
</li>
<li><p>现在双击即可运行</p>
</li>
<li><p>在打开的Gui中导入json即可</p>
</li>
<li><p>如果想把Shadowsocks添加到启动器,参考<a href="/Notes/Linux/Utuntu%E2%80%94%E2%80%94%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%88%B0%E5%90%AF%E5%8A%A8%E5%99%A8.html" title="/Notes/Linux/Utuntu——添加自定义的软件到启动器.html">Utuntu——添加自定义的软件到启动器</a></p>
</li>
</ul>
<h4 id="浏览器配置"><a href="#浏览器配置" class="headerlink" title="浏览器配置"></a>浏览器配置</h4><ul>
<li>以上两种版本安装配置后，想要用浏览器科学上网还需要配置浏览器</li>
</ul>
<h5 id="Firefox"><a href="#Firefox" class="headerlink" title="Firefox"></a>Firefox</h5><pre><code>&gt;Menu
-&gt; Preferences
-&gt; Network Proxy Setting
-&gt; Manual proxy configuration
-&gt; Socks Host: 127.0.0.1:1080
-&gt; SOCKS v5</code></pre>
<h5 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h5><ul>
<li><strong>分两种方式，一种是不用安装任何插件，在命令行启动时制定代理即可，一种需要安装SwitchyOmega插件</strong></li>
<li><strong>直接使用命令行</strong></li>
</ul>
<pre><code>    # 不用安装任何代理
    google-chrome --proxy-server=socks5://127.0.0.1:1080</code></pre>
<ul>
<li><strong>安装代理： Switchy Omega</strong></li>
<li>安装SwitchyOmega插件时需要科学上网，所以可以先用上面的命令行启动，然后使用Chrome安装SwitchyOmega插件*</li>
<li>配置方面参考SwitchyOmega官网(自动切换模式配置方便)： <a href="https://www.switchyomega.com/settings/" target="_blank" rel="noopener">SwitchyOmega官网/Settings</a>*</li>
</ul>
<hr>
<h3 id="安装wps和字体"><a href="#安装wps和字体" class="headerlink" title="安装wps和字体"></a>安装wps和字体</h3><ul>
<li>安装<a href="https://packages.debian.org/zh-cn/wheezy/amd64/libpng12-0/download" target="_blank" rel="noopener">libpng</a><ul>
<li>网站里面有多个源，可随意选一个下载</li>
<li>也可使用命令直接下载</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://ftp.cn.debian.org/debian/pool/main/libp/libpng/libpng12-0_1.2.49-1+deb7u2_amd64.deb</span><br></pre></td></tr></table></figure>

<ul>
<li>安装<a href="http://wps-community.org/download.html?vl=2016#download" target="_blank" rel="noopener">wps-office</a></li>
<li>安装字体<a href="http://wps-community.org/download.html?vl=fonts#download" target="_blank" rel="noopener">wps-office-fonts_1.0_all.deb</a></li>
</ul>
<hr>
<h3 id="为Ubuntu添加Windows的字体"><a href="#为Ubuntu添加Windows的字体" class="headerlink" title="为Ubuntu添加Windows的字体"></a>为Ubuntu添加Windows的字体</h3><p><strong><em>这个字体可以保证终端等能使用中文，同时能保证wps能正常使用</em></strong></p>
<ul>
<li>从Windows系统的 Windows/Fonts/ 拷贝所有文件，大小大概500M+</li>
<li>在Ubuntu上新建文件夹，建议放到/usr/share/fonts/下面， 比如/usr/share/fonts/win-fonts/</li>
<li>将字体文件拷贝到新建立的文件夹下面</li>
<li>执行下面命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /usr/share/fonts/</span><br><span class="line">sudo mkfontdir</span><br><span class="line">sudo mkfontscale</span><br><span class="line">sudo fc-cache</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="设置截图快捷键"><a href="#设置截图快捷键" class="headerlink" title="设置截图快捷键"></a>设置截图快捷键</h3><p><strong><em>在Windows和Mac下使用QQ快捷键截图习惯了后，到了Ubuntu会不习惯, 因为Ubuntu默认截图是保存到Pictures文件夹的，不是粘贴板</em></strong><br><strong><em>这里给出一种利用Ubuntu自带的软件gnome-screenshot实现QQ快捷键相同功能的方法</em></strong></p>
<ul>
<li>添加快捷键Ctrl+Alt+A绑定到命令<code>gnome-screenshot -ac</code></li>
<li><code>gnome-screenshot -c</code>是全屏截图，也可以设置成自己喜欢的快捷键</li>
<li>gnome-screenshot参数简介<ul>
<li><code>-c</code>, 保存到剪贴板</li>
<li><code>-w</code>, 截取当前窗口</li>
<li><code>-a</code>, 选择一个区域</li>
<li>如果没有任何参数默认是将当前全屏截图，并保存到Pictures文件夹</li>
<li>更多gnome-screenshot参数参考<code>gnome-screenshot -h</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="安装sublime"><a href="#安装sublime" class="headerlink" title="安装sublime"></a>安装sublime</h3><ul>
<li><p>添加安装源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -</span><br><span class="line">sudo apt-get install apt-transport-https</span><br><span class="line">echo &quot;deb https://download.sublimetext.com/ apt/stable/&quot; | sudo tee /etc/apt/sources.list.d/sublime-text.list</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
</li>
<li><p>用apt-get命令安装sublime</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install sublime-text</span><br></pre></td></tr></table></figure>
</li>
<li><p>Sublime中输入中文</p>
</li>
<li><p>说明: 如果不执行这一步,Sublime在Ubuntu中是不能直接输入中文的*</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone --depth=1 https://github.com/lyfeyaj/sublime-text-imfix.git</span><br><span class="line">cd sublime-text-imfix</span><br><span class="line">./sublime-imfix</span><br><span class="line">cd ..</span><br><span class="line">rm -rf sublime-text-imfix</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加一点说明</p>
<ul>
<li>如果上面的方法不能安装最新版或者安装失败,可以尝试自己下载软件发行版并自己添加sublime到启动器,添加自定义程序到启动器的方法可参考 <a href="/Notes/Linux/Utuntu%E2%80%94%E2%80%94%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%88%B0%E5%90%AF%E5%8A%A8%E5%99%A8.html" title="/Notes/Linux/Utuntu——添加自定义的软件到启动器.html">Utuntu——添加自定义的软件到启动器</a></li>
</ul>
</li>
</ul>
<p><del>下面是一些没经过测试的，之后会测试验证</del></p>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——xgboost包使用笔记</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94xgboost%E5%8C%85%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>xgboost包中包含了XGBoost分类器,回归器等, 本文详细介绍<code>XGBClassifier</code>类</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="安装和导入"><a href="#安装和导入" class="headerlink" title="安装和导入"></a>安装和导入</h3><ul>
<li><p>安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install xgboost</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import xgboost as xgb</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clf = xgb.XGBClassifier()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h3><h4 id="普通参数"><a href="#普通参数" class="headerlink" title="普通参数"></a>普通参数</h4><p><em>以下参数按照我理解的重要性排序</em></p>
<ul>
<li><code>booster</code>: <ul>
<li>‘gbtree’: 使用树模型作为基分类器</li>
<li>‘gbliner’: 使用线性模型作为基分类器</li>
<li>默认使用模型树模型即可,因为使用线性分类器时XGBoost相当于退化成含有L1和L2正则化的逻辑回归(分类问题中)或者线性回归(回归问题中) </li>
</ul>
</li>
<li><code>n_estimators</code>: 基分类器数量<ul>
<li>每个分类器都需要一轮训练,基分类器越多,训练所需要的时间越多</li>
<li>经测试发现,开始时越大越能提升模型性能,但是增加到一定程度后模型变化不大,甚至出现过拟合</li>
</ul>
</li>
<li><code>max_depth[default=3]</code>: 每棵树的最大深度<ul>
<li>树越深,越容易过拟合</li>
</ul>
</li>
<li><code>objective[default=&quot;binary:logistic&quot;]</code>: 目标(损失函数)函数,训练的目标是最小化损失函数<ul>
<li>‘binary:logistic’: 二分类回归, <code>XGBClassifier</code>默认是这个,因为<code>XGBClassifier</code>是分类器</li>
<li>‘reg:linear’: 线性回归, <code>XGBRegressor</code>默认使用这个</li>
<li>‘multi:softmax’: 多分类中的softmax</li>
<li>‘multi:softprob’: 与softmax相同,但是每个类别返回的是当前类别的概率值而不是普通的softmax值</li>
</ul>
</li>
<li><code>n_jobs</code>: 线程数量<ul>
<li>以前使用的是<code>nthread</code>, 现在已经不使用了,直接使用<code>n_jobs</code>即可</li>
<li>经测试发现并不是越多越快, 猜测原因可能是因为各个线程之间交互需要代价</li>
</ul>
</li>
<li><code>reg_alpha</code>: L1正则化系数</li>
<li><code>reg_lambda</code>: L2正则化系数</li>
<li><code>subsample</code>: 样本的下采样率</li>
<li><code>colsample</code>: 构建每棵树时的样本特征下采样率</li>
<li><code>scale_pos_weight</code>: 用于平衡正负样本不均衡问题, 有助于样本不平衡时训练的收敛<ul>
<li>具体调参实验还需测试[待更新]</li>
<li>这个值可以作为计算损失时正样本的权重</li>
</ul>
</li>
<li><code>learning_rate</code>: shrinkage参数<ul>
<li>更新叶子结点权重时,乘以该系数,避免步长过大,减小学习率,增加学习次数</li>
<li>在公式中叫做<code>eta</code>, 也就是 \(\eta\)</li>
</ul>
</li>
<li><code>min_child_weight[default=1]</code>: [待更新]</li>
<li><code>max_leaf_nodes</code>: 最大叶子结点数目<ul>
<li>也是用于控制过拟合, 和<code>max_depth</code>的作用差不多</li>
</ul>
</li>
<li><code>importance_type</code>: 指明特征重要性评估方式, 只有在<code>booster</code>为’gbtree’时有效<ul>
<li>‘gain’: [默认], is the average gain of splits which use the feature</li>
<li>‘cover’: is the average coverage of splits which use the feature</li>
<li>‘weight’: is the number of times a feature appears in a tree</li>
<li>‘total_gain’: 整体增益</li>
<li>‘total_cover’: 整体覆盖率</li>
</ul>
</li>
</ul>
<h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><ul>
<li><p><code>feature_importances_</code>: </p>
<ul>
<li>返回特征的重要性列表</li>
<li>特征重要性可以由不同方式评估</li>
<li>特征重要性评估指标(<code>importance_type</code>)在创建时指定, 使用<code>plot_importance</code>函数的话,可以在使用函数时指定</li>
</ul>
</li>
<li><p><code>plot_importance</code>: 按照递减顺序给出每个特征的重要性排序图</p>
<ul>
<li><p>使用方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from xgboost import plot_importance</span><br><span class="line">   from matplotlib import pyplot</span><br><span class="line">   plot_importance(model) </span><br><span class="line">   pyplot.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>详细定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plot_importance(booster, ax=None, height=0.2,</span><br><span class="line">                    xlim=None, ylim=None, title=&apos;Feature importance&apos;,</span><br><span class="line">                    xlabel=&apos;F score&apos;, ylabel=&apos;Features&apos;,</span><br><span class="line">                    importance_type=&apos;weight&apos;, max_num_features=None,</span><br><span class="line">                    grid=True, show_values=True, **kwargs):</span><br><span class="line">    &quot;&quot;&quot;Plot importance based on fitted trees.</span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    booster : Booster, XGBModel or dict</span><br><span class="line">        Booster or XGBModel instance, or dict taken by Booster.get_fscore()</span><br><span class="line">    ax : matplotlib Axes, default None</span><br><span class="line">        Target axes instance. If None, new figure and axes will be created.</span><br><span class="line">    grid : bool, Turn the axes grids on or off.  Default is True (On).</span><br><span class="line">    importance_type : str, default &quot;weight&quot;</span><br><span class="line">        How the importance is calculated: either &quot;weight&quot;, &quot;gain&quot;, or &quot;cover&quot;</span><br><span class="line">        * &quot;weight&quot; is the number of times a feature appears in a tree</span><br><span class="line">        * &quot;gain&quot; is the average gain of splits which use the feature</span><br><span class="line">        * &quot;cover&quot; is the average coverage of splits which use the feature</span><br><span class="line">          where coverage is defined as the number of samples affected by the split</span><br><span class="line">    max_num_features : int, default None</span><br><span class="line">        Maximum number of top features displayed on plot. If None, all features will be displayed.</span><br><span class="line">    height : float, default 0.2</span><br><span class="line">        Bar height, passed to ax.barh()</span><br><span class="line">    xlim : tuple, default None</span><br><span class="line">        Tuple passed to axes.xlim()</span><br><span class="line">    ylim : tuple, default None</span><br><span class="line">        Tuple passed to axes.ylim()</span><br><span class="line">    title : str, default &quot;Feature importance&quot;</span><br><span class="line">        Axes title. To disable, pass None.</span><br><span class="line">    xlabel : str, default &quot;F score&quot;</span><br><span class="line">        X axis title label. To disable, pass None.</span><br><span class="line">    ylabel : str, default &quot;Features&quot;</span><br><span class="line">        Y axis title label. To disable, pass None.</span><br><span class="line">    show_values : bool, default True</span><br><span class="line">        Show values on plot. To disable, pass False.</span><br><span class="line">    kwargs :</span><br><span class="line">        Other keywords passed to ax.barh()</span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    ax : matplotlib Axes</span><br><span class="line">    &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h3 id="单调性保证"><a href="#单调性保证" class="headerlink" title="单调性保证"></a>单调性保证</h3><ul>
<li>XGBoost自带单调性保证功能:<ul>
<li>参数使用示例是<code>monotone_constraints=&quot;(1,0,-1,0,0)&quot;</code>，表示输出结果随着<ul>
<li>第一个参数单调递增</li>
<li>第三个参数单调递减</li>
<li>其他参数不做约束 </li>
</ul>
</li>
<li>这个参数的实现逻辑是： <ul>
<li>monotone_constraints 参数通过在梯度提升树的分裂过程中加入额外的限制来实现单调性。具体来说，在选择最佳分裂点时，XGBoost 不仅考虑分裂的增益（如基尼不纯度减少或均方误差减少），还会检查分裂是否满足指定的单调性约束。如果一个潜在的分裂点违反了单调性约束，那么即使它能带来较大的增益，也不会被选作最佳分裂点。 </li>
</ul>
</li>
<li>在现实场景中会出现修改单调特征值以后，模型预测结果为0的问题 <ul>
<li>表现：实际使用时体现为输出值全是相同的（单调确没有意义）</li>
<li>原因：此时主要原因是数据本身不具有单调性，特别是当label不随着单调特征单调时，容易出现学到的许多区间上模型值相同 </li>
<li>测试（对于单调递增场景，单调递减的正常）： <ul>
<li>测试一：如果样本中存在太多不单调的数据，甚至希望单增，但数据单调递减，则会导致模型预估值随目标特征值变化，基本相同 * 测试二：如果样本中的目标特征和label是满足单调的，但是存在一些随机值，则在某些区间上容易出现单调值，特别是没有见过的区间上，预估值会是完全一致的 </li>
<li>注意：测试时，需要限定其他特征都不变，只有当前特征变化才可以，否则无法保证单调性</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Demo示例：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import xgboost as xgb</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.datasets import make_regression</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"># 创建一个简单的回归数据集</span><br><span class="line">X, y = make_regression(n_samples=1000, n_features=5, noise=0.1)</span><br><span class="line"># 假设我们有5个特征，并且我们知道第一个特征应该与目标变量呈现正相关，</span><br><span class="line"># 第二个特征应该与目标变量呈现负相关，其余特征没有特定的单调性要求。</span><br><span class="line">monotone_constraints = (1, -1, 0, 0, 0)</span><br><span class="line"># 划分数据集</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><br><span class="line"># 将数据转换为 DMatrix 格式，这是 XGBoost 所需的数据格式</span><br><span class="line">dtrain = xgb.DMatrix(X_train, label=y_train)</span><br><span class="line">dtest = xgb.DMatrix(X_test, label=y_test)</span><br><span class="line"># 定义 XGBoost 参数</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;objective&apos;: &apos;reg:squarederror&apos;,  # 对于回归任务</span><br><span class="line">    &apos;monotone_constraints&apos;: str(monotone_constraints)  # 应用单调性约束</span><br><span class="line">&#125;</span><br><span class="line"># 训练模型</span><br><span class="line">model = xgb.train(params, dtrain, num_boost_round=100)</span><br><span class="line"># 预测</span><br><span class="line">predictions = model.predict(dtest)</span><br><span class="line"># 打印部分预测结果</span><br><span class="line">print(predictions[:10])</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu-Jupyter——修改默认路径</title>
    <url>/Notes/Python/Jupyter/Ubuntu-Jupyter%E2%80%94%E2%80%94%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E8%B7%AF%E5%BE%84.html</url>
    <content><![CDATA[<p><em>Ubuntu下修改Jupyter的默认路径</em></p>
<ul>
<li>经测试，按照本文的方法修改后不一定生效</li>
<li>一个比较优雅的方法是每次进入指定路径，从该路径启动<code>jupyter notebook</code><ul>
<li>这种方法可确保Jupyter根目录为指定目录</li>
</ul>
</li>
</ul>
<hr>
<h3 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h3><ul>
<li>首先查看是已经生成Jupyter的配置文件，默认新装的Jupyter根目录为<code>～/</code>, 是没有配置文件的<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看配置文件命令</span><br><span class="line">cat ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h3><ul>
<li><p>如果没有配置文件，则首先需要生成新的配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成配置文件命令</span><br><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure>

<ul>
<li>root用户执行时可能需要按照要求加上参数</li>
</ul>
</li>
</ul>
<hr>
<h3 id="编辑配置文件有两种方式"><a href="#编辑配置文件有两种方式" class="headerlink" title="编辑配置文件有两种方式"></a>编辑配置文件有两种方式</h3><h4 id="相对路径方式"><a href="#相对路径方式" class="headerlink" title="相对路径方式"></a>相对路径方式</h4><p><strong>打开配置文件修改相对目录<code>c.NotebookApp.default_url</code>，这里默认为<code>/tree</code>， 修改成自己想要的地址即可</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># /tree表示当前路径pwd， 如下则表示当前终端路径下的/Coding路径</span><br><span class="line">c.NotebookApp.default_url = &apos;/tree/Coding&apos;</span><br><span class="line"># 这种情况以后打开Jupyter时都需要到指定的文件目录下才行，适用于多个jupyter路径的情况</span><br></pre></td></tr></table></figure>

<p><em>一般使用这种情况时建议直接保留相对目录<code>/tree</code>即可，这样可以在启动Jupyter时直接在那个文件目录下启动</em></p>
<h4 id="绝对路径方式"><a href="#绝对路径方式" class="headerlink" title="绝对路径方式"></a>绝对路径方式</h4><p><strong>打开配置文件配置绝对目录<code>c.NotebookApp.notebook_dir</code></strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 这种情况下无论在哪里打开都是绝对目录 	</span><br><span class="line">c.NotebookApp.notebook_dir = u&apos;/home/jiahong/JupyterWorkspace&apos;</span><br></pre></td></tr></table></figure>

<p><strong>如果两种情况都配置了，默认以第一中情况为主，因为第一个是redirect的</strong><br><strong>如果两种情况都没配置，默认等价于配置了<code>c.NotebookApp.default_url = &#39;/tree&#39;</code></strong></p>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-Jupyter——安装pip包</title>
    <url>/Notes/Python/Jupyter/Ubuntu-Jupyter%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85pip%E5%8C%85.html</url>
    <content><![CDATA[<p><em>安装pip包</em></p>
<hr>
<h3 id="在Jupyter环境中安装pip包"><a href="#在Jupyter环境中安装pip包" class="headerlink" title="在Jupyter环境中安装pip包"></a>在Jupyter环境中安装pip包</h3><ul>
<li><p>为了方便使用，我们一般会选择将Jupyter的内核切换为某个特定的虚拟Python环境，切换后，Jupyter的Python命令会按照新环境运行，但是安装软件包不会默认安装到当前环境（而是base环境）</p>
</li>
<li><p>直接使用下面的命令安装会安装在base环境中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!pip install xxx</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果想要安装在当前运行环境中，可以使用下面的命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line">!&#123;sys.executable&#125; -m pip install xxx</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——如何优雅的导入自定义的Python包</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E5%AF%BC%E5%85%A5%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84Python%E5%8C%85.html</url>
    <content><![CDATA[<hr>
<h3 id="对于IDEA的管理"><a href="#对于IDEA的管理" class="headerlink" title="对于IDEA的管理"></a>对于IDEA的管理</h3><ul>
<li>在IDEA中运行文件时,当前项目根目录会默认被添加为Python的搜索路径</li>
<li>根目录下的文件可以直接被访问</li>
<li>根目录的子目录需要每一层都添加<code>__init__.py</code>文件,将其编译为包</li>
</ul>
<hr>
<h3 id="IDEA项目用命令行运行"><a href="#IDEA项目用命令行运行" class="headerlink" title="IDEA项目用命令行运行"></a>IDEA项目用命令行运行</h3><ul>
<li><p>在运行的第一个文件中添加IDEA项目的根目录作为搜索路径</p>
<ul>
<li><p>这里的根目录最好是绝对路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line">sys.path.append(&quot;/home/xxx/xxx/xxx&quot;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果是想使用相对路径,从而方便移植也行,但需要获取当前文件的文件路径,然后从当前文件与根目录的关系解析出根目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">thisFileDir = os.path.split(os.path.realpath(__file__))[0]</span><br><span class="line"># we have to modify the tail if we change the path of this python file</span><br><span class="line">tail = 6</span><br><span class="line">rootPath = thisFileDir[0:-tail]</span><br><span class="line">if rootPath[-1] != &quot;/&quot; and rootPath[-1] != &quot;\\&quot;:</span><br><span class="line">    message = &quot;Error root path [%s]\n\tPlease check whether you changed the path of current file&quot; % rootPath</span><br><span class="line">    raise Exception(message)</span><br><span class="line">print &quot;Current root path: %s&quot; % rootPath</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(rootPath)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>其他的和在IDEA中运行一样,需要添加的库加上<code>__init__.py</code>即可</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——list返回最大的K个元素的索引</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94list%E8%BF%94%E5%9B%9E%E6%9C%80%E5%A4%A7%E7%9A%84K%E4%B8%AA%E5%85%83%E7%B4%A0%E7%9A%84%E7%B4%A2%E5%BC%95.html</url>
    <content><![CDATA[<p><em>自定义的一个有用的Python方法</em></p>
<hr>
<h3 id="返回最大的K个值的索引"><a href="#返回最大的K个值的索引" class="headerlink" title="返回最大的K个值的索引"></a>返回最大的K个值的索引</h3><p><a href="https://stackoverflow.com/questions/13070461/get-index-of-the-top-n-values-of-a-list-in-python" target="_blank" rel="noopener">引用自Stack Overflow</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_top_k_indexes_of_list(target_list, k, is_max=True, min_value=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    get the top k indexes of elements in list</span><br><span class="line">    Example:</span><br><span class="line">    Problem: I have a list say a = [5,3,1,4,10],</span><br><span class="line">            and I need to get a index of top two values of the list viz 5 and 10.</span><br><span class="line">            Is there a one-liner that python offers for such a case?</span><br><span class="line">    Usage: get_top_k_indexes_of_list(target_list=a, k=2, is_max=True)</span><br><span class="line">    link: https://stackoverflow.com/questions/13070461/get-index-of-the-top-n-values-of-a-list-in-python</span><br><span class="line">    :param target_list: target list</span><br><span class="line">    :param k: the number of indexes</span><br><span class="line">    :param is_max: True means max else False means min</span><br><span class="line">    :param min_value: if min_value is not None</span><br><span class="line">            filter the indexes whose value less than or equals to min_value</span><br><span class="line">    :return: a list of indexes</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    indexes = sorted(range(len(target_list)), key=lambda i: target_list[i], reverse=is_max)[:k]</span><br><span class="line">    result = list()</span><br><span class="line">    if min_value is not None:</span><br><span class="line">        for index in indexes:</span><br><span class="line">            if target_list[index] &lt;= min_value:</span><br><span class="line">                break</span><br><span class="line">            result.append(index)</span><br><span class="line">    else:</span><br><span class="line">        result = indexes</span><br><span class="line">    return result</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="一次性索引出多个元素"><a href="#一次性索引出多个元素" class="headerlink" title="一次性索引出多个元素"></a>一次性索引出多个元素</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_elements_from_list(target_list, indexes):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    get elements from target_list by indexes</span><br><span class="line">    :param target_list: target list</span><br><span class="line">    :param indexes: a list of indexes</span><br><span class="line">    :return: a list of elements</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    elements = [target_list[i] for i in indexes]</span><br><span class="line">    return elements</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-Jupyter——配置跨机器使用</title>
    <url>/Notes/Python/Jupyter/Python-Jupyter%E2%80%94%E2%80%94%E9%85%8D%E7%BD%AE%E8%B7%A8%E6%9C%BA%E5%99%A8%E4%BD%BF%E7%94%A8.html</url>
    <content><![CDATA[<p><em>Jupyter notebook默认是不能跨机器使用的,只能使用localhost访问,本文讲述了让其能跨机器访问的方法</em></p>
<hr>
<h3 id="Jupyter-notebook跨机器使用配置说明"><a href="#Jupyter-notebook跨机器使用配置说明" class="headerlink" title="Jupyter notebook跨机器使用配置说明"></a>Jupyter notebook跨机器使用配置说明</h3><h4 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure>

<pre><code>* 如果是`root`用户可能会存在问题,按照提示操作即可</code></pre>
<h4 id="生成密码"><a href="#生成密码" class="headerlink" title="生成密码"></a>生成密码</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jupyter notebook password</span><br></pre></td></tr></table></figure>

<pre><code>* 按照提示输入密码,这就是以后在远程访问时使用的密码
* 生成的秘钥在`~/.jupyter/jupyter_notebook_config.json`下</code></pre>
<h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim  jupyter_notebook_config.py</span><br></pre></td></tr></table></figure>

<ul>
<li>需要修改以下行<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;*&apos; # 这里可以写成本机的公网ip,或者是局域网ip</span><br><span class="line">c.NotebookApp.password = u&apos;sha1:***&apos; # 这里需要复制修改成刚才生成的密文</span><br><span class="line">c.NotebookApp.open_browser = False # 启动Jupyter时是否自动打开Web页面</span><br><span class="line">c.NotebookApp.port =8888 #可自行指定一个端口, 访问时使用该端口</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——Logstash时区问题</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94Logstash%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p><em>有时候Linux系统重启或者有修改后会造成时区被修改，默认一般是UTC时间，有时候会变成CST时间</em><br>在阿里云主机扩展后出现过一次，造成了Logstash 使用JDBC连接数据库时读取报错：大概是<code>JavaLang::IllegalArgumentException: HOUR_OF_DAY: 2 -&gt; 3</code>这样的错</p>
<hr>
<h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><h4 id="改系统时区"><a href="#改系统时区" class="headerlink" title="改系统时区"></a>改系统时区</h4><ul>
<li>将系统的时区改回去，然后重启<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改时区命令：	</span><br><span class="line">timedatectl set-timezone UTC</span><br><span class="line"># 重启命令：	</span><br><span class="line">shutdown -r now</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>重启后继续运行之前出错的程序，成功解决问题</strong></p>
<h4 id="链接时加参数"><a href="#链接时加参数" class="headerlink" title="链接时加参数"></a>链接时加参数</h4><ul>
<li><p>在连接mysql-connector时加上一个时区参数</p>
</li>
<li><p>以logstash为例子</p>
<ul>
<li><p>在mysql.config中修改连接语句原来为(连接到名称为”docker“的数据库，但是没有指定参数)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jdbc:mysql://localhost:3306/docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改后连接语句为(制定参数为UTC)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jdbc:mysql://localhost:3306/docker?serverTimezone=UTC</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p><strong>注意，这种方式修改后我的系统时区可为任意的，比如我的系统时区就一直为CST</strong></p>
<h4 id="替换JDBC版本【未测试】"><a href="#替换JDBC版本【未测试】" class="headerlink" title="替换JDBC版本【未测试】"></a>替换JDBC版本【未测试】</h4><ul>
<li>网上有博客指出可以通过换jdbc connector的版本解决问题</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——find命令详解</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94find%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3.html</url>
    <content><![CDATA[<p><em>本文介绍Linux下find命令的使用方法</em></p>
<ul>
<li>参考博客: <a href="https://www.cnblogs.com/weijiangbao/p/7653588.html" target="_blank" rel="noopener">https://www.cnblogs.com/weijiangbao/p/7653588.html</a></li>
</ul>
<hr>
<h3 id="常用方法说明"><a href="#常用方法说明" class="headerlink" title="常用方法说明"></a>常用方法说明</h3><ul>
<li><p>find命令用于在给定目录下查找符合给定条件的文件</p>
</li>
<li><p>用法: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage: find [-H] [-L] [-P] [-Olevel] [-D help|tree|search|stat|rates|opt|exec] [path...] [expression]</span><br></pre></td></tr></table></figure>
</li>
<li><p>常用的用法:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find   path   -option   [   -print ]   [ -exec   -ok   command ]   &#123;&#125; \;</span><br><span class="line">find  目标目录   选项 					查找完成后执行的操作</span><br></pre></td></tr></table></figure>
</li>
<li><p>实例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -name &quot;*a*.py&quot; -exec cat &#123;&#125; \; | grep test</span><br></pre></td></tr></table></figure>

<ul>
<li>解析:将当前文件夹下的名字能匹配<code>*a*.py</code>的文件找出来,并输出其中含有”test”的行<ul>
<li><code>.</code>对应<code>path</code>参数,代表搜索路径为当前文件夹, 默认会递归搜索</li>
<li><code>-name &quot;*a*.py&quot;</code>对应选项,指出找文件的筛选条件</li>
<li><code>-exec cat {} \; | grep test</code>指明找到文件后需要执行的操作,<code>{}</code>用于表示前面被固定下来的接受参数, 由于<code>{}</code>后面必须使用分号<code>;</code>,而命令行中分号需要转义字符转义,所以就是<code>\;</code><ul>
<li>注意: 这里<code>{} \;</code>中间必须空一格,且<code>{} \;</code>中间不能加任何字符或命令等</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find . -maxdepth 1 -name &quot;test*.py&quot; -exec cat &#123;&#125; \;| grep test</span><br></pre></td></tr></table></figure>

<pre><code>* 在前面的基础上指定只从第一层目录中查找,不递归</code></pre>
<h3 id="参数和选项详解"><a href="#参数和选项详解" class="headerlink" title="参数和选项详解"></a>参数和选项详解</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find   path   -option   [   -print ]   [ -exec   -ok   command ]   &#123;&#125; \;</span><br></pre></td></tr></table></figure>

<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li><code>-print</code>： find命令将匹配的文件输出到标准输出。</li>
<li><code>-exec</code>： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为’command’ { } ;，注意{ }和；之间的空格。</li>
<li><code>-ok</code>： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行</li>
</ul>
<h3 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h3><ul>
<li><code>-name</code>: 按照文件名查找文件。</li>
<li><code>-perm</code>: 按照文件权限来查找文件。</li>
<li><code>-prune</code>: 使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。</li>
<li><code>-user</code>: 按照文件属主来查找文件。</li>
<li><code>-group</code>: 按照文件所属的组来查找文件。</li>
<li><code>-mtime -n +n</code>: 按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前。find命令还有-atime和-ctime 选项，但它们都和-m time选项。</li>
<li><code>-nogroup</code>: 查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。</li>
<li><code>-nouser</code>: 查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。</li>
<li><code>-newer file1 ! file2</code>: 查找更改时间比文件file1新但比文件file2旧的文件。</li>
<li><code>-type</code>: 查找某一类型的文件<ul>
<li>默认是查找所有类型</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">参数</th>
<th align="center">文件类型</th>
</tr>
</thead>
<tbody><tr>
<td align="center">d</td>
<td align="center">目录</td>
</tr>
<tr>
<td align="center">c</td>
<td align="center">字符设备文件</td>
</tr>
<tr>
<td align="center">p</td>
<td align="center">管道文件</td>
</tr>
<tr>
<td align="center">f</td>
<td align="center">普通文件</td>
</tr>
<tr>
<td align="center">l</td>
<td align="center">符号链接文件</td>
</tr>
<tr>
<td align="center">b</td>
<td align="center">块设备文件</td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-Jupyter——将ipynb文件转成py文件</title>
    <url>/Notes/Python/Jupyter/Python-Jupyter%E2%80%94%E2%80%94%E5%B0%86ipynb%E6%96%87%E4%BB%B6%E8%BD%AC%E6%88%90py%E6%96%87%E4%BB%B6.html</url>
    <content><![CDATA[<pre><code>jupyter nbconvert --to script test.ipynb </code></pre>
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——共轭梯度法和最速下降法</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E5%92%8C%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95.html</url>
    <content><![CDATA[<p><em>本文介绍共轭梯度法和最速下降法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>最速下降法（Steepest Descent Method）和共轭梯度法（Conjugate Gradient Method, CG）也是求解无约束最优化问题的优化方法，其他无约束优化方法见：<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%B1%82%E8%A7%A3%E6%97%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html">ML——求解无约束最优化问题的优化方法</a></li>
<li>共轭梯度法是以共轭方向为搜索方向的一类算法，最早用于求解线性方程组，后来用于求解无约束最优化问题</li>
<li>最速下降法和共轭梯度法都仅限于求解正定矩阵对应的方程或正定二次型对应的无约束最优化问题</li>
</ul>
<h3 id="优化问题定义"><a href="#优化问题定义" class="headerlink" title="优化问题定义"></a>优化问题定义</h3><ul>
<li>在满足一定要求的情况下，求解线性方程组的解和求解最优化问题有一定的等价性</li>
<li>当\(A\)为对称正定矩阵时，有下面的两个问题等价：<ul>
<li>问题一：求解线性方程组<br>  $$ Ax = b $$</li>
<li>问题二：求解最优化问题的解<br>  $$ \min_x \frac{1}{2}(x - x^*)^TA(x - x^*)  $$<ul>
<li>其中\(x^*\)为线性方程组的解</li>
</ul>
</li>
<li>等价原因：正定矩阵\(A\)使得二次型\((x - x^*)^TA(x - x^*) \)一定大于等于0，当二次型\((x - x^*)^TA(x - x^*) = 0 \)时，一定有\(x=x^*\)</li>
</ul>
</li>
<li>对以上最优化问题进一步化简有：<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugate-Gradient-Method-1.png">
<ul>
<li>上图中用到了以下性质：<ul>
<li>对称正定矩阵的性质：\(A = A^T\)</li>
<li>\(x^*\)为线性方程组的解：\(Ax^* = b\)</li>
</ul>
</li>
<li>上图中\((Ax,x)\)表示内积</li>
</ul>
</li>
<li>思考：对于参数数量为2的问题，实际上原始优化问题\((x - x^*)^TA(x - x^*)\)的等值线可以理解为是一个椭圆，原始问题的最优解在多个椭圆的圆心<ul>
<li>为什么是一个椭圆，因为<strong>正定矩阵可以被对角化为一个对角线上元素都是正数的对角矩阵</strong>，即正定二次型一定可以化简为系数全为正的标准型</li>
<li>假设\(A\)已经是一个对角矩阵，对角线上的数字为\(d_1, d_2\)，那么原始二次型对应的\(c\)等值线为：\((x - x^*)^TA(x - x^*) = d_1x_1^2+d_2x_2^2 = c\)</li>
</ul>
</li>
</ul>
<h3 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h3><ul>
<li><p>最速下降法推导</p>
<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Steepest-Descent-Method-1.png">
<ul>
<li>上面的式子中，最后一步的推导是直接展开以后得到<br>$$<br>\begin{align}<br>&amp;=\frac{1}{2} (A(x+\alpha r, x+\alpha r)) - (b, (x+\alpha r)) \\<br>&amp;= \frac{1}{2} (x^TA^Tx + \alpha r^TA^Tx + \alpha x^TA^Tr + \alpha^2r^TA^Tr) - (b^Tx+\alpha b^T r) \\<br>&amp;= \frac{1}{2}( 2\alpha r^TA^Tx) + \frac{1}{2}\alpha^2r^TA^Tr - \alpha b^T r + \frac{1}{2}x^TA^Tx - b^Tx\\<br>&amp;= \alpha r^TA^Tx + \frac{1}{2}\alpha^2r^TA^Tr - \alpha (Ax + r)^T r + \frac{1}{2}x^TA^Tx - b^Tx\\<br>&amp;= \alpha r^TA^Tx + \frac{1}{2}\alpha^2r^TA^Tr - \alpha x^TA^Tr + \alpha r^Tr + \frac{1}{2}x^TA^Tx - b^Tx\\<br>&amp;= \alpha r^TA^Tx - \alpha x^TA^Tr + \frac{1}{2}\alpha^2r^TA^Tr + \alpha r^Tr + \frac{1}{2}x^TA^Tx - b^Tx\\<br>&amp;= 0 + \frac{1}{2}\alpha^2r^TA^Tr + \alpha r^Tr + \frac{1}{2}x^TA^Tx - b^Tx\\<br>&amp;= \frac{1}{2}\alpha^2r^TA^Tr + \alpha r^Tr + \frac{1}{2}x^TA^Tx - b^Tx\\<br>\end{align}<br>$$</li>
<li>上面的推导中使用到了\(b = Ax + r\)，和\(x^TA^Tr = x^TAr = (x, Ar) = (Ar, x) = r^TA^Tx \)</li>
<li>其中\( \frac{1}{2}x^TA^Tx - b^Tx\)与\(\alpha\)无关</li>
</ul>
</li>
<li><p>最速下降法的流程及收敛性分析</p>
<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Steepest-Descent-Method-2.png">
<ul>
<li>收敛速度与特征值有关，当特征值之间的差距过大时，收敛可能会比较慢</li>
</ul>
</li>
<li><p>最速下降法的收敛性图示分析</p>
<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Steepest-Descent-Method-3.png">
<ul>
<li>如果运气好，也可能一步到位，如果运气不好则可能要很久才收敛</li>
</ul>
</li>
</ul>
<h3 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h3><ul>
<li>共轭梯度法的引入<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugate-Gradient-Method-2.png">
<ul>
<li>\(p_{k-1}\)是上次迭代的方向， \(p_{k}\)是本次迭代的方向，初始该值设置为\(p_{-1} = 0\)，或第一步按照\(p_0 = r_0\)迭代</li>
<li>步长推导和最速下降法推导一致</li>
</ul>
</li>
<li>共轭梯度法的方向<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugate-Gradient-Method-3.png">
<ul>
<li>共轭的含义为：<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugacy.png"></li>
<li><strong>共轭梯度法名字的由来</strong>：<ul>
<li><strong>共轭</strong>来自于：\( \forall i, \quad (Ap_{i},p_{k}) = 0\)，每一步迭代的方向与之前的所有步都是共轭相对于矩阵\(A\)共轭的</li>
<li><strong>梯度</strong>则来自于下降的方向\(r\)是原始目标函数\(\frac{1}{2}(x - x^*)^TA(x - x^*) = \frac{1}{2}(Ax, x)- (b, x)\)的梯度，由于\(r\)同时也是方程\(Ax = b\)的残差\(r = b-Ax\)，所以也会称\(r\)为残差</li>
</ul>
</li>
</ul>
</li>
<li>共轭梯度法的性质<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugate-Gradient-Method-4.png"></li>
<li>共轭梯度法的流程<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugate-Gradient-Method-5.png">
<ul>
<li>注意：第0步使用\(p_0 = r_0\)</li>
</ul>
</li>
<li>共轭梯度法收敛性<img src="/Notes/ML/ML——共轭梯度法和最速下降法/Conjugate-Gradient-Method-6.png">








</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——损失函数总结</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>各种损失函数(Loss Function)总结,持续更新</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="名词概念"><a href="#名词概念" class="headerlink" title="名词概念"></a>名词概念</h3><p><em>在机器学习和统计学中，成本函数（Cost Function）、经验风险（Empirical Risk）和损失函数（Loss Function）是三个密切相关但又有所区别的概念</em></p>
<ul>
<li><strong>损失函数</strong>（Loss Function）： 损失函数衡量的是单个训练样本的预测值与实际值之间的差异。它是模型预测误差的量化表示。常见的损失函数包括均方误差（Mean Squared Error, MSE）、交叉熵损失（Cross-Entropy Loss）等</li>
<li><strong>成本函数</strong>（Cost Function）： 又名代价函数，等价于损失函数</li>
<li><strong>期望风险</strong>（Expected Risk）：所有样本（训练样本+验证样本+测试样本+未知样本）的损失函数的期望，用于评估模型的泛化能力</li>
<li><strong>经验风险</strong>（Empirical Risk）： 经验风险是在给定数据集上（一般是训练集），模型的平均损失。它是所有训练样本损失函数值的平均，用于评估模型在特定数据集上的表现。经验风险可以视为模型在有限数据集上的泛化能力的估计，本质是对期望风险的一种估计</li>
<li><strong>结构风险</strong>（Structural Risk）：用于平衡模型对样本的拟合能力和复杂度<br>$$<br>结构风险 = 经验风险 + \alpha 正则化项<br>$$</li>
<li>注意：<strong>在一般的书籍或者博客论文中，尝尝也用 <em>损失函数</em> 或 <em>成本函数</em> 笼统的表达了 <em>成本函数、经验风险、损失函数、成本函数、甚至结构风险</em> 等所有相关概念</strong>，所以本文下面也会比较笼统称为损失函数</li>
</ul>
<h3 id="损失函数总体说明"><a href="#损失函数总体说明" class="headerlink" title="损失函数总体说明"></a>损失函数总体说明</h3><ul>
<li>损失函数(Loss Function)又称为代价函数(Cost Function)</li>
<li>损失函数用于评估预测值与真实值之间的不一致程度</li>
<li>损失函数/成本函数是模型的优化目标函数,(神经网络训练的过程就是最小化损失函数的过程)</li>
<li>损失函数/成本函数越小,说明预测值越接近于真实值,模型表现越好</li>
</ul>
<hr>
<h3 id="各种损失函数介绍"><a href="#各种损失函数介绍" class="headerlink" title="各种损失函数介绍"></a>各种损失函数介绍</h3><h4 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h4><p><em>最常用的回归损失函数</em></p>
<ul>
<li>基本形式<br>$$loss = (y - f(x))^{2}$$</li>
<li>对应模型<ul>
<li>线性回归<ul>
<li>使用的均方误差来自于平方损失函数<br>$$loss_{MSE} = \frac{1}{m}\sum_{i=1}^{m}(y_{i} - f(x_{i}))^2$$</li>
</ul>
</li>
<li>其他扩展，RMSE（Root Mean Squared Error，常用作指标而不是损失函数）<br>$$loss_{RMSE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_{i} - f(x_{i}))^2}$$</li>
</ul>
</li>
</ul>
<h4 id="MSLE-RMSLE损失函数"><a href="#MSLE-RMSLE损失函数" class="headerlink" title="MSLE/RMSLE损失函数"></a>MSLE/RMSLE损失函数</h4><ul>
<li>MSLE，Mean Squared Logarithmic Error<br>$$loss_{MSLE} = \frac{1}{m}\sum_{i=1}^{m} \left(\log(1+y_{i}) - \log(1+f(x_{i})) \right)^2$$</li>
<li>RMSLE，Root Mean Squared Logarithmic Error</li>
<li>损失函数形式<br>$$loss_{RMSLE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m} \left(\log(1+y_{i}) - \log(1+f(x_{i})) \right)^2}$$</li>
<li>MSLE和RMSLE可缓解长尾变量导致的异常值问题</li>
</ul>
<h4 id="MAPE-MSPE-RMAPE-RMSPE损失函数"><a href="#MAPE-MSPE-RMAPE-RMSPE损失函数" class="headerlink" title="MAPE/MSPE/RMAPE/RMSPE损失函数"></a>MAPE/MSPE/RMAPE/RMSPE损失函数</h4><ul>
<li>MAPE，Mean Absolute Percentage Error<br>$$loss_{MAPE} = \frac{1}{m}\sum_{i=1}^{m} \left|\frac{y_i-f(x_i)}{y_i}\right|$$</li>
<li>MSPE，Mean Squared Percentage Error<br>$$loss_{MSPE} = \frac{1}{m}\sum_{i=1}^{m} \left(\frac{y_i-f(x_i)}{y_i}\right)^2$$</li>
<li>RMAPE/RMSPE在MAPE/MSPE的基础上开根号即可</li>
<li>MAPE/MSPE/RMAPE/RMSPE都可缓解长尾变量导致的异常值问题</li>
</ul>
<h4 id="绝对值损失函数"><a href="#绝对值损失函数" class="headerlink" title="绝对值损失函数"></a>绝对值损失函数</h4><p><em>最常用的回归损失函数</em></p>
<ul>
<li>基本形式<br>$$loss = |y - f(x)|$$</li>
<li>对应的经验风险:<br>$$loss_{MAE} = \frac{1}{m}\sum_{i=1}^{m}|y_{i} - f(x_{i})|$$</li>
</ul>
<h4 id="交叉熵损失函数（对数似然损失函数）"><a href="#交叉熵损失函数（对数似然损失函数）" class="headerlink" title="交叉熵损失函数（对数似然损失函数）"></a>交叉熵损失函数（对数似然损失函数）</h4><p><em>最常见的损失函数<strong>交叉熵损失函数</strong>,又名<strong>对数似然损失函数</strong></em></p>
<ul>
<li>基本形式（目标：在已知X时，样本标签Y出现的概率最大化，损失函数在概率前加个负号即可）<br>$$loss=L(P_{\theta}(Y|X))=-logP_{\theta}(Y|X)$$ </li>
<li><strong>二分类</strong>中的交叉熵损失函数:<br>$$loss_{CE} = \frac{1}{N} \sum_{i}^{N}-y_ilogy_i’ - (1-y_i)log(1-y_i’)$$<ul>
<li>二分类中对于单个样本的损失一般写为:<br>  $$loss(x_i) = -y_ilogy_i’ - (1-y_i)log(1-y_i’)$$</li>
<li>写成最容易看清楚的形式为:<br>  $$<br>  \begin{align}<br>  loss(x_i) &amp;= -logy_i’, &amp;\quad y_i = 1 \\<br>  loss(x_i) &amp;= -log(1-y_i’), &amp;\quad y_i = 0<br>  \end{align}<br>  $$<ul>
<li>\(y_i\) 为样本 \(x_i\)的真实类别</li>
<li>\(y_i’\) 为样本\(x_i\)在模型中的预测值(这个值在二分类中为Sigmoid函数归一化后的取值,代表样本分类为\(y_i=1\)的概率)</li>
<li>\(y_i’\) 也可表达为 \(p_{i,1}\), 即样本\(i\) 分类为1的概率</li>
</ul>
</li>
</ul>
</li>
<li><strong>多分类</strong>中的交叉熵损失函数:<br>$$loss_{CE} = -\frac{1}{N} \sum_{i}^{N}\sum_{c=1}^{C} y_{i,c}\log p_{i,c}^{\theta}$$<ul>
<li>\(y_{i,c} \in \{0, 1\}\)：当样本\(i\)的真实分类是\(c\)时，\(y_{i,c}=1\)，否则\(y_{i,c}=0\)</li>
<li>\(p_{i,c}^{\theta}\)：表示样本\(i\)为分类\(c\)的预估概率，由 \(y_{i,c} \in \{0, 1\}\)可知损失函数不需要关注样本预测为其他错误类别的概率，仅关注真实样本对应类别的概率即可</li>
<li>二分类的场景实际上是多分类的特定形式，\(1-y_i’\)可以用来表示分类为0时的概率</li>
</ul>
</li>
<li><strong>凡是极大似然估计作为学习策略的模型,损失函数都为交叉熵损失函数（对数似然损失函数）</strong><ul>
<li>因为极大化似然函数等价于极小化对数似然损失函数，推导：<br>  $$<br>  \begin{align}<br>  \theta^{*} &amp;= \arg\max_{\theta} \prod_i P_{\theta}(y_i|x_i) \\<br>  &amp;= \arg\max_{\theta} \sum_i \log P_{\theta}(y_i|x_i) \\<br>  &amp;= \arg\min_{\theta} - \sum_i \log P_{\theta}(y_i|x_i) \\<br>  &amp;= \arg\min_{\theta} - \frac{1}{N} \sum_{i}^{N}\sum_{c=1}^{C} y_{i,c}\log p_{i,c}^{\theta}<br>  \end{align}<br>  $$ </li>
<li>&lt;&lt;统计学习方法&gt;&gt;第十二章中LR使用的是极大似然估计但是对应的损失函数是逻辑斯蒂损失函数,这里可以证明LR中对数似然损失函数和逻辑斯蒂损失函数完全等价,证明见统计学习方法212页笔记</li>
</ul>
</li>
<li>对应模型<ul>
<li>所有使用极大似然估计的模型<ul>
<li>可以证明，<strong>极大似然估计法最大化样本出现概率</strong>的目标是<strong>最小化真实分布和预估分布的KL散度</strong><ul>
<li>为了方便证明，下面把\(P_{\theta}(y_i|x_i)\)写作\(P_{\theta}(x_i)\)，这里\(x_i\)包含了样本的label（\(y_i\)）信息<br>$$<br>\begin{align}<br>\theta^{*} &amp;= \arg\min_{\theta} - \frac{1}{N} \sum_{i}^{N}\sum_{c=1}^{C} y_{i,c}\log p_{i,c}^{\theta} &amp;\quad — 交叉熵损失函数\\<br>&amp;= \arg\min_{\theta} - \sum_i \log P_{\theta}(x_i)  &amp;\quad — 交叉熵损失函数\\<br>&amp;= \arg\max_{\theta} \sum_i \log P_{\theta}(x_i) \\<br>&amp;= \arg\max_{\theta} \prod_i P_{\theta}(x_i) &amp;\quad — 极大似然法\\<br>&amp;= \arg\max_{\theta} \sum_i \log P_{\theta}(x_i) \\<br>&amp;\approx \arg\max_{\theta} \mathbb{E}_{x \sim P_{data}}\log P_{\theta}(x) \\<br>&amp;= \arg\max_{\theta} \int_{x} P_{data}(x) \log P_{\theta}(x) dx \\<br>&amp;= \arg\max_{\theta} \int_{x} P_{data}(x) \log P_{\theta}(x) dx - \int_{x} P_{data}(x) \log P_{data}(x) dx &amp;\quad — 减去一项与\theta无关的项\\<br>&amp;= \arg\max_{\theta} \int_{x} P_{data}(x) \log \frac{P_{\theta}(x)}{P_{data}(x)} dx \\<br>&amp;= \arg\max_{\theta} -\int_{x} P_{data}(x) \log \frac{P_{data}(x)}{P_{\theta}(x)} dx \\<br>&amp;= \arg\min_{\theta} \int_{x} P_{data}(x) \log \frac{P_{data}(x)}{P_{\theta}(x)} dx \\<br>&amp;= \arg\min_{\theta} KL(P_{data}|| P_{\theta})  &amp;\quad — KL散度\\<br>\end{align}<br>$$ </li>
<li>上式表明：似然函数最大化(极大似然法，对应交叉熵损失最小化)，等价于最小化真实分布与预估分布的KL散度</li>
<li>注：式中\(x_i\)样本表示&lt;特征,标签&gt;对，\(P_{\theta}(x_i)\)表示在模型\(\theta\)下，真实&lt;特征,标签&gt;出现的概率</li>
</ul>
</li>
<li>同理，可以证明，<strong>最小化交叉熵损失函数</strong>的目标也是<strong>最小化真实分布和预估分布的KL散度</strong></li>
</ul>
</li>
<li>最大化后验概率等价于最小化对数似然损失函数<br>$$\theta^{\star} = \arg\max_{\theta} LL(\theta) = \arg\min_{\theta} -LL(\theta) = \arg\min_{\theta} -\log P(Y|X) = \arg\min_{\theta} -\sum_{i=1}^{N}\log p_{\theta}(y_{i}|x_{i})$$</li>
</ul>
</li>
</ul>
<h4 id="指数损失函数"><a href="#指数损失函数" class="headerlink" title="指数损失函数"></a>指数损失函数</h4><p><em>提升方法的损失函数</em></p>
<ul>
<li>基本形式<br>$$loss=L(y,f(x))=e^{-yf(x)}$$</li>
<li>对应模型<ul>
<li>提升方法</li>
</ul>
</li>
</ul>
<h4 id="0-1损失函数"><a href="#0-1损失函数" class="headerlink" title="0-1损失函数"></a>0-1损失函数</h4><p><em>最理想的损失函数,但是不光滑,不可导</em></p>
<ul>
<li>基本形式<br>$$loss=L(y,f(x))=0, \text{if} \  yf(x)&gt;0 \\<br>loss=L(y,f(x))=1, \text{if} \  yf(x)&lt;0$$</li>
<li>在由\(f(x)\)符号判断样本的类别的二分类问题中<ul>
<li>分类正确时总有\(yf(x)&gt;0\),损失为0</li>
<li>分类错误时总有\(yf(x)&lt;0\),损失为1</li>
</ul>
</li>
<li>在特定的模型中,比如要求\(f(x)=y\)才算正确分类的模型中<ul>
<li>0-1损失函数可定义为如下<br>$$loss=L(y,f(x))=0, \text{if} \  y=f(x) \\<br>loss=L(y,f(x))=1, \text{if} \  y\neq f(x)$$</li>
</ul>
</li>
</ul>
<h4 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h4><p><em>支持向量机的损失函数</em></p>
<ul>
<li>基本形式<br>$$loss=L(y,f(x))=[1-yf(x)]_{+}$$</li>
<li>\([z]_{+}\)表示<ul>
<li>\(z&gt;0\)时取\(z\)</li>
<li>\(z\leq 0\)时取0</li>
</ul>
</li>
<li>对应模型<ul>
<li>支持向量机</li>
</ul>
</li>
</ul>
<h4 id="感知机的损失函数"><a href="#感知机的损失函数" class="headerlink" title="感知机的损失函数"></a>感知机的损失函数</h4><p><em>感知机特有的损失函数&lt;&lt;统计学习方法&gt;&gt;</em></p>
<ul>
<li>基本形式<br>$$loss=L(y,f(x))=[-yf(x)]_{+}$$</li>
<li>与合页损失函数对比<ul>
<li>相当于函数图像整体左移一个单位长度</li>
<li>合页损失函数比感知机的损失函数对学习的要求更高</li>
<li>这使得感知机对分类正确的样本就无法进一步优化(分类正确的样本损失函数为0),学到的分类面只要能对样本正确分类即可(不是最优的,而且随机梯度下降时从不同点出发会有不同结果)</li>
<li>而SVM则需要学到最优的才行,因为即使分类正确的样本,依然会有一个较小的损失,此时为了最小化损失函数,需要不断寻找,直到分类面为最优的分类面位置</li>
</ul>
</li>
<li>对应模型<ul>
<li>感知机</li>
</ul>
</li>
</ul>
<h4 id="感知损失函数"><a href="#感知损失函数" class="headerlink" title="感知损失函数"></a>感知损失函数</h4><p><em>与感知机没有任何关系</em></p>
<ul>
<li>基本形式<br>$$L(y,f(x))=1, \text{if} \   \left | y-f(x)\right |&gt;t \\<br>L(y,f(x))=0,  \text{if} \  \left | y-f(x)\right |&lt; t$$</li>
<li>这里”感知”的意思是在一定范围内认为\(y\approx f(x)\),满足小范围差距时,损失函数为0</li>
</ul>
<h4 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h4><p><em>论文原文为: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" target="_blank" rel="noopener">ICCV 2017: Focal Loss for Dense Object Detection</a></em></p>
<ul>
<li>主要是为了解决正负样本严重失衡的问题</li>
<li>是交叉熵损失函数的一种改进</li>
<li>回归交叉熵损失函数的表达式为:<br>$$<br>\begin{align}<br>loss(x_i) &amp;= -logy_i’, &amp;\quad y_i = 1 \\<br>loss(x_i) &amp;= -log(1-y_i’), &amp;\quad y_i = 0<br>\end{align}<br>$$</li>
<li>Focal Loss的损失函数如下<br>$$<br>\begin{align}<br>loss(x_i) &amp;= -(1-y_i’)^\gamma logy_i’, &amp;\quad y_i = 1 \\<br>loss(x_i) &amp;= -y_i^\gamma log(1-y_i’), &amp;\quad y_i = 0<br>\end{align}<br>$$<ul>
<li>\(\gamma\)的取值在原始论文中使用了 0, 0.5, 1, 2, 5 等</li>
<li>当 \(\gamma &gt; 0\) 时显然有<ul>
<li>分类输出与真实标签越大的样本,他们的损失权重越大</li>
<li>分类输出与真实标签越接近的样本,他们的损失权重越小</li>
<li>以上两点给了模型重视分类错误样本的提示</li>
</ul>
</li>
<li>\(\gamma = 0\)时Focal Loss退化为交叉熵损失函数</li>
<li>\(\gamma\) 越大,说明, 分类错误的样本占的损失比重越大 </li>
</ul>
</li>
<li>实际使用中, 常加上\(\alpha\)平衡变量<br>$$<br>\begin{align}<br>loss(x_i) &amp;= -\alpha(1-y_i’)^\gamma logy_i’, &amp;\quad y_i = 1 \\<br>loss(x_i) &amp;= -(1-\alpha)y_i^\gamma log(1-y_i’), &amp;\quad y_i = 0<br>\end{align}<br>$$<ul>
<li>\(\alpha\) 用于平衡正负样本的重要性</li>
<li>\(\gamma\) 用于加强对难分类样本的重视程度</li>
</ul>
</li>
<li>假设正样本数量太少, 负样本数量太多, 那么该损失函数将降低负样本在训练中所占的权重, 可以理解为一种<strong>困难样本挖掘</strong><ul>
<li>困难样本挖掘的思想就是找到分类错误的样本(难以分类的样本), 然后重点关注这些错误样本</li>
</ul>
</li>
<li>原论文中的实验结果:<img src="/Notes/ML/ML——损失函数总结/focal_loss_Overview.png">

</li>
</ul>
<h4 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h4><ul>
<li>基本思路是在最小二乘的基础上加上L2正则<br>$$loss(\theta) = \frac{1}{m}\sum_{i=1}^{m}(y_{i} - f(x_{i}))^2 + \lambda \theta^2$$</li>
<li>其中\(\lambda \theta^2\)项是L2正则项，也称为<strong>收缩惩罚项（shrinkage penalty）</strong><ul>
<li>它试图缩小模型的参数，引入偏差来缓解参数估计中的方法问题，原始的最小二乘是无偏估计，但是引入了L2正则以后会变成有偏的，但是方差更小的参数估计</li>
<li>实际上，shrinkage参数，也称为<strong>收缩参数</strong>，是统计学里面的一个概念，是用于缓解参数估计时离群点带来的问题</li>
</ul>
</li>
</ul>
<h4 id="几个回归损失函数的对比"><a href="#几个回归损失函数的对比" class="headerlink" title="几个回归损失函数的对比"></a>几个回归损失函数的对比</h4><ul>
<li>参考链接补充：<a href="https://www.cnblogs.com/nxf-rabbit75/archive/2019/02/26/10440805.html" target="_blank" rel="noopener">https://www.cnblogs.com/nxf-rabbit75/archive/2019/02/26/10440805.html</a></li>
</ul>
<hr>
<h3 id="不同模型的损失函数"><a href="#不同模型的损失函数" class="headerlink" title="不同模型的损失函数"></a>不同模型的损失函数</h3><h4 id="决策树的损失函数"><a href="#决策树的损失函数" class="headerlink" title="决策树的损失函数"></a>决策树的损失函数</h4><ul>
<li>决策树有两个解释<ul>
<li>if-then规则</li>
<li>条件概率分布</li>
</ul>
</li>
<li>&lt;&lt;统计学习方法&gt;&gt;: 决策树的损失函数是对数似然<ul>
<li>决策树可以看作是对不同概率空间的划分</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Loss-Function-vs-Cost-Function"><a href="#Loss-Function-vs-Cost-Function" class="headerlink" title="Loss Function vs Cost Function"></a>Loss Function vs Cost Function</h3><ul>
<li>损失函数（Loss Function）应用于一个特定的样本计算误差</li>
<li>成本函数（Cost Function）是对所有样本而言的误差</li>
</ul>
<hr>
<h3 id="损失函数相关思考"><a href="#损失函数相关思考" class="headerlink" title="损失函数相关思考"></a>损失函数相关思考</h3><h4 id="分类问题为什么不能用MSE？"><a href="#分类问题为什么不能用MSE？" class="headerlink" title="分类问题为什么不能用MSE？"></a>分类问题为什么不能用MSE？</h4><ul>
<li>参考链接：<ul>
<li><a href="https://blog.csdn.net/CCSUXWZ/article/details/123695477?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-2-123695477.pc_agg_new_rank&utm_term=%E5%88%86%E7%B1%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8mse&spm=1000.2123.3001.4430" target="_blank" rel="noopener">为什么分类问题不使用MSE(平方损失函数)</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA4ODUxNjUzMQ==&mid=2247497812&idx=2&sn=a89412d3f803727758d2848d4c4cdc54&chksm=902a4a88a75dc39e76c72727eb494893e4108e3c2e543151d989144cf27a455fd10fb915a538&mpshare=1&scene=1&srcid=0323TN8Afh6zKFPclQcibgi8&sharer_sharetime=1648029215231&sharer_shareid=2b3e20f48a2bd45a6e7e36b7b6821670#rd" target="_blank" rel="noopener">深究交叉熵损失(Cross-entropy)和平方损失(MSE)的区别</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/463812174" target="_blank" rel="noopener">为什么回归问题用MSE？</a></li>
</ul>
</li>
<li>原因：<ul>
<li>在sigmoid函数拟合概率的情况下，使用MSE会导致预估值接近0或者为1时的梯度都接近0，不利于模型学习收敛</li>
<li>在sigmoid函数拟合概率的情况下，MSE是非凸优化问题，容易陷入局部最优；交叉熵损失则是凸优化问题，不会陷入局部最优</li>
</ul>
</li>
<li><strong>最小化交叉熵损失函数等价于不做任何假设的极大似然估计</strong>，其本质是在最小化真实样本分布和预估分布的KL散度（这里的分布可以是已知X时，label的条件分布）</li>
<li><strong>最小化MSE损失函数等价于假设噪声服从高斯分布时的极大似然估计</strong><ul>
<li><em>推导可参考：<a href="https://zhuanlan.zhihu.com/p/541655548" target="_blank" rel="noopener">MSE的推导</a>、<a href="https://zhuanlan.zhihu.com/p/501569029?utm_psn=1753542229158043648&utm_id=0" target="_blank" rel="noopener">MSE，MLE和高斯分布的关系</a>、<a href="https://zhuanlan.zhihu.com/p/463812174" target="_blank" rel="noopener">为什么回归问题用MSE？</a></em></li>
<li>假设在回归问题中\(y = z + \epsilon\)，样本噪声\(\epsilon\)服从均值为0，方差为\(\sigma\)的高斯分布，即\(\epsilon \sim N(0,\sigma)\)，这里的\(\sigma&gt;0\)</li>
<li>则有原始样本\(y\)服从均值为\(z\)方差为\(\sigma\)的高斯分布\(y \sim N(z,\sigma)\)，我们的目标是用模型拟合非噪声部分\(y_{pred} = \theta^T x = z\)，此时有\(y = y_{pred} + \epsilon\)，即\(y\)服从高斯分布\(y \sim N(y_{pred},\sigma)\)</li>
<li>正太分布的概率密度函数为\(p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-u)^2}{2\sigma^2}}\)，则此时单个样本出现的概率为\(\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(y-\theta^T x)^2}{2\sigma^2}}\)。</li>
<li>用<strong>极大似然法</strong>估计参数\(\theta\)，即最大化多个样本的联合概率为:<br>  $$<br>  \begin{align}<br>  \theta^* &amp;= \arg\max_\theta \prod_i \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(y^i-\theta^T x^i)^2}{2\sigma^2}} \\<br>  &amp;= \arg\max_\theta \prod_i e^{-\frac{(y^i-\theta^T x^i)^2}{2\sigma^2}} &amp;\quad —去掉与\theta无关的常数项 \\<br>  &amp;= \arg\max_\theta \sum_i -\frac{(y^i-\theta^T x^i)^2}{2\sigma^2} &amp;\quad —取对数 \\<br>  &amp;= \arg\max_\theta \sum_i -(y^i-\theta^T x^i)^2 &amp;\quad —继续去除常数项 \\<br>  &amp;= \arg\min_\theta \sum_i (y^i-\theta^T x^i)^2 \\<br>  \end{align}<br>  $$</li>
<li>即当假设噪声服从高斯分布时，用极大似然法估计参数与MSE损失函数得到的参数相同</li>
</ul>
</li>
</ul>
<h4 id="MSE和RMSE对反向传播过程一样吗？"><a href="#MSE和RMSE对反向传播过程一样吗？" class="headerlink" title="MSE和RMSE对反向传播过程一样吗？"></a>MSE和RMSE对反向传播过程一样吗？</h4><ul>
<li>答案是不一样<ul>
<li>MSE的梯度是：<br>$$ \nabla_{w} loss_{MSE} = \frac{1}{M}\sum_{i=1}^M 2(f_w(x_i)-y_i) \cdot \nabla_{w}f_w(x_i) $$</li>
<li>RMSE的梯度是：<br>$$ \nabla_{w} loss_{RMSE} = \frac{1}{2} \left(\frac{1}{M}\sum_{i=1}^M (f_w(x_i)-y_i)^2\right)^{-\frac{1}{2}} \cdot \frac{1}{M}\sum_{i=1}^M 2(f_w(x_i)-y_i) \cdot \nabla_{w}f_w(x_i) $$</li>
</ul>
</li>
<li>由于两者的梯度不同，所以两者对参数的影响也不同，由于\(\frac{1}{2} \left(\frac{1}{M}\sum_{i=1}^M (f_w(x_i)-y_i)^2\right)^{-\frac{1}{2}}\)不是固定值，所以MSE和RMSE对梯度的影响也不是简单的固定倍数关系<ul>
<li>可以简单理解为：<ul>
<li>RMSE的梯度相当于在MSE的基础上乘以\(\frac{1}{2} \left(\frac{1}{M}\sum_{i=1}^M (f_w(x_i)-y_i)^2\right)^{-\frac{1}{2}}\)，在不同的Batch中，该值不同，Loss越大，该值越大，Loss越小，该值越小</li>
</ul>
</li>
</ul>
</li>
<li>MSE是在假设误差服从均值为0的正太分布（即\(\epsilon \sim N(0,\sigma)\)）的基础上基于<strong>极大似然法</strong>求得的目标函数</li>
<li>在同一个Batch内来看，RMSE与MSE的目标实际上是完全一致的，即MSE最小时，RMSE也最小；但是从不同Batch之间来看，RMSE的梯度系数不同无法实现在所有训练集上MSE最小<ul>
<li><strong>当所有数据只有一个Batch时，RMSE和MSE对梯度的影响是常数倍数关系（基于所有样本计算Loss得到的常数）</strong></li>
</ul>
</li>
<li>为了实现多次采样Batch后实现MSE（满足极大似然法推导结果），一般常用MSE作为损失函数，而不是RMSE，RMSE更多是一个指标</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——模型评估指标总结</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>各种模型评估指标总结,持续更新</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h3><ul>
<li>参考：<a href="https://www.cnblogs.com/zongfa/p/9431807.html" target="_blank" rel="noopener">https://www.cnblogs.com/zongfa/p/9431807.html</a></li>
</ul>
<h4 id="Accuracy，准确率"><a href="#Accuracy，准确率" class="headerlink" title="Accuracy，准确率"></a>Accuracy，准确率</h4><ul>
<li>公式：$$ \text{Accuracy} = \frac{TP+TN}{TP+TN+FP+FN} $$</li>
<li>直观，但不利于不平衡样本</li>
</ul>
<h4 id="Recall，召回率"><a href="#Recall，召回率" class="headerlink" title="Recall，召回率"></a>Recall，召回率</h4><ul>
<li>公式：$$ \text{Recall} = \frac{TP}{TP+FN} $$</li>
</ul>
<h4 id="Precision，精确率"><a href="#Precision，精确率" class="headerlink" title="Precision，精确率"></a>Precision，精确率</h4><ul>
<li>公式：$$ \text{Precision} = \frac{TP}{TP+FP} $$</li>
</ul>
<h4 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h4><ul>
<li>公式：$$ \text{F1 Score} = \frac{2 * \text{Precision} * \text{Recall}}{\text{Precision}+\text{Recall}} $$</li>
<li>综合考虑模型”求精“和”求全“的能力</li>
</ul>
<h4 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h4><ul>
<li>形式化定义：AUC是ROC曲线下方的面积，其中ROC曲线的横坐标是伪阳性率（也叫假正类率，False Positive Rate），纵坐标是真阳性率（真正类率，True Positive Rate）</li>
<li>本质：任意取两个样本，一个正样本和一个负样本（\( \forall x^+, x^-\)），模型预测正样本为正的概率分为\(Score_\theta(y=1|x^+)\)，模型预测正样本为正的概率分为\(Score_\theta(y=1|x^-)\)，则AUC为：<br>$$ AUC_\theta = P(Score_\theta(y=1|x^+)&gt;Score_\theta(y=1|x^-))$$</li>
<li>真实实现时，可以统计所有正负样本对，若正样本预估值大于负样本，则累计分数+1，最后用累计分数除以所有可能的正负样本对数量<br>$$ AUC_\theta = \frac{Count(S_+&gt;S_-)}{Count(S_+) * Count(S_-)}$$</li>
<li>具体代码实现：<ul>
<li>将样本按照预估分数倒序排列，从大到小</li>
<li>定义四个变量：正样本总数\(M\)，负样本总数\(N\)，已访问负样本数量\(X=0\)，正样本大于负样本的样本对数量\(Z=0\)</li>
<li>依次访问所有样本：<ul>
<li>若为正样本，则\(Z = Z + (N-X)\)</li>
<li>若为负样本，则\(X = X + 1\)</li>
</ul>
</li>
<li>最后计算：\(AUC = \frac{Z}{M * N}\)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="回归模型"><a href="#回归模型" class="headerlink" title="回归模型"></a>回归模型</h3><ul>
<li>参考：<a href="https://www.cnblogs.com/HuZihu/p/10300760.html" target="_blank" rel="noopener">https://www.cnblogs.com/HuZihu/p/10300760.html</a></li>
<li>参考：<a href="https://blog.csdn.net/guolindonggld/article/details/87856780" target="_blank" rel="noopener">https://blog.csdn.net/guolindonggld/article/details/87856780</a><h4 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h4></li>
<li>MSE (Mean Square Error，均方误差)</li>
<li>也常用作损失函数<h4 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h4></li>
<li>EMSE (Root Mean Square Error，根均方误差)<h4 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h4></li>
<li>MAE (Mean Absolute Error，平均绝对误差)<h4 id="MAPE"><a href="#MAPE" class="headerlink" title="MAPE"></a>MAPE</h4></li>
<li>MAPE (Mean Absolute Percentage Error，平均绝对百分比误差)<h4 id="SMAPE"><a href="#SMAPE" class="headerlink" title="SMAPE"></a>SMAPE</h4></li>
<li>MAPE (Symmetric Mean Absolute Percentage Error，对称平均绝对百分比误差)</li>
</ul>
<hr>
<h3 id="排序模型"><a href="#排序模型" class="headerlink" title="排序模型"></a>排序模型</h3><ul>
<li>参考：<a href="https://www.cnblogs.com/by-dream/p/9403984.html" target="_blank" rel="noopener">https://www.cnblogs.com/by-dream/p/9403984.html</a><h4 id="DCG"><a href="#DCG" class="headerlink" title="DCG"></a>DCG</h4></li>
<li>DCG（Discounted Cumulative Gain， 累计收益折扣）<h4 id="NDCG"><a href="#NDCG" class="headerlink" title="NDCG"></a>NDCG</h4></li>
<li>(NDCG)Normalized Discounted Cumulative Gain</li>
</ul>
<hr>
<h3 id="校准模型"><a href="#校准模型" class="headerlink" title="校准模型"></a>校准模型</h3><ul>
<li>参考：<a href="http://t.zoukankan.com/eilearn-p-14164687.html" target="_blank" rel="noopener">推荐系统（2）—— 评估指标</a></li>
<li>参考：<a href="https://www.6aiq.com/article/1628642001831" target="_blank" rel="noopener">阿里妈妈展示广告预估校准技术演进之路</a></li>
</ul>
<h4 id="COPC"><a href="#COPC" class="headerlink" title="COPC"></a>COPC</h4><ul>
<li>COPC（Click over Predicted Click）<ul>
<li>COPC = 实际的点击率/模型预测的点击率，主要衡量model整体预估的偏高和偏低，同样越接近1越好，一般情况下在1附近波动。这个指标在展示广告上应用多一些</li>
</ul>
</li>
</ul>
<h4 id="PCOC"><a href="#PCOC" class="headerlink" title="PCOC"></a>PCOC</h4><ul>
<li>PCOC（Predicted Click over Click)<ul>
<li>PCOC = 模型预估的点击率/实际点击率，与COPC用途相同</li>
<li>COPC的倒数</li>
</ul>
</li>
</ul>
<blockquote>
<p>PCOC指标是校准之后的点击率与后验点击率（近似真实概率）的比值，越接近于1，意味着在绝对值上越准确，大于1为高估，小于1为低估，是一种常用的高低估评价指标。但是PCOC存在一定局限性，举个例子：2万个样本，其中1万个样本的预估概率是0.2，后验概率是0.4，计算出PCOC是0.2/0.4=0.5，是显著低估的，另1万个样本PCOC是0.8/0.6= 1.3，明显是高估的。所以校准效果并不好，但是样本放一起看，校准后概率是（0.2+0.8）/2=0.5，后验概率是（0.4+0.6）/2=0.5，整体PCOC是1.0，表现完全正常。所以<strong>单一PCOC指标不能表征样本各维度下的校准水平。</strong></p>
</blockquote>
<hr>
<h3 id="净胜率模型"><a href="#净胜率模型" class="headerlink" title="净胜率模型"></a>净胜率模型</h3><ul>
<li>ANLP<ul>
<li>参考：论文《Scalable Bid Landscape Forecasting in Real-time Bidding》</li>
<li>链接：<a href="https://arxiv.org/pdf/2001.06587.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.06587.pdf</a></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——假设检验</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C.html</url>
    <content><![CDATA[<p><em>本文介绍各种常见假设检验方法及使用示例</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考链接1：<a href="https://zhuanlan.zhihu.com/p/124072225" target="_blank" rel="noopener">知乎：T检验、F检验、卡方检验详细分析及应用场景总结</a></li>
<li>参考链接2：<a href="https://www.zhihu.com/zvideo/1355465438965551105" target="_blank" rel="noopener">知乎视频：5分钟带你了解卡方检验</a></li>
</ul>
<h3 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h3><p><em>《概率论与数理统计》</em></p>
<h3 id="p-value的含义"><a href="#p-value的含义" class="headerlink" title="p-value的含义"></a>p-value的含义</h3><ul>
<li>在假设检验中，对p-value的的一种直观理解：在假设\(H_0\)：假设目标样本属于某个正太分布，这个样本不是从这个正太分布中采样的概率就是p值（即p值的本质是一个概率）<ul>
<li>进一步的理解，已知一个正太分布和一个目标样本，那么这个目标样本对应的p值就是：重新在这个正太分布中重新采样一个新样本，新样本离中心位置\(u_0\)的距离大于等于目标样本的概率（该概率就是正太分布的两边区间积分和）</li>
<li>p值越小，说明这个目标样本越不可能是从这个正太分布采样出来的，越应该拒绝原假设\(H_0\)（即越应该接受\(H_A\))</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——凸优化问题和拉格朗日对偶性</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E5%87%B8%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98%E5%92%8C%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7.html</url>
    <content><![CDATA[<p><em>凸优化&amp;拉个朗日对偶性的直观理解</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="凸优化相关书籍"><a href="#凸优化相关书籍" class="headerlink" title="凸优化相关书籍"></a>凸优化相关书籍</h3><ul>
<li>参考链接：<a href="https://git.nju.edu.cn/201300035/NJUAI-Notes/-/tree/master/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95" target="_blank" rel="noopener">优化方法-资料集合</a></li>
</ul>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><ul>
<li>一般优化问题的定义:</li>
</ul>
<p>$$<br>\begin{align}<br>\min_{x}\quad  &amp;f(x) \\<br>s.t. \quad &amp;g_{i}(x) \leq 0, i = 1,2,\dots,k \\<br>      &amp;h_{j}(x) = 0, j = 1,2,\dots,l<br>\end{align}<br>$$</p>
<h4 id="凸优化问题"><a href="#凸优化问题" class="headerlink" title="凸优化问题"></a>凸优化问题</h4><ul>
<li>如果原始优化问题满足:<ul>
<li>\(f(x)\)和\(g_{i}(x)\)都是\(\mathbb{R}^{n}\)上连续可微的凸函数</li>
<li>\(h_{j}(x)\)为\(\mathbb{R}^{n}\)上的仿射函数 (仿射函数是满足\(h_{j}(x) = a\cdot x + b\)的函数)</li>
</ul>
</li>
<li>那么,原始优化问题是<strong>凸优化问题</strong></li>
</ul>
<h4 id="凸二次优化问题"><a href="#凸二次优化问题" class="headerlink" title="凸二次优化问题"></a>凸二次优化问题</h4><ul>
<li>若上述<strong>凸优化问题</strong>还满足:<ul>
<li>\(f(x)\)是二次函数</li>
<li>\(g_{i}(x)\)是\(\mathbb{R}^{n}\)上的仿射函数</li>
</ul>
</li>
<li>那么,原始优化问题是<strong>凸二次优化问题</strong></li>
</ul>
<h4 id="朗格朗日对偶性"><a href="#朗格朗日对偶性" class="headerlink" title="朗格朗日对偶性"></a>朗格朗日对偶性</h4><h5 id="拉格朗日对偶性的推导"><a href="#拉格朗日对偶性的推导" class="headerlink" title="拉格朗日对偶性的推导"></a>拉格朗日对偶性的推导</h5><ul>
<li>原始问题定义为</li>
</ul>
<p>$$<br>\begin{align}<br>\min_{x}\quad  &amp;f(x) \\<br>s.t. \quad &amp;g_{i}(x) \leq 0, i = 1,2,\dots,k \\<br>      &amp;h_{j}(x) = 0, j = 1,2,\dots,l<br>\end{align}<br>$$</p>
<ul>
<li><p>引进拉格朗日函数有<br>$$<br>\begin{align}<br>L(x,\alpha, \beta) = f(x) + \sum_{i=1}^{k}\alpha_{i}g_{i}(x) + \sum_{j=1}^{l}\beta_{j}h_{j}(x)<br>\end{align}<br>$$</p>
</li>
<li><p>考虑到<br>$$<br>\begin{align}<br>\max_{\alpha,\beta:\alpha_{i}\geq 0}L(x,\alpha, \beta)<br>\end{align}<br>$$</p>
<ul>
<li>当\(x\)满足原始问题的解时,上面的式子等价与\(f(x)\)</li>
<li>否则上面的式子等于无穷大\(+\infty\)</li>
</ul>
</li>
<li><p>所以假设原始问题的最优值为\(p^{\star}\),那么<br>$$<br>\begin{align}<br>p^{\star} = \min_{x}\max_{\alpha,\beta:\alpha_{i}\geq 0}L(x,\alpha, \beta)<br>\end{align}<br>$$</p>
<ul>
<li>(\(p^{\star}\)不是最优参数,是最优参数对应的函数值,参数应该用\(\arg\max_{x}\)而不是\(\max_{x}\))</li>
</ul>
</li>
<li><p>定义对偶问题为:<br>$$<br>\begin{align}<br>&amp;\max_{\alpha,\beta:\alpha_{i}\geq 0}\min_{x}L(x,\alpha, \beta) \\<br>&amp;s.t.\quad \alpha_{i} \geq 0,\quad i=1,2,\dots,k<br>\end{align}<br>$$</p>
</li>
<li><p>假设原始问题的最优解为\(d^{\star}\),则原始问题与对偶问题的关系为:<br>$$<br>\begin{align}<br>d^{\star} = \max_{\alpha,\beta:\alpha_{i}\geq 0}\min_{x}L(x,\alpha, \beta) = \min_{x}\max_{\alpha,\beta:\alpha_{i}\geq 0}L(x,\alpha, \beta) = p^{\star}<br>\end{align}<br>$$</p>
</li>
<li><p>满足下面条件的最优化问题,\(x^{\star}\)和\((\alpha^{\star}, \beta^{\star})\)分别是原始问题和对偶问题的解的<strong>充分必要条件</strong>是\((x^{\star}, \alpha^{\star}, \beta^{\star})\)<strong>满足KKT条件</strong></p>
<ul>
<li>原始问题是<strong>凸优化问题</strong></li>
<li>不等式约束\(c_{i}(x) &lt; 0\)是严格可行的,即:<br>$$\exists x \quad \forall i, \quad c_{i}(x) &lt; 0$$</li>
</ul>
</li>
<li><p>需要强调的是:</p>
<ul>
<li>KKT条件是强对偶性成立的<strong>必要条件</strong></li>
<li>当原始问题是<strong>凸优化问题</strong>且<strong>不等式约束\(c_{i}(x) &lt; 0\)是严格可行的</strong>时: KKT条件是强对偶性成立的<strong>充要条件</strong></li>
</ul>
</li>
</ul>
<h5 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件:"></a>KKT条件:</h5><p>$$<br>\begin{align}<br>\nabla_{x}L(x^{\star}, \alpha^{\star}, \beta^{\star}) &amp;= 0 \\<br>\nabla_{\alpha}L(x^{\star}, \alpha^{\star}, \beta^{\star}) &amp;= 0 \\<br>\nabla_{\beta}L(x^{\star}, \alpha^{\star}, \beta^{\star}) &amp;= 0 \\<br>h_{j}(x^{\star}) &amp;= 0, j=1,2,\dots,l \\<br>c_{i}(x^{\star}) &amp;\leq 0, i=1,2,\dots,k \\<br>\alpha_{i}^{\star} &amp;\geq 0, i=1,2,\dots,k \\<br>\alpha_{i}^{\star}c_{i}(x^{\star}) &amp;= 0, i=1,2,\dots,k<br>\end{align}<br>$$</p>
<ul>
<li>补充说明: <ul>
<li>\(x^{\star}\)是原始问题的解</li>
<li>\((\alpha^{\star}, \beta^{\star})\)是对偶问题的解</li>
</ul>
</li>
<li>理解:<ul>
<li>前三个无约束最优化时求极值的方法,导数为0</li>
<li>中间两个是原始问题中的要求</li>
<li>倒数第二个\(\alpha_{i}^{\star} \geq 0\)是为了保证\(f(x)\)的梯度和\(c_{i}(x)\)的梯度方向相反,详情参考周志华&lt;&lt;机器学习&gt;&gt;P404</li>
<li>最后一个KKT条件可通过梯度方向推出,详情参考周志华&lt;&lt;机器学习&gt;&gt;P404</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——函数空间总结</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E7%A9%BA%E9%97%B4%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>希尔伯特空间,欧几里得空间,巴拿赫空间或者拓扑空间都是<strong>函数空间</strong></em></p>
<p>参考博客:<a href="https://blog.csdn.net/weixin_36811328/article/details/81207753" target="_blank" rel="noopener">欧几里得空间与希尔伯特空间</a></p>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——因果推断</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD.html</url>
    <content><![CDATA[<p><em>本文介绍因果推断相关概念</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>讲的比较好的入门材料：<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/616895131" target="_blank" rel="noopener">闲聊因果效应（1）：当我们聊因果时，我们在聊什么</a></li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/622399450" target="_blank" rel="noopener">闲聊因果效应（2）: 理解因果效应的计算以及PSM、IPW</a></li>
</ul>
</li>
<li>阿里发的一篇论文，用神经网络实现的二分类Uplift模型：<ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/629853695" target="_blank" rel="noopener">KDD‘22 阿里｜DESCN: Deep Entire Space Cross Networks | 多任务、端到端、IPW的共舞</a></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——奇异值分解-SVD</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3-SVD.html</url>
    <content><![CDATA[<p><em>本文从不同角度给出奇异值分解的物理意义</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考知乎回答1:<a href="https://www.zhihu.com/collection/244516557" target="_blank" rel="noopener">奇异值的物理意义是什么？</a></li>
<li>参考知乎回答2:<a href="https://www.zhihu.com/question/263722514/answer/272977924" target="_blank" rel="noopener">人们是如何想到奇异值分解的？</a></li>
</ul>
<hr>
<h3 id="公式说明"><a href="#公式说明" class="headerlink" title="公式说明"></a>公式说明</h3><p>$$A=U\Sigma V^{T}$$</p>
<ul>
<li>\(U,V\)都是正交矩阵,\(\Sigma\)是对角矩阵,对角上的元素是矩阵\(A\)的奇异值</li>
<li>若保留对角元素最大的K个值<ul>
<li>\(K=r=Rank(A)\)时为紧奇异值分解,对应的是无损压缩,此时由于奇异值保留数量与原始矩阵相同,能做到对原始矩阵A的完全还原</li>
<li>\(K&lt; r=Rank(A)\)时为截断奇异值分解,对应的是有损压缩,此时由于奇异值保留数量比原始矩阵的小,做不到对原始矩阵A的完全还原,但是如果K足够大就能做到对矩阵A的较完美近似</li>
</ul>
</li>
</ul>
<hr>
<h3 id="图像处理方面"><a href="#图像处理方面" class="headerlink" title="图像处理方面"></a>图像处理方面</h3><ul>
<li>直观上可以理解为奇异值分解是将矩阵分解为若干个秩一矩阵之和,用公式表示就是:<br>$$A=\sigma_{1}u_{1}v_{1}^{T}+\sigma_{2}u_{2}v_{2}^{T}+…+\sigma_{r}u_{r}v_{r}^{T}$$<ul>
<li>式子中每一项的系数\(\sigma\)就是奇异值</li>
<li>\(u,v\)都是列向量,每一个\(uv^{T}\)都是秩为1的矩阵</li>
<li>奇异值按照从小到大排列</li>
</ul>
</li>
<li>从公式中按照从大到小排序后,保留前面系数最大的项目后效果<ul>
<li>对于一张450x333的图片,只需要保留前面的50项即可得到相当清晰的图像</li>
<li>从保留项1到50,图片越来越清晰</li>
</ul>
</li>
<li>结论: <ul>
<li><strong>奇异值越大的项,越能体现出来图片的效果</strong>,奇异值隐含着某种对于A矩阵来说很重要的信息</li>
<li>加权的秩一矩阵能体现整个大矩阵的值,<strong>奇异值就是对应秩一矩阵对于A矩阵的权重</strong></li>
</ul>
</li>
</ul>
<hr>
<h3 id="线性变换方面"><a href="#线性变换方面" class="headerlink" title="线性变换方面"></a>线性变换方面</h3><h4 id="几何含义"><a href="#几何含义" class="headerlink" title="几何含义"></a>几何含义</h4><ul>
<li>对于任何的一个矩阵，我们要找到一组两两正交单位向量序列，使得矩阵作用在此向量序列上后得到新的向量序列保持两两正交.奇异值的几何含义为:<strong>这组变换后的新的向量序列的长度</strong></li>
</ul>
<h4 id="更直观的几何含义"><a href="#更直观的几何含义" class="headerlink" title="更直观的几何含义"></a>更直观的几何含义</h4><ul>
<li>公式:<br>  $$E_{m}={y\in C^{m}: y=Ax, x\in C^{n},\left | x\right |_{2}=1}$$</li>
<li>二维矩阵A:<ul>
<li>矩阵A将二维平面中的单位圆变换为椭圆,而<strong>两个奇异值正好是椭圆的半轴长度</strong>.</li>
</ul>
</li>
<li>m维矩阵<ul>
<li>矩阵A将高维平面中的单位球变换为超椭球,<strong>矩阵的奇异值恰好就是超椭球的每条半轴长度</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="奇异值分解的降维理解"><a href="#奇异值分解的降维理解" class="headerlink" title="奇异值分解的降维理解"></a>奇异值分解的降维理解</h3><h4 id="代码编写"><a href="#代码编写" class="headerlink" title="代码编写"></a>代码编写</h4><ul>
<li><p>代码</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##encoding=utf-8</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">A = np.array([[4, 5, 6], [4, 5, 6], [8, 10, 12], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line">print(&quot;原始矩阵:&quot;)</span><br><span class="line">print(A)</span><br><span class="line"></span><br><span class="line">U, Sigma, Vt = np.linalg.svd(A)</span><br><span class="line">U = np.array(U)</span><br><span class="line">Sigma = np.array(Sigma)</span><br><span class="line">Vt = np.array(Vt)</span><br><span class="line"></span><br><span class="line">print(&quot;分解后的原始U，S，V：&quot;)</span><br><span class="line">print(U)</span><br><span class="line">Sigma_ = np.zeros((4, 3), dtype=np.float64)</span><br><span class="line">Sigma_[:3][:3] = np.diag(Sigma)</span><br><span class="line">print Sigma_</span><br><span class="line">print(Vt)</span><br><span class="line"></span><br><span class="line"># 不做任何处理，直接恢复原始矩阵</span><br><span class="line">A_ = np.dot(np.dot(U, Sigma_), Vt)</span><br><span class="line">print(A_)</span><br><span class="line"></span><br><span class="line"># 原始矩阵的秩为1，所以可以拆解到只剩下一个奇异值，压缩到一维，也能完整恢复原始矩阵，实现将4x3的矩阵变成两个向量+一个数字，当矩阵维度变大时，这里的压缩会更加明显</span><br><span class="line">U = U[:, :1]</span><br><span class="line">Sigma_ = Sigma_[:1, :1]</span><br><span class="line">Vt = Vt[:1, :]</span><br><span class="line"></span><br><span class="line">print(&quot;降维后的U，S，V：&quot;)</span><br><span class="line">print(U)</span><br><span class="line">print(Sigma_)</span><br><span class="line">print(Vt)</span><br><span class="line">A_ = np.dot(np.dot(U, Sigma_), Vt)</span><br><span class="line">print(&quot;降维后的恢复矩阵，与原矩阵相同：&quot;)</span><br><span class="line">print(A_)</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<blockquote>
<p>原始矩阵:<br>[[ 4  5  6]<br>[ 4  5  6]<br>[ 8 10 12]<br>[ 4  5  6]]<br>分解后的原始U，S，V：<br>[[-3.77964473e-01 -9.25820100e-01  1.52792960e-16  0.00000000e+00]<br>[-3.77964473e-01  1.54303350e-01  9.12870929e-01  0.00000000e+00]<br>[-7.55928946e-01  3.08606700e-01 -3.65148372e-01 -4.47213595e-01]<br>[-3.77964473e-01  1.54303350e-01 -1.82574186e-01  8.94427191e-01]]<br>[[2.32163735e+01 0.00000000e+00 0.00000000e+00]<br>[0.00000000e+00 1.22295087e-15 0.00000000e+00]<br>[0.00000000e+00 0.00000000e+00 5.18334466e-32]<br>[0.00000000e+00 0.00000000e+00 0.00000000e+00]]<br>[[-0.45584231 -0.56980288 -0.68376346]<br>[ 0.02454097  0.75988299 -0.64959647]<br>[-0.88972217  0.31289378  0.3324033 ]]<br>[[ 4.  5.  6.]<br>[ 4.  5.  6.]<br>[ 8. 10. 12.]<br>[ 4.  5.  6.]]<br>降维后的U，S，V：<br>[[-0.37796447]<br>[-0.37796447]<br>[-0.75592895]<br>[-0.37796447]]<br>[[23.21637353]]<br>[[-0.45584231 -0.56980288 -0.68376346]]<br>降维后的恢复矩阵，与原矩阵相同：<br>[[ 4.  5.  6.]<br>[ 4.  5.  6.]<br>[ 8. 10. 12.]<br>[ 4.  5.  6.]]</p>
</blockquote>
</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——线性规划的直观理解</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<p><em>本文是对线性规划的直观理解，不严谨，后续有新的问题/理解持续更新</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<ul>
<li>参考链接：<ul>
<li><a href="https://www.zhihu.com/question/27471863/answer/123244103" target="_blank" rel="noopener">运筹学中应该如何理解互补松弛性。这条性质又该如何运用?</a></li>
<li><a href="https://github.com/Operations-Research-Science/Ebook-Linear_Programming/blob/master/docs/chapter4/chapter4.md" target="_blank" rel="noopener">第4章 对偶理论和敏感度分析</a></li>
<li><a href="https://www.zhihu.com/question/426396827/answer/2052511472" target="_blank" rel="noopener">线性规划对偶问题的定义，有什么直觉上的解释吗？</a>：原始问题到对偶问题最好的一种很简洁的解释</li>
</ul>
</li>
</ul>
<h3 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h3><ul>
<li>问题描述：<ul>
<li>假设你是一个木匠有200单位的木头和90单位的时间</li>
<li>木匠可以制作桌子或者椅子<ul>
<li>桌子成本为5单位木头+2单位时间，售价10元</li>
<li>椅子成本为2单位木头+1单位时间，售价3元</li>
</ul>
</li>
</ul>
</li>
<li>目标：在已有资源情况下，最大化收入，应该生产多少桌子和椅子？</li>
<li>问题形式化描述:<ul>
<li>假设应该生产\(x_1\)把桌子和\(x_1\)把椅子<br>$$<br>\begin{align}<br>\max \ \ 10x_1 &amp;+ 3x_2 \\<br>5x_1 + 2x_2 &amp;&lt;= 200 \\<br>3x_1 + \ \  x_2 &amp;&lt;= 90 \\<br>x_1,x_2 &amp;&gt;= 0 \\<br>\end{align}<br>$$</li>
</ul>
</li>
<li>作图法可求得最优解为\(x_1^* = 30, x_2^*=0\)，此时最大收益为300<ul>
<li>在二维坐标轴上先画出可行域，然后按照目标直线斜率找到最优点</li>
</ul>
</li>
</ul>
<h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><ul>
<li>对偶问题描述：<ul>
<li>上述原始问题可以换一个视角看</li>
<li>假设现在你是一个原材料收购商（想要以最低价格收购木匠的原材料）</li>
<li>目标：对单位木头和单位时间进行出价，以最低的价格买完木匠的资源（假设木匠愿意卖出的前提是收购上出价的最小值不小于木匠原始问题中收益的最大值）<ul>
<li>实际上最好是刚好等于木匠原始问题的最大收益</li>
</ul>
</li>
</ul>
</li>
<li>对偶问题形式化描述<br>$$<br>\begin{align}<br>\min \ \ 200p_1 &amp;+ 90p_2 \quad  – 总付款 \\<br>5p_1 + 3p_2 &amp;&gt;= 10 \quad    – 一张桌子的资源售价不低于一张桌子的收益 \\<br>2p_1 + \ \  p_2 &amp;&gt;= 3 \quad   – 一张椅子的资源售价不低于一张椅子的收益 \\<br>p_1,p_2 &amp;&gt;= 0 \quad     – 售价不为负数 \\<br>\end{align}<br>$$</li>
<li>其中\(p_1, p_2\)分别称为单位木头和单位时间的<strong>影子价格</strong></li>
<li>作图法可求得最优解为\(p_1^* = 0, p_2^* = 3.3\)，此时最小支付金额为300</li>
</ul>
<h3 id="互补松弛定理的理解"><a href="#互补松弛定理的理解" class="headerlink" title="互补松弛定理的理解"></a>互补松弛定理的理解</h3><h4 id="从原始问题的约束视角出发"><a href="#从原始问题的约束视角出发" class="headerlink" title="从原始问题的约束视角出发"></a>从原始问题的约束视角出发</h4><p><em>等价于从对偶问题的解出发</em></p>
<ul>
<li>对偶问题中，最优解是\(p_1^* = 0, p_2^* = 3.3\)<ul>
<li>\(p_1^* = 0\)意味着我们的木材过量了，其实不需要这么多木材，原始问题中，最优解对应的木材约束是松的（\(5x_1^* + 2x_2^*=150 &lt; 200\)）</li>
<li>\(p_2^* = 3.3\)说明时间资源非常紧俏，原始问题中，最优解对应的时间约束是紧的（\(3x_1^* + \ \  x_2^* = 90\)）</li>
</ul>
</li>
<li>对应互补松弛的含义：<ul>
<li>如果在最优条件下一个约束不等式是松的（木材），那么这个约束对应的影子价格为0。</li>
<li>反过来说，如果某个约束对应的影子价格严格大于0，那么这个约束不等式一定是紧的。</li>
<li>总的来说，原始问题的约束和对偶问题变量(影子价格)总有一个要为0</li>
</ul>
</li>
</ul>
<h4 id="从对偶问题的约束视角出发"><a href="#从对偶问题的约束视角出发" class="headerlink" title="从对偶问题的约束视角出发"></a>从对偶问题的约束视角出发</h4><p><em>等价于从原始问题的解出发</em></p>
<ul>
<li>原始问题中，最优解是\(x_1^* = 30, x_2^*=0\)<ul>
<li>\(x_1^* = 30\)意味着桌子非常合算，应该多生产桌子，对偶问题中，桌子约束是紧的（\(5p_1^* + 3p_2^* = 10\)）</li>
<li>\(x_2^*=0\)以为这椅子不合算，不应该生产椅子，对偶问题中，椅子的约束是松的（\(2p_1^* + \ \  p_2^* = 3.3 &gt; 3\)）</li>
</ul>
</li>
<li>补充互补松弛的含义：<ul>
<li>如果在对偶最优条件下一个约束不等式是松的（椅子），那么这个约束对应的原始问题变量最优解（\(x_2^*\)）为0。</li>
<li>反过来说，如果某个原始问题变量（桌子）对应的解（\(x_1^*\)）严格大于0，那么对偶问题中这个约束不等式一定是紧的。</li>
<li>总的来说，对偶问题的约束和对应原始问题变量总有一个要为0</li>
</ul>
</li>
</ul>
<h4 id="互补松弛定理的公式化"><a href="#互补松弛定理的公式化" class="headerlink" title="互补松弛定理的公式化"></a>互补松弛定理的公式化</h4><p>$$<br>(5p_1^* + 3p_2^* - 10)x_1^* = 0 \\<br>(2p_1^* + p_2^* - 3)x_2^* = 0 \\<br>(5x_1^* + 2x_2^* - 200)p_1^* = 0 \\<br>(3x_1^* + x_2^* - 90)p_2^* = 0 \\<br>$$</p>
<hr>
<h3 id="其他相关补充"><a href="#其他相关补充" class="headerlink" title="其他相关补充"></a>其他相关补充</h3><p><em>持续更新</em></p>
<h4 id="线性规划相关论文推导"><a href="#线性规划相关论文推导" class="headerlink" title="线性规划相关论文推导"></a>线性规划相关论文推导</h4><ul>
<li><a href="https://www.arvinzyy.cn/2022/06/18/A-Unified-Solution-to-Constrained-Bidding-in-Online-Display-Advertising/" target="_blank" rel="noopener">《A Unified Solution to Constrained Bidding in Online Display Advertising》——论文阅读</a><ul>
<li>这篇文章的约束很多，每个商家都有自己的约束</li>
<li>推导时用到的对偶变换和互补松弛定理均可由本文推导得出【有时间再详细推导】</li>
</ul>
</li>
<li><a href="https://arxiv.org/pdf/1802.08365.pdf" target="_blank" rel="noopener">《Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising》——论文原文</a><ul>
<li>这篇文章中的问题定义比较简单，整体只有一个预算约束<img src="/Notes/Math/Math——线性规划的直观理解/Budget-Constrained-Bidding-Paper-Problem.png"></li>
<li>上述结果详细的推导可以参考：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/532453764" target="_blank" rel="noopener">智能出价——BCB求解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/588970232" target="_blank" rel="noopener">互联网广告算法漫谈——浅谈广告中的出价技术</a>。注意：该参考链接中没有把预算约束相关的互补松弛定理写出来，且2.3中存在一些较为明显的小bug，但整体求解思路和结论没问题</li>
</ul>
</li>
<li>推导结果\(bid = \frac{v_i}{\lambda}\)与常用的方法(RL-MPCA)结果不一致，但可以证明本质是等价的<img src="/Notes/Math/Math——线性规划的直观理解/Budget-Constrained-Bidding-Paper-Derivation.png"></li>
</ul>
</li>
<li>单位置拍卖的CPM计费场景，CPC约束下最大化商家点击量的推导：<ul>
<li>推导过程可参考论文<a href="https://arxiv.org/abs/1905.10928" target="_blank" rel="noopener">Bid Optimization by Multivariable Control in Display Advertising</a></li>
<li>基本推导思路：先通过拉格朗日乘子法得到最优解的形式，再将原始问题转换成对偶问题，进一步分情况讨论得到最终解<img src="/Notes/Math/Math——线性规划的直观理解/CPC-Constrained-Max-Click.png">

</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——运筹优化开源求解器-GLPK的使用</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E8%BF%90%E7%AD%B9%E4%BC%98%E5%8C%96%E5%BC%80%E6%BA%90%E6%B1%82%E8%A7%A3%E5%99%A8-GLPK%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
    <content><![CDATA[<p><em>本文介绍各种运筹优化开源求解器-GLPK的使用</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>GLPK是一款完全开源免费的运筹优化求解器，可以任意商用</li>
</ul>
<h3 id="Ubuntu安装GLPK"><a href="#Ubuntu安装GLPK" class="headerlink" title="Ubuntu安装GLPK"></a>Ubuntu安装GLPK</h3><ul>
<li><p>据说Ubuntu安装较为方便，所以建议首选Ubuntu</p>
</li>
<li><p>在网站下载文件：<a href="https://ftp.gnu.org/gnu/glpk/" target="_blank" rel="noopener">https://ftp.gnu.org/gnu/glpk/</a></p>
<ul>
<li>可以下载任意版本，建议选最新</li>
</ul>
</li>
<li><p>安装命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf glpk-xxx.tar.gz</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装后直接执行可能出现错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">error while loading shared libraries: libglpk.so.36:...</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决方案（<a href="https://groups.google.com/g/humann-users/c/rFtEV6vXZR8?pli=1" target="_blank" rel="noopener">原始解决方案地址</a>）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https://github.com/rstudio/renv/issues/1881</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Ubuntu下GLPK的使用"><a href="#Ubuntu下GLPK的使用" class="headerlink" title="Ubuntu下GLPK的使用"></a>Ubuntu下GLPK的使用</h3><ul>
<li><p>下列式子参考了：<a href="https://www.cnblogs.com/Iambda/archive/2012/12/12/3933510.html" target="_blank" rel="noopener">线性规划工具　GLPK 的安装及基本使用</a></p>
</li>
<li><p>创建问题描述文件<code>glpkDemo.mod</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/* Variables */</span><br><span class="line">var x1 &gt;= 0;</span><br><span class="line">var x2 &gt;= 0;</span><br><span class="line">var x3 &gt;= 0;</span><br><span class="line"></span><br><span class="line">/* Object function */</span><br><span class="line">maximize z: 3*x1 + x2 +2*x3;</span><br><span class="line"></span><br><span class="line">/* Constrains */</span><br><span class="line">s.t. con1: x1 + x2 + 3*x3 &lt;= 30;</span><br><span class="line">s.t. con2: 2*x1 +2*x2 + 5*x3 &lt;= 24;</span><br><span class="line">s.t. con3: 4*x1 + x2 + 2*x3 &lt;= 36;</span><br><span class="line"></span><br><span class="line">end;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行命令解决问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">glpsol -m glpkDemo.mod -o ./output/glpkDemo.sol</span><br></pre></td></tr></table></figure>
</li>
<li><p>输出文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Problem:    glpkDemo</span><br><span class="line">Rows:       4</span><br><span class="line">Columns:    3</span><br><span class="line">Non-zeros:  12</span><br><span class="line">Status:     OPTIMAL</span><br><span class="line">Objective:  z = 28 (MAXimum)</span><br><span class="line"></span><br><span class="line">   No.   Row name   St   Activity     Lower bound   Upper bound    Marginal</span><br><span class="line">------ ------------ -- ------------- ------------- ------------- -------------</span><br><span class="line">     1 z            B             28                             </span><br><span class="line">     2 a            B             12                          30 </span><br><span class="line">     3 b            NU            24                          24      0.166667 </span><br><span class="line">     4 c            NU            36                          36      0.666667 </span><br><span class="line"></span><br><span class="line">   No. Column name  St   Activity     Lower bound   Upper bound    Marginal</span><br><span class="line">------ ------------ -- ------------- ------------- ------------- -------------</span><br><span class="line">     1 x1           B              8             0               </span><br><span class="line">     2 x2           B              4             0               </span><br><span class="line">     3 x3           NL             0             0                   -0.166667 </span><br><span class="line"></span><br><span class="line">Karush-Kuhn-Tucker optimality conditions:</span><br><span class="line"></span><br><span class="line">KKT.PE: max.abs.err = 0.00e+00 on row 0</span><br><span class="line">        max.rel.err = 0.00e+00 on row 0</span><br><span class="line">        High quality</span><br><span class="line"></span><br><span class="line">KKT.PB: max.abs.err = 0.00e+00 on row 0</span><br><span class="line">        max.rel.err = 0.00e+00 on row 0</span><br><span class="line">        High quality</span><br><span class="line"></span><br><span class="line">KKT.DE: max.abs.err = 2.22e-16 on column 1</span><br><span class="line">        max.rel.err = 3.17e-17 on column 1</span><br><span class="line">        High quality</span><br><span class="line"></span><br><span class="line">KKT.DB: max.abs.err = 0.00e+00 on row 0</span><br><span class="line">        max.rel.err = 0.00e+00 on row 0</span><br><span class="line">        High quality</span><br><span class="line"></span><br><span class="line">End of output</span><br></pre></td></tr></table></figure>
</li>
<li><p>Activity这一列就是想要的解</p>
</li>
<li><p>其他输出项如何理解？</p>
</li>
</ul>
<h3 id="自动化生成问题"><a href="#自动化生成问题" class="headerlink" title="自动化生成问题"></a>自动化生成问题</h3><ul>
<li>使用shell或者Python自动生成<code>.mod</code>文件，然后自然解析<code>.sol</code>文件，实现自动化测试参数</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——统计学中的Bootstrap方法</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%AD%E7%9A%84Bootstrap%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<ul>
<li><p>参考链接：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/24851814" target="_blank" rel="noopener">【机器学习】Bootstrap详解 - 文兄的文章 - 知</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/107978939" target="_blank" rel="noopener">Bootstrapping算法（附python代码） - 商胜彭的文章 - 知乎</a></li>
</ul>
</li>
<li><p>Bootstrap: 原意为，独自创立；靠一己之力做成，在统计学中特指通过多批次重复采样估计统计量的方法</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——调和级数的和</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E8%B0%83%E5%92%8C%E7%BA%A7%E6%95%B0%E7%9A%84%E5%92%8C.html</url>
    <content><![CDATA[<p><em>调和级数的和是发散的,并不收敛,但是这又是一个非常常用的级数,本文总结了调和级数的和的求法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="一个事实"><a href="#一个事实" class="headerlink" title="一个事实"></a>一个事实</h3><ul>
<li>调和级数的和与n的自然对数的差值收敛到欧拉-马歇罗尼常数<br>$$<br>\begin{align}<br>lim_{n \to \infty}(\sum_{i=1}^{n}\frac{1}{i} - ln(n)) = \gamma<br>\end{align}<br>$$<ul>
<li>\(\gamma\)为欧拉-马歇罗尼常数<ul>
<li>(欧拉常数又称欧拉-马斯克若尼常数，近似值为γ≈0.57721 56649 01532 86060 65120 90082 40243 10421 59335)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><ul>
<li>事实上,调和级数的和为<br>$$<br>\begin{align}<br>\sum_{i=1}^{n}\frac{1}{i} = ln(n) + \gamma + \epsilon_{n}<br>\end{align}<br>$$<ul>
<li>\(\gamma\)为欧拉-马歇罗尼常数</li>
<li>\(\epsilon_{n}\)约等于\(\frac{1}{2n}\),随着n的不断增大,\(\epsilon_{n}\)趋于0</li>
</ul>
</li>
</ul>
<hr>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><ul>
<li>两个不同的调和数之间的差值永远不是整数</li>
<li>除了n=1时以外,没有任何一个调和数是整数</li>
</ul>
]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Math——样本均值方差和总体均值方差的关系</title>
    <url>/Notes/Math/Math%E2%80%94%E2%80%94%E6%A0%B7%E6%9C%AC%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E5%92%8C%E6%80%BB%E4%BD%93%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%9A%84%E5%85%B3%E7%B3%BB.html</url>
    <content><![CDATA[<p><em>本文介绍随机变量样本均值方差和整体均值方差的关系，同时还介绍样本和均值的方差和总体方差的关系</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>其他参考链接：<a href="https://www.zhihu.com/question/357835857/answer/2728936255" target="_blank" rel="noopener">在统计学里如何理解样本均值的方差等于总体方差➗n? - 蘇雲的回答 - 知乎</a></li>
</ul>
<h3 id="样本均值的方差与总体方差的关系"><a href="#样本均值的方差与总体方差的关系" class="headerlink" title="样本均值的方差与总体方差的关系"></a>样本均值的方差与总体方差的关系</h3><!-- * 详细推导如下，有时间再重新用文字梳理一下
<img src="/Notes/Math/Math——样本均值方差和总体均值方差的关系/var-of-mean-of-samples-vs-var-of-population.png"> -->
<ul>
<li>其他证明方式：<img src="/Notes/Math/Math——样本均值方差和总体均值方差的关系/var-of-mean-of-samples-vs-var-of-population-v2.png">
<ul>
<li>上式中，如果总体方差未知，想要用样本方差来作为总体方差的无偏估计，则样本方差的定义是应该是 \(S^2 = \frac{1}{n-1}\sum_i^n(X_i-\bar{X})^2\)，（此时样本方差是总体方差的无偏估计）</li>
</ul>
</li>
</ul>
<h3 id="样本方差为什么要除以n-1"><a href="#样本方差为什么要除以n-1" class="headerlink" title="样本方差为什么要除以n-1?"></a>样本方差为什么要除以n-1?</h3><ul>
<li>样本方差的定义：<br>$$<br>\sigma^2 \approx S^2 = \frac{1}{n-1}\sum_i^n(X_i-\bar{X})^2<br>$$</li>
<li>为什么样本方差是乘以\(\frac{1}{n-1}\)而不是\(\frac{1}{n}\)？<ul>
<li>因为这样使用\(\frac{1}{n}\)会低估总体方差，此时样本方差不是总体方差的无偏估计</li>
<li>样本方差低估了总体方差的原因是因为从总体里面抽出来的数据会更倾向于集中，极端情况下，一个样本对应的方差为0</li>
</ul>
</li>
<li>换个视角想，是因为我们不知道总体的均值，所以计算方差时使用的均值也是从样本中求平均得到的，这使得我们基于该均值得到的方差不够离散（因为使用了样本均值，所以自由度需要减一），也就是低估了总体方差<ul>
<li>怎么理解自由度？<ul>
<li>采样一个样本以后无法计算方差，此时方差为0，因为此时均值就等于样本本身，此时自由度为0</li>
<li>采样两个样本以后得到的方差只根第二个样本到第一个样本的距离有关（样本均值与这两个样本强相关），此时自由度为1</li>
<li>当采样的样本数非常多（假设为n）时，实际上单个样本与均值的关系很小了，此时自由度为n-1</li>
</ul>
</li>
</ul>
</li>
<li>当已知总体均值\(\mu\)时（个人理解：这里的\(\mu\)可以是其他采样方式下获得的近似均值，只要不跟当前用于计算方差的样本相关即可），样本方差可以使用：<br>$$<br>\sigma^2 \approx S^2 = \frac{1}{n-1}\sum_i^n(X_i-\mu)^2<br>$$</li>
<li>样本方差经过\(\frac{1}{n-1}\)修正以后可以用来估计总体方差（修正以后是总体方差的无偏估计）<ul>
<li>这个修正叫做贝塞尔修正</li>
</ul>
</li>
</ul>
<!-- * 证明方式1：
<img src="/Notes/Math/Math——样本均值方差和总体均值方差的关系/var-of-sampels-vs-var-of-population.png">
<pre><code>* 最后那里使用约等于符号的原因是在n不够大的时候，用样本估计总体总是估计不准确的（但是经过修正的样本方差是总体方差的无偏估计），所以用了约等于</code></pre>
<ul>
<li>证明方式2：<a href="https://zhuanlan.zhihu.com/p/315504747">统计学——样本方差和总体方差 - 我是一双烂胶鞋的文章 - 知乎</a><img src="/Notes/Math/Math——样本均值方差和总体均值方差的关系/var-of-sampels-vs-var-of-population-v2.png"> -->
<li>证明 from ：<a href="https://www.bilibili.com/video/BV1pq4y1p7nu/?vd_source=2679bd500199d28bea912c94f550dd3c" target="_blank" rel="noopener">Bilibili-样本方差为什么除以n-1?</a><img src="/Notes/Math/Math——样本均值方差和总体均值方差的关系/var-of-sampels-vs-var-of-population-v3-aux.png">
<img src="/Notes/Math/Math——样本均值方差和总体均值方差的关系/var-of-sampels-vs-var-of-population-v3.png">

</li>

]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>控制系统——PID相关</title>
    <url>/Notes/Others/%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F%E2%80%94%E2%80%94PID%E7%9B%B8%E5%85%B3.html</url>
    <content><![CDATA[<p><em>PID相关笔记</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="PID相关参考"><a href="#PID相关参考" class="headerlink" title="PID相关参考"></a>PID相关参考</h3><ul>
<li>一个简短的PID描述：<a href="https://www.bilibili.com/video/BV1GD4y1x7bV" target="_blank" rel="noopener">PID控制原理，看了开头，你就会看到结尾！</a><ul>
<li>包含修改参数带来的系统输出变化可视化示例</li>
</ul>
</li>
<li>非常详细的讲解：<a href="https://www.bilibili.com/video/BV1B54y1V7hp" target="_blank" rel="noopener">从不懂到会用！PID从理论到实践~</a></li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>熵——机器学习中各种熵的总结</title>
    <url>/Notes/Others/%E7%86%B5%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%90%84%E7%A7%8D%E7%86%B5%E7%9A%84%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>机器学习中的熵一般包括信息熵,条件熵,交叉熵几种</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><h4 id="为什么信息熵中有log"><a href="#为什么信息熵中有log" class="headerlink" title="为什么信息熵中有log?"></a>为什么信息熵中有log?</h4><p><em>参考博客:<a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">https://www.cnblogs.com/kyrieng/p/8694705.html</a></em></p>
<h5 id="自信息-I-x"><a href="#自信息-I-x" class="headerlink" title="自信息\(I(x)\)"></a>自信息\(I(x)\)</h5><ul>
<li>一条信息的信息量大小和它的不确定性有直接的关系, 信息量的度量就等于不确定性的多少</li>
<li>考虑一个离散随机变量\(x\),随机变量的信息量由概率分布\(p(x)\)确定</li>
<li>我们的目标是找到一个函数\(I(x) = f(p(x))\)(自变量是\(p(x)\),因为随机变量的信息量由概率分布确定),能够衡量\(x\)信息内容的多少<ul>
<li>且必须是概率分布\(p(x)\)的单调函数?</li>
</ul>
</li>
<li>现在考虑同时出现两个相互独立的随机变量\(x\)和\(y\),那么他们的信息量和应该是<br>$$I(x,y) = I(x)+I(y) = f(p(x)) + f(p(y))$$<ul>
<li>同时他们的联合概率分布为<br>$$p(x,y) = p(x)p(y)$$</li>
</ul>
</li>
<li>由上面的两个表达式可以知道\(f(\cdot)\)应该是一个对数相关的函数,因为<br>$$log_{a}(MN) = log_{a}M + log_{a}N$$</li>
<li>所以我们可以考虑把原始信息量定义为<br>$$I(x) = f(p(x)) = -log p(x)$$<ul>
<li>其中负号是用来保证信息量是正数或者零</li>
<li>log 函数基的选择是任意的（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特nats）</li>
</ul>
</li>
<li>\(I(x)\)也被称为随机变量\(x\)的自信息(self-information)，描述的是随机变量的某个事件发生所带来的信息量<img src="/Notes/Others/熵——机器学习中各种熵的总结/self-information.png">

</li>
</ul>
<h5 id="信息熵-H-x"><a href="#信息熵-H-x" class="headerlink" title="信息熵\(H(x)\)"></a>信息熵\(H(x)\)</h5><ul>
<li>现在假设一个发送者想传送一个随机变量的值给接收者</li>
<li>那么在这个过程中，他们传输的平均信息量可以通过求 \(I(x)=−logp(x)\)关于概率分布\(p(x)\)的期望得到:<br>$$<br>\begin{align}<br>H(x) &amp;= \sum_{x}p(x)(-logp(x)) \\<br>&amp;= -\sum_{x}p(x)logp(x) \\<br>&amp;= -\sum_{i=1}^{n}p(x_{i})logp(x_{i})<br>\end{align}<br>$$</li>
<li>\(H(X)\)就被称为随机变量\(x\)的熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望</li>
<li>从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大,当随机分布为均匀分布时，熵最大，且<br>$$0 ≤ H(X) ≤ logn$$<ul>
<li>证明<br>$$p(1)+p(2)+\dots+p(n)=1$$</li>
<li>目标函数<br>$$f(p(1),p(2),\dots,p(n))=-(p(1)logp(1)+p(2)logp(2)+\dots+p(n)logp(n))$$</li>
<li>约束条件<br>$$g(p(1),p(2),\dots,p(n),\lambda)=p(1)+p(2)+\dots+p(n)-1=0$$</li>
<li>定义拉格朗日函数<br>$$<br>L(p(1),p(2),\dots,p(n),\lambda)=-(p(1)logp(1)+p(2)logp(2)+\dots+p(n)logp(n))+\lambda(p(1)+p(2)+\dots+p(n)-1)<br>$$</li>
<li>对\(p(i)\)和\(\lambda\)求偏导,并令其等于0得<br>$$p(1)=p(2)=\dots=p(n)=\displaystyle\frac{1}{n}$$</li>
<li>带入目标函数得<br>$$<br>\begin{align}<br>f(\displaystyle\frac{1}{n},\frac{1}{n},\dots,\frac{1}{n}) &amp;= -(\frac{1}{n}log\frac{1}{n} + \frac{1}{n}log\frac{1}{n} + \dots + \frac{1}{n}log\frac{1}{n}) \\<br>&amp; = -log(\frac{1}{n}) \\<br>&amp; = log(n)<br>\end{align}<br>$$</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>趣味题——寻找最优者</title>
    <url>/Notes/Others/%E8%B6%A3%E5%91%B3%E9%A2%98%E2%80%94%E2%80%94%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BC%98%E8%80%85.html</url>
    <content><![CDATA[<p><em>寻找最优者：不可逆的做出选择，如何使得选到最优者的概率最大</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><ul>
<li>由于无人知道缘分何时到来,假设人的一生会遇到100个异性,这100个人参差不齐,其中有个人是最好的,100人随机的排着队出现,而我们对每个人必须决定是否接受,而且一旦决定后不能修改,那么用什么策略选择更有机会选到最好的那一个呢?</li>
</ul>
<hr>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="策略设想"><a href="#策略设想" class="headerlink" title="策略设想"></a>策略设想</h4><ul>
<li>拒绝前k个人,然后从第k+1个开始,只要<strong>优于前面的所有人</strong>,则接受</li>
<li>k值的设定比较难,太小了容易找到不好的,太多了容易错过最好的</li>
</ul>
<h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><ul>
<li><p>假设最优者出现在第\(i(k&lt; i \leq n)\)个(事件A),那么最优者被选中(事件B)的概率为<strong>前i-1个中的最优者在前k个人中的概率</strong><br>$$ P(B|A) = \frac{k}{i-1} $$</p>
<ul>
<li>\(P(B|A) = P(最优者被选中|第i个为最优者)\)</li>
</ul>
</li>
<li><p>最优者出现在第\(i\)个的概率为<br>$$P(B) = \frac{1}{n}$$</p>
</li>
<li><p>最优者被选中的概率为</p>
</li>
<li><p>接下来的证明用到了调和级数的和,详情参考<a href="/Notes/Others/Math%E2%80%94%E2%80%94%E8%B0%83%E5%92%8C%E7%BA%A7%E6%95%B0%E7%9A%84%E5%92%8C.html">调和级数的和</a><br>$$<br>\begin{align}<br>\sum_{i=1}^{n}\frac{1}{i} = ln(n) + \gamma + \epsilon_{n}<br>\end{align}<br>$$</p>
<ul>
<li>\(\gamma\)为欧拉-马歇罗尼常数,近似值为γ≈0.57721,下面的推导都不需要精确解的,所以可以使用该公式</li>
<li>\(\epsilon_{n}\)约等于\(\frac{1}{2n}\),随着n的不断增大,\(\epsilon_{n}\)趋于0<br>$$<br>\begin{align}<br>P(B) &amp;= \sum_{A}P(B,A) \\<br>&amp;= \sum_{A}P(B|A)P(A) \\<br>&amp;= \sum_{i=k+1}^{n}\frac{k}{i-1}\frac{1}{n} \\<br>&amp;= \sum_{i=k+1}^{n}\frac{k}{n}\frac{1}{i-1} \\<br>&amp;= \frac{k}{n}\sum_{i=k+1}^{n}\frac{1}{i-1} \\<br>&amp;= \frac{k}{n}\sum_{i=k}^{n-1}\frac{1}{i} \\<br>&amp;= \frac{k}{n}(\sum_{i=1}^{n-1}\frac{1}{i} - \sum_{i=1}^{k-1}\frac{1}{i}) \\<br>&amp;\approx \frac{k}{n}((ln(n-1)+C)-(ln(k-1)+C)) \\<br>&amp;= \frac{k}{n}ln(\frac{n-1}{k-1}) \\<br>\end{align}<br>$$</li>
</ul>
</li>
<li><p>当n和k足够大时有</p>
<ul>
<li>(<em>不能证明k也会足够大,但是直觉上n足够大k也会足够大,因为n足够大时往往需要拒绝更多的人</em>)<br>$$\frac{n-1}{k-1} = \frac{n}{k}$$</li>
</ul>
</li>
<li><p>令\(x=\frac{k}{n}\),则有<br>$$<br>\begin{align}<br>f(x) &amp;= xln\frac{1}{x} \\<br>{f}’(x) &amp;= ln(\frac{1}{x})+(-\frac{1}{x^2}\cdot x \cdot x) \\<br>&amp;= ln(\frac{1}{x})-1<br>\end{align}<br>$$</p>
</li>
<li><p>求\(f(x)\)的最大值,令\({f}’(x)=0\)有<br>$$ ln(\frac{1}{x})-1 = 0 $$</p>
</li>
<li><p>解方程得<br>$$ x = \frac{1}{e} $$</p>
</li>
<li><p>也就是说当\(\frac{k}{n} = \frac{1}{e} \)时,找到最优者的概率最大</p>
</li>
</ul>
<h4 id="策略描述"><a href="#策略描述" class="headerlink" title="策略描述"></a>策略描述</h4><ul>
<li>拒绝前\(\frac{n}{e}\)个人(也就是拒绝前37%的人),然后从第\(\frac{n}{e}+1\)个开始,只要<strong>优于前面的所有人</strong>,则接受</li>
<li>这种方式可以保证我们有最大的概率找到最优者</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>趣味题——优惠券收集问题</title>
    <url>/Notes/Others/%E8%B6%A3%E5%91%B3%E9%A2%98%E2%80%94%E2%80%94%E4%BC%98%E6%83%A0%E5%88%B8%E6%94%B6%E9%9B%86%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<hr>
<h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><ul>
<li>餐馆有12生肖优惠券,每天给小明等概率随机发放一张,问: 小明集齐12中生肖优惠券的天数</li>
<li>相关延伸: 掷骰子, 要求每个面都出现一次, 求投掷次数的期望</li>
</ul>
<hr>
<h3 id="问题求解"><a href="#问题求解" class="headerlink" title="问题求解"></a>问题求解</h3><ul>
<li><p>思路: </p>
<ul>
<li>把天数\(X\)这个随机变量变成12个随机变量的和$$X = \sum_{i=1}^{12}x_{i}$$</li>
<li>于是可以把期望分解为12个随机变量的期望和$$E(X) = \sum_{i=1}^{12}E(x_{i})$$</li>
</ul>
</li>
<li><p>第一种优惠券(12种中的任意一种均可):</p>
<ul>
<li>第一天随机拿到一个优惠券一定能够对应其中某一个生肖,概率为\(p_{1} = 1\)</li>
<li>期望天数\(E(x_{1})\)与概率的关系为\(E(x_{1})\cdot p_{1} = 1\)<ul>
<li>一个直观的说明,为什么\(E(x_{1})\cdot p_{1} = 1\):由于每一天成功的概率为\(p_{1}\),不成功概率为\(1-p_{1}\),显然为二项分布,由<strong>二项分布的期望应该为1次</strong>,即\(np_{1}=1\),其中n就是我们天数的期望(注意:二项分布的期望与这里的天数期望不同,后者为使得二项分布期望为1所需要的<strong>天数</strong>)</li>
<li>上面的证明不够严谨,<strong>严谨的数学证明</strong>如下:<ul>
<li>假设期望天数是 \(E\), 每天成功的概率为 \(p\), 下面我们求成功和不成功的概率分布</li>
<li>第一天成功的概率为 \(p\), 成功时只需要 \(1\) 天即可, 即有 \(p\) 的概率需要 \(1\)天</li>
<li>第一天没成功的概率为 \(1-p\), 不成功时需要将当前日子(\(1\)天)算上,然后回到原点,重新开始,即有 \(1-p\) 的概率需要 \(1+E\) 天</li>
<li>期望公式<br>$$E = p*1 + (1-p)(1+E)$$</li>
<li>化简为:<br>$$pE = 1$$ </li>
</ul>
</li>
</ul>
</li>
<li>需要天数的期望为$$E(x_{1}) = \frac{1}{p_{1}} = 1$$</li>
</ul>
</li>
<li><p>第二种优惠券(11种中的任意一种均可): </p>
<ul>
<li>每天随机收到的优惠券有\(p_{2} = \frac{11}{12}\)的概率满足剩余的11个生肖中的一个</li>
<li>期望天数\(E(x_{2})\)与概率的关系为\(x_{2}\cdot p_{2} = 1\)</li>
<li>需要的天数期望为$$E(x_{2}) = \frac{1}{p_{2}} = \frac{12}{11}$$</li>
</ul>
</li>
<li><p>…</p>
</li>
<li><p>以此类推可得:<br>$$<br>\begin{align}<br>E(X) &amp;= \sum_{i=1}^{n}E(x_{i}) \\<br>&amp;= \sum_{i=0}^{11}\frac{12}{12-i} \\<br>&amp;= \sum_{i=1}^{12}\frac{12}{i} \\<br>&amp;= 12\sum_{i=1}^{12}\frac{1}{i} \\<br>\end{align}<br>$$</p>
</li>
<li><p>由于调和级数的和与n的自然对数有关系,详情参考<a href="/Notes/Others/Math%E2%80%94%E2%80%94%E8%B0%83%E5%92%8C%E7%BA%A7%E6%95%B0%E7%9A%84%E5%92%8C.html">调和级数的和</a><br>$$<br>\begin{align}<br>\sum_{i=1}^{n}\frac{1}{i} = ln(n) + \gamma + \epsilon_{n}<br>\end{align}<br>$$</p>
<ul>
<li>\(\gamma\)为欧拉-马歇罗尼常数,近似值为γ≈0.57721</li>
<li>\(\epsilon_{n}\)约等于\(\frac{1}{2n}\),随着n的不断增大,\(\epsilon_{n}\)趋于0</li>
</ul>
</li>
<li><p>所以有<br>$$<br>\begin{align}<br>E(X) &amp;= 12\sum_{i=1}^{12}\frac{1}{i} \\<br>&amp;\approx 12(ln(12)+\gamma+\frac{1}{24})<br>\end{align}<br>$$</p>
</li>
<li><p>将12生肖推广到n,有<br>$$<br>\begin{align}<br>E(X) &amp;= n\sum_{i=1}^{n}\frac{1}{i} \\<br>&amp;\approx n(ln(n)+\gamma+\frac{1}{2n}) \\<br>&amp;\approx nln(n)<br>\end{align}<br>$$</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>趣味题——绳子剪三段后能构成三角形的概率</title>
    <url>/Notes/Others/%E8%B6%A3%E5%91%B3%E9%A2%98%E2%80%94%E2%80%94%E7%BB%B3%E5%AD%90%E5%89%AA%E4%B8%89%E6%AE%B5%E5%90%8E%E8%83%BD%E6%9E%84%E6%88%90%E4%B8%89%E8%A7%92%E5%BD%A2%E7%9A%84%E6%A6%82%E7%8E%87.html</url>
    <content><![CDATA[<p><em>初始题目为1米长的绳子剪成三段后能够构成三角形的概率，实际上绳子的长度可以是任意的，解法都一样</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><ul>
<li>一根绳子长度为n，剪三段后能构成三角形的概率是多少？</li>
<li>由于长度与概率无关，所以接下来我们把绳子当做长度为1米来处理</li>
</ul>
<hr>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul>
<li><p>三段绳子的长度分别为x,y,1-x-y</p>
</li>
<li><p>首先需要满足<br>$$<br>\begin{align}<br>x &amp;&gt; 0 \\<br>y &amp;&gt; 0\\<br>1-x-y &amp;&gt; 0 \\<br>\end{align}<br>$$</p>
<ul>
<li>以x为横坐标，y为纵坐标画图得到阴影区域面积为\(\frac{1}{2}\)</li>
</ul>
</li>
<li><p>图片来源于：<a href="https://blog.csdn.net/fanoluo/article/details/40374571" target="_blank" rel="noopener">https://blog.csdn.net/fanoluo/article/details/40374571</a>*</p>
<img src="/Notes/Others/趣味题——绳子剪三段后能构成三角形的概率/original.png">
</li>
<li><p>进一步分析三段绳子能生成组成三角形的概率<br>$$<br>\begin{align}<br>x+y  &amp;&gt; 1-x-y \\<br>y + 1-x-y &amp;&gt; x\\<br>x + 1-x-y &amp;&gt; y \\<br>\end{align}<br>$$</p>
<ul>
<li>进一步分析得到阴影部分面积为\(\frac{1}{8}\)</li>
</ul>
</li>
<li><p>图片来源于：<a href="https://blog.csdn.net/fanoluo/article/details/40374571" target="_blank" rel="noopener">https://blog.csdn.net/fanoluo/article/details/40374571</a>*</p>
<img src="/Notes/Others/趣味题——绳子剪三段后能构成三角形的概率/triangle.png"></li>
<li><p>所以有<br>$$P = \frac{1/8}{1/2} = \frac{1}{4}$$</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——做算法题时应该注意的细节</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%81%9A%E7%AE%97%E6%B3%95%E9%A2%98%E6%97%B6%E5%BA%94%E8%AF%A5%E6%B3%A8%E6%84%8F%E7%9A%84%E7%BB%86%E8%8A%82.html</url>
    <content><![CDATA[<p><em>和Java类似,Python实现了丰富的基本数据结构,但是使用方式和Java有所不同</em></p>
<hr>
<h3 id="字符类型"><a href="#字符类型" class="headerlink" title="字符类型"></a>字符类型</h3><ul>
<li>Python中没有Java以及C++一样的字符类型,只有字符串类型</li>
</ul>
<h4 id="字符和数字的比较"><a href="#字符和数字的比较" class="headerlink" title="字符和数字的比较"></a>字符和数字的比较</h4><ul>
<li><code>&#39;a&#39;</code>表示的不是字符’a’, 而是字符串”a”,也就是说<code>0 &lt; &#39;a&#39;</code>这样的写法是不可以的,因为<code>&#39;a&#39;</code>不是一个数字</li>
<li>若非要和数字比较,方式如下:<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">string = &quot;12_abc*ABZ-=&quot;</span><br><span class="line">alphanumeric_str = &quot;&quot;</span><br><span class="line">for char in string:</span><br><span class="line">    if (ord(char) &gt;= ord(&apos;0&apos;) and ord(char) &lt;= ord(&apos;9&apos;))\</span><br><span class="line">            or (ord(char) &gt;= ord(&apos;a&apos;) and ord(char) &lt;= ord(&apos;z&apos;))\</span><br><span class="line">            or (ord(char) &gt;= ord(&apos;A&apos;) and ord(char) &lt;= ord(&apos;Z&apos;)):</span><br><span class="line">        alphanumeric_str += char</span><br><span class="line">print(alphanumeric_str)</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">12abcABZ</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="集合类型的复制"><a href="#集合类型的复制" class="headerlink" title="集合类型的复制"></a>集合类型的复制</h3><h4 id="浅拷贝"><a href="#浅拷贝" class="headerlink" title="浅拷贝"></a>浅拷贝</h4><p><em>以list为例</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1, 2, 3]</span><br><span class="line"># method 1</span><br><span class="line">lcopy = l[:] # notice: ls.append(l[:]), 添加的是副本!!!</span><br><span class="line"># method 2</span><br><span class="line">lcopy = l.copy()</span><br><span class="line"># method 3</span><br><span class="line">import copy</span><br><span class="line">lcopy = copy.copy(l)</span><br></pre></td></tr></table></figure>

<h4 id="深拷贝"><a href="#深拷贝" class="headerlink" title="深拷贝"></a>深拷贝</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line"></span><br><span class="line">l = [1, 2, 3, [4, 5, 6]]</span><br><span class="line">l = copy.deepcopy(l)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="format函数的使用"><a href="#format函数的使用" class="headerlink" title="format函数的使用"></a>format函数的使用</h3><ul>
<li><p>format是用于格化式字符串的函数</p>
</li>
<li><p>format是字符串自带的函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># format string</span><br><span class="line">print &quot;&#123;&#125; &#123;&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;)</span><br><span class="line">print &quot;&#123;0&#125; &#123;1&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;) </span><br><span class="line">print &quot;&#123;1&#125; &#123;0&#125; &#123;1&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;)</span><br><span class="line"># output</span><br><span class="line">&apos;hello world&apos;</span><br><span class="line">&apos;hello world&apos;</span><br><span class="line">&apos;world hello world&apos;</span><br><span class="line"></span><br><span class="line"># format number</span><br><span class="line">print &quot;&#123;:.2f&#125;&quot;.format(3.1415926)</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他使用方式之后再补充</p>
</li>
</ul>
<hr>
<h3 id="字符串到数字的转换"><a href="#字符串到数字的转换" class="headerlink" title="字符串到数字的转换"></a>字符串到数字的转换</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># string to integer, int(str, base=10)</span><br><span class="line">a = &quot;101&quot;</span><br><span class="line">integer = int(a, 10)</span><br><span class="line">integer = int(a, 2)</span><br><span class="line">integer = int(a, 7)</span><br><span class="line"></span><br><span class="line"># string to float, float(str)</span><br><span class="line">b = &quot;123.4&quot;</span><br><span class="line">f = float(b)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="进制转换"><a href="#进制转换" class="headerlink" title="进制转换"></a>进制转换</h3><h4 id="转换示例"><a href="#转换示例" class="headerlink" title="转换示例"></a>转换示例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># binary, octonary, hexadecimal to decimal(from string)</span><br><span class="line">dec_num = int(&quot;1001&quot;, 2) # or int(&quot;0b1001&quot;, 2)</span><br><span class="line">dec_num = int(&quot;657&quot;, 8) # or int(&quot;0657&quot;, 8)</span><br><span class="line">dec_num = int(&quot;ff&quot;, 16) # or int(&quot;0xff&quot;, 16)</span><br><span class="line"></span><br><span class="line"># decimal to binary, octonary, hexadecimal(or string)</span><br><span class="line">bin_num = bin(123) # or str(bin(123))</span><br><span class="line">oct_num = oct(123) # or str(oct(123))</span><br><span class="line">hex_num = hex(123) # or str(hex(123))</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h4><ul>
<li>词的表示为十进制(<code>123</code>), 二进制(<code>0b110</code>), 八进制(<code>0567</code>), 十六进制(<code>0xff1</code>)</li>
<li>词的转换函数如下:</li>
</ul>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">十进制</th>
<th align="center">二进制</th>
<th align="center">八进制</th>
<th align="center">十六进制</th>
</tr>
</thead>
<tbody><tr>
<td align="center">表示</td>
<td align="center">123</td>
<td align="center">0b110</td>
<td align="center">0567</td>
<td align="center">0xff1</td>
</tr>
<tr>
<td align="center">函数</td>
<td align="center">int</td>
<td align="center">bin</td>
<td align="center">oct</td>
<td align="center">hex</td>
</tr>
</tbody></table>
<hr>
<h3 id="map函数的使用"><a href="#map函数的使用" class="headerlink" title="map函数的使用"></a>map函数的使用</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li><p>Python3</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map(function, iterable, ...) -&gt; map</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map(function, iterable, ...) -&gt; list</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Python2</span><br><span class="line">print map(lambda x: x**2, [1, 2, 3])</span><br><span class="line"># output:</span><br><span class="line">[1, 4, 9]</span><br><span class="line"></span><br><span class="line">print map(lambda x, y: x+y, [1, 2, 3], [5, 6, 7])</span><br><span class="line"># output:</span><br><span class="line">[6, 8, 10]</span><br><span class="line"></span><br><span class="line"># Python3</span><br><span class="line">result = map(lambda x: x**2, [1, 2, 3])</span><br><span class="line">print(list(result))</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Java——注解</title>
    <url>/Notes/Java/Java%E2%80%94%E2%80%94%E6%B3%A8%E8%A7%A3.html</url>
    <content><![CDATA[<ul>
<li>参考链接: <a href="https://blog.csdn.net/qq1404510094/article/details/80577555" target="_blank" rel="noopener">Java注解-最通俗易懂的讲解</a></li>
<li>一句话: 注解只能修饰实际代码，不会影响执行逻辑，但是其他执行逻辑（一般是框架代码）会读取注解并根据注解进行不同的操作，看起来像是注解影响了代码的执行逻辑</li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java——自定义对象作为Map的Key</title>
    <url>/Notes/Java/Java%E2%80%94%E2%80%94%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%B1%A1%E4%BD%9C%E4%B8%BAMap%E7%9A%84Key.html</url>
    <content><![CDATA[<hr>
<h3 id="自定义的HashKey类"><a href="#自定义的HashKey类" class="headerlink" title="自定义的HashKey类"></a>自定义的HashKey类</h3><h4 id="什么样的自定义类可以用作HashMap的Key？"><a href="#什么样的自定义类可以用作HashMap的Key？" class="headerlink" title="什么样的自定义类可以用作HashMap的Key？"></a>什么样的自定义类可以用作HashMap的Key？</h4><ul>
<li>实现了HashCode方法和equals方法的类</li>
</ul>
<h4 id="作为Key后的对象有什么要求？"><a href="#作为Key后的对象有什么要求？" class="headerlink" title="作为Key后的对象有什么要求？"></a>作为Key后的对象有什么要求？</h4><ul>
<li>该对象的值不能再修改，否则将导致containsKey找不到已经存储的Key</li>
<li>注意，Key值被修改后是无论如何都找不到的，因为hash对象变化导致hash方式变了<ul>
<li>能正确找到hash桶的对象与目标对象（修改后的值）不相等</li>
<li>与对象与目标对象（修改后的值）相等的对象找不到正确的Hash桶</li>
<li>除非将修改的值改回来</li>
</ul>
</li>
</ul>
<hr>
<h3 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h3><ul>
<li>如果是使用TreeMap，则不是考虑hashCode方法，而是其他方法</li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——RNN</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94RNN.html</url>
    <content><![CDATA[<p><em>本文将介绍一般RNN,RNN的变种GRU(门控循环单元)和LSTM(长短期记忆网络)等神经网络结构</em></p>
<ul>
<li>LSTM原始论文: <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">NIPS 2014, Sequence to Sequence Learning with Neural Networks</a></li>
<li>GRU原始论文: <a href="https://www.aclweb.org/anthology/D14-1179" target="_blank" rel="noopener">EMNLP 2014, Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></li>
</ul>
<hr>
<h3 id="一般的RNN"><a href="#一般的RNN" class="headerlink" title="一般的RNN"></a>一般的RNN</h3><p><em>循环神经网络(Recurrent Neural Network, RNN)</em></p>
<ul>
<li>将神经单元展开后的示意图如下:<img src="/Notes/DL/DL——RNN/RNN_Overview.png">

</li>
</ul>
<h4 id="RNN与CNN优缺点比较"><a href="#RNN与CNN优缺点比较" class="headerlink" title="RNN与CNN优缺点比较"></a>RNN与CNN优缺点比较</h4><p><em>此处我们之比较CNN和RNN在序列预测问题中的优劣</em></p>
<ul>
<li>关于前馈神经网络:<ul>
<li>是一种最简单的神经网络</li>
<li>各神经元分层排列, 每个神经元只接受前一层的输入,输出到下一层,已知到输出层,</li>
<li>整个网络中没有反馈(后面的backward求梯度不能算反馈,这里指一次输入到输出计算各层间没有反馈,数据流只能从上一层到下一层单向流动)</li>
<li>可以用一个有向无环图表示</li>
</ul>
</li>
<li>CNN:<ul>
<li>是一种前馈神经网络(多层感知机),是多层感知机的变体</li>
<li>采用固定大小的输入并生成固定大小的输出</li>
<li>CNN是图像和视频处理的理想选择</li>
</ul>
</li>
<li>RNN:<ul>
<li>不是前馈神经网络, 有环</li>
<li>可以处理任意长度的输入输出</li>
<li>使用时间序列信息,能用之前的状态影响下一个状态</li>
<li>RNN是文本和语音分析的理想选择</li>
</ul>
</li>
<li>RNN在处理序列信息的缺点:<ul>
<li>对短期输入非常敏感,但是对长期输入不敏感</li>
<li>难以处理长文本中长距离的单词间的关系</li>
</ul>
</li>
</ul>
<hr>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p><em>长短期记忆网络(Long Short-Term Memory, LSTM)</em></p>
<ul>
<li>整体示意图如下:<img src="/Notes/DL/DL——RNN/LSTM_Overview.png"></li>
<li>核心是”(Gate)门”结构<img src="/Notes/DL/DL——RNN/Gate_Overview.png">
<ul>
<li>其中Sigmoid是输出0到1的数值,描述每个部分有多少量可以通过当前门</li>
<li>0表示拒绝所有, 1表示接受所有</li>
</ul>
</li>
</ul>
<h4 id="推导流程图"><a href="#推导流程图" class="headerlink" title="推导流程图"></a>推导流程图</h4><ul>
<li><img src="/Notes/DL/DL——RNN/LSTM_Forget.png"></li>
<li><img src="/Notes/DL/DL——RNN/LSTM_New.png"></li>
<li><img src="/Notes/DL/DL——RNN/LSTM_Ct.png"></li>
<li><img src="/Notes/DL/DL——RNN/LSTM_Output.png">

</li>
</ul>
<hr>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><em>门控循环单元(Gated Recurrent Unit, GRU)</em></p>
<ul>
<li>整体示意图及推导如下:<img src="/Notes/DL/DL——RNN/GRU_Overview.png">


</li>
</ul>
<hr>
<h3 id="GRU与LSTM对比"><a href="#GRU与LSTM对比" class="headerlink" title="GRU与LSTM对比"></a>GRU与LSTM对比</h3><ul>
<li>二者提出的时间基本相同</li>
<li>都是用来解决RNN不能处理长期依赖的问题</li>
<li>LSTM有三个”门”结构, GRU只有两个”门结构”</li>
<li>GRU较简单,参数比较少,不容易过拟合, LSTM比较复杂一点,但是常用</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——激活函数总结</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<ul>
<li>参考博客: <a href="https://juejin.im/entry/5ca186c4e51d4534624698ae?from=singlemessage" target="_blank" rel="noopener">一篇写得非常好的文章:深度学习激活函数的理解与总结</a></li>
</ul>
<hr>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><em>修正线性单元(Rectified Linear Unit)</em></p>
<h4 id="x为0时的导数问题"><a href="#x为0时的导数问题" class="headerlink" title="x为0时的导数问题"></a>x为0时的导数问题</h4><ul>
<li>在实际场景中，x为0的概率非常低</li>
<li>实际场景中，当x为0时，原函数不可导，可以给此时的导数制定一个值，如0，或1</li>
<li>神经元永久死亡问题：<a href="https://www.zhihu.com/question/67151971" target="_blank" rel="noopener">https://www.zhihu.com/question/67151971</a><ul>
<li>当对于任意的训练样本，某一个神经元输出都小于0时，该神经元的反向梯度为0，所以神经元上层与神经元相连接的所有节点梯度都为0(链式法则，梯度相乘)，也就是对所有的训练样本，这些节点相关的参数都不会被更新(因为梯度为0)，下一次正向计算到这些节点时值都不会变</li>
<li>一个常见的疑问：如果当前神经元上层的神经元其他参数发生改变而导致当前神经元的输入数据发生改变呢？</li>
<li>回答：可能有这种情况，一个神经元的反向梯度为0以后，所有只与该层该神经元相关的参数都不会变了，但与该层其他神经元相关的节点参数还是会变的！【需要进一步探讨这个问题】</li>
<li>补充回答：可以确定的是，如果当前层的其他神经元不影响该神经元上层的相关参数，那么，这个神经元将永久性死亡</li>
<li>所以正确的描述是：对于任意的输入(不管当前层输入为何值时，当前神经元的输出都等于0，也就是说，虽然上层的输入可能会变，但是当前节点的输出已经不会变化了！)，都有当前神经元的输出等于0，此时反向梯度永远为0</li>
<li>注意：这不是说神经元从开始就怎么样，而是神经元相关的参数偏向太多，导致当前神经元的值永远为0</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——消失的进程</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E6%B6%88%E5%A4%B1%E7%9A%84%E8%BF%9B%E7%A8%8B.html</url>
    <content><![CDATA[<p><em>某个用户的所有进程突然消失,top,ps aux,或者时pstree都看不到相关的进程</em><br><em>但cat /proc/进程pid/status能看到进程信息,且相关端口确实被进程占用</em></p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Sklearn——模型方法和参数使用笔记</title>
    <url>/Notes/ML/Sklearn/Sklearn%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95%E5%92%8C%E5%8F%82%E6%95%B0%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>Sklearn——模型方法和参数使用笔记</em><br><em>本文不定期更新</em></p>
<hr>
<h3 id="模型的一般使用流程"><a href="#模型的一般使用流程" class="headerlink" title="模型的一般使用流程"></a>模型的一般使用流程</h3><ul>
<li>init</li>
<li>fit(x_train, y_train)</li>
<li>predict(x_test)</li>
</ul>
<hr>
<h3 id="模型输出预测概率"><a href="#模型输出预测概率" class="headerlink" title="模型输出预测概率"></a>模型输出预测概率</h3><ul>
<li>方法名称: predict_proba(x_test)</li>
<li>该方法对于预测时使用概率或者分数的算法来说直接返回概率值,对于不能返回概率的类来说一般返回交叉验证结果的平均值等<ul>
<li>NB: 概率值</li>
<li>LR: 逻辑回归的分数</li>
<li>SVM: 交叉验证生成的平均值, 这里的结果与predict预测结果可能有偏差</li>
</ul>
</li>
</ul>
<hr>
<h3 id="关于参数"><a href="#关于参数" class="headerlink" title="关于参数"></a>关于参数</h3><pre><code>* </code></pre>
]]></content>
      <tags>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas——为DataFrame的某一列实行OneHot编码</title>
    <url>/Notes/Python/Pandas/Pandas%E2%80%94%E2%80%94%E4%B8%BADataFrame%E7%9A%84%E6%9F%90%E4%B8%80%E5%88%97%E5%AE%9E%E8%A1%8COneHot%E7%BC%96%E7%A0%81.html</url>
    <content><![CDATA[<p><em>为DataFrame的某一列实行OneHot编码</em></p>
<hr>
<h3 id="使用OneHotEncoder进行编码"><a href="#使用OneHotEncoder进行编码" class="headerlink" title="使用OneHotEncoder进行编码"></a>使用OneHotEncoder进行编码</h3><ul>
<li><p>基本实现思路:</p>
<ul>
<li>生成一个OneHotEncoder对象</li>
<li>取出对应的列并处理成N*1维的数组,用其训练OneHotEncoder对象并进行编码转换</li>
<li>将新编码的数据生成为新的DataFrame对象<ul>
<li>为新的编码每一列生成新的列名称</li>
<li>为新的每行索引赋值为原始DataFrame对应的索引</li>
</ul>
</li>
<li>按照列合并两个DataFrame</li>
<li>删除之前的列</li>
</ul>
</li>
<li><p>实现代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.preprocessing import OneHotEncoder</span><br><span class="line"></span><br><span class="line">def one_hot_for_column(df, column):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    encode column of df with one hot method</span><br><span class="line">    :param df: an object of DataFrame</span><br><span class="line">    :param column: the name of column in df</span><br><span class="line">    :return: new object of DataFrame and object of OneHotEncoder</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    ohe = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">    # ohe.fit(df[column].values.reshape(-1, 1))</span><br><span class="line">    # col_series = ohe.transform(df[column].values.reshape(-1, 1)).toarray()</span><br><span class="line">    # &lt;==&gt;</span><br><span class="line">    col_series = ohe.fit_transform(df[column].values.reshape(-1, 1)).toarray()</span><br><span class="line">    </span><br><span class="line">    columns = [&quot;%s_%s&quot; % (column, str(m)) for m in range(1, col_series.shape[1] + 1)]</span><br><span class="line">    sub_df = pd.DataFrame(col_series, columns=columns, dtype=int, index=df.index)</span><br><span class="line">    new_df = pd.concat([df, sub_df], axis=1)</span><br><span class="line">    new_df.drop(columns=column, inplace=True)</span><br><span class="line">    return new_df, ohe</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Pandas</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL——Error code 28; No space left on device</title>
    <url>/Notes/MySQL/MySQL%E2%80%94%E2%80%94Error-code-28-No-space-left-on-device.html</url>
    <content><![CDATA[<p><strong>首先说明这是一个MySQL错误，描述的是MySQL临时文件不能打开，空间不足</strong></p>
<hr>
<h3 id="问题源头"><a href="#问题源头" class="headerlink" title="问题源头"></a>问题源头</h3><p>在Ubuntu系统中，爬取数据过多，数量百万级到千万级，而且存储都是以小文件的方式存储的，造成inode的大量使用，从而发生异常，发生异常时服务器尚有20多G的硬盘空间</p>
<hr>
<h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><p>将数据打包起来，然后删除打包后的数据</p>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在做数据分析时最好不要使用小文件存储数据，如果非要存储收也尽量注意打包压缩暂时不必要使用的文件</p>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Shadowsocks——混淆设置</title>
    <url>/Notes/Others/Shadowsocks%E2%80%94%E2%80%94%E6%B7%B7%E6%B7%86%E8%AE%BE%E7%BD%AE.html</url>
    <content><![CDATA[<p><em>obfs: obfuscating</em></p>
<ul>
<li>Follow Github项目<a href="https://github.com/shadowsocks/simple-obfs" target="_blank" rel="noopener">https://github.com/shadowsocks/simple-obfs</a></li>
<li>可选<code>http</code>和<code>tls</code>两种混淆方式,根据需求修改即可</li>
</ul>
]]></content>
      <tags>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP——词嵌入的前世今生</title>
    <url>/Notes/NLP/NLP%E2%80%94%E2%80%94%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F.html</url>
    <content><![CDATA[<p><em>词嵌入的发展,NNLM,CBOW,skip-gram</em><br><em>词嵌入(Word Embedding)也称为词向量(Word2Vec): 指的是将词转换成向量的形式</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="基本问题"><a href="#基本问题" class="headerlink" title="基本问题"></a>基本问题</h3><ul>
<li>用语言模型做预训练<ul>
<li>语言模型: 如何计算一段文本序列在某种语言下出现的概率?</li>
</ul>
</li>
<li>如何表示文本中的一个词?</li>
</ul>
<hr>
<h3 id="文本表示模型"><a href="#文本表示模型" class="headerlink" title="文本表示模型"></a>文本表示模型</h3><h4 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h4><ul>
<li>One-Hot: 向量维度与词的数量相同,某个维度为1,其他都为0</li>
<li>TF-IDF: 计算词对文档的贡献(权重)</li>
<li>TextRank: 类似于PageRank的思想<ul>
<li>如果一个单词出现在很多单词后面的话，那么说明这个单词比较重要</li>
<li>一个TextRank值很高的单词后面跟着的一个单词，那么这个单词的TextRank值会相应地因此而提高</li>
</ul>
</li>
</ul>
<h5 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h5><ul>
<li>OneHot:<ul>
<li>维度灾难</li>
<li>语义鸿沟</li>
</ul>
</li>
<li>TF-IDF和TextRank<ul>
<li>[个人理解]也无法表示语义,所以存在语义鸿沟</li>
</ul>
</li>
</ul>
<h4 id="主题模型"><a href="#主题模型" class="headerlink" title="主题模型"></a>主题模型</h4><ul>
<li>LSA: 矩阵分解(SVD)的方式,又名 Latent Semantic Indexing</li>
<li>pLSA: 简单的概率隐式语义模型</li>
<li>LDA: 无监督聚类常用的模型,有Gibbs Sampling和变分推断等实现,网上有很多资料,也有很多实现</li>
<li>L-LDA: LDA的有监督版本,核心思想就是在LDA吉布斯实现的版本基础上加上采样限制,使得每次采样只能控制在对应主题上,效果比较好,网上没有官方的实现,大部分实现不完全,感兴趣可以试试我的实现<a href="https://github.com/JoeZJH/Labeled-LDA-Python" target="_blank" rel="noopener">Labeled-LDA-Python</a><ul>
<li>L-LDA理论上可以得到比LDA质量更高,效果更好的词向量</li>
</ul>
</li>
</ul>
<h5 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点:"></a>优缺点:</h5><ul>
<li>LSA:<ul>
<li>利用全局语料特征</li>
<li>但SVD求解计算复杂度大</li>
</ul>
</li>
</ul>
<h4 id="基于词向量的固定表征"><a href="#基于词向量的固定表征" class="headerlink" title="基于词向量的固定表征"></a>基于词向量的固定表征</h4><ul>
<li>Word2vec: 基于上下文训练, 拥有相同上下文的词相近,基于分布式假设(相同上下文语境的词有似含义)</li>
<li>FastText: 通过对”词-词”共现矩阵进行分解从而得到词表示的方法,基于分布式假设(相同上下文语境的词有似含义)</li>
<li>GloVe: 基于全局预料，结合了LSA和word2vec的优点, 论文: <a href="https://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a></li>
</ul>
<h5 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h5><ul>
<li>目前提到的以上所有都是静态词向量,无法解决一次多义等问题</li>
<li>Word2vec和FastText: <ul>
<li>优化效率高，但是基于局部语料</li>
<li>本质上二者也都是语言模型</li>
</ul>
</li>
<li>GloVe: <ul>
<li>基于全局语料库、并结合上下文语境构建词向量, 结合了LSA和word2vec的优点</li>
<li>可以看作是更换了目标函数和权重函数的全局word2vec</li>
</ul>
</li>
</ul>
<h4 id="基于词向量的动态表征"><a href="#基于词向量的动态表征" class="headerlink" title="基于词向量的动态表征"></a>基于词向量的动态表征</h4><ul>
<li>ELMo: <ul>
<li>采样LSTM提取特征</li>
<li>采用双向语言模型</li>
<li>实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比BERT一体化融合特征方式弱</li>
</ul>
</li>
<li>GPT: <ul>
<li>采样Transformer进行特征提取</li>
<li>采用单向语言模型</li>
</ul>
</li>
<li>BERT: <ul>
<li>采用Transformer进行特征提取</li>
<li>采用双向语言模型</li>
</ul>
</li>
</ul>
<h5 id="优缺点-3"><a href="#优缺点-3" class="headerlink" title="优缺点"></a>优缺点</h5><ul>
<li>三者都是基于语言模型的动态词向量,能解决一次多义问题</li>
<li>ELMo: <ul>
<li>采用1层静态向量+2层LSTM，多层提取能力有限</li>
</ul>
</li>
<li>GPT和BERT: <ul>
<li>Transformer可采用多层，并行计算能力强</li>
</ul>
</li>
<li>很多任务表明: <ul>
<li>Transformer特征提取能力强于LSTM</li>
</ul>
</li>
<li>GPT和BERT都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；BERT的双向语言模型则采用encoder部分，采用了完整句子</li>
</ul>
<h4 id="NNLM-RNNLM"><a href="#NNLM-RNNLM" class="headerlink" title="NNLM/RNNLM"></a>NNLM/RNNLM</h4><ul>
<li>词向量为副产物，存在效率不高等问题；</li>
</ul>
<hr>
<h3 id="不同模型的对比"><a href="#不同模型的对比" class="headerlink" title="不同模型的对比"></a>不同模型的对比</h3><h4 id="word2vec-vs-NNLM"><a href="#word2vec-vs-NNLM" class="headerlink" title="word2vec vs NNLM"></a>word2vec vs NNLM</h4><ul>
<li>本质上都可以看做是语言模型</li>
<li>对NNLM来说,目不是词向量,是语言模型</li>
<li>word2vec虽然本质上也是语言模型,但是更关注词向量本身,因此做了很多优化来提高计算效率<ul>
<li>与NNLM相比,词向量直接sum(CBOW),而不是拼接,并取消隐藏层</li>
<li>考虑到Softmax需要遍历整个词汇表,采用 Hierarcal Softmax和 Negative Sampling进行优化</li>
<li>word2vec所做的一切都是为了一切为了快速生成词向量</li>
</ul>
</li>
</ul>
<h4 id="word2vec-vs-fastText"><a href="#word2vec-vs-fastText" class="headerlink" title="word2vec vs fastText"></a>word2vec vs fastText</h4><ul>
<li>都可以进行无监督学习, fastText训练词向量时会考虑subword(子词)<ul>
<li>在fastText中，每个中心词被表示成子词的集合。下面我们用单词<code>“where”</code>作为例子来了解子词是如何产生的。首先，我们在单词的首尾分别添加特殊字符<code>“&lt;”</code>和<code>“&gt;”</code>以区分作为前后缀的子词。然后，将单词当成一个由字符构成的序列来提取n元语法。例如，当n=3时，我们得到所有长度为3的子词：<code>“&lt;wh&gt;”</code> <code>“whe”</code> <code>“her”</code> <code>“ere”</code> <code>“&lt;re&gt;”</code>以及特殊子词<code>“&lt;where&gt;”</code>。</li>
</ul>
</li>
<li>fastText还可以进行有监督学习进行文本分类:<ul>
<li>结构与CBOW类似，但学习目标是人工标注的分类结果, 而不是中心词 \(w\)</li>
<li>采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；</li>
<li>引入N-gram，考虑词序特征；</li>
<li><strong>引入subword(子词)来处理长词，处理未登陆词问题</strong></li>
</ul>
</li>
</ul>
<h4 id="word2vec-vs-GloVe-vs-LSA"><a href="#word2vec-vs-GloVe-vs-LSA" class="headerlink" title="word2vec vs GloVe vs LSA"></a>word2vec vs GloVe vs LSA</h4><ul>
<li>word2vec vs GloVe <ul>
<li>语料库:<ul>
<li>word2vec是局部语料库训练的,特征提取基于滑动窗口,可以在线学习</li>
<li>GloVe的滑动窗口是为了建立”词-词”共现矩阵,需要统计共现概率,不能在线学习</li>
</ul>
</li>
<li>无监督学习:<ul>
<li>word2vec是无监督学习</li>
<li>GloVe通常被认为是无监督学习(无需标注)，但实际上GloVe还是有label的，即共现次数 \(log(X_{ij})\)</li>
</ul>
</li>
<li>损失函数:<ul>
<li>word2vec损失函数实质上是带权重的交叉熵，权重固定；</li>
<li>GloVe的损失函数是最小平方损失函数，权重可以做映射变换</li>
</ul>
</li>
<li>总结:<ul>
<li>word2vec的损失函数是<strong>带权重的交叉熵</strong>, <strong>权重是固定</strong>的</li>
<li>GloVe可以看作是<strong>目标函数为MSE</strong>,<strong>权重函数可变</strong>的全局word2vec</li>
</ul>
</li>
</ul>
</li>
<li>GloVe vs LSA<ul>
<li>LSA（Latent Semantic Analysis）可以基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高；</li>
<li>GloVe可看作是对LSA一种优化的高效矩阵分解算法，采用Adagrad对最小平方损失进行优化；</li>
</ul>
</li>
</ul>
<h4 id="ELMo、GPT、BERT"><a href="#ELMo、GPT、BERT" class="headerlink" title="ELMo、GPT、BERT"></a>ELMo、GPT、BERT</h4><ul>
<li>三者使用了两种不同的NN组件提取特征<ul>
<li>ELMo: LSTM</li>
<li>GPT, BERT: Transformer</li>
</ul>
</li>
<li>三者使用了两种不同的语言模型:<ul>
<li>GPT: 单向语言模型</li>
<li>ELMo, BERT: 双向语言模型</li>
<li>ELMo实际上是两个方向相反的单向语言模型的拼接, 融合特征的能力比BERT那种一体化的融合特征方式弱</li>
<li>GPT和BERT都采用Transformer,Transformer是 Encoder-Decoder结构<ul>
<li>GPT为单向语言模型,采用的是Decoder部分, Decoder部分见到的都是不完整的句子</li>
<li>BERT为双向语言模型,采用的是Encoder部分,采用了完整的句子</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p><strong><em>下面是一些模型的详细介绍</em></strong></p>
<hr>
<h3 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h3><h4 id="模型目标"><a href="#模型目标" class="headerlink" title="模型目标"></a>模型目标</h4><ul>
<li>给定一段长度为m的序列时,预测下一个词(第m+1)个词出现的概率</li>
<li>参考博客<a href="/Notes/DL/DL%E2%80%94%E2%80%94NNLM.html">DL——NNLM</a><img src="/Notes/NLP/NLP——词嵌入的前世今生/NNLM_Overview.png">

</li>
</ul>
<hr>
<h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><ul>
<li>此处只做简单介绍,更详细的讲解请参考博客<a href="http://xtf615.com/2018/10/05/word2vec/" target="_blank" rel="noopener">word2vec学习笔记</a></li>
<li>损失函数: 带权重的交叉熵损失函数(权重固定)</li>
</ul>
<h4 id="两类实现"><a href="#两类实现" class="headerlink" title="两类实现"></a>两类实现</h4><ul>
<li>一般实现: 每次训练一个样本 \((w_{in}, w_{out})\), 需要计算所有词和当前词一起对应的 softmax, 很耗时间,需要 \(O(V)\)复杂度的时间</li>
<li>优化实现: Hierarcal Softmax 和 Negative Sampling 两种方式<ul>
<li>Hierarcal Softmax: 每次训练一个样本 \((w_{in}, w_{out})\), 只需要更新平均 \(O(log V)\) 个非叶子节点(每个非叶子结点就是一个词向量)即可</li>
<li>Negative Sampling: 每次训练一个样本 \((w_{in}, w_{out})\), 只需要更新 \({w_j|w_j\in {w_O}\bigcup W_{neg}}\)个词向量, 通常负样本的集合大小取 \(log(V)\) 量级甚至更小</li>
</ul>
</li>
</ul>
<h5 id="层次Softmax模型框架"><a href="#层次Softmax模型框架" class="headerlink" title="层次Softmax模型框架"></a>层次Softmax模型框架</h5><p><em>层次Softmax Hierarchical Softmax模型</em></p>
<ul>
<li>核心思想: 输出层由语料库中<strong>词出现的频数当作权值</strong>构造出的<strong>哈夫曼树</strong>作为输出</li>
<li>引入<strong>哈夫曼树</strong>, Hierarcal Softmax是一种有效计算Softmax的方式,使用二叉树来表示</li>
<li>其中Hierarchical Softmax模型的输出层由语料库中<strong>词出现的频数当作权值</strong>构造出的<strong>哈夫曼树</strong>作为输出<img src="/Notes/NLP/NLP——词嵌入的前世今生/hierarchical_overview.jpg"></li>
<li>CBOW<ul>
<li>输入层: 2c个词向量</li>
<li>投影层: 2c个词向量的累加</li>
<li>输出层: 哈夫曼树(重点是词w所在的叶子节点, 以及w到根节点的路径)<ul>
<li>所有单词没有输出向量表示形式,每个内部结点有一个输出向量 \(v\)</li>
<li>输出层共有 \(V - 1\) 个非叶节点, 也就是要学习 \(V - 1\) 个输出向量</li>
<li>看起来并没有什么优化,但是每次迭代训练一个样本\(w_{in}, w_{out}\) 时,我们只需要优化从根节点到输出单词 \(w_{out}\) 的路径上的输出向量即可, 平均共 \(O(log V)\) 个向量</li>
<li>每个单词的概率计算需要考虑叶子节点来求出[待更新]</li>
</ul>
</li>
</ul>
</li>
<li>Skip-gram<ul>
<li>输入层：词w的向量</li>
<li>投影层：依旧是词w的向量</li>
<li>输出层：哈夫曼树（重点是词w的上下文窗内2c个词所在的叶子节点，以及各自到根节点的路径）</li>
</ul>
</li>
</ul>
<h5 id="Negative-Sampling模型框架"><a href="#Negative-Sampling模型框架" class="headerlink" title="Negative Sampling模型框架"></a>Negative Sampling模型框架</h5><ul>
<li>实际上就是简单的对每一个样本中每一个词都进行负例采样(本质上就是用负采样代替了层次softmax的哈弗曼树)</li>
<li>负采样是噪声对比估计的一个简化版本,目的是提高训练速度并改善所得词向量的质量</li>
<li>核心思想: 在每次迭代过程中, 有大量的输出向量需要更新,为了解决这一困难, Negative Sampling的做法是只更新其中一部分输出向量</li>
<li>利用不同词在语料中出现的频次多少来决定被采样到的概率(单词出现的频次越大,越可能被选做负样本)</li>
<li>负采样: 利用不同词在语料库中出现的频次多少来决定该词被采样到的概率</li>
<li>每次训练一个样本 \((w_{in}, w_{out})\), 只需要更新 \({w_j|w_j\in {w_O}\bigcup W_{neg}}\)个词向量, 通常负样本的集合大小取 \(log(V)\) 量级甚至更小</li>
<li>CBOW<ul>
<li>输入层： 2c个词向量</li>
<li>投影层：2c个词向量的累加</li>
<li>输出层：负采样词集(重点是词w的负词词集的参数 \(\theta\) ，负词的概率永远是1-Sigmoid函数值)</li>
</ul>
</li>
<li>Skip-gram模型:<ul>
<li>输入层：词w的向量</li>
<li>投影层：依旧是词w的向量</li>
<li>输出层：每个上下文词u的负采样(重点是词w的负词词集的参数 \(\theta\) ，负词的概率永远是1-Sigmoid函数值)</li>
</ul>
</li>
</ul>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul>
<li>层次Softmax模型:<ul>
<li>CBOW模型的一次更新是：<ul>
<li>输入2c个词向量的累加(简单的<code>sum</code>,而不是拼接)</li>
<li>对中心词 \(w\) 上的路径节点系数进行更新(输出层)</li>
<li>对所有的上下文词的词向量进行整体一致更新(输入层到隐藏层的参数, 修改多个词向量)</li>
</ul>
</li>
<li>Skip-gram模型的一次更新是：<ul>
<li>输入中心词 \(w\) 的词向量(直接输入,无其他操作)</li>
<li>对每个上下文词 \(u(i)\) 所在的路径上的节点系数进行更新(输出层,修改多个上下文的结点)</li>
<li>对词w的词向量进行单独更新(输入层到隐藏层的参数,只修改一个词向量)</li>
</ul>
</li>
<li>CBOW一次训练更新计算量小,Skip-gram一次训练计算量大(更新的系数更多?)</li>
</ul>
</li>
<li>CBOW看起来更新的更平滑，适合小量文本集上的词向量构建，Skip-gram每次更新都更加有针对性，所以对于大文本集上表现更好</li>
</ul>
<h4 id="word2vec的缺陷"><a href="#word2vec的缺陷" class="headerlink" title="word2vec的缺陷"></a>word2vec的缺陷</h4><ul>
<li>不能解决多义词问题(只能把几种意思同时编码进向量里)<ul>
<li>这是所有固定词向量的通病</li>
<li>解决方案是动态词向量(允许不同上下文使用不同词向量表达同一个词),包括ELMo, GPT, BERT</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>GitHub——主题推荐算法</title>
    <url>/Notes/Others/GitHub%E2%80%94%E2%80%94%E4%B8%BB%E9%A2%98%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<p><em>Topic Suggestions for Millions of Repositories</em></p>
<p><a href="https://githubengineering.com/topics/" title="https://githubengineering.com/topics/" target="_blank" rel="noopener">Github官网原文</a></p>
<hr>
<h3 id="总体流程图"><a href="#总体流程图" class="headerlink" title="总体流程图"></a>总体流程图</h3><!-- ![process-flow](Github-topics-method/Github-topics-process-flow.png) -->
<img src="/Notes/Others/GitHub——主题推荐算法/Github-topics-process-flow.png" title="process-flow">


<h3 id="分为七个步骤："><a href="#分为七个步骤：" class="headerlink" title="分为七个步骤："></a>分为七个步骤：</h3><hr>
<h4 id="Readme-预处理与清除"><a href="#Readme-预处理与清除" class="headerlink" title="Readme 预处理与清除"></a>Readme 预处理与清除</h4><p><em>(Readme preprocessing and cleanup)</em></p>
<ul>
<li>移除不要的文本块(Remove unwanted blocks)<ul>
<li>去除没用文本：一些块是没用的，比如code, table 和image链接等</li>
</ul>
</li>
<li>文本划分(Text segmentation)<ul>
<li>提取有用的文本：一个启发式的README tagger，借助格式分析：分析缩进(indentation)，空格(spacing)，和反撇号( `, backtick)等决定是否是有用信息【说明：这里语法分析是没必要的，我们只关心有用的文本，其他的都是为噪音(noise)】</li>
<li>提取到有用文本后，删除拓展部分：HTML标签，路径和其他处理出来的部分等</li>
<li>最后将剩下的文本进行粗粒度分割【用标点符号(punctuation marks)和README中的一些符号，比如临近哈希符号(contiguous hash symbols)】</li>
</ul>
</li>
</ul>
<hr>
<h4 id="生成候选主题"><a href="#生成候选主题" class="headerlink" title="生成候选主题"></a>生成候选主题</h4><p><em>(Generate candidate topics)</em></p>
<ul>
<li>用自定义的停用词去将词单元划分出来(Use custom stop words to break text into word units)<ul>
<li>停用词去除</li>
<li>n-gram分段小于等于4(测试发现小于4的比较好，太长的主题过于具体了，不合适)</li>
</ul>
</li>
</ul>
<hr>
<h4 id="移除噪音主题"><a href="#移除噪音主题" class="headerlink" title="移除噪音主题"></a>移除噪音主题</h4><p><em>(Eliminate noisy topics)</em></p>
<ul>
<li>用逻辑回归模型删减“bad”主题(Use a logistic regression model to prune “bad” topics)<ul>
<li>监督逻辑回归模型(supervised logistic regression model)主要针对除了频数比较小的外，一些不好的，比如”great”, “cool”等，<strong>这里的模型是一个分类模型，分为两类(good[positive] and bad(negative))</strong>, 我们称之为关键词过滤模型<ul>
<li>手动收集大约300个数据集作为训练集</li>
<li>单个动词一般都是Bad类<code>请教师兄</code></li>
<li>其他的(<code>Other features we used were occurrence of user names, n-gram size of a phrase, length of a phrase, and numeric content within a phrase.</code>)</li>
<li>以后打算加入回馈机制(来自用户的)去更新这个关键词过滤模型：接受度高的词作为positive的，接受度低的作为停用词或者negative的</li>
</ul>
</li>
</ul>
</li>
<li>移除不满足最小频数的主题(Eliminate topics not satisfying minimum frequency count)</li>
</ul>
<hr>
<h4 id="给主题评分"><a href="#给主题评分" class="headerlink" title="给主题评分"></a>给主题评分</h4><p><em>(Score Topics)</em></p>
<ul>
<li>用混合tf-idf分数，以主题频率和n元词作为打分标准(Use combination of tf-idf scores, topic frequency and n-gram size for scoring)<ul>
<li>评估多种指标后，选择了点互信息指标(PMI)： <a href="https://blog.csdn.net/luo123n/article/details/48574123" title="https://blog.csdn.net/luo123n/article/details/48574123" target="_blank" rel="noopener">Pointwise Mutual Information</a></li>
<li>考虑另一种指标tf-idf：参考论文<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf" title="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Using TF-IDF to Determine Word Relevance in Document Queries.pdf</a>或者<a href="https://blog.csdn.net/baimafujinji/article/details/51476117" title="https://blog.csdn.net/baimafujinji/article/details/51476117" target="_blank" rel="noopener">Python的实现</a></li>
<li><code>The second approach we tried uses the average tf-idf scores of individual words in a phrase weighted by the phrase frequency (if it’s more than one word long) and n-gram size.</code></li>
<li>两种方法比较：<ul>
<li>PMI： 强调独特性，越特殊的短语评分越高，但有些可能只是拼写错误(Typo)或者是没有被删除的代码片段</li>
<li>tf-idf： 不强调独特性，最终选择是tf-idf,原因是这个指标较好的平衡了独特性和主题与仓库(repository)的相关程度</li>
</ul>
</li>
<li>在tf-idf的基础上还添加了一下其他的比如boosting scores等方法</li>
<li><em>下面是TF-IDF的说明*</em><!-- ![tf-idf](Github-topics-method/tf-idf.png) -->
<img src="/Notes/Others/GitHub——主题推荐算法/tf-idf.png" title="tf-idf">

</li>
</ul>
</li>
</ul>
<hr>
<h4 id="规范化主题"><a href="#规范化主题" class="headerlink" title="规范化主题"></a>规范化主题</h4><p><em>(Canonicalize topics)</em></p>
<ul>
<li><p>使用内部词典规范主题形式(Use an in-house dictionary to suggest canonical form of topics)</p>
<ul>
<li><p>解决文字层面的差别和变化等,比如下面四个主题</p>
<blockquote>
<p>neural-network<br>neural-networks<br>neuralnetwork<br>neuralnetworks</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="移除相似的主题"><a href="#移除相似的主题" class="headerlink" title="移除相似的主题"></a>移除相似的主题</h4><p><em>(Eliminate near similar topics)</em></p>
<ul>
<li><p>用基于Jaccard相似性评分的贪心算法(Greedy approach using Jaccard similarity scoring)</p>
<ul>
<li><p>motivation：在得到Top-N的主题后，有些主题其实很相似，虽然都有用，但是他们只是在不同粒度描述了同一个主题而已，因此我们需要删除一些重复的，比如下面的例子</p>
<blockquote>
<p>machine learning<br>deep learning<br>general-purpose machine learning<br>machine learning library<br>machine learning algorithms<br>distributed machine learning<br>machine learning framework<br>deep learning library<br>support vector machine<br>linear regression</p>
</blockquote>
</li>
<li><p>method： 两个短语的相似性计算使用的是基于词的Jaccard相似性(两个短语的差集与并集的比值，因为它对较短的短语很有效，而且分数是[0-1]的，很方便设置阈值(thresholds))，用贪心算法，如果两个主题相似，去除分数较低的那一个，上面的例子去除相似主题后的结果是：</p>
<blockquote>
<p>machine learning<br>deep learning<br>support vector machine<br>linear regression</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="返回Top-K主题作为最终结果"><a href="#返回Top-K主题作为最终结果" class="headerlink" title="返回Top-K主题作为最终结果"></a>返回Top-K主题作为最终结果</h4>]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Go语言——String与Slice深度解析</title>
    <url>/Notes/Go/Go%E8%AF%AD%E8%A8%80%E2%80%94%E2%80%94String%E4%B8%8ESlice%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90.html</url>
    <content><![CDATA[<p><em>Note: 他们都是struct类型的</em></p>
<hr>
<h3 id="String"><a href="#String" class="headerlink" title="String"></a>String</h3><pre><code>type StringHeader struct {
    Data uintptr
    Len  int
}</code></pre>
<hr>
<h3 id="Slice"><a href="#Slice" class="headerlink" title="Slice"></a>Slice</h3><pre><code>type SliceHeader struct {
    Data uintptr
    Len  int
    Cap  int
}</code></pre>
<hr>
<h3 id="把Slice转换为String的语法为"><a href="#把Slice转换为String的语法为" class="headerlink" title="把Slice转换为String的语法为"></a>把Slice转换为String的语法为</h3><pre><code>[]byte(&quot;string&quot;)
string([]byte{1,2,3})</code></pre>
<p><strong>注意：这种实现会有拷贝操作</strong></p>
<hr>
<h3 id="如何避免拷贝操作呢？"><a href="#如何避免拷贝操作呢？" class="headerlink" title="如何避免拷贝操作呢？"></a>如何避免拷贝操作呢？</h3><p>答案是自己实现指针转换（也可用反射实现头部转换），省去复制数据部分，同时注意这种实现后底层的数据不能再更改了，不然容易引发错误</p>
<h4 id="直接修改指针类型并构建相应头部"><a href="#直接修改指针类型并构建相应头部" class="headerlink" title="直接修改指针类型并构建相应头部"></a>直接修改指针类型并构建相应头部</h4><pre><code>func String2Slice(s string) []byte {
    sp := *(*[2]uintptr)(unsafe.Pointer(&amp;s))
    bp := [3]uintptr{sp[0], sp[1], sp[1]}
    return *(*[]byte)(unsafe.Pointer(&amp;bp))
}

func BytesToString(b []byte) string {
    return *(*string)(unsafe.Pointer(&amp;b))
}</code></pre>
<h4 id="使用反射机制获取到头部再进行转换"><a href="#使用反射机制获取到头部再进行转换" class="headerlink" title="使用反射机制获取到头部再进行转换"></a>使用反射机制获取到头部再进行转换</h4><pre><code>func Slice2String(b []byte) (s string) {
    pbytes := (*reflect.SliceHeader)(unsafe.Pointer(&amp;b))
    pstring := (*reflect.StringHeader)(unsafe.Pointer(&amp;s))
    pstring.Data = pbytes.Data
    pstring.Len = pbytes.Len
    return
}


func String2Slice(s string) (b []byte) {
    pbytes := (*reflect.SliceHeader)(unsafe.Pointer(&amp;b))
    pstring := (*reflect.StringHeader)(unsafe.Pointer(&amp;s))
    pbytes.Data = pstring.Data
    pbytes.Len = pstring.Len
    pbytes.Cap = pstring.Len
    return
}</code></pre>
]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL——事务深度理解</title>
    <url>/Notes/MySQL/MySQL%E2%80%94%E2%80%94%E4%BA%8B%E5%8A%A1%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<hr>
<h3 id="autocommit"><a href="#autocommit" class="headerlink" title="@@autocommit"></a>@@autocommit</h3><ul>
<li>变量 @@autocommit<br>  select @@autocommit;<br>  set @@autocommit = 0;</li>
<li>@@autocommit 为0时表示不以显示事务开头的语句或者以事务开头(<code>begin;</code> 或者 <code>start transaction;</code>)都会被缓存起来并且在<code>commit;</code>提交前都可以用<code>rollback;</code>回滚</li>
<li>@@autocommit 为1时表示必须以事务开头的语句才会被缓存，否则一个sql语句将会被当做一个事务提交，将不能使用<code>rollback;</code>语句回滚</li>
</ul>
<p><strong>说明：不是所有引擎都支持事务，常用的支持事务的引擎是InnoDB</strong></p>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL——引擎比较说明</title>
    <url>/Notes/MySQL/MySQL%E2%80%94%E2%80%94%E5%BC%95%E6%93%8E%E6%AF%94%E8%BE%83%E8%AF%B4%E6%98%8E.html</url>
    <content><![CDATA[<hr>
<h3 id="关于引擎"><a href="#关于引擎" class="headerlink" title="关于引擎"></a>关于引擎</h3><ul>
<li>查看各种引擎 <code>show engines;</code></li>
<li>查看当前默认引擎 <code>show variables like &#39;%storage_engine%&#39;;</code></li>
<li>查看指定表的引擎 <code>show create table tableName;</code></li>
<li>修改指定表的引擎 <code>alter table tableName engine = innodb;</code></li>
<li>创建表时指定引擎 <code>create table mytable (id int, titlechar(20)) engine = innodb</code></li>
<li>修改默认存储引擎<br>  <strong>在mysql配置文件（linux下为/etc/my.cnf），在mysqld后面增加default-storage-engine=INNODB即可</strong></li>
</ul>
<hr>
<h3 id="不同引擎的事务支持说明"><a href="#不同引擎的事务支持说明" class="headerlink" title="不同引擎的事务支持说明"></a>不同引擎的事务支持说明</h3><p>MySQL数据库有多种引擎，一般使用的是InnoDB（从MySQL5.5.5以后，InnoDB是默认引擎），InnoDB存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比Myisam的存储引擎，InnoDB写的处理效率差一些并且会占用更多的磁盘空间以保留数据和索引。</p>
<p>其他的引擎不支持事务等，但是存储空间占的比较小，而且操作比较快一些。</p>
<p>MySQL有多种存储引擎，每种存储引擎有各自的优缺点，可以择优选择使用：MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE。</p>
<p>虽然MySQL里的存储引擎不只是MyISAM与InnoDB这两个，但常用的就是两个。</p>
<ul>
<li>InnoDB支持事务，MyISAM不支持，这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而MyISAM就不可以了。</li>
<li>MyISAM适合查询以及插入为主的应用，InnoDB适合频繁修改以及涉及到安全性较高的应用</li>
<li>InnoDB支持外键，MyISAM不支持</li>
<li>从MySQL5.5.5以后，InnoDB是默认引擎</li>
<li>InnoDB不支持FULLTEXT类型的索引</li>
<li>InnoDB中不保存表的行数，如select count(*) from table时，InnoDB需要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count(*)语句包含where条件时MyISAM也需要扫描整个表</li>
<li>对于自增长的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中可以和其他字段一起建立联合索引</li>
<li>清空整个表时，InnoDB是一行一行的删除，效率非常慢。MyISAM则会重建表</li>
<li>InnoDB支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’</li>
</ul>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Go语言——struct类型作为函数参数</title>
    <url>/Notes/Go/Go%E8%AF%AD%E8%A8%80%E2%80%94%E2%80%94struct%E7%B1%BB%E5%9E%8B%E5%81%9A%E4%B8%BA%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0.html</url>
    <content><![CDATA[<p><em>本文先给出Go语言中struct类型作为函数参数传入的例子，然后给出总结</em></p>
<hr>
<h3 id="首先看下面代码"><a href="#首先看下面代码" class="headerlink" title="首先看下面代码"></a>首先看下面代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func changeValue(person Person)&#123;</span><br><span class="line">	person.name = &quot;zhoujiahong&quot;</span><br><span class="line">	person.age = 22</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func changePoint(person *Person)&#123;</span><br><span class="line">	person.name = &quot;zhoujiahong&quot;</span><br><span class="line">	person.age = 22</span><br><span class="line">&#125;</span><br><span class="line">func main()  &#123;</span><br><span class="line">	person := Person&#123;</span><br><span class="line">		name:&quot;origin&quot;,</span><br><span class="line">	&#125;</span><br><span class="line">	changeValue(person)</span><br><span class="line">	fmt.Printf(&quot;Person:\n person.name: %s---person.age: %d\n&quot;, person.name, person.age)</span><br><span class="line">	changePoint(&amp;person)</span><br><span class="line">	fmt.Printf(&quot;*Person:\n person.name: %s---person.age: %d\n&quot;, person.name, person.age)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//output:</span><br><span class="line">//Person:</span><br><span class="line">// person.name: origin---person.age: 0</span><br><span class="line">//*Person:</span><br><span class="line">// person.name: zhoujiahong---person.age: 22</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>struct类型的传递是按值传递的，与数组[5]byte等的传递相似，也可以理解为和C++一样</li>
<li>struct类型传递与切片不同，切片事实上是一个指针（栈）指向一块数组（堆）这样的数据结构，所以无论如何都是传入指针或者是指针（栈）的指针（指向栈）</li>
<li>可以理解为: 切片类似于Java（切片还多了个特色，通过传入切片可以修改指针（栈）本身，而Java是做不到的）， 而数组类似于C++</li>
</ul>
<p><strong>注意：struct类型与切片传值方式不同而与数组相同</strong></p>
]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>Go语言——切片和数组的区别</title>
    <url>/Notes/Go/Go%E8%AF%AD%E8%A8%80%E2%80%94%E2%80%94%E5%88%87%E7%89%87%E5%92%8C%E6%95%B0%E7%BB%84%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p><em>关于切片与数组的区别，这里给出定义和传值等使用上的区别，最后给出总结</em></p>
<hr>
<h3 id="数组定义"><a href="#数组定义" class="headerlink" title="数组定义"></a>数组定义</h3><ul>
<li>申明类型定义</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var arr [2]byte</span><br><span class="line">arr[0] = 10</span><br></pre></td></tr></table></figure>

<ul>
<li>直接赋值定义</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">arr := [2]byte&#123;1,2&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="切片定义"><a href="#切片定义" class="headerlink" title="切片定义"></a>切片定义</h3><ul>
<li>申明类型定义</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var sli []byte</span><br></pre></td></tr></table></figure>

<ul>
<li>直接赋值定义</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sli := make([]byte, 5)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="作为参数传入"><a href="#作为参数传入" class="headerlink" title="作为参数传入"></a>作为参数传入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import &quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">func changeSlicePoint(p *[]byte)&#123;</span><br><span class="line">	(*p)[0] = 100</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func changeSlice(a []byte)&#123;</span><br><span class="line">	a[4] = 100</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func changeArrayPoint(p *[5]byte)&#123;</span><br><span class="line">	(*p)[0] = 100</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func changeArray(a [5]byte)&#123;</span><br><span class="line">	a[4] = 100</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func main()  &#123;</span><br><span class="line">	array := [5]byte&#123;1,2,3,4,5&#125;</span><br><span class="line">	var sli1 []byte</span><br><span class="line">	sli1 = make([]byte, 5)</span><br><span class="line">	slice := []byte&#123;1,2,3,4,5&#125;</span><br><span class="line"></span><br><span class="line">	changeSlicePoint(&amp;sli1)</span><br><span class="line">	changeSlice(sli1)</span><br><span class="line">	changeArray(array)</span><br><span class="line">	changeArrayPoint(&amp;array)</span><br><span class="line">	changeSlicePoint(&amp;slice)</span><br><span class="line">	changeSlice(slice)</span><br><span class="line"></span><br><span class="line">	fmt.Printf(&quot;slice[0]: %d\nslice[4]: %d\n&quot;, slice[0], slice[4])</span><br><span class="line">	fmt.Printf(&quot;sli1[0]: %d\nsli1[4]: %d\n&quot;, sli1[0], sli1[4])</span><br><span class="line">	fmt.Printf(&quot;array[0]: %d\narray[4]: %d\n&quot;, array[0], array[4])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//output: </span><br><span class="line">//slice[0]: 100</span><br><span class="line">//slice[4]: 100</span><br><span class="line">//sli1[0]: 100</span><br><span class="line">//sli1[4]: 100</span><br><span class="line">//array[0]: 100</span><br><span class="line">//array[4]: 5</span><br><span class="line">//</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>切片传入时是按照引用传递的，加上指针甚至可以修改引用（内存地址）本身</p>
<p>  比如下面的代码会是的传入的引用重新指向nil指针：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func changeSlicePoint(p *[]byte)&#123;</span><br><span class="line">(*p)[0] = 100</span><br><span class="line">*p = nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>数组的传递是按值传递的，使用了指针可实现传入地址，从而实现对数组的修改 </li>
</ul>
]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Hexo——命令总结</title>
    <url>/Notes/Hexo/Hexo%E2%80%94%E2%80%94%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>Hexo命令归纳整理</em></p>
<hr>
<h3 id="init"><a href="#init" class="headerlink" title="init"></a>init</h3><p><code>hexo init [folder]</code> <br><br># 在cmd命令下，cd到你所需要建立博客的文件夹，执行此命令，其中folder为可选指令，若不写，则默认当前目录。</p>
<hr>
<h3 id="hexo-new"><a href="#hexo-new" class="headerlink" title="hexo new"></a>hexo new</h3><p><code>hexo new [layout] \&lt;title\&gt; </code><br><br>#layout为可选项，默认使用_config.yml中的default_layout。新建文章的指令</p>
<hr>
<h3 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h3><p><code>hexo generate</code> <br><br># 生成静态文件，<br><br># 可选参数：<br><br>-d ,–deploy 文件生成后立即部署网站<br><br>-w , –watch 件事文件变动</p>
<hr>
<h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><p><code>hexo deploy</code> <br><br># 发布到网站，这里就是发布到 _config.yml中deploy中设置的网址上。<br><br># 参数 <br><br>-g , –generate 部署前生成静态文件</p>
<hr>
<h3 id="npm-install"><a href="#npm-install" class="headerlink" title="npm install"></a>npm install</h3><p>每一个rn项目都有一个package.json文件，里面有很多组件信息，使用npm install将按照package.json安装所需要的组件放在生成的node_modules文件夹中，rn项目下的每一个文件中都可以通过import引入node_modules的组件来加以使用</p>
<hr>
<h3 id="hexo-clean"><a href="#hexo-clean" class="headerlink" title="hexo clean"></a>hexo clean</h3><p><code>hexo clean</code> 清除缓存文件(db.json)和已生成的静态文件(public)，通常更换主题后，无效时，可以运行此命令。</p>
<hr>
<h3 id="hexo-server"><a href="#hexo-server" class="headerlink" title="hexo server"></a>hexo server</h3><p><code>hexo server</code> <br><br># 启动server，就可以在本地预览效果。<br><br>#参数，默认网址<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> <br><br>-p , –port 重设端口 <br><br>-s , –static 只是用静态文件<br><br>-l , –log 启动日志记录，使用覆盖记录格式<br><br>-i , –ip 重新制定服务器ip</p>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo——项目文件目录说明</title>
    <url>/Notes/Hexo/Hexo%E2%80%94%E2%80%94%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E8%AF%B4%E6%98%8E.html</url>
    <content><![CDATA[<p><em>Hexo 项目文件目录说明归纳整理</em></p>
<hr>
<h3 id="package-json"><a href="#package-json" class="headerlink" title="./package.json"></a>./package.json</h3><p>这个文件指定了hexo框架的参数和依赖插件</p>
<hr>
<h3 id="package-lock-json"><a href="#package-lock-json" class="headerlink" title="./package-lock.json"></a>./package-lock.json</h3><p>package.json里面定义的是版本范围（比如 1.0.0），具体跑npm install的时候安的什么版本，要解析后才能决定，这里面定义的依赖关系树，可以称之为逻辑树（logical tree）。<br><br>node_modules文件夹下才是npm实际安装的确定版本的东西，这里面的文件夹结构我们可以称之为物理树（physical tree）。安装过程中有一些去重算法，所以你会发现逻辑树结构和物理树结构不完全一样。<br><br>package-lock.json可以理解成对结合了逻辑树和物理树的一个快照（snapshot），里面有明确的各依赖版本号，实际安装的结构，也有逻辑树的结构。其最大的好处就是能获得可重复的构建（repeatable build），当你在CI（持续集成）上重复build的时候，得到的artifact是一样的，因为依赖的版本都被锁住了。在npm5以后，其内容和npm-shrinkwrap.json一模一样。</p>
<hr>
<h3 id="scaffolds"><a href="#scaffolds" class="headerlink" title="./scaffolds"></a>./scaffolds</h3><p>scaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new ‘title’）的时候，hexo是根据这个目录下的文件进行构建的。scaffolds模版 文件夹。Hexo的模板是指在新建的markdown文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。</p>
<hr>
<h3 id="source"><a href="#source" class="headerlink" title="./source"></a>./source</h3><p>source 资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。</p>
<h4 id="source-posts"><a href="#source-posts" class="headerlink" title="./source/_posts"></a>./source/_posts</h4><p>需要新建的博文都放在 _posts 目录下。_posts 目录下是一个个 markdown 文件。你应该可以看到一个 hello-world.md 的文件，文章就在这个文件中编辑。_posts 目录下的md文件，会被编译成html文件，放到 public （此文件现在应该没有，因为你还没有编译过）文件夹下。</p>
<hr>
<h3 id="themes"><a href="#themes" class="headerlink" title="./themes"></a>./themes</h3><p>网站主题目录，hexo有非常好的主题拓展，支持的主题也很丰富。该目录下，每一个子目录就是一个主题。<br>More info: <a href="https://hexo.io/themes" target="_blank" rel="noopener">hexo主题</a></p>
<hr>
<h3 id="config-yml"><a href="#config-yml" class="headerlink" title="./_config.yml"></a>./_config.yml</h3><p>_config.yml 采用YAML语法格式，<a href="https://my.oschina.net/u/1861837/blog/526142?p=%3C!--%EF%BF%BC0--%3E" target="_blank" rel="noopener">具体语法在这里</a> 。<br>具体配置可以参考<a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="noopener">官方文档</a>，_config.yml 文件中的内容，并对主要参数做简单的介绍</p>
<hr>
<h3 id="public"><a href="#public" class="headerlink" title="./public"></a>./public</h3><p>public 生成的网站文件，发布的站点文件。</p>
<hr>
<h3 id="tag"><a href="#tag" class="headerlink" title="./tag"></a>./tag</h3><p>tag 标签文件夹</p>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——LightGBM概述</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94LightGBM%E6%A6%82%E8%BF%B0.html</url>
    <content><![CDATA[<p><em>本文介绍LightGBM的一些特性和并行实现方法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考博客: <a href="https://www.cnblogs.com/ldt-/p/10206356.html" target="_blank" rel="noopener">https://www.cnblogs.com/ldt-/p/10206356.html</a></li>
</ul>
<hr>
<h3 id="LightGBM-vs-XGBoost"><a href="#LightGBM-vs-XGBoost" class="headerlink" title="LightGBM vs XGBoost"></a>LightGBM vs XGBoost</h3><ul>
<li>树结点切分方式不同:<ul>
<li>XGBoost树节点切分是 <strong>Level-wise</strong><img src="/Notes/ML/Models/ML——LightGBM概述/level-wise.png"></li>
<li>LightGBM树节点切分是 <strong>Leaf-wise</strong><img src="/Notes/ML/Models/ML——LightGBM概述/leaf-wise.png"></li>
</ul>
</li>
<li>LightGBM<strong>直接支持类别特征</strong>，对类别特征<strong>不必</strong>进行 <strong>One-Hot</strong> 处理</li>
<li>实现方面: <ul>
<li><strong>直方图算法</strong>(近似切分算法)最初由LightGBM提出,后来的XGBoost算法也实现了直方图算法</li>
</ul>
</li>
<li>XGBoost的近似搜索算法和LightGBM的直方图算法不同<ul>
<li>XGBoost的近似搜索算法是保存所有样本的二阶梯度,用分位数确定划分方法,他的分割点是可以直接通过计算总的样本梯度和和分位数点得到的.<br>      * LightGBM算法是将所有样本放进对应的桶中,并以当前桶作为划分点,计算左右桶的最大增益,它的最优点是遍历所有的桶才能得到的.</li>
</ul>
</li>
<li>LightGBM 通信优化 比 XGBoost 做得好<ul>
<li>这里有亲身体验: XGBoost使用多个处理器时,有时候处理器数量增加训练速度不增加,甚至反而变慢,<code>xgboost.XGBClassifier</code></li>
</ul>
</li>
<li>LightGBM 使用了 <strong>GOSS</strong>(Gradient-based One-Side Sampling) 来做采样算法<ul>
<li>GOSS 是通过区分不同梯度的实例，保留较大梯度实例同时对较小梯度随机采样的方式减少计算量，从而达到提升效率的目的</li>
<li><strong>GOSS 算法流程</strong>: <ul>
<li>根据样本的梯度将样本降序排序</li>
<li>保留前 \(a \ (0 &lt; a &lt; 1)\) 比例大小的数据样本，作为数据子集 \(Z_1\), 也就是保留 \(a * len(all\_samples)\) 的数据样本</li>
<li>对于剩下的数据的样本，随机采样获得大小为 \(b \ (0 &lt; b &lt; 1)\) 的数据子集 \(Z_2\), 也就是采样 \(b * len(all\_samples)\) 的数据样本</li>
<li>计算信息增益时对采样的 \(Z_2\) 样本的梯度数据乘以 \((1-a)/b\)（目的是不改变原数据的分布）</li>
</ul>
</li>
<li>GOSS的思想是,梯度大的实例正常使用,梯度小的实例就通过采样实现部分拟合总体的方法(拟合时不改变原来的分布,除以采样率就行了)</li>
</ul>
</li>
<li>LightGBM 使用了 <strong>EFB</strong> (Exclusive Feature Bundling)<ul>
<li>EFB 通过特征捆绑的方式减少特征维度(其实是降维技术)的方式,提升计算效率</li>
<li>通常被捆绑的特征都是互斥的(一个特征值为0,一个特征值不为0), 这样两个特征捆绑起来也不会造成信息丢失</li>
<li>当两个特征不是完全互斥时,可以用一个指标对特征不互斥程度进行评估,称为<strong>冲突比率</strong></li>
<li>冲突比率较小时,我们可以将他们捆绑而不太影响训练结果</li>
<li>EFB 算法流程: <ul>
<li>将特征按照非零值的个数进行排序</li>
<li>计算不同特征之间的冲突比率</li>
<li>遍历每个特征并尝试合并特征，使冲突比率最小化</li>
</ul>
</li>
</ul>
</li>
<li>LightGBM 的内存优化<ul>
<li>XGBoost 中 需要对每个特征进行预排序(注意:这里不能在算是XGBoost的缺点了,后来的XGBoost也实现了这个直方图算法,不需要预排序了)</li>
<li>LightGBM使用直方图算法替代了预排序</li>
</ul>
</li>
</ul>
<hr>
<h3 id="LightGBM的优点"><a href="#LightGBM的优点" class="headerlink" title="LightGBM的优点"></a>LightGBM的优点</h3><ul>
<li>相对XGBoost:<ul>
<li>速度快 (GOSS, EFB, Histogram等)</li>
<li>内存少 (XGBoost中排序)</li>
<li>精度高(效果不明显, 与XGBoost相当, 本人测试, 实际使用中有时候不如XGBoost, 可能是参数调节的问题)</li>
</ul>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>虽然官方一再强调LightGBM的精度不比XGBoost低,而且可能超过XGBoost,但是实践中, LightGBM除了比XGBoost快以外, 精度方面没什么优势, 甚至精度还不如XGBoost(注意: 也可能是我调参技术不行)<ul>
<li>问题: 为什么某些数据集上出现LightGBM比XGBoost精度差的情况? </li>
<li>回答: (个人理解)因为GOSS和EFB会带来一定的精度损失</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>LightGBM = XGBoost + Histogram + GOSS + EFB</strong><ul>
<li>XGBoost: 不同于XGBoost的, 树节点切分不同, LightGBM使用了 <strong>Leaf-wise</strong> 的生长策略</li>
<li>Histogram: <ul>
<li>Histogram方法牺牲了一定的精度,但是作者强调了在实验中精度降低并不多</li>
<li>开始的XGBoost使用的是预先排序的方式, 后来在 XGBoost 中也实现了Histogram</li>
<li>LightGBM 对 Histogram 算法进一步加速</li>
<li>一个叶子节点的Histogram可以直接由父节点的Histogram和兄弟节点的Histogram做差得到, 一般情况下，构造Histogram需要遍历该叶子上的所有数据，通过该方法，只需要遍历Histogram的k个捅, 速度提升了一倍</li>
</ul>
</li>
<li>GOSS: 对于梯度小的样本, 使用采样部分代替总体的方法省时间</li>
<li>EFB: 互斥特征捆绑,提升计算效率</li>
</ul>
</li>
<li>LightGBM 真正做到了把并行做到极致<ul>
<li>特征并行: 在不同的机器在不同的特征集合上分别寻找最优的分割点, 然后再机器间同步最优的分割点.</li>
<li>数据并行: 让不同的机器先在本地构造直方图, 然后进行全局的合并,然后在合并的直方图上寻找最优的分割点.</li>
</ul>
</li>
<li>LightGBM 支持类别特征<ul>
<li>无需将类别特征 One-Hot</li>
</ul>
</li>
<li>问题: 为什么XGBoost也使用了直方图,但是速度依然比较慢?<br>  * 直方图算法的实现有两种:<ul>
<li><ol>
<li><strong>全局构造</strong>,在树的构造过程中只建立一次直方图, 每次分裂都从缓存的直方图中寻找分裂点</li>
</ol>
</li>
<li><ol start="2">
<li><strong>局部构造</strong>, 每次树分裂到一层的时候就建立一次直方图</li>
</ol>
</li>
<li>XGBoost使用的是局部构造的方式, 所以速度会较慢</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——LR-逻辑回归</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94LR-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<hr>
<h3 id="手动推导流程"><a href="#手动推导流程" class="headerlink" title="手动推导流程"></a>手动推导流程</h3><ul>
<li>假设有m样本:\(X = (x_{1}, x_{2}, x_{3},\dots x_{m})\)<ul>
<li>样本点为: \(((x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3}),\dots (x_{m}, y_{m}))\)</li>
</ul>
</li>
<li>假设\(w, \theta, x_{i}\)等所有向量都为列向量</li>
</ul>
<h4 id="确定分类决策函数"><a href="#确定分类决策函数" class="headerlink" title="确定分类决策函数"></a>确定分类决策函数</h4><ul>
<li>线性回归模型<br>$$f(x) = w^{T}x + b$$</li>
<li>令<br>$$<br>\begin{align}<br>\theta &amp;= (w; b) \\<br>x_{i} &amp;= (x_{i}; 1)<br>\end{align}<br>$$</li>
<li>有<br>$$f(x) = \theta^{T} x$$</li>
<li>逻辑回归决策函数<strong>在线性回归模型上加一个sigmoid函数</strong><br>$$<br>\begin{align}<br>h_{\theta}(x) &amp;= sigmoid(f(x)) \\<br>&amp;= \frac{1}{1+e^{-f(x)}} \\<br>&amp;= \frac{1}{1+e^{-\theta^{T} x}} \\<br>\end{align}<br>$$</li>
<li>即<br>$$<br>\begin{align}<br>p(y=1|x) &amp;= h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T} x}} = \frac{e^{\theta^{T} x}}{1+e^{\theta^{T} x}}\\<br>p(y=0|x) &amp;= 1-h_{\theta}(x) = \frac{e^{-\theta^{T} x}}{1+e^{-\theta^{T} x}} = \frac{1}{1+e^{\theta^{T} x}} \\<br>\end{align}<br>$$</li>
<li><strong>对数几率(log odds, 也称为logit)</strong>定义为: \(ln \frac{p}{1-p}\) ,在LR中有:<br>$$<br>\begin{align}<br>ln\frac{h_{\theta}(x)}{1-h_{\theta}(x)} = \theta^T x<br>\end{align}<br>$$</li>
<li>分类超平面不确定,与最终的阈值有关,\(\alpha\)的值与最终阈值相关<br>$$w^{\star}x + b^{\star} = \alpha$$<ul>
<li>分类超平面由\((w, b)\)和阈值唯一确定,(注意: SVM的分类超平面由\((w, b)\)唯一确定)</li>
<li>这一点和SVM不同,SVM的分类超平面是确定的,详情参看<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94SVM-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html">ML——SVM-支持向量机</a></li>
</ul>
</li>
</ul>
<h4 id="确定优化目标"><a href="#确定优化目标" class="headerlink" title="确定优化目标"></a>确定优化目标</h4><ul>
<li>LR中使用<strong>极大似然法</strong><br>$$<br>\begin{align}<br>L(\theta) &amp;= p(Y|X;\theta) \\<br>&amp;= \prod_{i=1}^{m}p(y_{i}|x_{i}; \theta) \\<br>&amp;= \prod_{i=1}^{m}(p(y_{i} = 1|x_{i};\theta))^{y_{i}}(p(y_{i} = 0|x_{i};\theta))^{1-y_{i}}<br>\end{align}<br>$$</li>
<li>上面的式子不易求导优化,我们使用与上面的式子单调性和最优点等价的<strong>对数似然函数</strong>为<br>$$<br>\begin{align}<br>LL(\theta) &amp;= log L(\theta) \\<br>&amp;= log \prod_{i=1}^{m}(p(y_{i} = 1|x_{i};\theta))^{y_{i}}(p(y_{i} = 0|x_{i};\theta))^{1-y_{i}} \\<br>&amp;= \sum_{i=1}^{m}\left (y_{i}log(p(y_{i} = 1|x_{i};\theta)) + (1-y_{i})log(p(y_{i} = 0|x_{i};\theta)) \right ) \\<br>&amp;= \sum_{i=1}^{m}\left (y_{i}log\frac{p(y_{i} = 1|x_{i};\theta)}{p(y_{i} = 0|x_{i};\theta)} + log(p(y_{i} = 0|x_{i};\theta))\right ) \\<br>\end{align}<br>$$<ul>
<li>上面的式子中：<br>$$\sum_{i=1}^{m}\left (y_{i}log(p(y_{i} = 1|x_{i};\theta)) + (1-y_{i})log(p(y_{i} = 0|x_{i};\theta)) \right )$$<br>加个负号即为为交叉熵损失函数<br>$$-\sum_{i=1}^{m}\left (y_{i}log(p(y_{i} = 1|x_{i};\theta)) + (1-y_{i})log(p(y_{i} = 0|x_{i};\theta)) \right )$$<br>所以交叉熵损失函数与逻辑回归的对数似然损失函数（=逻辑回归对数似然函数的负数）是等价的</li>
</ul>
</li>
<li>由前面的推导有<br>$$<br>\begin{align}<br>log \frac{p(y_{i} = 1|x_{i};\theta)}{p(y_{i} = 0|x_{i};\theta)} = log\frac{\frac{e^{\theta^{T} x}}{1+e^{\theta^{T} x}}}{\frac{1}{1+e^{\theta^{T} x}}} = log e^{\theta^{T} x} = \theta^{T}x\\<br>\end{align}<br>$$<br>$$log(p(y_{i} = 0|x_{i};\theta)) = log(\frac{1}{1+e^{\theta^{T} x}}) = -log(1+e^{\theta^{T} x})$$</li>
<li>故而有<br>$$<br>\begin{align}<br>LL(\theta) &amp;= \sum_{i=1}^{m}\left (y_{i}log\frac{p(y_{i} = 1|x_{i};\theta)}{p(y_{i} = 0|x_{i};\theta)} + log(p(y_{i} = 0|x_{i};\theta))\right )\\<br>&amp;= \sum_{i=1}^{m}\left ( y_{i}\theta^{T}x -  log(1+e^{\theta^{T}x}) \right)<br>\end{align}<br>$$</li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><ul>
<li>最大化似然函数等价于最小化似然函数的负数</li>
<li>LR中使用<strong>极大似然法</strong>,所以对应的损失函数自然为<strong>对数似然损失函数</strong><br>$$loss(\theta) = -LL(\theta) = \sum_{i=1}^{m}\left (- y_{i}\theta^{T}x + log(1+e^{\theta^{T}x}) \right)$$</li>
</ul>
<h4 id="梯度下降法优化"><a href="#梯度下降法优化" class="headerlink" title="梯度下降法优化"></a>梯度下降法优化</h4><p><em>注意: 这里优化目标也可以用牛顿法</em></p>
<ul>
<li>目标,求一个\(\theta^{\star}\)满足似然函数最大化或者损失函数最小化<br>$$\theta^{\star} = \arg\max_{\theta} LL(\theta) = \arg\min_{\theta} -LL(\theta) = \arg\min_{\theta} loss(\theta)$$</li>
<li>对数似然函数对参数\(\theta\)求导有<br>$$<br>\begin{align}<br>\frac{\partial loss(\theta)}{\partial\theta} &amp;= \sum_{i=1}^{m}\left ( -y_{i}x_{i} + \frac{x_{i}e^{\theta^{T}x}}{1+e^{\theta^{T}}x}\right ) \\<br>&amp;= \sum_{i=1}^{m}x_{i}\left ( -y_{i} + \frac{e^{\theta^{T}x}}{1+e^{\theta^{T}}x}\right ) \\<br>&amp;= \sum_{i=1}^{m}x_{i}\left ( -y_{i} + h_{\theta}(x_{i})\right ) \\<br>\end{align}<br>$$</li>
<li>LR模型的梯度下降参数迭代公式<br>$$<br>\begin{align}<br>\theta^{t+1} &amp;= \theta^{t} - \alpha\sum_{i=1}^{m}x_{i}\left ( -y_{i} + h_{\theta^{t}}(x_{i})\right ) \\<br>&amp;= \theta^{t} + \alpha\sum_{i=1}^{m}x_{i}\left ( y_{i} - h_{\theta^{t}}(x_{i})\right )<br>\end{align}<br>$$<ul>
<li>其中\(\alpha\)为步长</li>
</ul>
</li>
<li><strong>线性回归</strong>和<strong>LR模型</strong>的梯度下降法参数迭代公式表达式看似相同,但是不同的模型对应的<strong>\(h_{\theta}\)函数并不相同</strong></li>
</ul>
<hr>
<h3 id="其他总结"><a href="#其他总结" class="headerlink" title="其他总结"></a>其他总结</h3><p><em>参考:<a href="https://www.cnblogs.com/ModifyRong/p/7739955.html" target="_blank" rel="noopener">https://www.cnblogs.com/ModifyRong/p/7739955.html</a></em></p>
<ul>
<li>LR中使用<strong>极大似然法</strong>,所以对应的损失函数自然为<strong>对数似然损失函数(对数损失函数)</strong><ul>
<li>对数似然损失函数定义为:<br>$$Loss(\theta) = -P(Y|X; \theta)$$</li>
<li>&lt;&lt;统计学习方法&gt;&gt;P213中定义LR的损失函数为逻辑斯蒂损失函数,在LR模型和最大熵模型中,逻辑斯蒂损失函数本质上与对数似然损失函数等价,可推导得到</li>
</ul>
</li>
<li>一句话概括逻辑回归:<ul>
<li><strong>逻辑回归是假设数据服从伯努利分布,通过极大似然函数的方法,运用梯度下降法或者牛顿法来求解参数,来达到将数据二分类的目的</strong></li>
<li>逻辑回归的假设: <strong>数据服从伯努利分布</strong>, \(p(y=1|x) = 1-p(y=0|x)\)</li>
<li>逻辑回归的损失函数: <strong>对数似然损失函数(交叉熵损失函数)</strong>, 也就是对数似然函数的负数</li>
<li>逻辑回归的求解方法: <strong>梯度下降法或牛顿法</strong></li>
<li>逻辑回归的目的: <strong>将数据二分类</strong></li>
<li>逻辑回归如何分类: <strong>预测结果是连续的[0-1]的数</strong>,我们一般选择0.5作为阈值来分类,但是这个值可能是可以变化的,因为损失函数最小并不意味着0.5时分类精度最高</li>
</ul>
</li>
<li>为什么要用极大似然法?等价于为什么要用对数似然损失函数作为损失函数?<ul>
<li>损失函数一般有平方损失函数,对数损失函数,合页损失函数,绝对值损失函数等,极大似然函数取对数后等同于对数损失函数,在逻辑回归这个模型中,推导可以得到,对数损失函数训练求解参数的迭代函数只与\(x_{i}, y_{i}\)相关,与sigmoid函数的梯度等无关.这样的参数更新自始至终都比较稳定</li>
<li>为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数,你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的,sigmod函数在它在定义域内的梯度都不大于0.25, 这样训练会非常的慢</li>
</ul>
</li>
<li>逻辑回归中,如果某些特征高度相关,甚至某些特征完全相同,会造成什么影响?<ul>
<li>损失函数收敛后,没有影响,因为特征的相关性并不影响分类器效果,重复特征会分化特征的权重(10个重复特征和单个特征训练结果差别在于前者每个特征的权重是后者的十分之一),本质上最终结果不变的</li>
<li>但是训练时由于特征重复,参数增多,模型复杂度增加,训练时长,内存等都会增加</li>
</ul>
</li>
<li>为什么需要去掉高度相关的特征?<ul>
<li>去掉高度相关的特征使得模型可解释性更好</li>
<li>提高训练时间,节约内存,减少参数数量</li>
<li>特征的提取本身也需要时间,实际工程项目中可以少提取一个特征往往能节约很多时间</li>
</ul>
</li>
<li><strong>logistic与logit的区别</strong>?<ul>
<li>logit: 又名 log adds , 指的是”对数几率”, 定义为\(ln\frac{p}{1-p}\)</li>
<li>logistic: 又叫Sigmoid函数, 指的是”对数几率函数”, 本质上是一种”Sigmoid”函数, 定义为 \(f(x) = \frac{1}{1+e^{-x}}\)</li>
</ul>
</li>
<li>简单介绍LR模型的优缺点:<ul>
<li>优点:<ul>
<li>模型简单,可解释性好,(如果对数据特征进行了归一化处理的话)可以从特征的权重看到不同特征对最终结果的影响</li>
<li>模型效果往往不错(特征工程做得好的话)</li>
<li>训练速度快, 成熟的SGD优化方法(SGD可以分布式)等</li>
<li>内存占用小</li>
<li>输出结果可以作为概率值,然后可以对阈值根据实际进行划分,不一定是确定的0.5,只是一般选择0.5而已</li>
</ul>
</li>
<li>缺点:<ul>
<li><strong>难以处理非线性数据,本质上是线性分类面</strong></li>
<li><strong>难以处理数据不平衡问题</strong>, 这里如果正例远远多于负例,那么全都预测为正例,整体损失函数也不会太大</li>
<li><strong>LR本身无法筛选特征</strong>,有时候会用GBDT和XGBoost来筛选特征,然后再用LR模型</li>
</ul>
</li>
</ul>
</li>
<li>扩展:逻辑回归可像SVM一样引入核函数处理非线性分类问题吗?<ul>
<li>一般来说不可以</li>
<li><strong>[存疑]</strong>理论上通过对原始样本非线性映射,似乎也可以,如果将\(f(x) = \theta^{T}x\)中的\(f(x)\)看做\(\theta\)看做变量,然后类比SVM的核函数,定义一个关于\(x_{i}\)的非线性映射<ul>
<li>这里\(x_{i}\)表示第\(i\)个样本, 用\(x_{i}^{j}\)表示第\(i\)个样本的第\(j\)个维度<br>$$x_{i}^{j} = \phi_{j}(x_{i}^{j})$$</li>
<li>基于上述非线性映射函数的定义,我们对每个样本都进行线性映射,每个维度用不同的映射函数(不同样本相同维度映射函数相同)</li>
<li>这里的非线性映射与SVM的核函数不同,SVM不使用核函数的话,也可以通过相同的非线性映射的方式实现非线性分类</li>
<li>使用核技巧后的LR模型将变得很慢,SVM与kernels是相配的,而LR与kernels会十分慢(来源<a href="https://www.cnblogs.com/yan2015/p/5183284.html" target="_blank" rel="noopener">SVM核技巧</a>)</li>
</ul>
</li>
</ul>
</li>
<li>LR模型训练完成后，输出概率多少的样本应该评估为正样本？【以下分析为个人理解，暂无严格证明】<ul>
<li><strong>LR模型的损失函数本质上是交叉熵损失函数，交叉熵损失函数本质是最小化预估分布与训练样本分布之间的差距，故而预估均值与真实训练样本均值应该相等</strong>，即LR模型的预估值均值理论上与训练样本标签均值相同（这里LR的预估值是Sigmoid的输出值，训练样本负样本标签为0，正样本标签为1）。【PS，一种辅助理解思考：假设训练集中的样本特征完全相同，但其中30%是正样本，另外70%为负样本，那么优秀的LR模型在预估该训练样本时应该输出约为0.3】</li>
<li>进一步来说，当预估的平均值大于训练样本的均值时，即可判断为正样本。<ul>
<li>举例，假设训练样本的均值为0.4，那么预估值大于0.4的样本均可视为正样本（思考：这种判定下模型的准确率\( \text{Accuracy} = \frac{TP+TN}{TP+TN+FP+FN}\)应该是最高的？）</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——XGBoost-vs-传统GBDT</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94XGBoost-vs-%E4%BC%A0%E7%BB%9FGBDT.html</url>
    <content><![CDATA[<p><em>本文主要介绍XGBoost和其他传统GBDT的比较的优劣</em><br><em>XGBoost又叫(Newton Boosting Tree)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>GBDT推导: <a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94GBDT-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B.html">ML——GBDT-梯度提升树-推导过程</a></li>
<li>XGBoost推导: <a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94XGBoost-%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B.html">ML——XGBoost-推导过程</a></li>
</ul>
<hr>
<h3 id="XGBoost的优点"><a href="#XGBoost的优点" class="headerlink" title="XGBoost的优点"></a>XGBoost的优点</h3><p><em>参考博客: <a href="https://www.cnblogs.com/massquantity/p/9794480.html" target="_blank" rel="noopener">https://www.cnblogs.com/massquantity/p/9794480.html</a></em></p>
<ul>
<li><p>XGBoost<strong>损失函数</strong>是<strong>二阶泰勒</strong>展开(与牛顿法对应),GBDT是<strong>一阶泰勒</strong>展开(与梯度下降法对应)</p>
<ul>
<li>传统 GBDT 在优化时只用到一阶导数信息, 所以传统GBDT也叫 (Gradient Boosting)</li>
<li>XGBoost 则对目标函数进行了二阶泰勒展开，同时用到了一阶和二阶导数, 所以XGBoost又叫(Newton Boosting Tree)</li>
</ul>
</li>
<li><p>XGBoost加了<strong>正则项</strong>,普通的GBDT没有,所以XGBoost学出来的模型更简单,能防止过拟合,提高模型的泛化性能</p>
</li>
<li><p>XGBoost Shrinkage(缩减)</p>
<ul>
<li>每次进行完一次迭代后,将叶子节点的权重乘以该系数(一般叫做<code>eta</code>\(\eta\))</li>
<li>可以理解为这里Shrinkage是将学习速率调小,从而需要的迭代次数增多</li>
<li>减小学习率实际上是减弱了每棵树对整体的影响,从而让后面的树有更多的学习空间</li>
<li>下面的表述还有待确定:<ul>
<li>进一步惩罚决策树叶节点的值(惩罚的意思是叶节点越大,惩罚越多,损失函数越大)</li>
<li>对叶节点的惩罚本身可以理解为一个正则化</li>
</ul>
</li>
</ul>
</li>
<li><p>结点分裂的<strong>增益计算公式</strong>不同</p>
<ul>
<li>传统 GBDT 一般采用的是<strong>最小二乘法</strong>作为内部分裂的增益计算指标(因为用的都是回归树)<ul>
<li>注意: 这里论文中描述的最小绝对偏差回归(LAD)是第一步损失函数的定义,不是这一步中的信息增益计算</li>
<li>查看源码: <code>sklearn.ensemble.GradientBoostingClassifier</code>在分裂结点时可以选择三种方式:<ul>
<li><code>friedman_mse</code>(默认), mean squared error with improvement score by Friedman</li>
<li><code>mse</code>: mean squared error</li>
<li><code>mae</code>: mean absolute error</li>
</ul>
</li>
</ul>
</li>
<li>而 XGBoost 使用的是经过优化推导后的式子<br>$$<br>\begin{align}<br>Gain = \frac{G_L^2}{H_L+ \lambda} + \frac{G_R^2}{H_R+ \lambda} - \frac{(G_L + G_R)^2}{H_L+ H_R + \lambda} - \gamma<br>\end{align}<br>$$<ul>
<li>注意: XGBoost中的信息增益计算形式固定为上面的计算方式,但是具体的值与损失函数的定义相关(因为 \(g_i\) 和 \(h_i\) 的是损失函数的一阶和二阶梯度)</li>
</ul>
</li>
</ul>
</li>
<li><p>XGBoost<strong>支持自定义的损失函数</strong>,支持一阶和二阶可导就行</p>
<ul>
<li>注意,这里的损失函数指的是 \(l(y_i,\hat{y}_i)\), 单个样本预测值与目标值的差异, 也就是单个样本的损失函数</li>
<li>从<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94XGBoost-%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B.html">ML——XGBoost-推导过程</a>中可知:<ul>
<li>\(g_i = l’(y_i,\hat{y}_i^{t-1})\) 为 \(l(y_i,\hat{y}_i)\) 对 \(\hat{y}_i\) 的一阶导数在 \(\hat{y}_i = \hat{y}_i^{t-1}\) 处的值 </li>
<li>\(h_i = l’’(y_i,\hat{y}_i^{t-1})\) 是 \(l(y_i,\hat{y}_i)\) 对 \(\hat{y}_i\) 的二阶导数在 \(\hat{y}_i = \hat{y}_i^{t-1}\) 处的值 </li>
</ul>
</li>
<li>XGBoost中只要损失函数二次可微分即可得到 \(g_i\) 和 \(h_i\)<ul>
<li>\(g_i\) 和 \(h_i\) 本身与损失函数的定义形式无关, 只要求损失函数二阶可微分即可</li>
</ul>
</li>
<li>只要有了 \(g_i\) 和 \(h_i\) 我们即可<ul>
<li>根据预先推导的叶子节点分数表达式 \(w_j^{\star} = -\frac{G_j}{H_j+\lambda}\) 求得叶子结点的分数</li>
<li>根据预先推导的信息增益公式 \(Gain = \frac{G_L^2}{H_L+ \lambda} + \frac{G_R^2}{H_R+ \lambda} - \frac{(G_L + G_R)^2}{H_L+ H_R + \lambda} - \gamma\) 确定分裂特征和分裂点</li>
</ul>
</li>
<li>GBDT 损失函数关系一般选择<strong>最小二乘回归</strong>或者<strong>最小绝对偏差回归</strong><ul>
<li><strong>最小方差回归</strong>: (Least-Squares Regression, LSR)<br>$$\begin{align} L(y,F(x)) = \frac{1}{2}(y-F(x))^{2} \end{align}$$</li>
<li><strong>最小绝对偏差回归</strong>: (Least Absolute Deviation Regression, LAD)<br>$$\begin{align} L(y,F(x)) = |y-F(x)| \end{align}$$</li>
<li>查看源码: <code>sklearn.ensemble.GradientBoostingClassifier</code>的损失函数是定义好的, 不能自己定义, 详细如下源码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LOSS_FUNCTIONS = &#123;&apos;ls&apos;: LeastSquaresError,</span><br><span class="line">                  &apos;lad&apos;: LeastAbsoluteError,</span><br><span class="line">                  &apos;huber&apos;: HuberLossFunction,</span><br><span class="line">                  &apos;quantile&apos;: QuantileLossFunction,</span><br><span class="line">                  &apos;deviance&apos;: None,    # for both, multinomial and binomial</span><br><span class="line">                  &apos;exponential&apos;: ExponentialLoss,</span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_SUPPORTED_LOSS = (&apos;deviance&apos;, &apos;exponential&apos;)</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">if (self.loss not in self._SUPPORTED_LOSS</span><br><span class="line">                or self.loss not in LOSS_FUNCTIONS):</span><br><span class="line">            raise ValueError(&quot;Loss &apos;&#123;0:s&#125;&apos; not supported. &quot;.format(self.loss))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>XGBoost 借鉴了随机森林的做法，<strong>支持列采样</strong>，不仅能降低过拟合，还能减少计算量，这也是 XGBoost 异于传统 GBDT 的一个特性</p>
<ul>
<li>列采样: 这借鉴于随机森林中的做法,每棵树不使用所有特征,而是部分特征参与训练</li>
<li>可以减少计算量,同时还能降低过拟合,简直优秀</li>
</ul>
</li>
<li><p>XGBoost 有<strong>缺失值自动处理</strong>, 在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销,在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向</p>
<ul>
<li>进一步理解稀疏数据的支持: [待更新]</li>
</ul>
</li>
<li><p><strong>并行化处理</strong>：由于 Boosting 本身的特性，传统 GBDT 无法像随机森林那样树与树之间的并行化</p>
<ul>
<li>XGBoost 的并行主要体现在特征粒度上，在对结点进行分裂时，由于已预先对特征排序并保存为block 结构，每个特征的增益计算就可以开多线程进行，极大提升了训练速度</li>
</ul>
</li>
<li><p><strong>剪枝策略</strong>不同</p>
<ul>
<li>传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合</li>
<li>XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝<ul>
<li>而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的[待更新:XGBoost剪枝的策略是怎样的?只依赖信息增益指标吗?]</li>
<li>和<a href="https://www.cnblogs.com/massquantity/p/9794480.html" target="_blank" rel="noopener">博客</a>作者指出的一样,我这里并不是”纯粹的”后剪枝,因为提前设定了最大深度</li>
</ul>
</li>
</ul>
</li>
<li><p>基分类器的选择不同:</p>
<ul>
<li><strong>传统GBDT</strong>中原始论文使用<strong>树回归</strong>,论文见<a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">Firedman 1999</a>,后来作者提出可以使用<strong>逻辑回归</strong>,论文见<a href="https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf" target="_blank" rel="noopener">Friedman 2000</a></li>
<li><strong>XGBoost</strong>后面的各种损失计算等都包含着树模型的复杂度,叶子节点分类等,所以是<strong>只能用CART,不能使用逻辑回归</strong>的</li>
<li>(从函数空间定义和后面的公式推导来看)<strong>XGBoost中基函数只使用CART回归树，不能使用逻辑回归</strong></li>
<li>但是事实上XGBoost的实现中是支持线性分类器作为基分类器的, 参数<code>booster[default=&#39;gbtree&#39;]</code>,可选为<code>booster=gblinear</code><ul>
<li>使用<strong>线性分类器作为基分类器</strong>时, XGBoost相当于带有L1正则化和L2正则化的:<ul>
<li>Logistic回归(分类问题)</li>
<li>线性回归(回归问题)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>分桶策略</strong>算法不同</p>
<ul>
<li>传统的GBDT分桶时每个样本的权重都是相同的</li>
<li>XGBoost中每个样本的权重为损失函数在该样本点的二阶导数(对不同的样本,计算得到的损失函数的二阶导数是不同的), 这里优点AdaBoost的思想,重点关注某些样本的感觉</li>
<li>这里影响的是划分点的位置(我们划分划分点[桶]时都是均匀划分样本到桶里面,当不同样本的权重不同时,每个桶里面的样本数量可能会不同)</li>
<li>下图是一个示例<img src="/Notes/ML/Models/ML——XGBoost-vs-传统GBDT/split_point_method_weighted_in_xgboost.png">

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="XGBoost的缺点"><a href="#XGBoost的缺点" class="headerlink" title="XGBoost的缺点"></a>XGBoost的缺点</h3><p><em>注意这里是</em></p>
<ul>
<li><strong>空间消耗大</strong><ul>
<li>因为要在训练之前先对<strong>每个特征</strong>进行<strong>预排序</strong>并将结果存储起来，所以空间消耗较大</li>
<li>GBDT无需预排序,但是每次重新排序很耗时间</li>
</ul>
</li>
<li>速度慢<ul>
<li>虽然XGBoost速度比传统 GBDT 快了不少, 但是不如 LightGBM 快, 且 LightGBM 占用内存更低</li>
</ul>
</li>
</ul>
<hr>
<h3 id="XGBoost为什么能够并行"><a href="#XGBoost为什么能够并行" class="headerlink" title="XGBoost为什么能够并行?"></a>XGBoost为什么能够并行?</h3><p><em>而GBDT是不能并行的，原因是：<a href="https://www.136.la/shida/show-187480.html" target="_blank" rel="noopener">https://www.136.la/shida/show-187480.html</a></em></p>
<ul>
<li>GBDT不能并行的原因是没有预排序（XGB的预排序结果会存储到block结构）等，在有了预排序结果后，同一个特征的切割方式可以并行尝试计算增益</li>
<li>决策树最耗时间(包括GBDT)的步骤是对特征值的排序<ul>
<li>用于确定最佳分割点</li>
</ul>
</li>
<li>XGBoost训练前,预先对数据进行了排序,称为预排序<ul>
<li>将预先排序的结果保存为block结构, 后面迭代的时候重复使用这个结构,从而实现一次排序,多次使用,大大减少计算量</li>
</ul>
</li>
<li>这个结构减少计算量的同时还为并行化提供了可能([待更新]实际上不用预排序也能并行的吧?只是每次需要先使用一个单一线程排序,然后再多个线程并行?只是不够并行)<ul>
<li>进行结点的分裂时,需要计算每个特征的增益,然后选择增益最大的那个特征分裂</li>
<li>这里我们可以同时使用多个线程计算不同特征的增益, 从而实现并行化</li>
</ul>
</li>
<li>总结为三方面的并行, ([待更新]但是具体实现了哪些并行不确定)<ul>
<li>同一层级的结点间每个结点的分裂可以并行</li>
<li>同一个结点内部不同特征增益的计算可以并行</li>
<li>同一个结点同一个特征的不同分裂点的增益计算可以并行</li>
</ul>
</li>
</ul>
<hr>
<h3 id="GBDT为什么不能自定义损失函数"><a href="#GBDT为什么不能自定义损失函数" class="headerlink" title="GBDT为什么不能自定义损失函数?"></a>GBDT为什么不能自定义损失函数?</h3><p><em>GBDT为什么不能像XGBoost一样自定义损失函数?</em></p>
<ul>
<li>查看<code>sklearn.ensemble.GradientBoostingClassifier</code>的源码发现, 确实不支持自定义的损失函数</li>
<li>[待更新],因为涉及到后面的参数更新方式?</li>
</ul>
<hr>
<h3 id="XGBoost如何使用自定义的损失函数"><a href="#XGBoost如何使用自定义的损失函数" class="headerlink" title="XGBoost如何使用自定义的损失函数?"></a>XGBoost如何使用自定义的损失函数?</h3><p><em>模型直接调用<code>fit</code>函数无法传入自定义的损失函数, 需要在模型开始定义的时候传入或者使用<code>xgb.train</code>函数调用</em></p>
<ul>
<li><p>使用方法1:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from xgboost import XGBClassifier</span><br><span class="line"></span><br><span class="line">clf = XGBClassifier(objective=MyLossFunction)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用方法2:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import xgboost as xgb</span><br><span class="line">from xgboost import XGBClassifier</span><br><span class="line"></span><br><span class="line">clf = XGBClassifier()</span><br><span class="line">xgb.train(xgb_model=clf, obj=MyLossFunction)</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm——AVL树和红黑树等各种树结构总结</title>
    <url>/Notes/Others/Algorithm%E2%80%94%E2%80%94AVL%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%E7%AD%89%E5%90%84%E7%A7%8D%E6%A0%91%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<hr>
<h3 id="各种树的介绍"><a href="#各种树的介绍" class="headerlink" title="各种树的介绍"></a>各种树的介绍</h3><h4 id="树"><a href="#树" class="headerlink" title="树"></a>树</h4><ul>
<li>一个根节点,每个结点可能有多个子节点</li>
</ul>
<h4 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h4><ul>
<li>一个根节点,或者为空</li>
<li>每个结点只有两个子节点</li>
</ul>
<h4 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h4><p><em>也叫二叉查找树</em></p>
<ul>
<li>首先是一棵二叉树</li>
<li>左边孩子结点的值都小于当前结点</li>
<li>右边孩子结点的值都大于当前结点</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>极端情况下,树模型会退化成链表,查找变成了 O(n) 复杂度的</li>
</ul>
<h4 id="平衡二叉搜索树-AVL树"><a href="#平衡二叉搜索树-AVL树" class="headerlink" title="平衡二叉搜索树(AVL树)"></a>平衡二叉搜索树(AVL树)</h4><p><em>也叫平衡二叉查找树</em></p>
<ul>
<li>首先是一棵二叉搜索树</li>
<li>对<strong>每个结点</strong>而言, <strong>左右孩子结点的深度差值不能超过1</strong>, 从而保证查找是 O(log n) 的</li>
<li>控制平衡方法: 参考链接<a href="https://mp.weixin.qq.com/s/dYP5-fM22BgM3viWg4V44A?utm_source=wechat_session&utm_medium=social&utm_oi=675997552948678656" target="_blank" rel="noopener">AVL树详解</a><ul>
<li>左-左型: 右旋</li>
<li>右-右型: 左旋</li>
<li>左-右型: 左旋 + 右旋<ul>
<li>第一步左旋后面部分,变成 左-左 型</li>
<li>第二步使用右旋修正 左-左 型</li>
</ul>
</li>
<li>右-左型: 右旋 + 左旋<ul>
<li>第一步右旋后面部分,变成 右-右 型</li>
<li>第二步使用左旋修正 右-右 型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>每棵树的<strong>左右子树高度最多差1</strong>这个<strong>要求太严</strong>了</li>
<li>几乎每次插入或者删除结点时都会造成规则破坏, 也就需要频繁的通过左旋或者右旋操作来修正</li>
<li>插入和删除太频繁的场景中不太适合使用AVL树, 性能会因为<strong>左右子树高度最多差1</strong>这个<strong>规则频繁被打破</strong>而降低</li>
</ul>
<h4 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h4><ul>
<li>首先是一棵<strong>二叉搜索树</strong></li>
<li>每个结点不是黑色就是红色</li>
<li><strong>根节点为黑色</strong></li>
<li>每个<strong>叶子结点都为黑色</strong>的<strong>空结点</strong>(NULL)<ul>
<li>注意: 叶子节点不存数据</li>
</ul>
</li>
<li>任何<strong>相邻</strong>结点<strong>不同时</strong>为<strong>红色</strong><ul>
<li>注意,相邻结点可以为黑色,且可以某条路径上全是黑色</li>
</ul>
</li>
<li>对<strong>每个</strong>结点而言,<strong>当前结点</strong>到<strong>叶子结点</strong>的<strong>所有路径</strong>包含<strong>相同</strong>的<strong>黑色结点数</strong></li>
</ul>
<h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>能保证查找时间复杂度是 O(log n) 的, 和AVL树差不多<ul>
<li>证明: [待更新]</li>
</ul>
</li>
<li>插入和删除操作中不会频繁破坏红黑树的规则</li>
</ul>
<h5 id="红黑树的应用"><a href="#红黑树的应用" class="headerlink" title="红黑树的应用"></a>红黑树的应用</h5><ul>
<li>容器集合 HashMap, TreeMap等<ul>
<li>HashMap是 链表 + 红黑树的实现, 当冲突时就需要使用红黑树加快检索</li>
<li>HashMap中 链表长度太短时使用链表, 太长时使用红黑树, 这个阈值一般设置为8</li>
</ul>
</li>
</ul>
<h4 id="二三树"><a href="#二三树" class="headerlink" title="二三树"></a>二三树</h4><ul>
<li>红黑树是二三树的一个变形,一般用红黑树就够了<br>[待更新]</li>
</ul>
<h4 id="B树"><a href="#B树" class="headerlink" title="B树"></a>B树</h4><ul>
<li>B树在大量的数据库和文件系统场景中都有使用<br>[待更新]</li>
</ul>
<h4 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h4><p>[待更新]</p>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>可以说<strong>红黑树是一种不严格的平衡树</strong></li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——heapq模块-最大堆最小堆</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94heapq%E6%A8%A1%E5%9D%97-%E6%9C%80%E5%A4%A7%E5%A0%86%E6%9C%80%E5%B0%8F%E5%A0%86.html</url>
    <content><![CDATA[<p><em>由于<code>queue</code>不是Python标准库,所以在LeetCode等OJ上面不能直接使用,我们可以选择<code>heapq</code>来使用最大最小堆</em></p>
<hr>
<h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><ul>
<li><p>堆排序示例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import heapq</span><br><span class="line"></span><br><span class="line">nums = [2, 3, 5, 1, 54, 23, 132]</span><br><span class="line">heap = []</span><br><span class="line">for num in nums:</span><br><span class="line">    heapq.heappush(heap, num)</span><br><span class="line"># 等价于</span><br><span class="line">heapq.heapify(nums)</span><br><span class="line"></span><br><span class="line"># heap sort by incresing</span><br><span class="line">print([heapq.heappop(heap) for _ in range(len(nums))])</span><br></pre></td></tr></table></figure>
</li>
<li><p>加入元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">heapq.heappush(heap, num)</span><br></pre></td></tr></table></figure>
</li>
<li><p>弹出元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num = heapq.heappop(heap)</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取最大最小值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import heapq</span><br><span class="line"></span><br><span class="line">nums = [1, 3, 4, 5, 2]</span><br><span class="line">print(heapq.nlargest(3, nums))</span><br><span class="line">print(heapq.nsmallest(3, nums))</span><br><span class="line"></span><br><span class="line">#Output:</span><br><span class="line">[5, 4, 3]</span><br><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取堆顶元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">top = nums[0]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="最大堆的实现"><a href="#最大堆的实现" class="headerlink" title="最大堆的实现"></a>最大堆的实现</h3><ul>
<li>由于Python <code>heapq</code>模块只实现了最小堆, 最大堆需要我们自己实现</li>
<li>一种简单可行的实现方案:<ul>
<li>在加入和弹出时,<strong>把元素取反</strong>,从而<strong>实现最大堆</strong></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——为什么Python中没有自增++和自减--操作?</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E4%B8%BA%E4%BB%80%E4%B9%88Python%E4%B8%AD%E6%B2%A1%E6%9C%89%E8%87%AA%E5%A2%9E++%E5%92%8C%E8%87%AA%E5%87%8F--%E6%93%8D%E4%BD%9C?.html</url>
    <content><![CDATA[<p><em>C++和Java等语言都有++和–操作,为什么以方便自居的Python却没有这种操作呢?</em></p>
<hr>
<h3 id="Python的数值对象"><a href="#Python的数值对象" class="headerlink" title="Python的数值对象"></a>Python的数值对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = 1</span><br><span class="line">b = 1</span><br><span class="line">c = 1000123</span><br><span class="line">d = 1000123</span><br><span class="line"></span><br><span class="line">print id(a)</span><br><span class="line">print id(b)</span><br><span class="line">print id(c)</span><br><span class="line">print id(d)</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">94181316498840</span><br><span class="line">94181316498840</span><br><span class="line">94181323965720</span><br><span class="line">94181323965720</span><br></pre></td></tr></table></figure>

<ul>
<li>Python数值对象都是不可变类型,与String一样,所以不能修改对象内部数据</li>
<li>C++中的<code>i++</code>修改内存中对象本身,数值增加1,而Python不能修改对象本身</li>
<li>与C++中字符串可以修改,Python中不能修改是一个道理</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——优先队列PriorityQueue</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97PriorityQueue.html</url>
    <content><![CDATA[<p><em>本文介绍Python中优先队列的用法</em></p>
<ul>
<li>注意: <code>queue</code>并不能算是Python标准库,所以在LeetCode等OJ环境中不能使用, 想要使用优先队列的话可以使用Python的标准库<code>heapq</code></li>
<li><code>heapq</code>的使用请参考 <a href="/Notes/Python/Python%E2%80%94%E2%80%94heapq%E6%A8%A1%E5%9D%97-%E6%9C%80%E5%A4%A7%E5%A0%86%E6%9C%80%E5%B0%8F%E5%A0%86.html">Python——heapq模块-最大堆最小堆</a></li>
</ul>
<hr>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from Queue import PriorityQueue</span><br><span class="line"># or</span><br><span class="line">from queue import PriorityQueue</span><br></pre></td></tr></table></figure>

<ul>
<li><code>queue</code>包名已经弃用,测试发现本地Python2.7环境可以用,但是LeetCode线上环境不能用</li>
<li>推荐使用<code>Queue</code></li>
</ul>
<hr>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from Queue import PriorityQueue</span><br><span class="line"></span><br><span class="line">pq = PriorityQueue()</span><br><span class="line"></span><br><span class="line">pq.put((10, &quot;start&quot;))</span><br><span class="line">pq.put((5, &quot;b&quot;, 12, 123))</span><br><span class="line">pq.put((5, &quot;a&quot;, 6))</span><br><span class="line">pq.put(1)</span><br><span class="line">pq.put(4)</span><br><span class="line">pq.put([0, &quot;a&quot;])</span><br><span class="line">pq.put([8, &quot;b&quot;])</span><br><span class="line">pq.put(&quot;avb&quot;)</span><br><span class="line">pq.put(None)</span><br><span class="line"></span><br><span class="line"># while pq.not_empty()</span><br><span class="line"># while not pq.empty()</span><br><span class="line">while pq.qsize():</span><br><span class="line">    print pq.get()</span><br><span class="line"># output:</span><br><span class="line">None</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">[0, &apos;a&apos;]</span><br><span class="line">[8, &apos;b&apos;]</span><br><span class="line">avb</span><br><span class="line">(5, &apos;a&apos;, 6)</span><br><span class="line">(5, &apos;b&apos;, 12, 123)</span><br><span class="line">(10, &apos;start&apos;)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>不能使用<code>not pq</code>这样的语句判断优先队列是否收敛,他不是普通的内嵌对象(list,str等是内嵌对象),除非<code>pq == None</code>否则,双端队列对象永远为<code>not pq == False</code></p>
</li>
<li><p>下面用法最优:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while pq.qsize():</span><br></pre></td></tr></table></figure>
</li>
<li><p>PriorityQueue中默认递增排序(这一点与Python中的sorted函数和sort()函数一样),每次get(),移除并返回最小的对象</p>
</li>
<li><p>PriorityQueue中,可以同时添加不同类别的对象</p>
</li>
<li><p>PriorityQueue会将对象首先按照类别排序,然后各个类别内部按照不同数值排序</p>
</li>
<li><p>若传入对象是可以直接比较大小的类型即可直接传入,包括tuple, list, str, int(long)等类型</p>
<ul>
<li>Python中list,tuple,str等都是可以直接比较大小的,默认使用他们的第一个元素比较大小,如果第一个元素相等,则比较第二个元素,以此类推</li>
<li>详细情况参考本文后面的说明</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Python中的内嵌对象比较大小"><a href="#Python中的内嵌对象比较大小" class="headerlink" title="Python中的内嵌对象比较大小"></a>Python中的内嵌对象比较大小</h3><ul>
<li>Python中类别间也可以比较大小,默认类别间大小为:tuple &gt; str &gt; list &gt; int(long) &gt; None, 但是记不清楚的话不建议使用Python的这个特性,容易造成错误</li>
<li>Python中list,tuple,str等都是可以直接比较大小的,默认使用他们的第一个元素比较大小,如果第一个元素相等,则比较第二个元素,以此类推</li>
<li>Python中类别间也可以比较大小,默认类别间大小为:tuple &gt; str &gt; list &gt; int(long) &gt; None, 但是记不清楚的话不建议使用Python的这个特性,容易造成错误<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls = [(1, &apos;b&apos;), (1, &apos;a&apos;), (2, &apos;a&apos;), [1, &apos;a&apos;], [1, &apos;b&apos;], [2, &apos;a&apos;], &apos;1a&apos;, &apos;1b&apos;, &apos;2a&apos;, 1, 2, 3, None]</span><br><span class="line">ls.sort()</span><br><span class="line">print ls</span><br><span class="line"># output</span><br><span class="line">[None, 1, 2, 3, [1, &apos;a&apos;], [1, &apos;b&apos;], [2, &apos;a&apos;], &apos;1a&apos;, &apos;1b&apos;, &apos;2a&apos;, (1, &apos;a&apos;), (1, &apos;b&apos;), (2, &apos;a&apos;)]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Python自定义对象比较大小"><a href="#Python自定义对象比较大小" class="headerlink" title="Python自定义对象比较大小"></a>Python自定义对象比较大小</h3><ul>
<li><p>在做算法题时,没必要的情况下不建议使用</p>
</li>
<li><p>在做工程时建议使用这种方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import Queue</span><br><span class="line">class Node():</span><br><span class="line">    def __init__(self, val):</span><br><span class="line">        self.val = val</span><br><span class="line"></span><br><span class="line">    def __lt__(self, other):</span><br><span class="line">        return self.val &lt; other.val</span><br><span class="line"></span><br><span class="line">pq = Queue.PriorityQueue()</span><br><span class="line">pq.put(Node(5))</span><br><span class="line">pq.put(Node(1))</span><br><span class="line"></span><br><span class="line">while pq.qsize():</span><br><span class="line">    print pq.get().val</span><br><span class="line"># output</span><br><span class="line">1</span><br><span class="line">5</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意<code>__lt__</code>函数中是小于号,说明递增排序,大于号,说明递减排序</p>
</li>
<li><p>PriorityQueue对象不是普通Python内嵌对象,不能使用Python内嵌的<code>len</code>函数</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——关于列表list的操作</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E5%88%97%E8%A1%A8list%E7%9A%84%E6%93%8D%E4%BD%9C.html</url>
    <content><![CDATA[<p><em>Python的list有很多强大的功能,有些比较罕见的操作可能很有用,需要我们记住</em></p>
<h4 id="list的常见操作"><a href="#list的常见操作" class="headerlink" title="list的常见操作"></a>list的常见操作</h4><h5 id="list子列表"><a href="#list子列表" class="headerlink" title="list子列表"></a>list子列表</h5><ul>
<li><strong>注意使用子列表时是一个新对象,操作子列表与原始list无关</strong></li>
<li>在快速排序和归并排序中不可将子列表传入,以期待可以从函数中修改原始列表的值</li>
</ul>
<h5 id="list反序子列表"><a href="#list反序子列表" class="headerlink" title="list反序子列表"></a>list反序子列表</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1, 2, 3]</span><br><span class="line">l1 = l[::-1]</span><br><span class="line">print l</span><br><span class="line">print l1</span><br><span class="line"># output:</span><br><span class="line">[1, 2, 3]</span><br><span class="line">[3, 2, 1]</span><br></pre></td></tr></table></figure>

<h4 id="list的罕见操作"><a href="#list的罕见操作" class="headerlink" title="list的罕见操作"></a>list的罕见操作</h4><h5 id="remove-object"><a href="#remove-object" class="headerlink" title="remove(object)"></a>remove(object)</h5><ul>
<li>移除列表中第一个与object相等的对象<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1, 2, 2, 3, 4]</span><br><span class="line">l.remove(2)</span><br><span class="line">print l</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">[1, 2, 3, 4]</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="pop-index"><a href="#pop-index" class="headerlink" title="pop(index)"></a>pop(index)</h5><ul>
<li>从列表中移除一个元素,并返回该元素,index为索引</li>
<li>默认移除最后一个<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l = [1, 2, 2, 3, 4, 5]</span><br><span class="line">l.pop(0)</span><br><span class="line">print l</span><br><span class="line">l.pop()</span><br><span class="line">print l</span><br><span class="line"># output:</span><br><span class="line">[2, 2, 3, 4, 5]</span><br><span class="line">[2, 2, 3, 4]</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——参数*args与**kwargs的意义***</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%8F%82%E6%95%B0*args%E4%B8%8E**kwargs%E7%9A%84%E6%84%8F%E4%B9%89.html</url>
    <content><![CDATA[<p><em>Python中常常使用\</em>args与**kwargs这样的参数形式定义函数的参数,他们表示该函数可以接受任何类型任何数量的参数*</p>
<ul>
<li>参考博客:<a href="https://blog.csdn.net/maliao1123/article/details/52152989" target="_blank" rel="noopener">https://blog.csdn.net/maliao1123/article/details/52152989</a></li>
</ul>
<hr>
<h3 id="一句话解释"><a href="#一句话解释" class="headerlink" title="一句话解释"></a>一句话解释</h3><ul>
<li>*args是非关键字参数，用于元</li>
<li>**kwargs是关键字参数，用于字典</li>
</ul>
<hr>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def test(*args, **kwargs):</span><br><span class="line">    print &quot;args: &quot;, args</span><br><span class="line">    print &quot;kwargs: &quot;, kwargs</span><br><span class="line"></span><br><span class="line">test(1,2,3)</span><br><span class="line">test(a=1, b=2)</span><br><span class="line">test(1, a=2, c=2)</span><br><span class="line"># output</span><br><span class="line">args:  (1, 2, 3)</span><br><span class="line">kwargs:  &#123;&#125;</span><br><span class="line">args:  ()</span><br><span class="line">kwargs:  &#123;&apos;a&apos;: 1, &apos;b&apos;: 2&#125;</span><br><span class="line">args:  (1,)</span><br><span class="line">kwargs:  &#123;&apos;a&apos;: 2, &apos;c&apos;: 2&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>注意: 关键字参数后面不能有非关键字参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test(1, a=1, 2)</span><br></pre></td></tr></table></figure>

<ul>
<li>上面的函数调用会造成语法错误</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>*args表示任何多个无名参数，它是一个tuple；</li>
<li>**kwargs表示关键字参数，它是一个dict</li>
<li>同时使用*args和**kwargs时，必须*args参数列要在**kwargs前<ul>
<li>否则提示语法错误“SyntaxError: non-keyword arg after keyword arg”</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——双端队列deque</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E5%8F%8C%E7%AB%AF%E9%98%9F%E5%88%97deque.html</url>
    <content><![CDATA[<p><em>本文介绍Python中双端队列(double-ended queue, 简称为deque)的用法</em></p>
<hr>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line">import collections</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import collections</span><br><span class="line">dq = collections.deque()</span><br><span class="line">dq.append(3)</span><br><span class="line">dq.append(4)</span><br><span class="line">dq.append(1)</span><br><span class="line">dq.appendleft(9)</span><br><span class="line">dq.appendleft(10)</span><br><span class="line">print dq</span><br><span class="line">print dq.pop()</span><br><span class="line">print dq</span><br><span class="line">print dq.popleft()</span><br><span class="line">print dq</span><br><span class="line"></span><br><span class="line">while len(dq):</span><br><span class="line">    print dq.pop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">deque([10, 9, 3, 4, 1])</span><br><span class="line">1</span><br><span class="line">deque([10, 9, 3, 4])</span><br><span class="line">10</span><br><span class="line">deque([9, 3, 4])</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<ul>
<li>注意,deque没有<code>qsize()</code>函数,但是可以像普通队列一样使用Python内嵌的<code>len</code>函数</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——装饰器decorator</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E8%A3%85%E9%A5%B0%E5%99%A8decorator.html</url>
    <content><![CDATA[<p><em>Python中的装饰器可以在不修改原始函数代码的基础上,在Python函数中插入一些额外操作</em></p>
<ul>
<li>参考博客:<a href="https://ask.hellobi.com/blog/pythoneer" target="_blank" rel="noopener">https://ask.hellobi.com/blog/pythoneer</a></li>
</ul>
<hr>
<h3 id="简单装饰器"><a href="#简单装饰器" class="headerlink" title="简单装饰器"></a>简单装饰器</h3><ul>
<li><p>装饰器定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def decorator(func):</span><br><span class="line">    def wrapper(*args, **kwargs):</span><br><span class="line">        print &quot;decorator&quot;</span><br><span class="line">        return func(*args, **kwargs)</span><br><span class="line">    return wrapper</span><br></pre></td></tr></table></figure>
</li>
<li><p>装饰器使用</p>
<ul>
<li><p>不带参数的函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@decorator</span><br><span class="line">def test():</span><br><span class="line">    print &quot;inner test&quot;</span><br><span class="line"></span><br><span class="line"># just like a normal function</span><br><span class="line">test()</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">decorator</span><br><span class="line">inner test</span><br></pre></td></tr></table></figure>
</li>
<li><p>带参数的函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@decorator</span><br><span class="line">def add(a, b):</span><br><span class="line">    print &quot;sum: &quot;, a+b</span><br><span class="line"></span><br><span class="line"># just like a normal function</span><br><span class="line">add(10, 20)</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">decorator</span><br><span class="line">sum:  30</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="装饰器是一种语法糖"><a href="#装饰器是一种语法糖" class="headerlink" title="装饰器是一种语法糖"></a>装饰器是一种语法糖</h3><ul>
<li>实际上上面的代码等价于<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def test():</span><br><span class="line">    print &quot;inner test&quot;</span><br><span class="line"></span><br><span class="line">test = decorator(test)</span><br><span class="line"></span><br><span class="line"># test is a normal</span><br><span class="line">test()</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">decorator</span><br><span class="line">inner test</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="带参数的装饰器"><a href="#带参数的装饰器" class="headerlink" title="带参数的装饰器"></a>带参数的装饰器</h3><ul>
<li>需要对装饰器进一步的封装</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def outterDecorator(tag):</span><br><span class="line">    def decorator(func):</span><br><span class="line">        def wrapper(*args, **kwargs):</span><br><span class="line">            print &quot;decorator: &quot; + tag</span><br><span class="line">            return func(*args, **kwargs)</span><br><span class="line">        return wrapper</span><br><span class="line">    return decorator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@outterDecorator(tag=&quot;123&quot;)</span><br><span class="line">def test():</span><br><span class="line">    print &quot;inner test&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@outterDecorator(&quot;abc&quot;)</span><br><span class="line">def add(a, b):</span><br><span class="line">    print &quot;sum: &quot;, a+b</span><br><span class="line"></span><br><span class="line">test()</span><br><span class="line"></span><br><span class="line">add(10, 20)</span><br></pre></td></tr></table></figure>

<pre><code>* 在原始的装饰器外面封装一层函数,用于接受参数,其他的不用改变</code></pre>
<ul>
<li>理解:<ul>
<li>等价于给装饰器加了一层接受参数的外层空间</li>
<li>实际上调用的时候除了参数外,其他的都没变</li>
<li>被装饰的函数依然是被作为内层函数的参数传入装饰器中</li>
</ul>
</li>
</ul>
<hr>
<h3 id="类装饰器"><a href="#类装饰器" class="headerlink" title="类装饰器"></a>类装饰器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Foo(object):</span><br><span class="line">    def __init__(self, func):</span><br><span class="line">        self._func = func</span><br><span class="line"></span><br><span class="line">    def __call__(self):</span><br><span class="line">        print (&apos;class decorator runing&apos;)</span><br><span class="line">        self._func()</span><br><span class="line">        print (&apos;class decorator ending&apos;)</span><br><span class="line"></span><br><span class="line">@Foo</span><br><span class="line">def bar():</span><br><span class="line">    print (&apos;bar&apos;)</span><br><span class="line"></span><br><span class="line">bar()</span><br></pre></td></tr></table></figure>

<ul>
<li>如上述代码所示,类装饰器必须有<code>__init__</code>和<code>__call__</code>两个函数</li>
<li><code>__init__</code>负责接受被装饰函数作为参数并存储该函数</li>
<li><code>__call__</code>负责执行函数调用过程并执行想要插入函数的代码</li>
<li>被装饰的函数被调用时本质上是<code>__call__</code>函数被调用</li>
</ul>
<h4 id="类装饰器的优点"><a href="#类装饰器的优点" class="headerlink" title="类装饰器的优点"></a>类装饰器的优点</h4><ul>
<li>灵活度高</li>
<li>高内聚,不像函数一样定义在外面</li>
<li>封装的好,容易阅读</li>
</ul>
<hr>
<h3 id="多个装饰器的顺序问题"><a href="#多个装饰器的顺序问题" class="headerlink" title="多个装饰器的顺序问题"></a>多个装饰器的顺序问题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@a</span><br><span class="line">@b</span><br><span class="line">@c</span><br><span class="line">def f ():</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>

<ul>
<li>函数可以同时被多个装饰器修饰</li>
<li>装饰器的顺序从靠近函数的那个开始从内向外一层层封装<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f = a(b(c(f)))</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="装饰器对原始函数的属性修改"><a href="#装饰器对原始函数的属性修改" class="headerlink" title="装饰器对原始函数的属性修改"></a>装饰器对原始函数的属性修改</h3><ul>
<li><p>涉及到<code>docstring</code>,<code>__name__</code>等属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 装饰器</span><br><span class="line">def logged(func):</span><br><span class="line">    def with_logging(*args, **kwargs):</span><br><span class="line">        print func.__name__      # 输出 &apos;with_logging&apos;</span><br><span class="line">        print func.__doc__       # 输出 None</span><br><span class="line">        return func(*args, **kwargs)</span><br><span class="line">    return with_logging</span><br><span class="line"></span><br><span class="line"># 函数</span><br><span class="line">@logged</span><br><span class="line">def f(x):</span><br><span class="line">   &quot;&quot;&quot;does some math&quot;&quot;&quot;</span><br><span class="line">   return x + x * x</span><br><span class="line"></span><br><span class="line">logged(f)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>functools.warps</code>装饰器可以修复原始函数的文档</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import wraps</span><br><span class="line">def logged(func):</span><br><span class="line">    @wraps(func)</span><br><span class="line">    def with_logging(*args, **kwargs):</span><br><span class="line">        print func.__name__      # 输出 &apos;f&apos;</span><br><span class="line">        print func.__doc__       # 输出 &apos;does some math&apos;</span><br><span class="line">        return func(*args, **kwargs)</span><br><span class="line">    return with_logging</span><br><span class="line"></span><br><span class="line">@logged</span><br><span class="line">def f(x):</span><br><span class="line">   &quot;&quot;&quot;does some math&quot;&quot;&quot;</span><br><span class="line">   return x + x * x</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="property装饰器"><a href="#property装饰器" class="headerlink" title="property装饰器"></a>property装饰器</h3><ul>
<li><p>用于类的属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Student(object):</span><br><span class="line">    def __init__(self, birth):</span><br><span class="line">        self._birth = birth</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def birth(self):</span><br><span class="line">        return self._birth</span><br><span class="line"></span><br><span class="line">    @birth.setter</span><br><span class="line">    def birth(self, value):</span><br><span class="line">        self._birth = value</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def age(self):</span><br><span class="line">        return 2014 - self._birth:</span><br></pre></td></tr></table></figure>
</li>
<li><p>当加上<code>property</code>装饰器后,函数就变成了一个只读属性,被修饰的函数不能再当成普通函数   </p>
<ul>
<li><p>当前函数不能有参数,除非是默认参数,因为当前函数变成属性后,直接调用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s.birth(10)</span><br></pre></td></tr></table></figure>

<ul>
<li>解析是<code>s.birth</code>返回一个属性值,然后,属性值不能被调用,所以抛出异常</li>
</ul>
</li>
</ul>
</li>
<li><p><code>property</code>装饰器会生成两个新的装饰<code>[method_name].setter</code>和<code>[method_name].getter</code>,分别用于代表当前函数对应属性的的写和读功能,读的功能默认加上了,写的功能需要的话我们可以使用<code>[method_name].setter</code>装饰器实现</p>
</li>
<li><p>总结: <code>property</code>装饰器可以将类的某个属性封装起来(在不暴露类属性的情况下提供<code>getter</code>方法和<code>setter</code>方法(后者需要自己显示定义))</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——HMM-MEMM-CRF</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94HMM-MEMM-CRF.html</url>
    <content><![CDATA[<p><em>本文主要区分隐马尔可夫模型(HMM),最大熵马尔可夫模型(MEMM),和条件随机场(CRF)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><ul>
<li>生成式模型</li>
<li>有向图模型,贝叶斯网络</li>
</ul>
<h4 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h4><ul>
<li>模型参数: \(\lambda = (A, B, \pi)\)<ul>
<li>A为状态转移矩阵</li>
<li>B为观测概率矩阵</li>
<li>\(\pi\)为初始状态概率向量</li>
</ul>
</li>
<li>HMM对</li>
</ul>
<h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><ul>
<li>观测序列之间独立</li>
<li>当前状态仅仅依赖于上一个状态</li>
</ul>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ul>
<li>概率计算问题: 给定模型\(\lambda = (A, B, \pi)\)和观测序列\(O=(o_{1}, o_{2},,,o_{n})\),计算在给定模型下观测序列出现的概率\(P(O|\lambda)\)</li>
<li>学习问题: 已知观测序列\(O=(o_{1}, o_{2},,,o_{n})\),估计模型参数\(\lambda = (A, B, \pi)\),使得在该模型下观测序列出现的概率\(P(O|\lambda)\)最大.(极大似然法,EM算法)</li>
<li>预测问题(解码问题): 给定模型\(\lambda = (A, B, \pi)\)和观测序列\(O=(o_{1}, o_{2},,,o_{n})\),求状态序列\(I=(i_{1}, i_{2},,,i_{n})\),使得\(P(I|O;\lambda)\)最大,(维特比算法)</li>
</ul>
<h4 id="序列标注问题"><a href="#序列标注问题" class="headerlink" title="序列标注问题"></a>序列标注问题</h4><ul>
<li>序列标注问题是已知观测序列\(O=(o_{1}, o_{2},,,o_{n})\),求状态序列\(I=(i_{1}, i_{2},,,i_{n})\),使得\(P(I|O)\)最大</li>
<li>实际上序列标注问题包括学习问题和预测问题两个问题:<ul>
<li>学习问题: 根据观测序列确定模型参数\(\lambda = (A, B, \pi)\), 极大似然法或EM算法(EM算法会同时估计得到最优状态序列(隐变量))</li>
<li>预测问题: 根据模型参数和观测序列确定最优状态序列\(I=(i_{1}, i_{2},,,i_{n})\),维特比算法</li>
</ul>
</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>算法成熟</li>
<li>效率高, 模型简单, 容易训练</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>序列标注问题中,当前状态(标注)往往不仅仅和前一个状态相关,还可能和观察序列相关(这里指的是整个序列)</li>
<li>也就是说每个状态可能还与整个观察序列的(除了当前观察值以外的)其他观察值(观察序列上下文)相关</li>
</ul>
<hr>
<h3 id="MEMM"><a href="#MEMM" class="headerlink" title="MEMM"></a>MEMM</h3><ul>
<li>判别式模型</li>
<li>有向图模型,贝叶斯网络</li>
</ul>
<h4 id="假设-1"><a href="#假设-1" class="headerlink" title="假设"></a>假设</h4><ul>
<li>当前状态仅依赖于上一状态和<strong>当前观测值</strong>(或<strong>所有观测值</strong>)</li>
<li>(问题: 为什么有些书上画出的图是当前状态依赖上一状态和<strong>所有观测值</strong>?,这里应该是<strong>当前观测值</strong>和<strong>所有观测值</strong>两种情况都是MEMM,&lt;&lt;百面&gt;&gt;画的是<strong>所有观测值的情况</strong>)</li>
</ul>
<h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><ul>
<li>MEMM似乎只用于序列标注,也就是在已知观测序列的情况下,寻找最优的状态序列</li>
<li><em>有其他应用的话再添加</em></li>
</ul>
<h4 id="序列标注问题-1"><a href="#序列标注问题-1" class="headerlink" title="序列标注问题"></a>序列标注问题</h4><ul>
<li>用于序列标注时,一般也包括两个问题: 学习问题和预测问题<ul>
<li>学习问题: 根据观测序列确定模型参数(每条边的(概率)值和初始状态?)</li>
<li>预测问题: 根据模型和观测序列确定维特比算法</li>
</ul>
</li>
</ul>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul>
<li>解决了观测独立性问题(观测独立性是只当前观测序列只与当前状态相关)[问题: 在MEMM中并不关心观测序列由谁影响,而是关心观测序列如何影响了状态序列]</li>
</ul>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>标签偏置(labeling bias)问题<ul>
<li>MEMM中概率最大路径往往容易出现在转移少的状态中</li>
<li>MEMM归一化在加和函数\(\sum\)计算内部,而CRF的归一化在加和函数\(\sum\)的外部,这使得MEMM只会关注加和函数[原始建模问题概率值\((y_{1…n}|x_{1…n})\)]的局部特征,而不是的整体特征,所以MEMM存在偏置问题</li>
</ul>
</li>
<li>比HMM复杂</li>
</ul>
<hr>
<h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><ul>
<li>判别式模型</li>
<li>无向图模型,马尔科夫网络</li>
</ul>
<h4 id="假设-2"><a href="#假设-2" class="headerlink" title="假设"></a>假设</h4><ul>
<li>当前状态仅依赖于上一状态和<strong>当前观测值</strong>(或<strong>所有观测值</strong>)</li>
<li>(问题: 为什么有些书上画出的图是当前状态依赖上一状态和<strong>所有观测值</strong>?,这里应该是<strong>当前观测值</strong>和<strong>所有观测值</strong>两种情况都是线性CRFs,&lt;&lt;百面&gt;&gt;画的是<strong>所有观测值的情况</strong>)</li>
<li>与MEMM的区别就是无向图模型与有向图模型的区别</li>
</ul>
<h4 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h4><h4 id="序列标注问题-2"><a href="#序列标注问题-2" class="headerlink" title="序列标注问题"></a>序列标注问题</h4><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul>
<li>模型复杂,能建模更多可能的特征</li>
<li>全局归一化(这里与MEMM的区别是,MEMM归一化在加和函数[原始建模问题概率值\((y_{1…n}|x_{1…n})\)]\(\sum\)计算内部,而CRF的归一化在加和函数[原始建模问题概率值\((y_{1…n}|x_{1…n})\)]\(\sum\)的外部)</li>
</ul>
<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>模型复杂,速度慢</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——EM算法</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<p><em>期望最大化(Exception Maximization Algorithm)EM算法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="不同教材的不同形式"><a href="#不同教材的不同形式" class="headerlink" title="不同教材的不同形式"></a>不同教材的不同形式</h3><h4 id="李航统计学习方法"><a href="#李航统计学习方法" class="headerlink" title="李航统计学习方法"></a>李航统计学习方法</h4><h5 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h5><ul>
<li><p>输入: 观测变量数据Y</p>
</li>
<li><p>输出: 模型(参数\(\theta\))</p>
</li>
<li><p><strong>E步:</strong> 计算\(Q(\theta, \theta^{i})\)<br>$$<br>\begin{align}<br>Q(\theta, \theta^{i}) &amp;= E_{Z}[logP(Y,Z|\theta)|Y,\theta^{i}] \\<br>  &amp;= E_{Z\sim P(Z|Y,\theta^{i})}[logP(Y,Z|\theta)] \\<br>  &amp;= \sum_{Z} P(Z|Y,\theta^{i})logP(Y,Z|\theta) \\<br>  &amp;= \sum_{Z} logP(Y,Z|\theta)P(Z|Y,\theta^{i})<br>\end{align}<br>$$</p>
</li>
<li><p><strong>M步:</strong> 求使得\(Q(\theta, \theta^{i})\)极大化的参数\(\theta=\theta^{i+1}\)<br>$$\theta^{i+1} = \arg\max_{\theta}Q(\theta, \theta^{i})$$</p>
</li>
<li><p>重复E步和M步,直到收敛</p>
</li>
<li><p>理解: \(Q(\theta, \theta^{i})\)可以理解为\(Q(\theta|\theta^{i})\),表示在参数\(\theta^{i}\)已知的情况下,对数似然函数关于隐变量后验分布的期望函数,函数的参数为\(\theta\)</p>
</li>
</ul>
<h5 id="隐变量的期望还是分布"><a href="#隐变量的期望还是分布" class="headerlink" title="隐变量的期望还是分布?"></a>隐变量的期望还是分布?</h5><p><em>参考博客:<a href="https://www.jianshu.com/p/c3ff1ae5cb66" target="_blank" rel="noopener">https://www.jianshu.com/p/c3ff1ae5cb66</a></em></p>
<h6 id="仅考虑隐变量的期望"><a href="#仅考虑隐变量的期望" class="headerlink" title="仅考虑隐变量的期望"></a>仅考虑隐变量的期望</h6><ul>
<li>应用场景为k-means聚类,但是k-means聚类E步求的是最可能的\(Z\)值(概率最大的\(Z\)值),而不是\(Z\)的期望</li>
</ul>
<table>
<thead>
<tr>
<th align="center">步骤</th>
<th align="center">具体细节</th>
</tr>
</thead>
<tbody><tr>
<td align="center">E步</td>
<td align="center">基于\(\theta^{i}\)推断隐变量\(Z\)的期望,记为\(Z^{i}\)</td>
</tr>
<tr>
<td align="center">M步</td>
<td align="center">基于已观测变量\(Y\)和\(Z^{i}\)对参数\(\theta\)做极大似然估计,得到\(\theta^{i+1}\) $$\theta^{i+1}=\arg\max_{\theta}P(Y,Z^{i}\mid\theta)$$</td>
</tr>
</tbody></table>
<h6 id="考虑隐变量的分布"><a href="#考虑隐变量的分布" class="headerlink" title="考虑隐变量的分布"></a>考虑隐变量的分布</h6><ul>
<li>应用场景为GMM模型聚类</li>
</ul>
<table>
<thead>
<tr>
<th align="center">步骤</th>
<th align="center">具体细节</th>
</tr>
</thead>
<tbody><tr>
<td align="center">E步</td>
<td align="center">基于\(\theta^{i}\)推断隐变量\(Z\)的后验分布\(P(Z\mid Y,\theta^{i})\)</td>
</tr>
<tr>
<td align="center">E步M步均可</td>
<td align="center">基于隐变量的后验分布\(P(Z\mid Y,\theta^{i})\)<br>计算对数似然函数\(logP(Y,Z\mid\theta)\)关于隐变量\(Z\)的后验分布\(P(Z\mid Y,\theta^{i})\)的期望\(Q(\theta,\theta^{i})\)<br>$$Q(\theta,\theta^{i}) = E_{P(Z\mid Y,\theta^{i})}logP(Y,Z\mid \theta)$$</td>
</tr>
<tr>
<td align="center">M步</td>
<td align="center">基于期望函数\(Q(\theta, \theta^{i})\),对参数\(\theta\)求极值(极大似然估计),得到$$\theta^{i+1}=\arg\max_{\theta}Q(\theta, \theta^{i})=\arg\max_{\theta}E_{P(Z\mid Y,\theta^{i})}logP(Y,Z\mid \theta)$$</td>
</tr>
</tbody></table>
<h5 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h5><ul>
<li><p>已知数据是观测数据Y,像极大似然法一样,使得<strong>似然函数最大化</strong>即可,这里为了方便计算使用<strong>对数似然</strong></p>
</li>
<li><p>我们的终极目标与极大似然法一样,求一个使得似然函数最大(可能是极大)的参数$$\theta^{\star}=\arg\max_{\theta}L(\theta)$$<br>其中<br>$$<br>\begin{align}<br>L(\theta)&amp;=logP(Y|\theta)\\<br>  &amp;=log\sum_{Z}P(Y,Z|\theta)\\<br>  &amp;=log\left(\sum_{Z}P(Y|Z,\theta)P(Z|\theta)\right)<br>\end{align}<br>$$</p>
</li>
<li><p>显然,上述似然函数比较难以求解,非凸,且涉及和(积分或加法)的对数等操作,难以展开(可能还有其他的原因)</p>
</li>
<li><p>所以我们使用EM算法迭代不断逼近原始似然函数的最优解\(\theta^{\star}\)(注意:我们只能得到局部最优,但是一般来说局部最优也够用了)</p>
</li>
<li><p>可用<strong>Jensen不等式</strong>得到\(L(\theta)\)的下界来作为优化目标不断迭代<br>$$<br>\begin{align}<br>L(\theta)&amp;=log\left(\sum_{Z}P(Y|Z,\theta)P(Z|\theta)\right)\\<br>  &amp;=log\left(\sum_{Z}P(Z|Y,\theta^{i})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{i})}\right)\\<br>  &amp;\geq \sum_{Z}P(Z|Y,\theta^{i})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{i})}\\<br>  &amp;=B(\theta,\theta^{i})<br>\end{align}<br>$$</p>
</li>
<li><p>由于此时\(B(\theta, \theta^{i})\)是\(L(\theta)\)的下界(\(B(\theta, \theta^{i})\)是固定\(\theta^{i}\)时关于\(\theta\)的凸函数且容易求导,可以求极大值),所以使得前者增大的参数\(\theta\)也能使得后者增大,为了使得后者尽可能的增大,我们对前者取极大值</p>
<ul>
<li>下面的推导基于事实: <strong>消去与\(\theta\)无关的项,极大值点(\(\theta^{i+1}\))不变</strong></li>
</ul>
</li>
</ul>
<p>$$<br>\begin{align}<br>\theta^{i+1} &amp;= \arg\max_{\theta}B(\theta,\theta^{i})\\<br>    &amp;=\arg\max_{\theta}\left( \sum_{Z}P(Z|Y,\theta^{i})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{i})}\right)\\<br>    &amp;=\arg\max_{\theta}\left( \sum_{Z}P(Z|Y,\theta^{i})logP(Y|Z,\theta)P(Z|\theta)-\sum_{Z}P(Z|Y,\theta^{i})logP(Z|Y,\theta^{i})\right)\\<br>    &amp;= \arg\max_{\theta}\left( \sum_{Z}P(Z|Y,\theta^{i})logP(Y|Z,\theta)P(Z|\theta)\right)\\<br>    &amp;=\arg\max_{\theta}Q(\theta,\theta^{i})<br>\end{align}<br>$$</p>
<ul>
<li>由上式可知,我们的EM算法步骤中E步和M步是正确的</li>
<li>问题:推导第二步中为什么选择\(P(Z|Y,\theta^{i})\)而不是其他分布呢?<ul>
<li>解答: \(B(\theta, \theta^{i})\)和\(L(\theta)\)什么时候相等呢?(前面推导中<strong>Jensen不等式</strong>什么时候能取等号呢?)</li>
<li><strong>Jensen不等式</strong>取等号当且仅当<strong>Jensen不等式</strong>中函数的值为常数,此处函数的值为\(log\frac{P(Y,Z|\theta)}{Q(Z)}\)<br>$$<br>\begin{align}<br>L(\theta)&amp;=log\left(\sum_{Z}P(Y,Z|\theta)\right)\\<br>&amp;=log\left(\sum_{Z}Q(Z)\frac{P(Y,Z|\theta)}{Q(Z)}\right)\\<br>&amp;\geq \sum_{Z}Q(Z)log\frac{P(Y,Z|\theta)}{Q(Z)}\\<br>\end{align}<br>$$</li>
<li>不等式中当且仅当\(log\frac{P(Y,Z|\theta)}{Q(Z)}\)为常数,也就是\(\frac{P(Y,Z|\theta)}{Q(Z)}=c\), c为常数,时等号成立</li>
<li>此时由于\(Q(Z)\)是一个分布(注意:正因为\(Q(Z)\)是一个分布才能用Jensen不等式),所以有<br>$$<br>\begin{align}<br>Q(Z)=\frac{P(Y,Z|\theta)}{\sum_{Z}P(Y,Z|\theta)}=\frac{P(Y,Z|\theta)}{P(Y|\theta)}=P(Z|Y,\theta)<br>\end{align}<br>$$</li>
</ul>
</li>
</ul>
<h4 id="吴恩达CS229"><a href="#吴恩达CS229" class="headerlink" title="吴恩达CS229"></a>吴恩达CS229</h4><ul>
<li>E步: 计算\(Q_{i}(Z)=P(Z|Y,\theta^{i})\)</li>
<li>M步: 求使得原始似然函数下界极大化的参数\(\theta=\theta^{i+1}\)<br>$$<br>\begin{align}<br>\theta^{i+1} &amp;= \arg\max_{\theta}\sum_{Z}P(Z|Y,\theta^{i})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{i})} \\<br>  &amp;= \arg\max_{\theta}\sum_{Z}Q_{i}(Z)log\frac{P(Y|Z,\theta)P(Z|\theta)}{Q_{i}(Z)}<br>\end{align}<br>$$<ul>
<li>进一步消除与\(\theta\)无关的项可以得到<br>$$<br>\begin{align}<br>\theta^{i+1} &amp;= \arg\max_{\theta}\sum_{Z}Q_{i}(Z)logP(Y|Z,\theta)P(Z|\theta)<br>\end{align}<br>$$</li>
</ul>
</li>
<li>推导步骤和李航统计学习方法一样,核心是运用<strong>Jensen不等式</strong></li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>以上两个不同课程的E步不同,但完全等价,吴恩达CS229课程中E步计算\(Q_{i}(Z)=P(Z|Y,\theta^{i})\)就等价于计算出了李航统计学习方法中的\(Q(\theta, \theta^{i})\),二者关系如下:<br>$$<br>\begin{align}<br>Q(\theta, \theta^{i})&amp;=\sum_{Z}P(Z|Y,\theta^{i})logP(Y|Z,\theta)P(Z|\theta) \\<br>  &amp;= \sum_{Z}Q_{i}(Z)logP(Y|Z,\theta)P(Z|\theta)<br>\end{align}<br>$$</li>
<li>M步中,二者本质上完全相同,但是吴恩达CS229中没消去与\(\theta\)无关的项,所以看起来不太简洁</li>
</ul>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><h5 id="实例一"><a href="#实例一" class="headerlink" title="实例一"></a>实例一</h5><ul>
<li>三个硬币的抛硬币问题<ul>
<li>第一次:抛硬币A,决定第二次抛C还是B,选中B的概率为\(\pi\)</li>
<li>第二次:抛硬币B或C,正面为1,反面为0</li>
</ul>
</li>
<li>(第一个硬币A抛中B的概率为隐变量)</li>
</ul>
<h5 id="实例二"><a href="#实例二" class="headerlink" title="实例二"></a>实例二</h5><ul>
<li>200个人不知道男女的身高拟合问题(性别为隐变量)</li>
<li>这里是高斯混合模型的代表</li>
</ul>
<h5 id="实例三"><a href="#实例三" class="headerlink" title="实例三"></a>实例三</h5><ul>
<li>K-Means聚类</li>
<li>参考&lt;&lt;百面机器学习&gt;&gt;P100-P101</li>
</ul>
<h4 id="进一步理解"><a href="#进一步理解" class="headerlink" title="进一步理解"></a>进一步理解</h4><h5 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h5><ul>
<li><p>初始化参数\(\theta^{0}\)</p>
</li>
<li><p>1.根据参数\(\theta^{i}\)计算当前隐变量的分布函数\(Q_{i}(Z)=P(Z|Y,\theta^{i})\)</p>
<ul>
<li>这一步的本质是使得在参数 \(\theta = \theta^i\) 时, 求得一个隐变量 \(Z\) 的分布,使得原始式子中的不等式取等号</li>
</ul>
</li>
<li><p>2.根据\(Q_{i}(Z)=P(Z|Y,\theta^{i})\)得到对数似然函数下界函数\(B(\theta, \theta^{i})\)(求原始似然函数的下界B函数是因为直接对原始似然函数求极大值很难)或者\(Q(\theta,\theta^{i})\)<br>$$\arg\max_{\theta}B(\theta, \theta^{i})=\arg\max_{\theta}Q(\theta,\theta^{i})$$<br>(\(Q(\theta,\theta^{i})\)可以看作是\(B(\theta, \theta^{i})\)的简化版,\(B(\theta, \theta^{i})\)才是原始似然函数的下界,\(Q(\theta,\theta^{i})\)不是原始似然函数的下界)</p>
<ul>
<li>这里 \(B(\theta, \theta^{i})\) 就是原始似然函数的下界(也就是不等式取到等号)</li>
</ul>
</li>
<li><p>3.求使得函数\(B(\theta, \theta^{i})\)极大化的参数\(\theta=\theta^{i+1}\)</p>
<ul>
<li>这一步是在固定隐变量 \(Z\) 的分布时, 用极大似然求一个使得下界 \(B(\theta, \theta^{i})\) 最大的参数 \(\theta = \theta^{i+1}\) 使得 \(B(\theta^{i+1}, \theta^{i}) = \text{max} B(\theta, \theta^{i})\) </li>
</ul>
</li>
<li><p>4.循环1,2,3直到收敛(相邻<strong>两次参数的变化</strong>或者是<strong>似然函数的变化</strong>足够小即可判断为收敛)</p>
<ul>
<li>\(||\theta^{i+1}-\theta^{i}||&lt;\epsilon_{1}\)或者\(||Q(\theta^{i+1},\theta^{i})-Q(\theta^{i},\theta^{i})||&lt;\epsilon_{2}\)</li>
</ul>
</li>
<li><p>总结:</p>
<ul>
<li>在吴恩达CS229课程中: E步包含1, M步包含2,3,其中第2步中求的是\(B(\theta, \theta^{i})\)</li>
<li>在李航&lt;&lt;统计学习方法&gt;&gt;中: E步包含1,2, M步包含3,其中第2步中求的是\(Q(\theta,\theta^{i})\)</li>
<li>两种表达等价</li>
</ul>
</li>
</ul>
<h5 id="图示理解"><a href="#图示理解" class="headerlink" title="图示理解"></a>图示理解</h5><ul>
<li>迭代图示如下图(图来自博客:<a href="https://www.cnblogs.com/xieyue/p/4384915.html" target="_blank" rel="noopener">https://www.cnblogs.com/xieyue/p/4384915.html</a>)</li>
</ul>
<img src="/Notes/ML/ML——EM算法/EM_explain.png">

<ul>
<li>也可参考李航&lt;&lt;统计学习方法&gt;&gt;第160页的图和解释</li>
</ul>
<h4 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h4><ul>
<li>参考李航&lt;&lt;统计学习方法&gt;&gt;第160页-第162页推导过程</li>
<li>参考&lt;&lt;百面机器学习&gt;&gt;第P099页-第P100页<ul>
<li>核心思想<strong>原始函数单调有界</strong></li>
<li>原始函数为\(L(\theta)\),函数下界为\(B(\theta,\theta^{i})\)</li>
<li>E步:<ul>
<li>找到使得在当前\(\theta^{i}\)确定时,原始函数的下界\(B(\theta, \theta^{i})\),在\(\theta^{i}\)处有<br>\(\)<br>$$<br>\begin{align}<br>L(\theta^{i}) = B(\theta,\theta^{i})<br>\end{align}<br>$$</li>
</ul>
</li>
<li>M步: <ul>
<li>找到使得函数\(B(\theta,\theta^{i})\)取得极大值的\(\theta^{i+1}\)</li>
</ul>
</li>
<li>i = i + 1,然后重新开始E和M步<br>$$<br>\begin{align}<br>L(\theta^{i+1}) &gt;= L(\theta^{i+1})<br>\end{align}<br>$$</li>
<li>所以函数是单调的</li>
<li>由于\(L(\theta)\)有界(这里原始函数有界可以从似然函数的定义得到)</li>
<li><strong>函数单调有界=&gt;函数收敛</strong>(数学分析中的定理)</li>
</ul>
</li>
</ul>
<h4 id="优劣性"><a href="#优劣性" class="headerlink" title="优劣性"></a>优劣性</h4><h5 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h5><ul>
<li>简单性</li>
<li>普适性</li>
</ul>
<h5 id="劣势"><a href="#劣势" class="headerlink" title="劣势"></a>劣势</h5><ul>
<li>不能保证收敛到最大值,只能保证收敛到极大值</li>
<li>对初始值敏感,不同初始值可能收敛到不同极值点</li>
<li>实际使用时通常采用多次选择不同的初始值来进行迭代,最终对估计值选最好的</li>
</ul>
<h4 id="EM算法的推广"><a href="#EM算法的推广" class="headerlink" title="EM算法的推广"></a>EM算法的推广</h4><ul>
<li>引入F函数</li>
</ul>
<h5 id="GEM1"><a href="#GEM1" class="headerlink" title="GEM1"></a>GEM1</h5><p><em>F函数极大-极大法</em></p>
<ul>
<li>初始化参数\(\theta^{0}\)</li>
<li>E步: 求使得\(F(\tilde{P},\theta^{i})\)极大化的\(\tilde{P}^{i+1}\)</li>
<li>M步: 求使得\(F(\tilde{P}^{i+1},\theta)\)极大化的\(\theta^{i+1}\)</li>
<li>重复E,M,直到收敛</li>
</ul>
<h5 id="GEM2"><a href="#GEM2" class="headerlink" title="GEM2"></a>GEM2</h5><ul>
<li>初始化参数\(\theta^{0}\)</li>
<li>E步: 计算\(Q(\theta,\theta^{i})\)</li>
<li>M步: 求\(\theta^{i+1}\)使得\(Q(\theta^{i+1},\theta^{i}) &gt; Q(\theta^{i},\theta^{i})\)</li>
<li>重复E,M,直到收敛</li>
<li>总结: \(Q(\theta,\theta^{i})\)的极大化难求时,这种方法可以简化计算</li>
</ul>
<h5 id="GEM3"><a href="#GEM3" class="headerlink" title="GEM3"></a>GEM3</h5><ul>
<li>初始化参数\(\theta^{0}\)</li>
<li>E步: 计算\(Q(\theta,\theta^{i})\)</li>
<li>M步: 对参数\(\theta^{i}\)的每一个维度k,固定参数的其他维度,求使得\(Q(\theta,\theta^{i})\)极大化的\(\theta_{k}^{i+1}\),最终得到\(\theta^{i+1}\)<ul>
<li>使得\(Q(\theta^{i+1},\theta^{i}) &gt; Q(\theta^{i},\theta^{i})\)</li>
</ul>
</li>
<li>重复E,M,直到收敛</li>
<li>总结: 一种特殊的GEM算法,将M步分解为参数\(\theta\)的维度次来条件极大化</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——AUC和GAUC</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94AUC%E5%92%8CGAUC.html</url>
    <content><![CDATA[<p><em>本文介绍AUC和GAUC</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/84350940" target="_blank" rel="noopener">图解AUC和GAUC-知乎</a></li>
</ul>
<h3 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def calculate_auc(ground_truth, predictions):</span><br><span class="line">    # 将预测结果和真实标签按照预测结果从大到小的顺序进行排序</span><br><span class="line">    sorted_predictions = [p for _, p in sorted(zip(predictions, ground_truth), reverse=True)]</span><br><span class="line">    print(sorted_predictions)</span><br><span class="line">    # 统计正样本和负样本的数量</span><br><span class="line">    positive_count = sum(ground_truth)</span><br><span class="line">    negative_count = len(ground_truth) - positive_count</span><br><span class="line"></span><br><span class="line">    neg_found_count = 0</span><br><span class="line">    pos_gt_neg_count = 0</span><br><span class="line">    # 计算正样本大于负样本的数量之和</span><br><span class="line">    for label in sorted_predictions:</span><br><span class="line">        if label == 1:</span><br><span class="line">            pos_gt_neg_count += negative_count - neg_found_count</span><br><span class="line">        else:</span><br><span class="line">            neg_found_count += 1</span><br><span class="line"></span><br><span class="line">    # 计算AUC</span><br><span class="line">    auc = 1.0 * pos_gt_neg_count / (positive_count * negative_count)</span><br><span class="line"></span><br><span class="line">    return auc</span><br><span class="line"></span><br><span class="line"># 真实标签</span><br><span class="line">ground_truth = [1, 0, 1, 0, 1, 1]</span><br><span class="line"># 预测结果</span><br><span class="line">predictions = [0.5, 0.3, 0.1, 0.2, 0.8, 0.9]</span><br><span class="line"></span><br><span class="line"># 计算AUC</span><br><span class="line">auc = calculate_auc(ground_truth, predictions)</span><br><span class="line"></span><br><span class="line">print(&quot;AUC:&quot;, auc)</span><br></pre></td></tr></table></figure>

<h3 id="SQL实现"><a href="#SQL实现" class="headerlink" title="SQL实现"></a>SQL实现</h3><ul>
<li><p>详情见：<a href="https://tracholar.github.io/machine-learning/2018/01/26/auc.html" target="_blank" rel="noopener">深入理解AUC</a></p>
</li>
<li><p>推导思路：</p>
<ul>
<li>统计每个正样本大于负样本的概率（排在该正样本后面的负样本数/总的负样本数）</li>
<li>对所有正样本的概率求均值</li>
</ul>
</li>
<li><p>SQL实现</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select</span><br><span class="line">    (ry - 0.5*n1*(n1+1))/n0/n1 as auc</span><br><span class="line">from(</span><br><span class="line">    select</span><br><span class="line">        sum(if(y=0, 1, 0)) as n0,</span><br><span class="line">        sum(if(y=1, 1, 0)) as n1,</span><br><span class="line">        sum(if(y=1, r, 0)) as ry</span><br><span class="line">    from(</span><br><span class="line">        select y, row_number() over(order by score asc) as r</span><br><span class="line">        from(</span><br><span class="line">            select y, score</span><br><span class="line">            from some.table</span><br><span class="line">        )A</span><br><span class="line">    )B</span><br><span class="line">)C</span><br></pre></td></tr></table></figure>
</li>
<li><p>SQL实现（分场景+pcoc实现）</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line">    scene,</span><br><span class="line">    (ry - 0.5*n1*(n1+1))/n0/n1 as auc,</span><br><span class="line">    n1/(n1+n0) as ctr,</span><br><span class="line">    pctr,</span><br><span class="line">    pctr/(n1/(n1+n0)) as pcoc,</span><br><span class="line">from(</span><br><span class="line">    select </span><br><span class="line">        scene,</span><br><span class="line">        sum(if(y=0, 1, 0)) as n0,</span><br><span class="line">        sum(if(y=1, 1, 0)) as n1,</span><br><span class="line">        sum(if(y=1, r, 0)) as ry,</span><br><span class="line">        avg(score) as pctr</span><br><span class="line">    from(</span><br><span class="line">        select scene, score, y, row_number() over(partition by scene order by score asc) as r</span><br><span class="line">        from(</span><br><span class="line">            select scene, y, score</span><br><span class="line">            from some.table</span><br><span class="line">        )A</span><br><span class="line">    )B</span><br><span class="line">)C</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——MCMC采样</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94MCMC%E9%87%87%E6%A0%B7.html</url>
    <content><![CDATA[<p><em>本文介绍MCMC采样法和他的两个常用方法:MH(Metropolis-Hastings)采样法和Gibbs采样法</em><br><em>马尔科夫蒙特卡洛(Markov Chain Monte Carlo, MCMC)采样法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>对于高维空间中的随机向量,拒绝采样和重要性采样经常难以找到合适的参考分布,容易导致采样效率低下(样本的接受概率太小或者重要性权重太低),此时可以考虑马尔科夫蒙特卡洛采样法(MCMC)</li>
<li>MCMC中常见的有两种,MH(Metropolis-Hastings)采样法和Gibbs采样法</li>
</ul>
<hr>
<h3 id="MCMC概述"><a href="#MCMC概述" class="headerlink" title="MCMC概述"></a>MCMC概述</h3><ul>
<li>马尔科夫蒙特卡洛(Markov Chain Monte Carlo, MCMC)采样法可分为两个部分(两个MC)描述</li>
</ul>
<h4 id="蒙特卡洛法"><a href="#蒙特卡洛法" class="headerlink" title="蒙特卡洛法"></a>蒙特卡洛法</h4><ul>
<li>蒙特卡洛法(Monte Carlo)是指: <strong>基于采样的数值型近似求解方法</strong></li>
</ul>
<h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><p><em>又称离散时间马尔可夫链(discrete-time Markov chain，缩写为DTMC),或者马氏链</em></p>
<ul>
<li>马尔科夫链(Markov Chain)是指: 状态空间中经过从一个状态到另一个状态的转换的随机过程, 该随机过程满足马尔科夫性</li>
<li>马尔科夫性(Markov property)是指: 当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。具有马尔可夫性质的过程通常称之为马尔可夫过程<ul>
<li>马尔科夫性的简单理解: “无记忆性”: 下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关</li>
</ul>
</li>
</ul>
<h4 id="MCMC基本思想"><a href="#MCMC基本思想" class="headerlink" title="MCMC基本思想"></a>MCMC基本思想</h4><ul>
<li>针对采样的目标分布,<strong>构造一个马尔科夫链</strong>,使得该<strong>马尔科夫链的平稳分布就是目标分布</strong></li>
<li>从任何一个初始状态出发,沿着马尔科夫链进行状态转移,直到<strong>马尔科夫链收敛</strong>(到达平稳状态)</li>
<li>继续在马尔科夫链上进行状态转移,<strong>收敛后继续采样得到的样本就是原始目标分布的样本</strong><ul>
<li><strong>burn-in处理</strong>: 现实应用中,我们需要丢弃收敛前的样本,只保留收敛后的样本</li>
<li>burn-in 原意为”老化,定型”之意,在这里表示我们只取后面马氏链定型(收敛)后采样得到的样本</li>
<li>假设采样到收敛用了n次采样,那么服从原始分布的k个样本为\((x^{n+1}, x^{n+2},,,, x^{n+k})\)有时候为了得到近似独立的样本,可以间隔每r次再取出其中一个样本\((x^{n+r}, x^{n+2r},,,, x^{n+kr})\)</li>
<li>真正独立同分布的k个样本可用多条k条不同的收敛后的马氏链得到,不同马氏链采样得到的样本是独立同分布的</li>
</ul>
</li>
<li>核心: <strong>马尔科夫链的构造</strong>,也就是<strong>确定马尔科夫链的状态转移概率</strong></li>
</ul>
<hr>
<h3 id="常见的MCMC方法"><a href="#常见的MCMC方法" class="headerlink" title="常见的MCMC方法"></a>常见的MCMC方法</h3><h4 id="MH-Metropolis-Hastings-采样法"><a href="#MH-Metropolis-Hastings-采样法" class="headerlink" title="MH(Metropolis-Hastings)采样法"></a>MH(Metropolis-Hastings)采样法</h4><ul>
<li>对于原始目标分布\(p(x)\)</li>
<li>选择一个参考条件分布\(q(x^{\star}|x)\), 定义接受概率\(A(x,x^{\star})\):<ul>
<li>(注意这里是参考条件分布,因为是马尔科夫链,所以每个状态都由上一个状态转移而来,需要定义的参考分布应该是条件分布,不是一般拒绝采样中的普通参考分布)<br>$$<br>A(x,x^{\star}) = min\left ( 1, \frac{p(x^{\star})q(x|x^{\star})}{p(x)q(x^{\star}|x)} \right )<br>$$</li>
</ul>
</li>
<li>MH采样法构建满足平稳分布就是目标分布\(p(x)\)的秘诀就是让<strong>每次采样时,当前状态以一定概率停留在上一状态</strong><ul>
<li>与拒绝采样对应: 接受意味着跳转到下一状态,拒绝意味着停留在当前状态</li>
</ul>
</li>
</ul>
<h5 id="采样过程"><a href="#采样过程" class="headerlink" title="采样过程"></a>采样过程</h5><ul>
<li>随机选取初始样本x^{0}</li>
<li>for t = 1,2,3,…:<ul>
<li>参考条件分布采样\(x^{\star} \sim q(x^{star}|x^{t-1})\)</li>
<li>均匀分布采样\(u \sim U(0,1)\)</li>
<li>判断是否接受: 如果\(u &lt; A(x^{t-1}, x^{\star})\),则接受,令: \(x^{t} = x^{\star}\), 否则拒绝,令: \(x^{t}=x^{t-1}\)</li>
</ul>
</li>
<li><strong>burn-in处理</strong>: 丢弃采样到平稳分布前的样本, 只保留平稳分布后的样本即为服从原始分布\(p(x)\)的样本<ul>
<li>假设采样到收敛用了n次采样,那么服从原始分布的k个样本为\((x^{n+1}, x^{n+2},,,, x^{n+k})\)有时候为了得到近似独立的样本,可以间隔每r次再取出其中一个样本\((x^{n+r}, x^{n+2r},,,, x^{n+kr})\)</li>
<li>真正独立同分布的k个样本可用多条k条不同的收敛后的马氏链得到,不同马氏链采样得到的样本是独立同分布的</li>
</ul>
</li>
<li>采样次数一般来说是凭经验选择一个足够大的值,现实是现实可以使用一些参数变化量这类的指标来判断采样是否收敛,参考<a href="/Notes/NLP/NLP%E2%80%94%E2%80%94LLDA%E7%9A%84Gibbs%E9%87%87%E6%A0%B7%E5%AE%9E%E7%8E%B0.html">NLP——LLDA的Gibbs采样实现</a></li>
</ul>
<h5 id="与拒绝采样的区别"><a href="#与拒绝采样的区别" class="headerlink" title="与拒绝采样的区别"></a>与拒绝采样的区别</h5><ul>
<li>MH采样基于拒绝采样来逼近平稳分布</li>
<li>拒绝采样中: 如果样本某一步被拒绝,那么该步不会产生新的样本,需要重新对当前步进行采样</li>
<li>MH中: 每一步都会产生一个样本,被拒绝后,就令当前样本和上一个样本相同即可<ul>
<li>因为这里是为了使得每个状态的转出概率等于转入概率,所以拒绝就意味着当前采样步骤状态不跳转</li>
<li>MH采样法最核心的思想就是一定概率停留在上一个状态来实现对马尔科夫链的构建的</li>
</ul>
</li>
</ul>
<h5 id="MH采样法正确性证明"><a href="#MH采样法正确性证明" class="headerlink" title="MH采样法正确性证明"></a>MH采样法正确性证明</h5><ul>
<li><p>MH采样法构造的马尔科夫链(状态转移概率矩阵)是正确的吗?</p>
</li>
<li><p><strong>细致平稳条件</strong>, 如果非周期的马氏链的状态转移矩阵P和分布\(\pi(x)\)满足下面的式子对任意的\(i,j\)都成立:<br>$$\pi(x^{i})P_{ij} = \pi(x^{j})P_{ji}$$</p>
<ul>
<li>上式为细致平稳分布条件(detailed balance condition)</li>
<li>其中\(\pi(x)\)为马氏链的平稳分布,在这里等于我们的原始分布\(p(x)\)</li>
</ul>
</li>
<li><p>证明\(\pi(x)\)为马氏链的平稳分布:<br>$$<br>\begin{align}<br>\sum_{i=1}^{\infty}\pi(x^{i})P_{ij} = \sum_{i=1}^{\infty}\pi(x^{j})P_{ji} = \pi(x^{j})\sum_{i=1}^{\infty}P_{ji} = \pi(x^{j}) \\<br>=&gt; \pi(x) P = \pi(x)<br>\end{align}<br>$$</p>
<ul>
<li>由于\(\pi(x)\)为方程\(\pi(x) P = \pi(x)\)的解,所以\(\pi(x)\)是状态转移矩阵P对应的马氏链的平稳分布</li>
</ul>
</li>
<li><p>在MH采样法中</p>
<ul>
<li>参考条件分布函数本对应状态转移矩阵的一个元素,\(q(x^{i}|x^{j}) = P_{ij}\)(注意: <strong>实际上一般不相等</strong>)</li>
<li>但是很难构造这样方便采样的函数,于是我们使用一个接受率来修正\(q(x^{i}|x^{j})\alpha(x^{j}, x^{i}) = P_{ij}\)<ul>
<li>\(\alpha(x^{j}, x^{i})\)表示从\(x^{j}\)跳转到\(x^{i}\)的接受率, 其值可如下求得:<br>$$<br>\begin{align}<br>\pi(x^{i}) P_{ij} &amp;= \pi(x^{j})P_{ji} \\<br>\pi(x^{i}) q(x^{j}|x^{i})\alpha(x^{i}, x^{j}) &amp;= \pi(x^{j})q(x^{i}|x^{j})\alpha(x^{j}, x^{i})<br>\end{align}<br>$$</li>
</ul>
</li>
<li>显然,直接取:<br>$$<br>\begin{align}<br>\alpha(x^{i}, x^{j}) &amp;= \pi(x^{j})q(x^{i}|x^{j})\\<br>\alpha(x^{j}, x^{i}) &amp;= \pi(x^{i})q(x^{j}|x^{i})\\<br>\end{align}<br>$$</li>
<li>即可</li>
</ul>
</li>
<li><p>但是由于\(\alpha(x^{j}, x^{i})\)一般来说太小,所以我们考虑将\(\alpha(x^{j}, x^{i})\)和\(\alpha(x^{i}, x^{j})\)同时扩大M倍,使得其中大的那个为1,即可得到最大的接受率</p>
<ul>
<li>使用原始接受率的方法称为一般MCMC方法</li>
<li>使用扩大M被接受率的方法称为MCMC的改进方法: MH方法</li>
</ul>
</li>
<li><p>改进后的接受率为从\(x^{i}\)跳转到\(x^{j}\)的接受率:<br>$$<br>\begin{align}<br>A(x^{i}, x^{j}) = min\left ( 1, \frac{p(x^{j})q(x^{i}|x^{j})}{p(x^{i})q(x^{j}|x^{i})} \right )<br>\end{align}<br>$$</p>
<ul>
<li>理解:<br>$$<br>\begin{align}<br>p(x^{j})q(x^{i}|x^{j}) &amp;&gt; p(x^{i})q(x^{j}|x^{i})时: A(x^{i}, x^{j}) = 1 \\<br>p(x^{j})q(x^{i}|x^{j}) &amp;&lt; p(x^{i})q(x^{j}|x^{i})时: A(x^{i}, x^{j}) = \frac{p(x^{j})q(x^{i}|x^{j})}{p(x^{i})q(x^{j}|x^{i})}<br>\end{align}<br>$$</li>
</ul>
</li>
<li><p>在MH中表现为从\(x\)跳转到\(x^{\star}\)的接受率:<br>$$<br>A(x,x^{\star}) = min\left ( 1, \frac{p(x^{\star})q(x|x^{\star})}{p(x)q(x^{\star}|x)} \right )<br>$$</p>
</li>
</ul>
<h4 id="Gibbs采样法"><a href="#Gibbs采样法" class="headerlink" title="Gibbs采样法"></a>Gibbs采样法</h4><p><em>Gibbs采样是MH采样法的一个特例,每次只更新样本的一个维度</em></p>
<ul>
<li>针对维度很高的多维向量,同时采样多个维度难度较高,且接受率很小</li>
<li>使用Gibbs采样每次采样一个维度可以解决这个问题</li>
</ul>
<h5 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h5><ul>
<li>求得已知其他维度下,每一维度的条件概率\(p(x_{i}|x_{1},,,x_{i-1}, x_{i+1},,,x_{d})\)</li>
</ul>
<h5 id="采样过程-1"><a href="#采样过程-1" class="headerlink" title="采样过程"></a>采样过程</h5><ul>
<li><p>随机选择初始状态\(x^{0} = (x_{1}^{0}, x_{2}^{0}, x_{3}^{0},,,, x_{d}^{0})\)</p>
</li>
<li><p>for t = 1,2,3,…:</p>
<ul>
<li>基于前一次产生的样本:\(x^{t-1} = (x_{1}^{t-1}, x_{2}^{t-1}, x_{3}^{t-1},,,, x_{d}^{t-1})\), 依次采样和更新每个维度的值,即依次按照:<ul>
<li>\(x_{1}^{t} \sim p(x_{1}|x_{2}^{t-1},,,x_{d}^{t-1})\)</li>
<li>\(x_{2}^{t} \sim p(x_{2}|x_{1}^{t}, x_{3}^{t-1},,,x_{d}^{t-1})\)</li>
<li>\(x_{i}^{t} \sim p(x_{i}|x_{1}^{t},,,x_{i-1}^{t}, x_{i+1}^{t-1},,,x_{d}^{t-1})\)</li>
<li>\(x_{d}^{t} \sim p(x_{d}|x_{1}^{t}, x_{2}^{t},,,x_{d-1}^{t})\)</li>
</ul>
</li>
<li>得到新的一个样本:\(x^{t} = (x_{1}^{t}, x_{2}^{t}, x_{3}^{t},,,, x_{d}^{t})\)</li>
</ul>
</li>
<li><p><strong>burn-in处理</strong>: 丢弃采样到平稳分布前的样本, 只保留平稳分布后的样本即为服从原始分布\(p(x)\)的样本</p>
<ul>
<li>假设采样到收敛用了n次采样,那么服从原始分布的k个样本为\((x^{n+1}, x^{n+2},,,, x^{n+k})\), 有时候为了得到近似独立的样本,可以间隔每r次再取出其中一个样本\((x^{n+r}, x^{n+2r},,,, x^{n+kr})\)</li>
<li>真正独立同分布的k个样本可用多条k条不同的收敛后的马氏链得到,不同马氏链采样得到的样本是独立同分布的</li>
<li>注意: 采样完成部分维度生成的中间样本如\((x_{1}^{t},,,x_{i-1}^{t}, x_{i}^{t}, x_{i+1}^{t-1},,,x_{d}^{t-1})\)是不能作为最终样本的,因为他们之间(同一轮次的多个中间样本)相互依赖性太强,不具有独立同分布的(虽然完全采样完成的也不能视为具有独立同分布,但是可近似的认为是独立同分布的)</li>
</ul>
</li>
<li><p>采样次数一般来说是凭经验选择一个足够大的值,现实是现实可以使用一些参数变化量这类的指标来判断采样是否收敛,参考<a href="/Notes/NLP/NLP%E2%80%94%E2%80%94LLDA%E7%9A%84Gibbs%E9%87%87%E6%A0%B7%E5%AE%9E%E7%8E%B0.html">NLP——LLDA的Gibbs采样实现</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——主动学习-半监督学习-直推学习</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E7%9B%B4%E6%8E%A8%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<p><strong>主动学习(Active Learning), 半监督学习(Semi-Supervised Learning)与直推学习(Transductive Learning)</strong><br><strong>半监督学习又称为归纳学习(Inductive Learning)</strong></p>
<hr>
<h3 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a>补充知识</h3><h4 id="“开放世界”假设"><a href="#“开放世界”假设" class="headerlink" title="“开放世界”假设"></a>“开放世界”假设</h4><ul>
<li>学得的模型能适用于训练过程中从未观察到的数据</li>
<li>也就是说:测试集未知</li>
</ul>
<h4 id="“封闭世界”假设"><a href="#“封闭世界”假设" class="headerlink" title="“封闭世界”假设"></a>“封闭世界”假设</h4><ul>
<li>学得的模型仅仅能适用于训练过程中观察到的未标记样本</li>
<li>也就是说:测试集就是训练时观察到的未标记数据</li>
</ul>
<hr>
<h3 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h3><ul>
<li>都是用于解决有少量标注数据和海量未标注数据的问题的算法</li>
<li>都是迭代扩充标记数据集的算法:<ul>
<li>每次迭代时添加如一部分新的标记数据(由未标记数据标记产生的)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><h4 id="主动学习"><a href="#主动学习" class="headerlink" title="主动学习"></a>主动学习</h4><ul>
<li>主动学习添加了专家知识(人工确认或者打标签),每次迭代时加入的新的标记数据都是由专家打出来的标签</li>
<li>半监督学习和直推学习都是全自动的(无需人工干预),主动学习是半自动的</li>
</ul>
<h4 id="半监督学习与直推学习"><a href="#半监督学习与直推学习" class="headerlink" title="半监督学习与直推学习"></a>半监督学习与直推学习</h4><ul>
<li>直推学习将当前的为标签数据看成是最终的测试数据</li>
<li>半监督学习和主动学习的测试集都是未知数据</li>
<li>半监督学习是基于”开放世界”假设的</li>
<li>直推学习是基于”封闭世界”假设的</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="总体概览"><a href="#总体概览" class="headerlink" title="总体概览"></a>总体概览</h4>

<h4 id="表格概览"><a href="#表格概览" class="headerlink" title="表格概览"></a>表格概览</h4><table>
<thead>
<tr>
<th align="left">学习算法</th>
<th align="center">是否需要专家知识(人工)</th>
<th align="center">是否具有泛化性</th>
</tr>
</thead>
<tbody><tr>
<td align="left">半监督学习</td>
<td align="center">否</td>
<td align="center">是(“开放世界”假设)</td>
</tr>
<tr>
<td align="left">主动学习</td>
<td align="center">是</td>
<td align="center">是(“开放世界”假设)</td>
</tr>
<tr>
<td align="left">直推学习</td>
<td align="center">否</td>
<td align="center">不具有,测试集是已知的未标记数据(“封闭世界假设”)</td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——分布的距离评估</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E7%9A%84%E8%B7%9D%E7%A6%BB%E8%AF%84%E4%BC%B0.html</url>
    <content><![CDATA[<h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><h3 id="TV距离"><a href="#TV距离" class="headerlink" title="TV距离"></a>TV距离</h3><ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/352946799" target="_blank" rel="noopener">Total Variation Distance 总变差</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——最小二乘与梯度下降</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.html</url>
    <content><![CDATA[<p><em>本文持续更新，主要总结各种与最小二乘和梯度下降相关的优化方法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="最小二乘与最小均方误差的区别"><a href="#最小二乘与最小均方误差的区别" class="headerlink" title="最小二乘与最小均方误差的区别"></a>最小二乘与最小均方误差的区别</h3><h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><ul>
<li>二乘也就是平方的意思，故又称最小平方法(英文全称Least Square Method, LSM或者LS)</li>
<li>是一种数学优化技术</li>
<li>通过最小化误差的平方和寻找数据的最佳函数匹配<ul>
<li>为什么最小化误差平方和可以得到最佳函数匹配呢？<ul>
<li>可以证明，只要误差是服从正太分布的，最小化误差平法和就能匹配到最佳函数(这个高斯证明过)</li>
<li>可以证明，误差的确是服从正太分布的，细节可以参考<a href="https://www.matongxue.com/madocs/589.html" title="https://www.matongxue.com/madocs/589.html" target="_blank" rel="noopener">为什么正太分布如此常见</a></li>
<li>中心极限定理说了，在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布 <ul>
<li>适当的条件包括三个要素:<ul>
<li>独立</li>
<li>随机</li>
<li>相加</li>
</ul>
</li>
<li>误差满足上面三个要素</li>
</ul>
</li>
<li>综上所述，最小二乘法(最小化误差平法和)可以的到最佳函数匹配</li>
</ul>
</li>
</ul>
</li>
<li>最小二乘法通过求导，并令导数为0，直接求得误差平方和(损失函数的一种)取最小值时的参数</li>
<li>公式:<br>$$\theta^{\star} = \arg\min_\theta L(x) = \sum_i^{m}(y_i-f(x_i;\theta))^2$$</li>
</ul>
<h4 id="加权最小二乘法"><a href="#加权最小二乘法" class="headerlink" title="加权最小二乘法"></a>加权最小二乘法</h4><ul>
<li>加权最小二乘中每个样本的误差前面会乘上相应的权重,然后再求和<br>$$\theta^{\star} = \arg\min_\theta L(x) = \sum_i^{m}w_i(y_i-f(x_i;\theta))^2$$</li>
</ul>
<h4 id="最小化均方误差"><a href="#最小化均方误差" class="headerlink" title="最小化均方误差"></a>最小化均方误差</h4><ul>
<li>最小二乘（Least Square, LS）问题是这样一类优化问题，目标函数是若干项的平方和</li>
<li>LS的一种更复杂也更灵活的变形: 加权最小二乘根据实际问题考虑每个求和项的重要程度，即为每一项加权值w</li>
<li>均方误(Mean Square Error, MSE)是一种加权最小二乘，它的权值是对应项的概率<br>$$\theta^{\star} = \arg\min_\theta L(x) = \sum_i^{m}\frac{1}{m}(y_i-f(x_i;\theta))^2 = \frac{1}{m}\sum_i^{m}(y_i-f(x_i;\theta))^2$$</li>
</ul>
<h4 id="周志华的解释"><a href="#周志华的解释" class="headerlink" title="周志华的解释"></a>周志华的解释</h4><ul>
<li>基于均方误差最小化来进行模型求解的方法称为最小二乘法 ——《机器学习》(周志华著)<ul>
<li>理解：周志华的意思应该是把均方误差与误差平方和视为一个东西，也就是说各项的权重相同，所以感觉这里定义其实是比较模糊的，不用太在意最小二乘法与最小化均方误差法的定义</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>三者可以理解为相同的表达式,其中<ul>
<li><strong>最小二乘法</strong>(LSM), 权重为1, 每个样本相同</li>
<li><strong>最小均方误差</strong>(MSE), 权重为\(\frac{1}{m}\), 每个样本相同</li>
<li><strong>加权最小二乘</strong>, 权重为自定义的值, 每个样本可以不同</li>
</ul>
</li>
<li><strong>最小均方误差</strong>(MSE) 是一种 <strong>加权最小二乘</strong>, 他的权重为样本的概率, 每个样本出现的概率相等, 就都乘以 \(\frac{1}{m}\) 即可</li>
</ul>
<hr>
<h3 id="梯度下降与梯度上升"><a href="#梯度下降与梯度上升" class="headerlink" title="梯度下降与梯度上升"></a>梯度下降与梯度上升</h3><h4 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h4><p><strong>为什么需要梯度下降或者梯度上升,而不是用直接对目标函数求导得到最优解?</strong> </p>
<h5 id="几点说明"><a href="#几点说明" class="headerlink" title="几点说明"></a>几点说明</h5><ul>
<li>对于凸函数而言:<ul>
<li>通过函数(损失函数等)对参数求导,令其导数为0,的确可以解得损失函数的最小(最大)值点对应的参数</li>
</ul>
</li>
<li>对于非凸函数而言:<ul>
<li>相同的方法可能只能得到极值点,而不是最小(最大)值点</li>
<li>但此时梯度下降法也不能找到最优点</li>
</ul>
</li>
<li>无论是梯度下降函数直接推到求解目标函数最优解,我们都需要求目标函数的导数,并且需要确保目标函数可导,不然是不能用这些方法的</li>
</ul>
<h5 id="几点原因"><a href="#几点原因" class="headerlink" title="几点原因"></a>几点原因</h5><ul>
<li>有时候目标方程(令导数为0后得到的方程)很复杂,求导数为0的参数的解(根)很难<ul>
<li>包括时间和空间两方面可能面临困难</li>
<li>比如大矩阵的乘法,大矩阵的逆等</li>
<li>求某一点的梯度一般是比较容易的(只要求出导数后将对应点的数值带入即可)<ul>
<li><em>这里与求导数为0的解难度不一样,在求出函数导数后,求某一点的梯度仅仅需要带入该点的值,而求导数为0的解需要的是很难的解题过程,甚至可能找不到解</em></li>
</ul>
</li>
</ul>
</li>
<li>有时候目标方程很奇怪,之前我们没见过,需要我们写新的代码教会计算机如何直接求解函数的根<ul>
<li>直接使用梯度下降的话是计算机可以理解的</li>
<li>梯度下降法不用重新写代码来教给计算机</li>
</ul>
</li>
</ul>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><ul>
<li>梯度下降是最小化目标函数<ul>
<li>\(\theta=\theta-\lambda\frac{\partial L(\theta)}{\partial\theta}\)</li>
<li>\(\lambda\)为步长</li>
<li>每轮迭代沿着负梯度方向移动</li>
</ul>
</li>
</ul>
<h4 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h4><ul>
<li>梯度上升是最大化目标函数<ul>
<li>\(\theta=\theta+\lambda\frac{\partial L(\theta)}{\partial\theta}\)</li>
<li>\(\lambda\)为步长</li>
<li>每轮迭代验证正梯度方向移动</li>
</ul>
</li>
</ul>
<hr>
<h3 id="梯度下降-上升-法与最小二乘法"><a href="#梯度下降-上升-法与最小二乘法" class="headerlink" title="梯度下降(上升)法与最小二乘法"></a>梯度下降(上升)法与最小二乘法</h3><h4 id="最小二乘法-1"><a href="#最小二乘法-1" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><ul>
<li>最小二乘法是一种通过<strong>最小化误差的平方和</strong>寻找数据的<strong>最佳函数匹配</strong>的<strong>数学优化技术</strong></li>
<li>与最小二乘法并列的是最小话误差的三次方或者四次方等方法<ul>
<li>这里的<strong>三次方和</strong>和<strong>四次方和</strong>只是为了和<strong>二次方和</strong>作对比，实践中很少遇到这样的优化技术</li>
</ul>
</li>
</ul>
<h4 id="梯度下降-上升-法"><a href="#梯度下降-上升-法" class="headerlink" title="梯度下降(上升)法"></a>梯度下降(上升)法</h4><ul>
<li>梯度下降(上升)法的目标是通过迭代(每次朝着最优方向，即梯度下降最快方向前进)不断逼近能使得目标函数最小(最大)时的最优参数值的方法</li>
<li>梯度下降(上升)是一种迭代法，也就是通过更新参数不停的逼近最优点，可以用来解决各种各样的问题(包括最小二乘问题，最小化误差三次方和等)<ul>
<li>这里最小二乘问题可以理解为用最小二乘法(最小化误差平法和)来寻找最佳函数的问题</li>
</ul>
</li>
</ul>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ul>
<li>最小二乘法与梯度下降(上升)法不在一个维度，不能对比</li>
</ul>
<h5 id="最小二乘法-2"><a href="#最小二乘法-2" class="headerlink" title="最小二乘法"></a>最小二乘法</h5><ul>
<li>目标：找到(或者逼近)最优函数，使得该函数能最拟合已知数据</li>
<li>方法：最小化误差平方和</li>
</ul>
<h5 id="梯度下降-上升-法-1"><a href="#梯度下降-上升-法-1" class="headerlink" title="梯度下降(上升)法"></a>梯度下降(上升)法</h5><ul>
<li>目标：找到(或者逼近)最优参数，使得该参数能最小化(最大化)目标函数</li>
<li>方法：是通过沿梯度方向迭代更新参数</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——参数化模型与非参数化模型</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E5%8F%82%E6%95%B0%E5%8C%96%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%9D%9E%E5%8F%82%E6%95%B0%E5%8C%96%E6%A8%A1%E5%9E%8B.html</url>
    <content><![CDATA[<p><em>本文介绍非参数化模型（Non-parametric models）和参数化模型（Parametric models）</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="参数化模型"><a href="#参数化模型" class="headerlink" title="参数化模型"></a>参数化模型</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>参数化模型</strong>指的是那些依赖于固定数量参数的模型，这些参数可以通过训练数据学习得到。参数化模型的特点是一旦参数确定，模型的复杂度也就固定了。</p>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>参数化模型的例子包括线性回归、逻辑回归、支持向量机（SVM）和神经网络等</p>
<h3 id="非参数化模型"><a href="#非参数化模型" class="headerlink" title="非参数化模型"></a>非参数化模型</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p><strong>非参数化模型</strong>则不对模型形式做出严格的假设，也不依赖于固定数量的参数。这类模型通常具有更大的灵活性，可以根据数据的复杂性来适应模型的复杂度。</p>
<h4 id="举例-1"><a href="#举例-1" class="headerlink" title="举例"></a>举例</h4><p>非参数化模型的例子包括高斯过程回归、决策树、k-最近邻（k-NN）算法、核方法以及各种基于模型的集成方法如随机森林和提升树（Boosting Trees）等</p>
<h3 id="其他说明"><a href="#其他说明" class="headerlink" title="其他说明"></a>其他说明</h3><ul>
<li><blockquote>
<p>尽管神经网络可以拥有大量的参数，使得它们具有高度的灵活性和强大的表达能力，但这些特点并不使其成为非参数化模型。非参数化模型指的是不对模型形式做出严格假设的模型，其参数的数量和模型的复杂度可以随着数据量的增加而增加。因此，即使神经网络在实际应用中可以非常复杂，它们仍然是基于固定参数数量的参数化模型。</p>
</blockquote>
</li>
<li>神经网络越来越复杂，现在参数量已经非常大，实际上可以看做是非参数模型？</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——模型性能评估之MAP</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E4%B9%8BMAP.html</url>
    <content><![CDATA[<p><em>MAP(Mean Average Precision)</em></p>
<ul>
<li>参考博客: <a href="https://blog.csdn.net/JNingWei/article/details/78955536" target="_blank" rel="noopener">https://blog.csdn.net/JNingWei/article/details/78955536</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——模型的方差与偏差</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html</url>
    <content><![CDATA[<p><em>本文讲解机器学习中模型的方差偏差关系</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="偏差与方差的定义"><a href="#偏差与方差的定义" class="headerlink" title="偏差与方差的定义"></a>偏差与方差的定义</h3><ul>
<li>方差：模型的预测值之间的离散程度</li>
<li>偏差：模型整体的预测值与真实值的偏离程度<img src="/Notes/ML/ML——模型的方差与偏差/Variance&Bias.png">

</li>
</ul>
<hr>
<h3 id="正则化与方差和偏差"><a href="#正则化与方差和偏差" class="headerlink" title="正则化与方差和偏差"></a>正则化与方差和偏差</h3><ul>
<li>参考博客：<a href="https://www.cnblogs.com/qkloveslife/p/9885500.html" target="_blank" rel="noopener">https://www.cnblogs.com/qkloveslife/p/9885500.html</a></li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>\(\lambda\)为正则化系数</li>
<li>当\(\lambda\)很小时，模型处于“高方差”状态，“训练误差”很小，“交叉验证误差”较大</li>
<li>当\(\lambda\)很大时，模型处于“高偏差”状态，“训练误差”和“交叉验证误差”都很大</li>
</ul>
<hr>
<h3 id="集成学习与方差和偏差"><a href="#集成学习与方差和偏差" class="headerlink" title="集成学习与方差和偏差"></a>集成学习与方差和偏差</h3><ul>
<li>参考博客：<a href="https://blog.csdn.net/xmu_jupiter/article/details/47314927" target="_blank" rel="noopener">https://blog.csdn.net/xmu_jupiter/article/details/47314927</a></li>
</ul>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><h5 id="集成学习分两类"><a href="#集成学习分两类" class="headerlink" title="集成学习分两类"></a>集成学习分两类</h5><ul>
<li>平均方法：例如随机森林， Bagging methods。在平均方法中，系统分别去建立多个基分类器，分类器之间没有任何联系。然后在分类或者回归阶段，各个分类器根据测试数据给出自己的答案，然后系统根据各个分类器给出的结果去综合出最后的结果，比如可以使投票的形式。</li>
<li>提升方法：例如梯度提升决策树GBDT，AdaBoost。在提升方法中，系统模型在训练过程中会先后建立一系列分类器，这些分类器单个可能是弱分类器，但是组合起来就成为一个强分类器。</li>
<li>Stacking方法：<ul>
<li>单层Stacking（普通Stacking）：降低方差，类似于bagging（因为类似于集成了多个模型投票，属于平均方法）</li>
<li>多层Stacking：同时降低方差和偏差，每层上有平均方法，多层之间是提升方法</li>
<li><a href="https://zhuanlan.zhihu.com/p/441394654" target="_blank" rel="noopener">方差与偏差、Bagging、Boosting、Stacking【实用机器学习5.1-5.4】</a></li>
</ul>
</li>
</ul>
<h5 id="不同类别的偏差与方差"><a href="#不同类别的偏差与方差" class="headerlink" title="不同类别的偏差与方差"></a>不同类别的偏差与方差</h5><ul>
<li>平均方法尝试去降低模型的方差<ul>
<li>所以平均方法通常比其任何一个基分类器效果好</li>
</ul>
</li>
<li>而提升方法尝试去降低模型的偏差</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——求解无约束最优化问题的优化方法</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E6%B1%82%E8%A7%A3%E6%97%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>本文介绍求解无约束最优化问题的优化方法：梯度下降法(Gradient Descent)、牛顿法(Newton’s Method)和拟牛顿法(Quasi-Newton Methods)</strong></p>
<ul>
<li>共轭梯度法也是一种无约束的参数优化方法，详情见<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E5%92%8C%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95.html">ML——共轭梯度法和最速下降法</a></li>
</ul>
<hr>
<h3 id="无约束参数优化方法概览"><a href="#无约束参数优化方法概览" class="headerlink" title="无约束参数优化方法概览"></a>无约束参数优化方法概览</h3><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><p>$$<br>\begin{align}<br>\theta^{\star} = \arg\max_{\theta}L(\theta)<br>\end{align}<br>$$</p>
<h4 id="直接法"><a href="#直接法" class="headerlink" title="直接法"></a>直接法</h4><h5 id="使用直接法需要满足两个条件"><a href="#使用直接法需要满足两个条件" class="headerlink" title="使用直接法需要满足两个条件"></a>使用直接法需要满足两个条件</h5><ul>
<li>\(L(\theta)\)为关于\(\theta\)的凸函数</li>
<li>\(\nabla L(\theta)=0\)有解析解(又名闭式解，指的是有限个常见运算的组合给出的形式)</li>
</ul>
<h5 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h5><ul>
<li>目标函数直接对参数求导后令目标函数导数为0: $$\nabla L(\theta)=0$$</li>
</ul>
<h4 id="迭代法"><a href="#迭代法" class="headerlink" title="迭代法"></a>迭代法</h4><ul>
<li>包括梯度下降法和牛顿法，牛顿法又可以拓展为拟牛顿法</li>
</ul>
<hr>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p><em>更详细的说明参考<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.html" title="/Notes/ML/ML——最小二乘与梯度下降.html">ML——最小二乘与梯度下降</a>中的介绍</em></p>
<ul>
<li>梯度下降法通过每次迭代沿着梯度下降最快的方向(梯度的负方向)前进一个\(\Delta \theta\)长度</li>
<li>\(\Delta \theta\)为步长\(\alpha\)(参数)乘以梯度的模(变化率)的方法，实现对极大(小)值的一步步逼近</li>
</ul>
<hr>
<h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><h4 id="数学中的牛顿法"><a href="#数学中的牛顿法" class="headerlink" title="数学中的牛顿法"></a>数学中的牛顿法</h4><ul>
<li>牛顿法的本质是求函数值\(f(x)\)为零(\(f(x)=0\))时参数(自变量\(x\))的值</li>
<li>本质上说，牛顿法是用逼近的方法来<strong>解方程</strong>的一种方法</li>
</ul>
<h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><ul>
<li>对于确定的方程\(f(x)=0\)，我们的目标是求解自变量\(x\)的值</li>
<li>1.初始值定义为\(x=x_{0}\)</li>
<li>2.牛顿法通过迭代每次从当前点\(x={k}\)沿着斜率\(\frac{\partial f(x)}{\partial x}, x=x_{k}\)出发做一条直线,并求出其表达式</li>
<li>3.求出该直线与x轴的交点\(x_{k+1}=x_{k}-\frac{f(x_{k})}{f{}’(x_{k})}\)</li>
<li>4.设置下一次迭代的点为\(x=x_{k+1}\)</li>
<li>5.重复2到4，直到斜率小于某个阈值\(\epsilon\)时停止迭代</li>
<li>6.迭代停止时\(x=x_{k+1}\)就是方程的近似解</li>
<li>迭代过程图示化如下：</li>
<li>图片引用自博客：<a href="https://www.cnblogs.com/lyr2015/p/9010532.html" target="_blank" rel="noopener">https://www.cnblogs.com/lyr2015/p/9010532.html</a>*

</li>
</ul>
<h5 id="迭代公式推导"><a href="#迭代公式推导" class="headerlink" title="迭代公式推导"></a>迭代公式推导</h5><ul>
<li>上述迭代流程中迭代公式\(x_{k+1}=x_{k}-\frac{f(x_{k})}{f{}’(x_{k})}\)的推导如下：


</li>
</ul>
<h4 id="最优化方法中的牛顿法"><a href="#最优化方法中的牛顿法" class="headerlink" title="最优化方法中的牛顿法"></a>最优化方法中的牛顿法</h4><ul>
<li>最优化方法的目标是学习函数\(f(\theta)\)的极小/大值(若\(f(\theta)\)是凸函数，则为最小/大值)</li>
<li>可以证明，目标函数(凸函数)取最优点(最大/小值)时，目标函数的导数\(\frac{\partial f(\theta)}{\partial \theta}=0, 其中\theta=\theta^{\star}\)</li>
<li>基于这个事实，在知道目标函数的导数\(f{}’(\theta)=\frac{\partial f(\theta)}{\partial \theta}\)的表达式后，我们可以列出方程\(f{}’(\theta)=0\)<ul>
<li>此时方程的解\(\theta^{\star}\)就是原始目标函数的最优解</li>
</ul>
</li>
<li>解上述方程\(\frac{\partial f(\theta)}{\partial \theta}=0\)时我们即可使用<strong>数学中的牛顿法</strong></li>
</ul>
<h5 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h5><ul>
<li>具体算法步骤与<strong>数学中的算法流程</strong>小节一致，不同的地方在于此时迭代公式变为\(\theta_{k+1}=\theta_{k}-\frac{f{}’(\theta_{k})}{f{}’{}’(\theta_{k})}\)</li>
<li>在具体的实现中，我们一般会直接推导出每一步的迭代公式，按照公式迭代即可得到最优解</li>
</ul>
<h4 id="迭代公式推导-1"><a href="#迭代公式推导-1" class="headerlink" title="迭代公式推导"></a>迭代公式推导</h4><ul>
<li>根据上述事实，此时我们的目标函数为\(f(\theta)\)，目标方程方程为\(f{}’(\theta)=0\)</li>
<li>此时迭代公式为$$\theta_{k+1}=\theta_{k}-\frac{f{}’(\theta_{k})}{f{}’{}’(\theta_{k})}$$</li>
<li>当参数数量不止一个时，\(\theta\)表示一个向量，此时迭代公式为$$\theta_{k+1}=\theta_{k}-H_{k}^{-1}\nabla_{\theta}f(\theta_{k})$$<ul>
<li>其中\(H_{k}^{-1}\)为海森矩阵\(H_{k}\)的逆, \(H_{k}=H(\theta_{k})\)</li>
<li>\(H_{k}^{-1}\)和\(\frac{1}{f{}’{}’(\theta_{k})}\)分别是多维和一维参数时目标函数的二阶导</li>
<li>海森矩阵是目标函数对参数的二阶导数，\(H(\theta)=\left [ \frac{\partial^{2}f}{\partial \theta^{i}\partial \theta^{j}} \right ]_{n\times n}\)</li>
</ul>
</li>
<li>推导步骤和上面的<strong>数学中的迭代公式</strong>推导一致</li>
</ul>
<hr>
<h3 id="牛顿法与梯度下降法的一种简单推导方法"><a href="#牛顿法与梯度下降法的一种简单推导方法" class="headerlink" title="牛顿法与梯度下降法的一种简单推导方法"></a>牛顿法与梯度下降法的一种简单推导方法</h3><ul>
<li>用泰勒展开来推导,详情待更新(参考&lt;&lt;百面&gt;&gt;P149)</li>
</ul>
<h4 id="迭代法的思想"><a href="#迭代法的思想" class="headerlink" title="迭代法的思想"></a>迭代法的思想</h4><ul>
<li>最小化目标函数<br>$$<br>\begin{align}<br>\delta_{t} &amp;= \arg\min_{\delta}L(\theta_{t} + \delta) \\<br>\theta_{t+1} &amp;= \theta_{t} + \delta_{t} \\<br>\end{align}<br>$$</li>
<li>注意,如果这里是max最大化目标函数的话,对应的是梯度上升法,此时后面推导的步骤中加入L2正则项时为了使得\(\left | \delta \right |_{2}^{2}\)的值最小,正则项的权重应该为负数,这样整体目标函数最大时正则项才能取得最小值,最终推导得到的结果也将变成梯度上升的表达式,(\(\alpha &gt; 0\)时对应的参数迭代公式变成加好而不是减号)</li>
<li>但是实际上最大化目标函数可以价格负号转变为最小化目标函数,所以其实掌握最小化目标函数的优化方法即可</li>
</ul>
<h4 id="一阶法——梯度下降法"><a href="#一阶法——梯度下降法" class="headerlink" title="一阶法——梯度下降法"></a>一阶法——梯度下降法</h4><ul>
<li>对目标函数在\(L(\theta_{t} + \delta)\)处做一阶泰勒展开<br>$$<br>\begin{align}<br>L(\theta_{t} + \delta) = L(\theta_{t}) + \nabla L(\theta_{t})^{T}\delta \\<br>\end{align}<br>$$</li>
<li>由于上式在\(\delta\)非常小时才成立，所以我们一般加上加入L2正则项（L2正则项可以保证\(\delta\)的值不会太大),为了保证\(\left | \delta \right |_{2}^{2}\)的值最小,要求\(\alpha &gt; 0\), 才能保证目标函数最小时正则项也对应最小 (梯度提升法中要求\(\alpha &lt; 0\)或者需要在正则项前面的系数加上负号,才能保证目标函数最大时正则项最小)<br>$$<br>\begin{align}<br>L(\theta_{t} + \delta) = L(\theta_{t}) + \nabla L(\theta_{t})^{T}\delta + \frac{1}{2\alpha}\left | \delta \right |_{2}^{2} \\<br>\end{align}<br>$$</li>
<li>直接对上式求导=0得<br>$$<br>\begin{align}<br>\delta_{t} &amp;= \arg\min_{\delta}L(\theta_{t} + \delta) \\<br>&amp;= \arg\min_{\delta}(L(\theta_{t}) + \nabla L(\theta_{t})^{T}\delta + \frac{1}{2\alpha}\left | \delta \right |_{2}^{2}) \\<br>&amp;= -\alpha \nabla L(\theta_{t})<br>\end{align}<br>$$</li>
<li>于是梯度下降法的更新表达式为<br>$$<br>\begin{align}<br>\theta_{t+1} &amp;= \theta_{t} + \delta_{t} \\<br>&amp;= \theta_{t} - \alpha \nabla L(\theta_{t})<br>\end{align}<br>$$</li>
</ul>
<h4 id="二阶法——牛顿迭代法"><a href="#二阶法——牛顿迭代法" class="headerlink" title="二阶法——牛顿迭代法"></a>二阶法——牛顿迭代法</h4><ul>
<li>对目标函数在\(L(\theta_{t} + \delta)\)处做二阶泰勒展开<br>$$<br>\begin{align}<br>L(\theta_{t} + \delta) = L(\theta_{t}) + \nabla L(\theta_{t})^{T}\delta + \frac{1}{2}\delta^{T}\nabla^{2}L(\theta_{t})\delta \\<br>\end{align}<br>$$</li>
<li>直接对上式求导=0得<br>$$<br>\begin{align}<br>\delta_{t} &amp;= \arg\min_{\delta}L(\theta_{t} + \delta) \\<br>&amp;= \arg\min_{\delta}(L(\theta_{t}) + \nabla L(\theta_{t})^{T}\delta + \frac{1}{2}\delta^{T}\nabla^{2}L(\theta_{t})\delta)  \\<br>&amp;= -\nabla^{2} L(\theta_{t})^{-1} \nabla L(\theta_{t}) \\<br>&amp;= -H(\theta_{t})^{-1}\nabla L(\theta_{t}) \\<br>\end{align}<br>$$</li>
<li>于是梯度下降法的更新表达式为<br>$$<br>\begin{align}<br>\theta_{t+1} &amp;= \theta_{t} + \delta_{t} \\<br>&amp;= \theta_{t} - \nabla^{2} L(\theta_{t})^{-1} \nabla L(\theta_{t})<br>\end{align}<br>$$</li>
</ul>
<hr>
<h3 id="比较梯度下降法与牛顿法"><a href="#比较梯度下降法与牛顿法" class="headerlink" title="比较梯度下降法与牛顿法"></a>比较梯度下降法与牛顿法</h3><ul>
<li>一般来说，牛顿法收敛速度快<ul>
<li>一种解释是梯度下降法用<strong>一次平面去拟合局部区域</strong>牛顿法是用<strong>二次曲面去拟合局部区域</strong>，所以后者能得到更好的拟合效果，迭代的方向也就越精确</li>
<li>另一种解释是梯度下降只能看到当前目标函数的下降方向，牛顿法可以看到当前当前目标函数的下降方向，同时还能看到一阶导数的变化趋势，所以拟合更快</li>
</ul>
</li>
<li>梯度下降法中有个超参数\(\alpha\)<ul>
<li>牛顿法中海森矩阵的逆在迭代过程中不断减小，可以起到逐步减小步长的效果(这里的步长与梯度下降中的\(\alpha\)步长不一样,梯度下降中的\(\alpha\)是超参数,表示每次向着负梯度移动的长度,而牛顿法中没有超参数)</li>
<li>为了防止震荡,这个步长\(\alpha\)也可以随着迭代次数不断减小</li>
</ul>
</li>
<li>梯度下降法可能造成震荡<ul>
<li>越接近最优值时，步长\(\alpha\)应该不断减小，否则会在最优值附近来回震荡</li>
<li>&lt;&lt;统计学习方法&gt;&gt;第一版附录A中,\(\alpha\)是不断变化的,能保证不会造成震荡</li>
</ul>
</li>
<li>牛顿法在多变量时需要求海森矩阵H的逆,比较复杂,时间和空间上有要求,不如梯度下降法来的容易</li>
</ul>
<hr>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><ul>
<li>解决牛顿法求取海森矩阵\(H\)的逆\(H^{-1}\)时比较耗时间的难题</li>
<li>用一个近似矩阵\(B\)代替海森矩阵\(H\)的逆\(H^{-1}\)</li>
<li>不同算法近似矩阵\(B\)的计算有差异</li>
<li>常见的拟牛顿法：BFGS,L-BFGS和OWL-QN等<ul>
<li>&lt;&lt;统计学习方法&gt;&gt;中讲解了三个拟牛顿算法: DFP,BFGS和Broyden类算法</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><em>归纳自&lt;&lt;统计学习方法&gt;&gt;</em></p>
<h4 id="相同迭代公式"><a href="#相同迭代公式" class="headerlink" title="相同迭代公式"></a>相同迭代公式</h4><ul>
<li>目标函数为\(f(\theta)\),最优化方法的终极目标是求\(f(\theta)\)的最大值点(或极大值点)</li>
<li>目标函数的一阶导数<ul>
<li>参数一维时为:\(f{}’(\theta)=\frac{\partial f(\theta)}{\partial \theta}\)</li>
<li>参数多维时为:\(\nabla f(\theta)\)</li>
</ul>
</li>
<li>目标函数的二阶导数<ul>
<li>参数一维时为:\(f{}’{}’(\theta)\)</li>
<li>参数多维时为:(海森矩阵)\(H(\theta)=\left [ \frac{\partial^{2}f}{\partial \theta^{i}\partial \theta^{j}} \right ]_{n\times n}\)</li>
</ul>
</li>
<li>梯度下降法,牛顿法和拟牛顿法的迭代参数均可表示为下面的迭代形式:<br>$$\theta_{k+1}=\theta_{k}+\lambda_{k}p_{k}$$</li>
<li>均通过一阶导数\(\left | \nabla f(\theta) \right |&lt;\epsilon\)为<strong>收敛判断条件</strong><ul>
<li>在&lt;&lt;统计学习方法&gt;&gt;中,梯度下降法还可以通过\(\left | \theta_{k+1}-\theta_{k+} \right |&lt;\epsilon\)或\(\left | f(\theta_{k+1})-f(\theta_{k+}) \right |&lt;\epsilon\)来判断收敛</li>
</ul>
</li>
</ul>
<h4 id="梯度下降法-1"><a href="#梯度下降法-1" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><ul>
<li>\(p_{k}=\nabla f(x)\) </li>
<li>\(\lambda_{k}\)是步长,满足<br>$$f(\theta_{k}+\lambda_{k}p_{k})=\min_{\lambda \geq 0}f(\theta_{k}+\lambda p_{k})$$</li>
</ul>
<h4 id="牛顿法-1"><a href="#牛顿法-1" class="headerlink" title="牛顿法"></a>牛顿法</h4><ul>
<li>\(p_{k}=-H_{k}^{-1}\nabla f(\theta)\)</li>
<li>\(\lambda_{k}=1\)是固定长度的,详情见<strong>数学中的牛顿法</strong>的推导,每次迭代到斜率与横轴的交点<br>$$\lambda_{k}=1$$</li>
</ul>
<h4 id="拟牛顿法-1"><a href="#拟牛顿法-1" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><ul>
<li>核心思想是找一个容易计算的近似矩阵(可以迭代计算,而不是每次重新计算逆矩阵的矩阵)替代原始牛顿法中的海森矩阵\(H\)(\(B\))或者海森矩阵的逆矩阵\(H^{-1}\)(\(G\))</li>
</ul>
<h5 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h5><ul>
<li>使用\(G_{k}\)逼近海森矩阵的逆矩阵\(H^{-1}\)</li>
<li>\(p_{k}=-G_{k}\nabla f(\theta)\)<ul>
<li>关于\(G_{k}\)的计算:<ul>
<li>初始化\(G(0)\)为正定对称矩阵</li>
<li>\(G_{k+1}=G_{k}+\frac{\delta_{k}\delta_{k}^{T}}{\delta_{k}^{T}y_{k}}-\frac{G_{k}y_{k}y_{k}^{T}G_{k}}{y_{k}^{T}G_{k}y_{k}}\)<ul>
<li>\(\delta_{k}=\theta_{k+1}-\theta_{k}\)</li>
<li>\(y_{k}=\nabla f(\theta_{k+1})-\nabla f(\theta_{k})\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>\(\lambda_{k}\)是步长,满足<br>$$f(\theta_{k}+\lambda_{k}p_{k})=\min_{\lambda \geq 0}f(\theta_{k}+\lambda p_{k})$$</li>
</ul>
<h5 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h5><ul>
<li>使用\(B_{k}\)逼近海森矩阵\(H_{k}\)</li>
<li>\(p_{k}=-B_{k}^{-1}\nabla f(\theta)\)<ul>
<li>关于\(B_{k}\)的计算:<ul>
<li>初始化\(B(0)\)为正定对称矩阵</li>
<li>\(B_{k+1}=B_{k}+\frac{y_{k}y_{k}^{T}}{y_{k}^{T}\delta_{k}}-\frac{B_{k}\delta_{k}\delta_{k}^{T}B_{k}}{\delta_{k}^{T}B_{k}\delta_{k}}\)<ul>
<li>\(\delta_{k}=\theta_{k+1}-\theta_{k}\)</li>
<li>\(y_{k}=\nabla f(\theta_{k+1})-\nabla f(\theta_{k})\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>\(\lambda_{k}\)是步长,满足<br>$$f(\theta_{k}+\lambda_{k}p_{k})=\min_{\lambda \geq 0}f(\theta_{k}+\lambda p_{k})$$</li>
</ul>
<h5 id="Broyden类算法-Broyden’s-algorithm"><a href="#Broyden类算法-Broyden’s-algorithm" class="headerlink" title="Broyden类算法(Broyden’s algorithm)"></a>Broyden类算法(Broyden’s algorithm)</h5><ul>
<li>使用\(G_{k}\)逼近海森矩阵的逆矩阵\(H^{-1}\)</li>
<li>在BFGS中取\(G^{BFGS}=B^{-1}\)</li>
<li>利用DFP和BFGS二者的<strong>线性组合</strong>选择合适的海森矩阵的逆矩阵\(H^{-1}\)的替代矩阵<br>$$G_{k+1}=\alpha G^{DFP}+(1-\alpha)G^{BFGS}$$</li>
<li>可以证明\(G\)此时是正定的</li>
<li>\(0\leq \alpha \leq 1\)</li>
<li>这样得到的一类拟牛顿法都称为Broyden类算法</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——训练集-验证集-测试集</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86.html</url>
    <content><![CDATA[<p><em>训练集(training set),验证集(validation set)和测试集(test set)</em></p>
<hr>
<h3 id="统计学习方法"><a href="#统计学习方法" class="headerlink" title="统计学习方法:"></a>统计学习方法:</h3><ul>
<li>训练集: 用于模型训练</li>
<li>验证集: 用于模型选择</li>
<li>测试集: 用于模型评估(原文:用于最终对学习方法的评估)</li>
</ul>
<hr>
<h3 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解:"></a>个人理解:</h3><ul>
<li>训练集: 用于模型拟合的数据样本</li>
<li>验证集: 用于模型的选择<ul>
<li>不参与训练过程</li>
<li>可用于超参数的调整</li>
<li>可用于判断模型是否过拟合(用于决策何时停止训练过程)</li>
<li>可多次使用</li>
</ul>
</li>
<li>测试集: 用于模型的评估 <ul>
<li>不参与训练过程</li>
<li>不可用于超参数的调整</li>
<li>只可以使用一次</li>
</ul>
</li>
</ul>
<hr>
<h3 id="一些场景说明"><a href="#一些场景说明" class="headerlink" title="一些场景说明"></a>一些场景说明</h3><h4 id="做kaggle题目时"><a href="#做kaggle题目时" class="headerlink" title="做kaggle题目时"></a>做kaggle题目时</h4><ul>
<li>不用测试集,因为数据集本来就少,测试集不能用于优化或者选择模型,只能用于评估</li>
<li>此时测试集可以理解为kaggle题目中未知的部分数据</li>
<li>此时从验证集中划分一部分验证集</li>
</ul>
<h4 id="写论文时"><a href="#写论文时" class="headerlink" title="写论文时"></a>写论文时</h4><ul>
<li>需要训练集用于训练,验证集用于模型选择</li>
<li>并且最终需要测试集用于评估模型和与其他人的模型对比</li>
</ul>
<p><strong>核心:验证集和测试集不能参与训练模型,测试集不能参与模型选择</strong><br><em>模型训练时不能让模型看见验证集</em><br><em>在模型确定前都不能让模型看见测试集</em></p>
<hr>
<h3 id="关于K折交叉验证法"><a href="#关于K折交叉验证法" class="headerlink" title="关于K折交叉验证法"></a>关于K折交叉验证法</h3><ul>
<li>统计学习方法中的描述: 交叉验证中每次划分为两部分,一部分用于训练模型,另一部分用于测试模型</li>
<li>理解:上面的两部分,一部分为训练集,另一部分为验证集,在这里面没有测试集</li>
</ul>
<hr>
<h3 id="关于网格搜索"><a href="#关于网格搜索" class="headerlink" title="关于网格搜索"></a>关于网格搜索</h3><ul>
<li>用来调超参数的一部分正好称为验证集</li>
<li>除了验证集外,加入写论文做实验需要和其他模型作比较的话需要使用测试集(和验证集以及训练集都不相交)</li>
</ul>
<hr>
<h3 id="关于数据划分的比重"><a href="#关于数据划分的比重" class="headerlink" title="关于数据划分的比重"></a>关于数据划分的比重</h3><ul>
<li>训练集：测试集 = 7：3</li>
<li>训练集：验证集：测试集 = 6：2：2</li>
<li>数据量非常多时（比如百万级别），验证集和测试集的比重可以适当缩小，因为验证集的主要目的是选择较好的模型，而测试集的目的是为了测试模型的效果，如果1万条数据足以验证模型的效果，则没必要非得给20万条数据</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——贝叶斯优化</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96.html</url>
    <content><![CDATA[<p><em>本文持续更新，主要主要介绍贝叶斯优化</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考链接：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/662879942" target="_blank" rel="noopener">一文详解贝叶斯优化（Bayesian Optimization）原理 - 下里巴程序员的文章 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/76269142" target="_blank" rel="noopener">贝叶斯优化/Bayesian Optimization - 代忠祥的文章 - 知乎</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="关于贝叶斯优化"><a href="#关于贝叶斯优化" class="headerlink" title="关于贝叶斯优化"></a>关于贝叶斯优化</h3><ul>
<li>贝叶斯优化（Bayesian Optimization，简称BO）主要用于解决超参数优化问题，目标是找到一组使得目标函数最大或者最小的超参数</li>
<li>BO的最大优势是可以使用非常少的步骤找到一个还不错的超参数组合，而且不需要知道目标函数（模型）的梯度</li>
<li>BO使用的场景特点：<ul>
<li>需要优化的函数（模型）是非常复杂的</li>
<li>需要优化的函数（模型）对指定的变量（一般指超参数）没有梯度信息（求不出梯度）</li>
</ul>
</li>
<li>BO不适合使用的场景包括：<ul>
<li>离散问题，即参数是离散的，则不适合使用BO</li>
<li>参数太多，即参数维度过高，此时不适合使用BO</li>
</ul>
</li>
<li>和BO的适用场景和进化算法（如CEM等）比较相似</li>
</ul>
<h3 id="贝叶斯优化的一般步骤"><a href="#贝叶斯优化的一般步骤" class="headerlink" title="贝叶斯优化的一般步骤"></a>贝叶斯优化的一般步骤</h3><ul>
<li>贝叶斯优化（Bayesian Optimization）算法是一个序列决策过程（Sequential Decision-making Problem），其核心框架是SMBO (Sequential Model-Based Optimization)，而贝叶斯优化狭义上特指代理模型为高斯过程回归模型的SMBO，SMBO的一般算法思路如下：<ul>
<li>第一步：初始化采样n组参数\(\{x_1,x_2,…,x_n\}\)</li>
<li>第二步：针对每组参数，观察目标函数（待优化模型）的值（这一步一般需要训练模型或者模拟环境仿真，是成本比较高的操作），将得到的结果组合生成数据集\(D=\{(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\}\)</li>
<li>第三步：根据数据集D拟合（训练）一个代理模型（Surrogate Model），该代理模型的功能是通过输入\(x_i\)预测输出\(y_i\)</li>
<li>第四步：根据采集函数（Acquisition Function，简称AC Function），选择一个最优参数\(x_i\)</li>
<li>第五步：观察目标函数（待优化模型）在参数\(x_i\)下的输出值\(y_i\)，并将\((x_i,y_i)\)添加到数据集D中</li>
<li>循环执行第三步到第五步，直到达到指定的迭代次数</li>
<li>最终：输出数据集D中最优的参数\(x^*\) </li>
</ul>
</li>
</ul>
<h3 id="代理模型"><a href="#代理模型" class="headerlink" title="代理模型"></a>代理模型</h3><ul>
<li>代理模型一般是非常容易计算的，或者有梯度的，给出任意的\(x_i\)能快速预估\(y_i\)的</li>
<li>常见的代理模型可以是：高斯过程回归、随机森林等等。其中，贝叶斯优化狭义上特指代理模型为高斯过程回归模型的SMBO。</li>
<li>高斯过程回归可参考：<ul>
<li><a href="https://www.zhihu.com/question/46631426/answer/1735470753" target="_blank" rel="noopener">如何通俗易懂地介绍 Gaussian Process？ - 石溪的回答 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/331591492" target="_blank" rel="noopener">快速入门高斯过程（Gaussian process）回归预测 - 摸鱼大王二百五的文章 - 知乎</a></li>
</ul>
</li>
</ul>
<h3 id="采集函数"><a href="#采集函数" class="headerlink" title="采集函数"></a>采集函数</h3><ul>
<li>通常做法是设计一个采集函数A，对每个采样点x进行打分，分数越高的点越值得被采样。</li>
<li>一般来说，采集函数需要满足下面的要求：<ul>
<li>在已有的采样点处采集函数的值更小，因为这些点已经被探索过，再在这些点处计算函数值对解决问题没有什么用</li>
<li>探索：在置信区间更宽（方差更大）的点处采集函数的值更大，因为这些点具有更大的不确定性，更值得探索</li>
<li>利用：对最大（小）化问题，在函数均值更大（小）的点处采集函数的值更大，因为均值是对该点处函数值的估计值，这些点更可能在极值点附近。</li>
</ul>
</li>
<li>常用的AC Function包括下面几种：<ul>
<li>Probability of Improvement：</li>
<li>Expected Improvement：</li>
<li>Entropy Search：</li>
<li>Upper Confidence Bound：</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——采样方法</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<p><em>本文介绍几种采样的方法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="计算机能做什么样的采样"><a href="#计算机能做什么样的采样" class="headerlink" title="计算机能做什么样的采样"></a>计算机能做什么样的采样</h3><h4 id="Uniform"><a href="#Uniform" class="headerlink" title="Uniform"></a>Uniform</h4><ul>
<li>本质上来讲,计算机只能实现对<strong>均匀分布</strong>(Uniform Distribution)的采样</li>
</ul>
<h4 id="numpy-random模块功能介绍"><a href="#numpy-random模块功能介绍" class="headerlink" title="numpy.random模块功能介绍"></a><code>numpy.random</code>模块功能介绍</h4><p><em><code>numpy.random</code>是用来实现随机数生成的库</em></p>
<ul>
<li>生成随机数<code>random</code></li>
<li>生成某个随机数的随机样本<code>random_sample</code></li>
<li>对序列做随机<code>shuffle</code>,<code>choice</code>等</li>
</ul>
<h4 id="numpy-random模块的采样"><a href="#numpy-random模块的采样" class="headerlink" title="numpy.random模块的采样"></a><code>numpy.random</code>模块的采样</h4><p><em><code>numpy.random</code>模块下实现了很多常见分布的采样函数(他们本质上都是计算机通过多次均匀分布采样实现的)</em></p>
<h5 id="单变量分布"><a href="#单变量分布" class="headerlink" title="单变量分布"></a>单变量分布</h5><ul>
<li><code>beta</code>:beta分布</li>
<li><code>binomial</code>:二项分布</li>
<li><code>chisquare</code>:卡方分布</li>
<li><code>exponential</code>:指数分布</li>
<li>还有更多…</li>
</ul>
<h5 id="多变量分布"><a href="#多变量分布" class="headerlink" title="多变量分布"></a>多变量分布</h5><ul>
<li><code>dirichlet</code> : Multivariate generalization of Beta distribution. 狄利克雷分布</li>
<li><code>multinomial</code>: Multivariate generalization of the binomial distribution. 多项分布</li>
<li><code>multivariate_normal</code>: Multivariate generalization of the normal distribution. 多变量正太(高斯)分布</li>
</ul>
<h5 id="标准分布"><a href="#标准分布" class="headerlink" title="标准分布"></a>标准分布</h5><ul>
<li><code>standard_cauchy</code>:      Standard Cauchy-Lorentz distribution.</li>
<li><code>standard_exponential</code>: Standard exponential distribution.</li>
<li><code>standard_gamma</code>:       Standard Gamma distribution.</li>
<li><code>standard_normal</code>:      Standard normal distribution.</li>
<li><code>standard_t</code>:           Standard Student’s t-distribution.</li>
</ul>
<hr>
<h3 id="复杂分布的采样方式"><a href="#复杂分布的采样方式" class="headerlink" title="复杂分布的采样方式"></a>复杂分布的采样方式</h3><p><em>在实践中,往往有很多复杂的分布,复杂到我们无法直接对他进行采样</em><br><em>有些时候我们甚至不知道目标函数的分布函数</em></p>
<h4 id="逆变换采样"><a href="#逆变换采样" class="headerlink" title="逆变换采样"></a>逆变换采样</h4><p><em>Inverse Transform Sampling</em></p>
<ul>
<li><p>目标函数: \(p(x)\)</p>
</li>
<li><p>相关补充: </p>
<ul>
<li>求函数的累积分布函数\(\Phi^{-1}(x) = \int_{- \infty}^{x}p(t)d_{t}\)</li>
<li>求累计分布函数的逆函数(反函数)</li>
</ul>
</li>
<li><p>采样步骤:</p>
<ul>
<li>均匀分布采样:\(u_{i} \sim U(0,1)\)</li>
<li>计算: \(x_{i} = \Phi^{-1}(u_{i})\), 其中\(\Phi^{-1}(\cdot)\)是\(p(x)\)的<strong>累积分布函数(CDF)的逆函数</strong></li>
<li>\(x_{i}\)服从\(p(x)\)分布</li>
</ul>
</li>
<li><p>优缺点:</p>
<ul>
<li>优点:<ul>
<li>仅需进行一个均匀分布采样即可</li>
</ul>
</li>
<li>缺点:<ul>
<li>需要求解累积分布函数的逆函数</li>
<li>累积分布函数的逆函数不一定容易求解,有些甚至无法求解</li>
</ul>
</li>
</ul>
</li>
<li><p>证明:</p>
<ul>
<li>示意图如下:  <img src="/Notes/ML/ML——采样方法/inverse_transform_sampling.png"></li>
<li>图中纵轴就是均匀分布采样的结果,然后丛纵轴对应到的累计分布函数\(\Phi(x)\)的曲线上概率越大的地方实际上也就是累积分布函数的导数(原始分布函数\(p(x)\))最大的地方</li>
<li>这个对应过程等价于我们将\(x\)轴和\(y\)轴互换,也就是求累积分布函数的逆函数即可</li>
</ul>
</li>
</ul>
<h4 id="拒绝采样"><a href="#拒绝采样" class="headerlink" title="拒绝采样"></a>拒绝采样</h4><p><em>别名: Accept-Reject Sampling, <strong>接受-拒绝采样</strong></em></p>
<ul>
<li><p>目标函数: \(p(x)\)</p>
</li>
<li><p>相关补充: </p>
<ul>
<li><strong>(参考分布寻找)</strong>: 寻找一个容易采样的分布\(q(x)\),满足\(p(x) \leq M\cdot q(x)\)</li>
<li>一般选择正太分布等</li>
<li>\(M &gt; 1\),从后面的证明可以知道, M越小越好</li>
</ul>
</li>
<li><p>采样步骤:</p>
<ul>
<li>参考分布采样:\(x_{i} \sim q(x)\)</li>
<li>均匀分布采样:\(u_{i} \sim U(0,1)\)</li>
<li>判断是否接受: 如果\(u_{i} &lt; \frac{p(x_{i})}{M\cdot q(x_{i})}\),则接受采样,否则拒绝本次采样(丢弃本次采样的\(x_{i}\))</li>
<li>\(x_{i}\)服从\(p(x)\)分布(不包括被拒绝的样本)</li>
</ul>
</li>
<li><p>证明: </p>
<ul>
<li>由采样步骤得到,最终得到的分布服从$$q(x)\cdot \frac{p(x)}{M\cdot q(x)} = \frac{1}{M}p(x)$$</li>
<li>对上述分布进行归一化即可得到采样的样本服从\(p(x)\)</li>
<li><em>从\(\frac{1}{M}p(x)\)这里也可以看出来采样的效率由M值的大小决定,M越小,采样效率越高</em></li>
</ul>
</li>
<li><p>优缺点:</p>
<ul>
<li>优点:<ul>
<li>复杂分布变成简单分布采样+一个均匀分布,有时候甚至可以将参考分布也使用均匀分布</li>
</ul>
</li>
<li>缺点:<ul>
<li>参考分布\(q(x)\)的选择很难的</li>
<li>不合适的参考分布可能导致采样效率低下</li>
</ul>
</li>
</ul>
</li>
<li><p>解决\(q(x)\)难以寻找的一种解决方案: <strong>自适应拒绝采样</strong>(Adaptive Rejection Sampling)</p>
<ul>
<li>只适用于目标函数为凸函数</li>
<li>使用分段线性函数来覆盖目标函数</li>
<li>下面是示意图片(图片来源: <a href="https://www.jianshu.com/p/3fb6f4d39c60" target="_blank" rel="noopener">https://www.jianshu.com/p/3fb6f4d39c60</a>)<img src="/Notes/ML/ML——采样方法/adaptive-rejection_sampling.png">

</li>
</ul>
</li>
</ul>
<h4 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h4><p><em>Importance Sampling</em></p>
<ul>
<li><p>重要性采样与前面两者不同,重要性采样解决的问题是在求一个函数的关于原始分布的期望时</p>
</li>
<li><p>目标定义: \(E_{x\sim p(x)}[f(x)] = \int_{x}f(x)p(x)d_{x}\)</p>
</li>
<li><p>直接对\(p(x)\)采样可能存在的两个问题:</p>
<ul>
<li>\(p(x)\)可能难以采样</li>
<li>\(p(x)\)采样到的样本大多都在\(f(x)\)比较小的地方,即\(f(x)\)与\(p(x)\)的差别太大导致有限次采样无法正确评估原始期望,采样次数不够大的话偏差可能很大<ul>
<li>这里一种解决方案是采样足够多的次数,多花点时间保证所有样本量足够,降低偏差</li>
</ul>
</li>
</ul>
</li>
<li><p>解决方案:</p>
<ul>
<li><p>引入一个容易采样的参考分布\(q(x)\),满足<br>$$<br>\begin{align}<br>E_{x\sim p(x)}[f(x)] &amp;= \int_{x}f(x)p(x)d_{x} \\<br>&amp;= \int_{x}f(x)\frac{p(x)}{q(x)}q(x)d_{x} \\<br>&amp;= \int_{x}f(x)w(x)q(x)d_{x} \\<br>\end{align}<br>$$</p>
</li>
<li><p>其中\(w(x) = \frac{p(x)}{q(x)}\)称为样本\(x\)的重要性权重(Importance Weight),不同样本的重要性权重不同</p>
</li>
<li><p>与直接从\(p(x)\)中采样相比,相同采样次数,最终得到期望偏差会更小,因为\(p(x)\)会<strong>增大在\(f(x)\)大但是\(p(x)\)极小处</strong>的样本被采样的概率,然后调低这个样本的权重</p>
</li>
<li><p>示意图片:</p>
<img src="/Notes/ML/ML——采样方法/importance_sampling.jpg">

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>逆采样和拒绝采样</strong>都是在通过简单分布采样来采样原始分布的样本,最终的样本就是服从原始分布的样本\(x_{i}\sim p(x)\)</li>
<li><strong>重要性采样</strong>本质上是通过简单分布来采样和重要性权重来估计某个函数在原始分布上的期望\(E_{x\sim p(x)}[f(x)] = \int_{x}f(x)p(x)d_{x}\)</li>
<li>对于高维空间中的随机向量,拒绝采样和重要性采样经常难以找到合适的参考分布,容易导致采样效率低下(样本的接受概率太小或者重要性权重太低),此时可以考虑马尔科夫蒙特卡洛采样法(MCMC),MCMC中常见的有两种,MH(Metropolis-Hastings)采样法和Gibbs采样法,关于MCMC详情可参考我的博客<a href="/Notes/ML/ML%E2%80%94%E2%80%94MCMC%E9%87%87%E6%A0%B7.html">ML——MCMC采样</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>RS——FMM模型</title>
    <url>/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94FMM%E6%A8%A1%E5%9E%8B.html</url>
    <content><![CDATA[<p><em>本文主要介绍FFM(FFM, Field-aware Factorization Machine)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="FFM模型"><a href="#FFM模型" class="headerlink" title="FFM模型"></a>FFM模型</h3><ul>
<li>FFM最初概念来自Yu-Chin Juan（阮毓钦，毕业于中国台湾大学，现在美国Criteo工作）与其比赛队员,他们借鉴了Michael Jahrer的论文中的field概念提出了FM的升级版模型</li>
</ul>
<h4 id="Field的概念"><a href="#Field的概念" class="headerlink" title="Field的概念"></a>Field的概念</h4><ul>
<li>FFM把相同性质的特征归于同一个Field,<ul>
<li>比如“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中</li>
</ul>
</li>
<li>简单来说,就是同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等</li>
</ul>
<h4 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h4><ul>
<li>FM 对每个特征 \(x_i\) 学习一个 \(k\) 维隐向量\(v_i\), 二次项参数数量为\(nk\)</li>
<li>FFM 对每个特征 \(x_i\) 和每个域(field) \(f_j\) 学习一个 \(k\) 维隐向量\(v_{i,f_{j}}\), 二次项参数数量为\(nfk\)<ul>
<li>假设样本的特征有 \(n\) 个</li>
<li>假设filed有 \(f\) 个</li>
</ul>
</li>
<li>建模方程<br>$$y(x) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle x_i x_j \label{eq:ffm}\tag{4}$$<ul>
<li>值得注意的是,上面的公式中,\(\mathbf{v}_{i, f_j}\)的第二个下标是 \(f_j\) ,不是 \(f_i\) ,表示的是, 同一个特征\(x_{i}\)在对不同的域(Field) \(f_j\) 中的特征 \(x_j\) 组合时,FFM考虑到 \(x_j\)的域不同,应该用不同的组合方式,所以使用不同的隐向量</li>
<li>FM中 \(x_i\) 的隐向量为 \(v_i\)</li>
<li>FFM中 \(x_i\) 的隐向量有多个,确切的说是隐矩阵, 对每个不同的域Field(包括\(x_i\)自身所在的域), 都有一个隐向量 \(v_{i,f_{j}}\) , 和不同类型的特征组合时,我们选择他们对应域的隐变量与之相乘</li>
</ul>
</li>
<li>当前模型的二次项一共有\(\frac{n(n-1)}{2}\)项, 计算复杂度为\(O(n^2)\) 与FM化简前相同, FFM这里不能化简, 所以训练和预测复杂度计算复杂度为 \(O(n^2)\)</li>
</ul>
<h4 id="FFM需要关注的细节"><a href="#FFM需要关注的细节" class="headerlink" title="FFM需要关注的细节"></a>FFM需要关注的细节</h4><ul>
<li>样本归一化, FFM默认进行样本归一化, 有个参数<code>pa.norm</code>设置为真即可;若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误<br>  *　因此，样本层面的数据是推荐进行归一化的<ul>
<li>[待更新],样本归一化的具体操作,样本归一化的作用是什么?</li>
</ul>
</li>
<li>特征归一化，这里由于样本归一化后categorical的特征会变得非常小</li>
<li>省略零值特征, 从建模方程可以看出, 零值特征对FFM模型完全没有贡献,包含零值特征的一切组合均为零</li>
</ul>
]]></content>
      <tags>
        <tag>RS</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>RS——FM-因子分解机</title>
    <url>/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94FM-%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA.html</url>
    <content><![CDATA[<p><em>本文主要介绍因子分解机(FM, Factorization Machine)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="FM模型"><a href="#FM模型" class="headerlink" title="FM模型"></a>FM模型</h3><ul>
<li>最早于2010年提出,目标是<strong>解决稀疏特征下的特征组合问题</strong></li>
</ul>
<h4 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h4><ul>
<li>假设训练数据为:\(\{(x, y)\}\)<ul>
<li>\(x\)特征维度为n(One-Hot编码后的维度), 初始时维度比较小,但是特征中含有特殊的字符串或者对象类型等,我们使用One-Hot编码来拓展每个特殊特征(如果一个特征有m个可能的取值,那么One-Hot编码下每个特征将被扩展为m个维度)<ul>
<li>由于用One-Hot来编码,所以实际上特征是非常稀疏的(某些特征很多样本都为0,只有少数的样本为1)</li>
</ul>
</li>
<li>\(y_{i}\)是样本的标签,表示是否点击(Clicked?), 1表示点击,0表示未点击</li>
</ul>
</li>
<li>一般模型建模<br>$$y(x) = w_0+ \sum_{i=1}^n w_i x_i \label{eq:poly}\tag{1}$$<ul>
<li>未挖掘到特征之间的关联关系,<ul>
<li>如“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响</li>
<li>如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等</li>
</ul>
</li>
</ul>
</li>
<li>FM建模<br>$$ y(x) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n w_{ij} x_i x_j $$<ul>
<li>n是样本的特征数量</li>
<li>\(x_{i}\)是样本的<strong>第\(i\)个特征</strong>,不是第\(i\)个样本</li>
<li>\(w_{0}, w_{i}, w_{ij}\)都是模型参数</li>
<li>显然模型组合特征的参数数量为\(\frac{n(n-1)}{2}\),且任意两个参数独立</li>
</ul>
</li>
<li>存在问题: 在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的<ul>
<li>原因: 每个参数\(w_{ij}\) 的训练需要大量\(x_{i}\) 和\(x_{j}\) 都非零的样本；由于样本数据本来就比较稀疏，满足“\(x_{i}\)和\(x_{j}\)都非零”的样本将会非常少。训练样本的不足，很容易导致参数\(w_{ij}\)不准确，最终将严重影响模型的性能</li>
</ul>
</li>
<li>解决问题的灵感<ul>
<li>基于模型的协同过滤中,一个User-Item的评分(rating)矩阵可以分解为User和Item两个矩阵,每个用户和商品都可以用一个隐向量表示<img src="/Notes/RecommenderSystem/RS——FM-因子分解机/CF-Matrix.png"></li>
</ul>
</li>
<li>FM解决问题的方法:<ul>
<li>用一个对称矩阵\(W\)代表所有二次项参数\(w_{ij}\),矩阵的两边对称的是参数,中间填充正实数</li>
<li>矩阵\(W\)可分解为<br>$$W=V^{T}V$$<ul>
<li>\(V\)的第 \(j\) 列就是第 \(j\) 维特征的隐向量</li>
<li>\(V\)是\(k x n\)维的向量(\(k &lt;&lt; n\))</li>
<li>此时有<br>$$w_{ij} = \boldsymbol{v}_i^T \boldsymbol{v}_j$$<br>$$ y(x) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \boldsymbol{v}_i^T \boldsymbol{v}_j x_{i} x_{j} $$</li>
<li>\(v_{i}\cdot v_{j}\)是两个隐向量的内积</li>
<li>此时\(w_{ij}\)和\(w_{mj}\)之间不在是独立的,他们有相同的内积项\(v_{j}\),这意味着所有包含“\(x_{i}\)的非零组合特征”(存在某个\(j\neq i\)，使得 \(x_{i}x_{j}\neq 0\))的样本都可以用来学习隐向量\(v_{i}\)(从而学习到\(w_{ij}\)), 这很大程度上<strong>避免了数据稀疏性造成的影响</strong><ul>
<li>问题: 以前不能用来学习吗?</li>
<li>回答: 以前的时候 \(w_{ij}\) 参数只能靠 \(x_{i}, x_{j}\) 均为1的样本 \(x\) 来训练,现在 \(w_{ij}\) 能由 \(v_{j}, v_{j}\) 生成,而 \(x_{i}\) 不为0的所有 \(x\) 都可以用来训练 \(v_{i}\),(当然,这里还需要存在某个 \(j\neq i\) ，使得 \(x_{i}x_{j}\neq 0\) , 只要当前样本不是只有\(x_{i}\)这个维度为1,这个条件肯定是成立的)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>当前的式子运算复杂度是\(O(kn^2)\),这意味着我们训练和预测时计算\(y(x)\)的值都需要\(O(n^2)\)的时间</li>
<li>考虑到上面的公式主要是复杂在二次项的计算,我们考虑对二次项进行化简<!-- 使用\boldsymbol{v}这个才对，但是使用这个会出现显示错误，待查
$$
\sum_{i=1}^{n-1}\sum_{j=i+1}^n(\boldsymbol{v}_i^T \boldsymbol{v}_j)x_ix_j = \frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n(\boldsymbol{v}_i^T \boldsymbol{v}_j)x_ix_j-\sum_{i=1}^n(\boldsymbol{v}_i^T \boldsymbol{v}_i)x_ix_i\right)
$$
-->

</li>
</ul>
<p>$$<br>\sum_{i=1}^{n-1}\sum_{j=i+1}^n(v_i^T v_j)x_ix_j = \frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n(v_i^T v_j)x_ix_j-\sum_{i=1}^n(v_i^T v_i)x_ix_i\right)<br>$$</p>
<p>$$<br>\begin{align}<br> &amp;=\frac{1}{2}\left(\sum_{i=1}^n\sum_{j=1}^n\sum_{l=1}^kv_{il}v_{jl}x_ix_j-\sum_{i=1}^n\sum_{l=1}^k v_{il}^2x_i^2\right)\\<br> &amp;=\frac{1}{2}\sum_{l=1}^k\left(\sum_{i=1}^n(v_{il}x_i)\sum_{j=1}^n(v_{jl}x_j)-\sum_{i=1}^nv_{il}^2x_i^2\right)\\<br> &amp;=\frac{1}{2}\sum_{l=1}^k\left(\left(\sum_{i=1}^n(v_{il}x_i)\right)^2-\sum_{i=1}^n (v_{il}x_i)^2\right)\\<br> \end{align}<br> $$</p>
<ul>
<li>上式中：<ul>
<li>现在二次项的复杂度是\(O(nk)\),最终模型的计算复杂度也是\(O(nk)\),注意这里是计算复杂度,不是参数数量(虽然FM模型的二次项参数数量碰巧也是\(nk\)个)</li>
<li>意味着我们可以在<strong>线性时间</strong>内对<strong>FM模型</strong>进行<strong>训练</strong>和<strong>预测</strong>,是非常高效的</li>
<li>\(v_i, v_j\)分别表示\(\boldsymbol{v}_i, \boldsymbol{v}_j\)</li>
</ul>
</li>
</ul>
<h4 id="参数数量"><a href="#参数数量" class="headerlink" title="参数数量"></a>参数数量</h4><ul>
<li>整体模型的参数一共是\(1+n+nk\)个<ul>
<li>偏执项一个\(w_{0}\)</li>
<li>一次项 \(n\) 个\((w_{1},\dots,w_{n})\)</li>
<li>二次项 \(nk\) 个\(v_{ij}\),其中\(i=1,2,\dots,n\), \(j=1,2,\dots,k\)</li>
</ul>
</li>
</ul>
<h4 id="和其他模型的对比"><a href="#和其他模型的对比" class="headerlink" title="和其他模型的对比"></a>和其他模型的对比</h4><ul>
<li>FM较为灵活,通过一些合适的特征变换,可以用来模拟<strong>二阶多项式核</strong>的(<strong>SVM</strong>,<strong>MF模型</strong>,<strong>SVD++模型</strong>)等</li>
<li>相比SVM的二阶多项式而言,FM在样本稀疏的情况下有优势?什么优势?[待更新],<ul>
<li>能想到的其中一个优势是FM训练/预测是线性复杂度,而二阶多项式核SVM需要计算核矩阵[待更新,关于核矩阵的理解?],所以复杂度是\(O(n^2)\)</li>
</ul>
</li>
<li>MF相当于只有 \(u,i\) 两类特征的FM模型[待更新:MF的详细推导]<ul>
<li>而FM模型中我们还可以加入任意多的特征,比如user的历史购买平均值,item的历史购买平均值等,但是MF只能局限在两类特征中</li>
<li>为什么MF相当于只有 \(u,i\) 两类特征的FM模型?<ul>
<li>证明,将MF中的每一项评分(rating)改写为:<br>$$r_{ui} \sim \beta_{u} + \gamma_i + x_u^Ty_i$$</li>
<li>显然可得结论</li>
</ul>
</li>
</ul>
</li>
<li>SVD++与MF类似,都是矩阵分解方法,在特征的扩展性上也不如FM模型[待更新: SVD++的推导和理解]</li>
<li>FFM是FM的改进模型,加入了Field的概念,参考<a href="/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94FMM%E6%A8%A1%E5%9E%8B.html">RS——FMM模型</a></li>
</ul>
]]></content>
      <tags>
        <tag>RS</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>RS——WCE-YouTube推荐论文</title>
    <url>/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94WCE-YouTube%E6%8E%A8%E8%8D%90%E8%AE%BA%E6%96%87.html</url>
    <content><![CDATA[<p><em>本文主要介绍WCE</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><em>原始论文：<a href="https://github.com/wzhe06/Reco-papers/blob/master/Industry%20Recommender%20System/[Youtube]%20Deep%20Neural%20Networks%20for%20YouTube%20Recommendations%20(Youtube%202016).pdf" target="_blank" rel="noopener">[Youtube] Deep Neural Networks for YouTube Recommendations (Youtube 2016)</a></em></p>
<hr>
<h3 id="WCE"><a href="#WCE" class="headerlink" title="WCE"></a>WCE</h3><ul>
<li>Weighted Cross Entropy，加权交叉熵，也叫做Weighted LR，Weighted Logistic Regression</li>
<li>用于解决回归问题<ul>
<li>主要是存在大量负样本（值为0）的回归问题</li>
<li>比如视频浏览时长问题（点击率就比较低）</li>
</ul>
</li>
<li>训练时使用损失函数：<br>$$<br>loss = \sum_i w_i y_ilog(p_i) + (1-y_i)log(1-p_i)<br>$$<ul>
<li>其中 \(p_i = \frac{1}{1+e^{-\theta^{T}\boldsymbol{x}}}\)</li>
<li>\(w_i\) = 回归值（如观看时长）</li>
<li>\(y_i\) = 是否为正值（即是否点击，未点击表示观看时长为0，视为负样本）</li>
<li>对任意样本，我们真实想要的预估目标是一个视频被点击且观看的概率\(pred = wp\)</li>
</ul>
</li>
<li>serving时使用下面的定义来表示回归值：<br>$$<br>pred = e^{\theta^{T}\boldsymbol{x}}<br>$$<ul>
<li>对于原始的CE损失函数，有\(Odds = \frac{p}{1-p} = e^{\theta^{T}\boldsymbol{x}}\)（补充：Odds表示样本为正的概率除以样本为负的概率，\(log(Odds) = \theta^{T}\boldsymbol{x}\)）就是logit</li>
<li>当前损失函数下，正负样本的比例（或权重）发生了变化，实际上\(Odds = \frac{p}{1-p} = e^{\theta^{T}\boldsymbol{x}}\)表示的值不再是原始样本中正负样本的比例，而是带权重的比例，详情看后续的证明</li>
</ul>
</li>
<li>可以证明上面的方法会造成预估值有偏</li>
</ul>
<h3 id="WCE改进"><a href="#WCE改进" class="headerlink" title="WCE改进"></a>WCE改进</h3><ul>
<li>改进后的损失函数<br>$$<br>loss = \sum_i w_i y_ilog(p_i) + log(1-p_i)<br>$$</li>
<li>改进前方案是有偏的，修改为上面的损失函数后，\(pred = Odds = e^{\theta^{T}\boldsymbol{x}}\)是无偏的</li>
<li>证明：<ul>
<li>假设在原始的CE损失函数下，正负样本的比例为A:B，此时有\(p = \frac{A}{A+B}\)【这里只是假设训练时遇到特征值完全相同的多个样本（有正有负），模型在遇到serving时遇到同一个特征值样本时，应该预估样本为正的概率为多少？】<ul>
<li>原始CE下，样本为正的概率就是正样本数/总样本数</li>
</ul>
</li>
<li>那么在上述加权的损失函数下，相当于正负样本的比例为\(wA:B+A\)，此时有\(p’ = \frac{wA}{wA+B+A}\)<ul>
<li>因为权重被修改了，可以证明<strong>样本不变，增加权重</strong>等价于<strong>权重不变，增加样本（重复采样）</strong></li>
<li>\(wA:B+A\)的原因是因为正样本被加了\(w\)倍的权重，而负样本则被增加了A个（原始的CE函数中正样本不会累加\(log(1-p)\)作为损失，但改进后的WCE会</li>
</ul>
</li>
<li>我们真实想要的预估值是：\(pred = wp = w * \frac{A}{A+B}\)<ul>
<li>可以表述为样本为正的概率乘以样本为正时的值（用户点击视频的概率*用户点击视频后观看的概率）</li>
</ul>
</li>
<li>经推导有：<br>$$<br>pred = e^{\theta^{T}\boldsymbol{x}} = \frac{p’}{1-p’} = \frac{\frac{wA}{wA+B+A}}{1-\frac{wA}{wA+B+A}} = w * \frac{A}{A+B} = wp<br>$$<ul>
<li>注意，我们需要的是\(pred = wp\)而不是\(pred = wp’\)<ul>
<li>因为\(p’\)是被我们修改权重后得到的模型输出(均值)</li>
</ul>
</li>
<li>真实serving时，模型的输出值\(p’\)是不用的，只使用\(pred = e^{\theta^{T}\boldsymbol{x}}\)就可以了</li>
<li>其他：<ul>
<li>对于原始CE，有：<br>  $$<br>  pred = e^{\theta^{T}\boldsymbol{x}} = \frac{p}{1-p} = \frac{\frac{A}{A+B}}{1-\frac{A}{A+B}} = \frac{A}{B} = \frac{p}{1-p}<br>  $$</li>
<li>对于YouTube的WCE，有：<br>  $$<br>  pred = e^{\theta^{T}\boldsymbol{x}} = \frac{p’’}{1-p’’} = \frac{\frac{wA}{wA+B}}{1-\frac{wA}{wA+B}} = w * \frac{A}{B} \approx w * \frac{A}{A+B} = wp<br>  $$<ul>
<li><strong>约等于符号成立的前提是正样本占比特别少</strong>，此时\(\frac{A}{B} \approx \frac{A}{A+B}\)</li>
<li>也就是说，在正样本占比特别少时，使用YouTube的WCE也是没问题的，但是为了保证无偏，建议使用改进后的WCE</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="扩展问题"><a href="#扩展问题" class="headerlink" title="扩展问题"></a>扩展问题</h3><ul>
<li>在面对回归问题是，WCE相对MSE真的有提升吗？</li>
</ul>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>WCE也可以用于分类问题中，目的是让模型更关注某些特殊样本</li>
</ul>
<h3 id="其他参考链接"><a href="#其他参考链接" class="headerlink" title="其他参考链接"></a>其他参考链接</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/61827629" target="_blank" rel="noopener">揭开YouTube深度推荐系统模型Serving之谜</a></li>
</ul>
]]></content>
      <tags>
        <tag>RS</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>RS——推荐系统综述</title>
    <url>/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ul>
<li>参考博客:<ul>
<li><a href="http://xtf615.com/2018/05/03/recommender-system-survey/" target="_blank" rel="noopener">http://xtf615.com/2018/05/03/recommender-system-survey/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27502172" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27502172</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="推荐系统分类-基于推荐依据分类"><a href="#推荐系统分类-基于推荐依据分类" class="headerlink" title="推荐系统分类(基于推荐依据分类)"></a>推荐系统分类(基于推荐依据分类)</h3><img src="/Notes/RecommenderSystem/RS——推荐系统综述/RS-Overview.jpg">
<p>[待更新]</p>
<h4 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h4><p><em>Content-based recommenders</em></p>
<ul>
<li><strong>推荐和用户曾经喜欢的商品相似的商品</strong></li>
<li><strong>主要是基于商品属性信息和用户画像信息的对比</strong></li>
<li>核心问题是<strong>如何刻画商品属性和用户画像以及效用的度量</strong></li>
<li>使用一个<strong>效用函数</strong>(utility function)来评价特定用户\(c \in C\)对特定项目\(s_{c}’ \in S\)的评分, \(UserProfit(c), ItemProfit(s)\)分别表示用户和商品的收益函数(Profit Function)[存疑: 待更新]<br>$$u(c,s) = score(UserProfit(c), ItemProfit(s))$$<ul>
<li>基于启发式的方法(Heuristic-based method): <ul>
<li>特征构建: 基于关键字提取等方法,使用TF-IDF等指标提取关键字作为特征</li>
<li>效用的度量: 使用启发式cosine等相似性指标, 衡量商品特征和用户画像的相似性,相似性越高,效用越大</li>
</ul>
</li>
<li>基于机器学习的方法(Machine learning-based mehod):<ul>
<li>特征构建: 使用机器学习算法来构建用户和商品的维度特征,例如建模商品属于某个类别,得到商品的刻画属性</li>
<li>效用的度量: 直接使用机器学习算法拟合效用函数</li>
</ul>
</li>
</ul>
</li>
<li>对于任意的用户\(c \in C\),我们可以通过选择商品\(s_{c}’ \in S\)来得到最大化的效用函数<br>$$\forall c \in C, s_{c}’ = \arg\max_{s \in S} u(c,s)$$</li>
</ul>
<h4 id="基于协同过滤的推荐"><a href="#基于协同过滤的推荐" class="headerlink" title="基于协同过滤的推荐"></a>基于协同过滤的推荐</h4><p><em>Collaborative filtering recommenders</em></p>
<h5 id="基于内存的推荐"><a href="#基于内存的推荐" class="headerlink" title="基于内存的推荐"></a>基于内存的推荐</h5><ul>
<li>直接对User-Item矩阵进行研究<ul>
<li>User-based CF: 推荐给特定用户列表中还没有发生过行为、而在<strong>相似用户列表中产生过行为</strong>的高频商品</li>
<li>Item-based CF: 推荐给特定用户列表中还没有发生过行为、并且和<strong>当前用户已经发生过行为的商品相似的商品</strong></li>
</ul>
</li>
</ul>
<h5 id="基于模型的推荐"><a href="#基于模型的推荐" class="headerlink" title="基于模型的推荐"></a>基于模型的推荐</h5><h6 id="损失函数-正则项-Loss-Function"><a href="#损失函数-正则项-Loss-Function" class="headerlink" title="损失函数+正则项(Loss Function)"></a>损失函数+正则项(Loss Function)</h6><h6 id="神经网络-层-Neural-Network"><a href="#神经网络-层-Neural-Network" class="headerlink" title="神经网络+层(Neural Network)"></a>神经网络+层(Neural Network)</h6><h6 id="图模型-圈-Graph-Model"><a href="#图模型-圈-Graph-Model" class="headerlink" title="图模型+圈(Graph Model)"></a>图模型+圈(Graph Model)</h6><h6 id="基于矩阵分解的推荐"><a href="#基于矩阵分解的推荐" class="headerlink" title="基于矩阵分解的推荐"></a>基于矩阵分解的推荐</h6><ul>
<li>Traditional SVD<ul>
<li>传统的SVD分解<br>$$R_{m\times n} = U_{m \times k} \Sigma_{k \times k} V_{k \times n}^T$$</li>
<li>缺点:<ul>
<li>普通SVD分解时矩阵必须是稠密的,即每个位置都必须有值</li>
<li>如果矩阵是稀疏的,有空值,那么需要先用均值或者其他统计学方法来填充矩阵</li>
<li>计算复杂度高,空间消耗大</li>
</ul>
</li>
</ul>
</li>
<li>FunkSVD<ul>
<li>分解为两个低秩矩阵而不是三个矩阵<br>$$\hat{r}_{u,i} = q_i^T p_u$$<ul>
<li>\(\hat{r}_{u,i}\)指用户对商品的评分</li>
<li>\(q_{i}\)是商品\(i\)在隐空间的隐向量表示</li>
<li>\(p_{u}\)是用户\(u\)在隐空间的隐向量表示</li>
</ul>
</li>
<li>不使用传统的矩阵分解方式,定一个损失函数(针对\(\hat{r}_{u,i}\)未缺失的地方,\(\hat{r}_{u,i}\)缺失的地方训练时不用管),然后用梯度下降法进行参数优化<ul>
<li>优化问题(最小化损失函数)定义如下<br>$$q^{\star}, p^{\star} = \min_{q, p} \sum_{(u,i) \in R_{train}} (r_{u,i} - q_i^T p_u)^2 + \lambda (||q_i||^2 + ||p_u||^2 )$$<ul>
<li>正则项是两个参数的L2正则</li>
<li>\(R_{train}\)是评分矩阵中<strong>可观测的数据对</strong>构成的集合, 缺失值不参与训练, 缺失值是需要我们最终预测的</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>BiasSVD<ul>
<li>BiasSVD是FunkSVD诸多变形版本的一个相对成功的方法</li>
<li>带有偏执项的SVD分解</li>
<li>基于假设: <ul>
<li>关于用户的假设: 天生存在偏好,有的喜欢给商品好评,有的喜欢给商品差评<ul>
<li>这些用户的固有属性与商品无关</li>
</ul>
</li>
<li>关于商品的假设: 天生存在优劣,有的容易被人给好评,有的容易被人给差评<ul>
<li>这些商品的固有属性与用户无关</li>
</ul>
</li>
</ul>
</li>
<li>优化问题(最小化损失函数)定义<br>$$<br>\begin{align}<br>&amp;\hat{r}_{ui} = \mu + b_u + b_i + q_i^Tp_u \\<br>&amp;min \sum_{r_{ui} \in R_{train}} \left(r_{ui} - \hat{r}_{ui} \right)^2 + \lambda\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\right)<br>\end{align}<br>$$    </li>
<li>梯度下降更新公式<br>$$<br>\begin{align}<br>\begin{split}b_u &amp;\leftarrow b_u &amp;+ \gamma (e_{ui} - \lambda b_u) \\<br>b_i &amp;\leftarrow b_i &amp;+ \gamma (e_{ui} - \lambda b_i)\\<br>p_u &amp;\leftarrow p_u &amp;+ \gamma (e_{ui} \cdot q_i - \lambda p_u)\\<br>q_i &amp;\leftarrow q_i &amp;+ \gamma (e_{ui} \cdot p_u - \lambda q_i)\end{split}<br>\end{align}<br>$$<ul>
<li>\(e_{ui} = r_{ui} - \hat{r}_{ui}\)</li>
<li>\(b_{u}\)[待更新]</li>
<li>\(b_{i}\)[待更新]</li>
<li>\(u\)[待更新]</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>SVD++<ul>
<li>SVD++是对BiasSVD进行改进的</li>
<li>基于假设:<ul>
<li>除了显示的评分行为以外，用户对于商品的浏览记录或购买记录（隐式反馈）也可以从侧面反映用户的偏好。相当于引入了额外的信息源，<strong>能够解决因显示评分行为较少导致的冷启动问题</strong></li>
</ul>
</li>
<li>优化问题定义[待更新]</li>
</ul>
</li>
<li>TimeSVD++<ul>
<li>之前的模型都是静态的,这里TimeSVD++是动态的,每个时间段学习一个参数,不同时间段使用该时间段的训练数据进行学习</li>
<li>优化问题定义[待更新]</li>
</ul>
</li>
<li>BiasSVDwithU</li>
<li>NMF</li>
<li>非负矩阵分解Nonnegative Matrix Factorization*</li>
<li>PMF</li>
<li>概率矩阵分解Probabilistic Matrix Factorization*</li>
<li>WRMF</li>
<li>weighted regularized matrix factorization*</li>
</ul>
<h4 id="混合的推荐"><a href="#混合的推荐" class="headerlink" title="混合的推荐"></a>混合的推荐</h4><ul>
<li>基于混合的推荐，顾名思义，是对以上算法的融合</li>
<li>淘宝上就既有基于内容的推荐也有协同过滤的推荐</li>
<li>对模型的融合可以参考集成学习的三种集成方式<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94Ensemble-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">ML——Ensemble-集成学习</a></li>
</ul>
<h3 id="推荐系统分类-基于推荐的最终输出形式分类"><a href="#推荐系统分类-基于推荐的最终输出形式分类" class="headerlink" title="推荐系统分类(基于推荐的最终输出形式分类)"></a>推荐系统分类(基于推荐的最终输出形式分类)</h3><h4 id="评分预测模型"><a href="#评分预测模型" class="headerlink" title="评分预测模型"></a>评分预测模型</h4><p><em>Rating prediction</em></p>
<ul>
<li>核心目的: 填充User-Item矩阵中的缺失值</li>
<li>模型衡量指标: 均方根误差(RMSE, Root Mean Squard Error), 平均绝对误差(MAE, Mean Absolute Error)</li>
</ul>
<h4 id="排序预测模型"><a href="#排序预测模型" class="headerlink" title="排序预测模型"></a>排序预测模型</h4><p><em>Ranking prediction (top-n recommendation)</em></p>
<ul>
<li>核心目的: 给定一个用户,推荐一个有序的商品列表</li>
<li>模型衡量指标: Precision@K, Recall@K等</li>
</ul>
<h4 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h4><p><em>Classification</em></p>
<ul>
<li>核心目的: 将候选商品分类,然后用于推荐</li>
<li>模型衡量指标: Accuracy</li>
</ul>
]]></content>
      <tags>
        <tag>RS</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——DT-决策树</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94DT-%E5%86%B3%E7%AD%96%E6%A0%91.html</url>
    <content><![CDATA[<p>本文参考: &lt;&lt;统计学习方法&gt;&gt;<br><em>决策树算法时很多优秀的集成算法的基础,包括RF,GBDT,提升树(Boosting Tree)等</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li>决策树(DT)是一种基本的分类和回归方法</li>
<li>分类问题中可以理解为if-then规则的集合</li>
<li>分类问题中也可以理解为定义在特征空间-&gt;类空间上的条件概率分布</li>
<li>分类问题中使用的是ID3和C4.5<ul>
<li>ID3 基于 最大化信息增益 来选择特征</li>
<li>C4.5基于 最大化信息增益比 来选择特征</li>
</ul>
</li>
<li>回归问题使用的是分类与回归树(Classification And Regression Tree, CART)<ul>
<li>分类树: 基于 最小化基尼(Gini Index)指数 来选择特征</li>
<li>回归树: 基于 最小化平方误差 来选择特征</li>
</ul>
</li>
<li>关于树模型的复杂度可以用下面的方式评估, 统计学习方法中<strong>CART</strong>选择使用树的<strong>节点总数</strong>来评估树的复杂度<ul>
<li>叶子节点的数量</li>
<li>节点总数</li>
<li>树的深度</li>
</ul>
</li>
</ul>
<hr>
<h3 id="树模型的优缺点"><a href="#树模型的优缺点" class="headerlink" title="树模型的优缺点"></a>树模型的优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>可解释性强</li>
<li>可处理混合类型特征(?)</li>
<li>具体伸缩不变性(不用归一化特征,LR模型需要归一化特征) </li>
<li>有特征组合的作用</li>
<li>可自然地处理缺失值(神经网络不能处理缺失值)</li>
<li>对异常点鲁棒</li>
<li>有特征选择作用</li>
<li>可扩展性强，容易并行</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>缺乏平滑性(回归预测时输出值只能 输出有限的若干种数值)</li>
<li>不适合处理高维稀疏数据<ul>
<li>数据稀疏时<ul>
<li>比如某个数据集有10000个样本</li>
<li>某个特征只有10个样本存在值，其他样本都不存在值</li>
</ul>
</li>
<li>决策树： </li>
<li>这样的话树容易将当前特征选中作为分类特征，这种划分可能在训练集上看起来很好，但测试集中表现可能不太好(这里不是简单的缺失值，而是数据很稀疏，这里需要进一步的理解[待更新])</li>
<li>LR等线性模型：<ul>
<li>因为现在的模型普遍都会带着正则项，而LR等线性模型的正则项是对权重的惩罚，也就是特征对应的权重一旦过大，惩罚就会很大，进一步压缩权重的值，使他不至于过大，而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="决策树训练的三个步骤"><a href="#决策树训练的三个步骤" class="headerlink" title="决策树训练的三个步骤"></a>决策树训练的三个步骤</h3><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><h5 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h5><ul>
<li>对数据集D的经验熵:\(H(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}\)</li>
<li>对特征A对数据集D的经验条件熵:\(H(D|A)=\sum_{n=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=-\sum_{n=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}\), n为特征A的取值个数</li>
</ul>
<h5 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h5><ul>
<li>特征A对训练数据D的信息增益比:\(g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}\)<ul>
<li>其中\(H_{A}=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}\), n为特征A的取值个数</li>
</ul>
</li>
</ul>
<h5 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h5><ul>
<li>对于分布\(p=(p_{1},…,p_{k})\)的基尼指数:\(Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}\)</li>
<li>对样本集合D,基尼指数为:\(Gini(p)=1-\sum_{k=1}^{K}\left(\frac{|C_{k}|}{|D|}\right)^{2}\)</li>
<li>在特征A的条件下,集合D的基尼指数:\(Gini(D,A)=\frac{|C_{1}|}{|D|}Gini(D_{1})+\frac{|C_{2}|}{|D|}Gini(D_{2})\)<ul>
<li>我们将集合D根据特征A分为两类: </li>
<li>是否取特征A的某个值,其中\(A=a\)的是\(D_{1}\),\(A\neq a\)的是\(D_{2}\)</li>
</ul>
</li>
</ul>
<h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><h5 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h5><ul>
<li>基于<strong>最大化信息增益</strong>来选择特征</li>
<li>选取所有信息增益最大的特征作为当前结点的特征</li>
<li>对取值数目较多的属性有所偏好</li>
<li>只有分类树,没有回归树</li>
<li>ID3相当于用极大似然法对模型进行选择(问题:如何理解?)</li>
</ul>
<h5 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h5><ul>
<li>基于<strong>最大化信息增益比</strong>来选择特征</li>
<li>选取信息增益比最大的特征作为当前结点的特征<ul>
<li>由于使用最大的信息增益比特征可能对取值数目少的特征属性有所偏好,所以C4.5算法一般不会直接选信息增益比最大的,而是<ul>
<li><strong>先从候选区属性中找出信息增益高于平均水平的</strong></li>
<li><strong>再从筛选结果中寻找信息增益比最大的</strong></li>
</ul>
</li>
</ul>
</li>
<li>处理连续型特征时使用二分法(bi-partition)</li>
<li>只有分类树,没有回归树</li>
</ul>
<h5 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h5><ul>
<li>CART的决策树是二叉树</li>
<li>分类树: 基于<strong>最小化基尼指数</strong>来选择特征<ul>
<li>输出变量为离散变量</li>
<li>基于基尼指数选取所有可能的特征A和所有可能的切分点a中,选择基尼指数最小的特征和切分点为最优特征和最优且分点</li>
</ul>
</li>
<li>回归树: 基于<strong>最小化平方误差</strong>来选择特征<ul>
<li>CART回归树一般称为最小二乘回归树(因为目标函数的优化是最小化误差的平方和实现的)</li>
<li>输出变量为连续变量</li>
<li>选取时选择最优划分变量(特征)j和最优切分点s,然后按照变量j的划分点s将结点分为两个结点,大于s的为第一个结点,小于s的为第二个结点</li>
</ul>
</li>
<li>一点说明:<ul>
<li>*&lt;&lt;统计学习方法&gt;&gt;*中:<ul>
<li>回归树的特征默认是连续变量,选取划分点s时使用的是<strong>连续变量的各种中间值</strong>作为候选值,<strong>大于s的分为一个结点,小于s的分为一个结点</strong></li>
<li>分类树的特征默认是离散变量,选取划分点a时使用的是<strong>离散变量的值</strong>作为候选值,<strong>等于a的分为一个结点,不等于a的分为另一个结点</strong></li>
<li>但是实际上无论时分类树还是回归树,CART都可以用相同手段处理连续值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><ul>
<li>剪枝的核心思想:<ul>
<li>就是加入考虑树的复杂度考量(决策树的生成时仅仅是考虑到信息增益和信息增益比,没有考量树的复杂度)<ul>
<li>树的深度越深,树越复杂</li>
</ul>
</li>
<li>整体上再考虑树变得更简单的同时保证分类误差较小</li>
</ul>
</li>
</ul>
<h5 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h5><ul>
<li>预剪枝发生在决策树生成过程中</li>
<li>在每个节点划分前先进行估计</li>
<li>若当前节点划分不能带来决策树准确率提升,则停止划分</li>
<li>在&lt;&lt;统计学习方法&gt;&gt;中未提到这个方法,只讲了一种简单的后剪枝算法</li>
<li>时间复杂度小,欠拟合风险大</li>
</ul>
<h5 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h5><ul>
<li>后剪枝发生在决策树生成后</li>
<li>自底向上的对非叶子节点进行考察(注意千万不可从根节点开始自顶向下的剪枝,可能失去整体最优的决策树)</li>
<li>若将该节点换成叶子节点能带来决策树准确率提升,则将该节点替换为叶子节点</li>
<li>时间复杂度大,欠拟合风险小</li>
</ul>
<h6 id="常见的后剪枝方法"><a href="#常见的后剪枝方法" class="headerlink" title="常见的后剪枝方法"></a>常见的后剪枝方法</h6><p><em>各种方法各有优劣,关注不同的优化角度</em></p>
<ul>
<li>REP 错误率降低剪枝(Reduced Error Pruning)</li>
<li>PEP 悲观剪枝(Pessimistic Error Pruning)</li>
<li>CCP 代价复杂度剪枝(Cost Complexity Pruning), 详细过程参考<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94DT-%E5%86%B3%E7%AD%96%E6%A0%91.html#CCP%E5%89%AA%E6%9E%9D%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0">CPP剪枝算法描述</a></li>
<li>MEP 最小误差剪枝(Minimum Error Pruning)</li>
<li>CVP (Critical Value Pruning)</li>
<li>OPP (Optimal Pruning)</li>
</ul>
<h5 id="ID3和C4-5的剪枝"><a href="#ID3和C4-5的剪枝" class="headerlink" title="ID3和C4.5的剪枝"></a>ID3和C4.5的剪枝</h5><h5 id="CART的剪枝"><a href="#CART的剪枝" class="headerlink" title="CART的剪枝"></a>CART的剪枝</h5><p><em>CART使用的是CCP剪枝</em></p>
<ul>
<li>对任意的子树,我们可以定义子树的损失<ul>
<li>\(C_{a}(T)=C(T)+\alpha|T|\)</li>
<li>子树的损失 = 子树的预测误差 + \(\alpha\) * 子树的节点数</li>
<li>对于回归树和分类树,子树的预测误差定义不一样,前者是误差的平方和,后者是基尼指数</li>
</ul>
</li>
<li>可以证明对于给定的Alpha,一定存在某个损失最小的子树,也就是我们要的最优子树</li>
<li>现实中实现时可以使用递归的方法实现</li>
</ul>
<h6 id="CCP剪枝算法描述"><a href="#CCP剪枝算法描述" class="headerlink" title="CCP剪枝算法描述"></a>CCP剪枝算法描述</h6><p><em>CPP剪枝也是一种后剪枝算法</em><br><strong>修正：统计学习方法CART算法第六步中应该跳到第二步，而不是第三步</strong></p>
<ul>
<li>1：计算所有节点对应的\(\alpha\)值: \(\alpha=\frac{C(t)-C(T_{t})}{|T_{t}|-1}\)<ul>
<li>\(C(t)\)是以<strong>t节点单一节点为树</strong>时单一节点树的损失函数</li>
<li>\(C(T_{t})\)是以<strong>t节点为根节点的子树</strong>时整棵子树的损失函数</li>
<li>\(|T_{t}|\)是以<strong>t节点为根节点的子树</strong>时整棵子树的节点数量</li>
</ul>
</li>
<li>2：对当前\(\alpha\)值最小的节点t剪枝，并存储中间结果的\(\alpha\)值和剪枝后的树结构<ul>
<li>当\(\alpha\)确定时，存在唯一的最小子树\(T_{\alpha}\)使损失函数\(C_{\alpha}(T)\)最小</li>
</ul>
</li>
<li>3：选取当前树为剪枝后的树，跳转到第1步，直到剪枝到只有三个节点的树时截止</li>
<li>4：截止后得到节点数量从大到小的多个子树\(T_{\alpha_{0}}, T_{\alpha_{1}},…,T_{\alpha_{n}}\). (其中\(T_{\alpha_{i}}\)也就对应着第i个\(\alpha\)值\(\alpha_{i}\))</li>
<li>5：用交叉验证法对\(\alpha\)的值进行选择(CART算法执行时\(\alpha\)类似超参数，整个算法学习的过程类似于用交叉验证法确定超参数的过程，\(\alpha\)的值确定了，对应的决策树也就确定了！)</li>
</ul>
<h4 id="关于连续特征"><a href="#关于连续特征" class="headerlink" title="关于连续特征"></a>关于连续特征</h4><h5 id="统计学习方法"><a href="#统计学习方法" class="headerlink" title="统计学习方法"></a>统计学习方法</h5><ul>
<li>在&lt;&lt;统计学习方法&gt;&gt;中回归树的特征默认是连续变量,分类树的特征默认是离散变量</li>
</ul>
<h5 id="机器学习-周志华"><a href="#机器学习-周志华" class="headerlink" title="机器学习 周志华"></a>机器学习 周志华</h5><ul>
<li>在&lt;&lt;机器学习&gt;&gt;中提到连续特征的一种解决方案:<ul>
<li>把该连续特征所有出现的取值排序</li>
<li>取临近两两之间的平均值作为划分点</li>
<li>像处理离散的点一样,使用信息增益最大化,信息增益率最大化,或者是基尼指数最小化实现对应的划分选择</li>
</ul>
</li>
</ul>
<h5 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h5><ul>
<li>处理连续型变量(特征的能力)<ul>
<li>ID3 不能处理连续型特征<ul>
<li>因为连续型特征往往使得每一个样本该特征取值都不一样,造成该特征对数据集D的经验条件熵为0?</li>
<li>使得ID3算法趋向于选择这个特征?</li>
</ul>
</li>
<li>C4.5 能处理连续型特征<ul>
<li>将数据排序后找到类别不同的分割线作为切分点</li>
<li>根据切分点把连续属性二分类装换为布尔类型</li>
<li>可多次使用连续属性分类</li>
</ul>
</li>
<li>CART 能处理连续型特征<ul>
<li>实际对连续型特征的处理<strong>与C4.5一样</strong></li>
<li>由于CART树构建时每次都会对特征进行二值化分,所以可以很好的适用与连续型变量</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="一些总结"><a href="#一些总结" class="headerlink" title="一些总结"></a>一些总结</h4><ul>
<li>算法ID3生成可能是多叉树,而CART一定是二叉树，*&lt;&lt;统计学习方法&gt;&gt;*中二者生成相同的数是巧合，除了不同评价方式的特征选择结果一样以外，还需要被选中的特征都是二值的！</li>
<li>ID3相当于用极大似然法对模型进行选择</li>
</ul>
<h4 id="一种很好的理解思路"><a href="#一种很好的理解思路" class="headerlink" title="一种很好的理解思路"></a>一种很好的理解思路</h4><h5 id="ID3-1"><a href="#ID3-1" class="headerlink" title="ID3"></a>ID3</h5><ul>
<li>ID3算法就是用<strong>信息增益</strong>大小来判断当前节点应该用什么特征来构建决策树，用计算出的<strong>信息增益最大的特征</strong>来建立决策树的当前节点</li>
</ul>
<h6 id="ID3的缺点"><a href="#ID3的缺点" class="headerlink" title="ID3的缺点:"></a>ID3的缺点:</h6><ul>
<li>ID3<strong>不能处理连续特征</strong></li>
<li>ID3对取值较多的特征有着偏好<strong>在相同条件下，取值比较多的特征比取值少的特征信息增益大</strong></li>
<li>ID3<strong>不能处理缺失值</strong></li>
<li>没有考虑过拟合的问题(问题: ID3没有剪枝吗?)</li>
</ul>
<h5 id="C4-5-1"><a href="#C4-5-1" class="headerlink" title="C4.5"></a>C4.5</h5><ul>
<li>C4.5可以看成是对ID3进行改进</li>
</ul>
<h6 id="C4-5对ID3的改进"><a href="#C4-5对ID3的改进" class="headerlink" title="C4.5对ID3的改进"></a>C4.5对ID3的改进</h6><ul>
<li><p>对于ID3不能处理连续特征，C4.5的思路是将连续的特征离散化.</p>
<ul>
<li>样本(m个样本)按照特征的取值排列后取相邻样本的平均值作为划分点(m-1个划分点),分别计算以每个划分点作为二元分类时的信息增益. 最终选择信息增益高的(问题:这里是C4.5,为什么不是使用信息增益比而是信息增益来作为区分?)</li>
<li>C4.5选择连续特征作为分类特征时,只分两类,但是后面(其他层结点)可以使用该特征分类,也就是说连续特征在C4.5中可以多次使用,但每次只分为两部分</li>
</ul>
</li>
<li><p>对于ID3信息增益最大指标会造成偏向于取值较多的特征的问题. C4.5使用信息增益比来解决问题</p>
</li>
<li><p>对于ID3不能处理缺失值的问题,C4.5主要解决两个问题</p>
<ul>
<li>1) 如何在属性值缺失条件下进行<strong>属性选择</strong><ul>
<li>没有缺失值的属性,正常处理</li>
<li>每个属性A有缺失值:<ul>
<li>对该特征进行划分时仅仅考虑在属性A上没有缺失的部分数据,有缺失的数据不考虑.</li>
<li>无缺失的数据计算收益时需要乘以一个权重(无缺失的样本总数/样本总数)</li>
<li>相当于信息增益适当缩小</li>
</ul>
</li>
</ul>
</li>
<li>2) 给定划分属性,若<strong>样本在该属性上的值缺失,如何对样本进行划分</strong><ul>
<li>在A属性没有缺失值的样本,正常划分</li>
<li>在A属性有缺失值的样本:<ul>
<li>给每个样本引入权重,初始值都为1 </li>
<li>同一个样本按照不同概率划入到不同的结点(当前叶节点)中去(概率是当前结点中样本数量/无缺失样本总数) </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>对于没有考虑过拟合的问题:</p>
<ul>
<li>C4.5引入了正则化系数剪枝(问题: ID3没有剪枝吗?)</li>
</ul>
</li>
</ul>
<h6 id="C4-5的缺点"><a href="#C4-5的缺点" class="headerlink" title="C4.5的缺点"></a>C4.5的缺点</h6><ul>
<li>[这个问题存疑,没有任何书籍显示C4.5的剪枝策略是PEP,&lt;&lt;统计学习方法&gt;&gt;中只是简单的介绍了<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94DT-%E5%86%B3%E7%AD%96%E6%A0%91.html#%E5%90%8E%E5%89%AA%E6%9E%9D">后剪枝</a>]C4.5的剪枝方法时PEP,PEP准确度高,但是存在下面两个子问题:<ul>
<li>1) PEP使用由上而下的剪枝策略,会导致与预先剪枝相同的问题,造成过度剪枝</li>
<li>2) 会造成剪枝失败的情况</li>
</ul>
</li>
<li>C4.5生成多叉树,计算机中很多时候多叉树运算效率不如二叉树来的高</li>
<li>C4.5只能用于分类</li>
<li>C4.5需要进行大量的对数运算(计算熵)</li>
</ul>
<h5 id="CART-1"><a href="#CART-1" class="headerlink" title="CART"></a>CART</h5><ul>
<li>可以理解为对C4.5进一步的改进</li>
</ul>
<h6 id="CART对C4-5的改进"><a href="#CART对C4-5的改进" class="headerlink" title="CART对C4.5的改进"></a>CART对C4.5的改进</h6><ul>
<li>CART使用CPP代价复杂度剪枝算法<ul>
<li>详细过程参考<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94DT-%E5%86%B3%E7%AD%96%E6%A0%91.html#CCP%E5%89%AA%E6%9E%9D%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0">CCP剪枝算法描述</a></li>
</ul>
</li>
<li>CART使用二叉树只生成二叉树,即使是离散特征<ul>
<li>CART对连续特征的处理与C4.5完全相同</li>
<li>CART对离散特征也是二分类且也是可以多次使用同一特征(ID3与C4.5中离散特征只能使用一次,且是多分叉)</li>
<li>所以CART是一颗二叉树</li>
</ul>
</li>
<li>CART可有分类树和回归树两种<ul>
<li>回归树的目标函数的优化是: 最小化误差的平方和</li>
<li>分类树以概率最大的类别作为叶节点的类别</li>
<li>回归树以中位数或者均值作为预测结果</li>
</ul>
</li>
<li>CART使用基尼指数作为结点混乱度的度量指标<ul>
<li>避免了对数计算(与熵比较)</li>
</ul>
</li>
</ul>
<h5 id="ID3-C4-5-CART的缺点"><a href="#ID3-C4-5-CART的缺点" class="headerlink" title="ID3,C4.5,CART的缺点"></a>ID3,C4.5,CART的缺点</h5><ul>
<li>每次之全责一个最优特征作为分类决策,而实际中其实可能需要多个特征一起决策<ul>
<li>解决方案: 多变量决策树(每次选择多个特征一起决策)<ul>
<li>单个特征决策可以看成是直线</li>
<li>多个特征决策可以看成是斜线</li>
</ul>
</li>
</ul>
</li>
<li>样本改变一点点都会造成树的结构改变很大<ul>
<li>解决方案: 随机森林等集成学习方法</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>DT</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——集成学习</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94Ensemble-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<p><em>集成学习(Ensemble)的本质是一种组合基础模型实现更高泛化能力和精度的技术框架</em><br>本文参考了博客: <a href="http://www.cnblogs.com/jasonfreak/p/5657196.html" target="_blank" rel="noopener">http://www.cnblogs.com/jasonfreak/p/5657196.html</a></p>
<hr>
<h3 id="集成学习的三族算法"><a href="#集成学习的三族算法" class="headerlink" title="集成学习的三族算法"></a>集成学习的三族算法</h3><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p><em>通过<strong>重采样技术</strong>生成若干<strong>不同子训练集</strong>,然后在<strong>每个训练集</strong>上训练<strong>一个分类器</strong>,最终采用<strong>投票方式</strong>产生模型最终结果</em></p>
<ul>
<li>m个基础模型</li>
<li>从原始训练集中抽样生成m个子训练集,用子训练集训练每个基础模型</li>
<li>最终预测结果: 对所有基础模型预测的结果进行综合产生

</li>
</ul>
<h5 id="代表模型"><a href="#代表模型" class="headerlink" title="代表模型"></a>代表模型</h5><h6 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h6><ul>
<li>RF = Bagging + DT (随机森林中特征的选择也是随机的,这一点不同于DT,也不同与Bagging)</li>
<li>随机森林详情可参考<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94RF.html" title="/Notes/ML/Models/ML——RF.html">ML——RF</a></li>
</ul>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p><em>每个<strong>训练样例都有权重</strong>，每次训练新分类器的时候都<strong>着重训练</strong>那些在<strong>上一次分类</strong>过程中<strong>分错的样例</strong>，<strong>权重</strong>会随着迭代次数的变化而<strong>变化</strong></em></p>
<ul>
<li>训练思想是,每一轮迭代都将重心放到分错类的样本上</li>
<li>训练过程为阶梯状</li>
<li>基础模型按照次序依次训练(实现时可做到并行)</li>
<li>前一个模型的预测结果修改后一个模型的样本权重(注意:模型的训练集时不会变的,只有每个样本的权重在变化,增大分错的样本的权重,使得后面训练时重视分错的样本),以此类推</li>
<li>最终预测结果: 对所有基础模型预测的结果进行线性综合产生

</li>
</ul>
<h5 id="代表模型-1"><a href="#代表模型-1" class="headerlink" title="代表模型"></a>代表模型</h5><h6 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h6><ul>
<li>Boosting Tree = AdaBoost + DT (AdaBoost是Boosting族算法的一种)</li>
</ul>
<h6 id="梯度提升树"><a href="#梯度提升树" class="headerlink" title="梯度提升树"></a>梯度提升树</h6><ul>
<li>GBDT = Gradient Boosting + DT (Gradient Boosting是Boosting族算法的一种)</li>
<li>GBDT详情可参考<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94GBDT.html" title="/Notes/ML/Models/ML——GBDT.html">ML——GBDT</a></li>
</ul>
<h4 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h4><p><em>每个分类器首先做一遍决策，然后将分类器们的决策送到更高一层的模型中，把他们当做特征再进行一次训练</em></p>
<ul>
<li>训练所有基础模型</li>
<li>获取所有基础模型的预测结果</li>
<li>第j个模型对某个训练样本产生的预测值作为该训练样本的第j个特征,标签不变,生成一个新的数据集(注意,样本的特征空间大小发生了变化,标签没变)</li>
<li>基于新的训练集进行训练,得到预测模型M()</li>
<li>测试时也要将特征转换后再用M进行预测</li>
<li>实质上就是先用一些模型提取特征(这些模型的输出作为更高层模型的特征),然后再用模型的输出作为最终模型的特征,从而实现模型的堆叠(Stacking)</li>
<li>理论上可以堆叠各种各样的分类器

</li>
</ul>
<hr>
<h3 id="方差与偏差"><a href="#方差与偏差" class="headerlink" title="方差与偏差"></a>方差与偏差</h3><ul>
<li>偏差与方差可以参考我的博客: <a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html">模型的偏差与方差</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——GBDT-梯度提升树-推导过程</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94GBDT-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B.html</url>
    <content><![CDATA[<p><em>本文介绍梯度提升树(族)的推导过程,包括传统的GBDT,XGBoost等</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="参数空间的优化"><a href="#参数空间的优化" class="headerlink" title="参数空间的优化"></a>参数空间的优化</h3><ul>
<li>参数空间中我们使用梯度下降法(一阶导数)和牛顿迭代法(二阶导数)来优化</li>
<li>关于无约束参数优化方法参考<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95-%E6%97%A0%E7%BA%A6%E6%9D%9F%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html">无约束参数优化方法</a></li>
</ul>
<hr>
<h3 id="从参数空间优化到函数空间的优化"><a href="#从参数空间优化到函数空间的优化" class="headerlink" title="从参数空间优化到函数空间的优化"></a>从参数空间优化到函数空间的优化</h3><ul>
<li>函数空间中我们使用GBDT(一阶导数)和XGBoost(二阶导数)来优化</li>
<li>函数空间的优化完全类比参数空间的优化<img src="/Notes/ML/Models/ML——GBDT-梯度提升树-推导过程/GradientDescend2GradientBoosting.png">
<img src="/Notes/ML/Models/ML——GBDT-梯度提升树-推导过程/NewtonsMethod2NewtonBoosting.png">

</li>
</ul>
<hr>
<h3 id="Boosting是一种加法模型"><a href="#Boosting是一种加法模型" class="headerlink" title="Boosting是一种加法模型"></a>Boosting是一种加法模型</h3><p><em>加法模型：additive training</em><br>$$<br>\begin{align}<br>F(x) = \sum_{t=0}^{T}f_{t}(x)<br>\end{align}<br>$$</p>
<ul>
<li>上式中\(f_{t}(x)\)为基分类器, 我们通常采用回归树[Friedman 1999] 和逻辑回归<br>[Friedman 2000]</li>
<li>树模型的优缺点可以参考<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94DT-%E5%86%B3%E7%AD%96%E6%A0%91.html">ML——DT-决策树</a></li>
</ul>
<hr>
<h3 id="GBDT算法原理"><a href="#GBDT算法原理" class="headerlink" title="GBDT算法原理"></a>GBDT算法原理</h3><p><em>这里只原论文中的Gradient Boosting Tree算法</em><br><em>Friedman于论文”GreedyFunctionApproximation…”中最早 出GBDT</em></p>
<h4 id="模型-F-的加法定义："><a href="#模型-F-的加法定义：" class="headerlink" title="模型\(F\)的加法定义："></a>模型\(F\)的加法定义：</h4><p>$$<br>\begin{align}<br>F(x) &amp;= \sum_{t=0}^{T}\alpha_{t} h_{t}(x;w_{t}) \\<br>&amp;= \sum_{t=0}^{T} f_{t}(x;w_{t})<br>\end{align}<br>$$</p>
<ul>
<li>其中, \(x\)为输入样本, \(h_{t}\)为分类回归树, \(w_{t}\)是树\(h_{t}\)的参数, \(\alpha_{t}\)是树\(h_{t}\)的权重</li>
</ul>
<h4 id="损失函数的定义"><a href="#损失函数的定义" class="headerlink" title="损失函数的定义"></a>损失函数的定义</h4><p>$$<br>\begin{align}<br>F^{\star} = \arg\max_{F}\sum_{i=1}^{N}L(y_{i}, F(x_{i};w))<br>\end{align}<br>$$</p>
<ul>
<li>其中，\(N\)为样本数量，所有样本的损失总和为总的损失函数</li>
<li>最小化损失函数即可得到最优模型</li>
</ul>
<h4 id="最优模型的求解"><a href="#最优模型的求解" class="headerlink" title="最优模型的求解"></a>最优模型的求解</h4><ul>
<li>直接列举所有可能的树——<strong>NP难问题</strong></li>
<li>所以GBDT算法使用<strong>贪心法, 迭代求局部最优解</strong></li>
<li>详细的Gradient Boosting Tree迭代过程如下<img src="/Notes/ML/Models/ML——GBDT-梯度提升树-推导过程/GradientBoostingTreeProcess.png"></li>
<li>上面的推导中<ul>
<li>2.1中：\(\tilde{y}_{i}\)是当前的损失函数\(L(y_{i}, F(x_{i}))\)关于当前函数\(F(x)\)(模型)在\(F(x)=F_{t-1}(x)\)处的负梯度(每个样本都有一个负梯度)，这个梯度也是GBDT名字中梯度的由来<ul>
<li>这个损失函数在使用不同回归方法时时定义各不相同</li>
<li>原始论文中提到两个损失函数定义,根据损失函数的不同,对应的归回方法不同: <strong>最小二乘归回</strong>或者<strong>最小绝对误差回归</strong></li>
</ul>
</li>
<li>2.2中：\(w^{\star}\)是指能够拟合当前负梯度的树\(h_{t}(x;w^{\star})\)的最佳参数，这里我们认为最佳参数就是最小二乘的最佳参数，实际上这个地方可以使用其他测拟合标准(这个标准是拟合当前负梯度的拟合标准，与后面的损失函数\(L(y, F(x;w))\)无关)，只是这里最小二乘是最简单也是最自然的选择<ul>
<li>原始论文中这里使用的基函数是\(\beta h(x;w)\)，其中\(\beta\)是当前基函数\(h(x;w)\)的权重，这里我认为直接使用\(h(x;w)\)作为基函数即可,权重\(\beta\)会自动被基函数学到的(可能原始论文中\(h(x;w)\)指的是简单的基函数，是不含权重学习功能的)</li>
<li>但是需要注意的是,如果我们使用的基分类器是逻辑回归,那么这里每个基分类器的结果都是在[0-1]之间的数,是需要前面的\(\beta\)的</li>
<li>这里我们推导的时候假定了基分类器是回归树,所以不需要使用 \(\beta\)</li>
</ul>
</li>
<li>2.3中：由于\(w^{\star}\)只能保证当前树\(h_{t}(x;w^{\star})\)是能拟合负梯度的，不能保证把当前这棵树添加到模型中时模型的损失函数是最小的，所以我们加了一个步长参数\(\rho\)，用来表示得到当前树的最优权重<ul>
<li>损失函数是平方损失函数时，这里的参数为\(\rho\)就是1，无需计算</li>
</ul>
</li>
<li>2.4中：将当前树的最优树(包括权重)一起加入到模型中</li>
</ul>
</li>
</ul>
<h4 id="不同损失函数和基函数对应不同的算法"><a href="#不同损失函数和基函数对应不同的算法" class="headerlink" title="不同损失函数和基函数对应不同的算法"></a>不同损失函数和基函数对应不同的算法</h4><p><em>上述式子中推导用到的基函数为树模型，实际使用中也可以使用逻辑回归模型[Friedman 2000]等基函数</em><br><em>本小节将介绍不同损失函数或者基函数带来的不同算法</em></p>
<ul>
<li>注意:包括Adaboost和GBDT在内<strong>Boosting框架中，基函数(基分类器)都不能使用线性模型</strong><ul>
<li>理解: Boosting框架本质上是一个加法模型，是对多个基函数的线性组合，得到更优的分类器，可以证明线性模型的线性组合还是线性模型，如果Boosting框架中使用线性模型，那么我们最终得到的分类器也是线性模型，这就局限了我们的整体模型的表达能力</li>
</ul>
</li>
</ul>
<h5 id="最小二乘回归-损失函数"><a href="#最小二乘回归-损失函数" class="headerlink" title="最小二乘回归(损失函数)"></a>最小二乘回归(损失函数)</h5><p><em>Least-Squares Regression</em></p>
<h6 id="损失函数定义"><a href="#损失函数定义" class="headerlink" title="损失函数定义"></a>损失函数定义</h6><ul>
<li>此时损失函数定义为<br>$$<br>\begin{align}<br>L(y,F(x)) = \frac{1}{2}(y-F(x))^{2}<br>\end{align}<br>$$</li>
</ul>
<h6 id="进一步理解推导过程"><a href="#进一步理解推导过程" class="headerlink" title="进一步理解推导过程"></a>进一步理解推导过程</h6><ul>
<li>2.1中\(\tilde{y}_{i}=y_{i}-F_{t-1}(x_{i})\)，这里直接对损失函数求导即可的到<br>$$<br>\begin{align}<br>\tilde{y_{i}} &amp;= -\left [\frac{\partial L(y,F(x_{i}))}{\partial F(x_{i})}\right ]_{F(x) = F_{t-1}(x)} \\<br>&amp;= -\left [\frac{\partial L(y,F_{t-1}(x_{i}))}{\partial F_{t-1}(x_{i})}\right ] \\<br>&amp;= -\left [\frac{\partial \frac{1}{2}(y_{i}-F_{t-1}(x_{i}))^{2}}{\partial F_{t-1}(x_{i})}\right ] \\<br>&amp;= 2 \cdot -\frac{1}{2}(y_{i}-F_{t-1}(x_{i})) \cdot -1 \\<br>&amp;= y_{i}-F_{t-1}(x_{i})<br>\end{align}<br>$$</li>
<li>2.2中正常拟合(使用线性回归和CART回归树均可)</li>
<li>2.3中基函数的权重\(\rho^{\star}\)是常数1，推导如下<br>$$<br>\begin{align}<br>L(y,F_{t}(x)) &amp;= \sum_{i=1}^{N}L(y_{i}, F_{t}(x_{i})) \\<br>&amp;= \frac{1}{2}\sum_{i=1}^{N}((y_{i}-F_{t}(x_{i}))^{2}) \\<br>&amp;= \frac{1}{2}\sum_{i=1}^{N}((y_{i}-F_{t-1}(x_{i}) - h_{t}(x_{i};w))^{2}) \\<br>&amp;= \frac{1}{2}\sum_{i=1}^{N}((\tilde{y}_{i}-h_{t}(x_{i};w))^{2}) \\<br>\end{align}<br>$$</li>
<li>显然，这个式子和2.2中拟合目标完全相同(只差着2倍常数权重)，所以2.2中得到的最优基函数\(h_{t}(x;w^{\star})\)就是2.3中使得模型损失函数\(L(y,F_{t}(x))\)最小的最优基函数，无需添加任何的权重系数</li>
</ul>
<h5 id="最小绝对偏差回归-损失函数"><a href="#最小绝对偏差回归-损失函数" class="headerlink" title="最小绝对偏差回归(损失函数)"></a>最小绝对偏差回归(损失函数)</h5><p><em>Least Absolute Deviation Regression, LAD</em></p>
<h6 id="损失函数定义-1"><a href="#损失函数定义-1" class="headerlink" title="损失函数定义"></a>损失函数定义</h6><ul>
<li>此时损失函数定义为<br>$$<br>\begin{align}<br>L(y,F(x)) = |y-F(x)|<br>\end{align}<br>$$</li>
</ul>
<h6 id="进一步理解推导过程-1"><a href="#进一步理解推导过程-1" class="headerlink" title="进一步理解推导过程"></a>进一步理解推导过程</h6><ul>
<li>2.1中\(\tilde{y}_{i}=sign(y_{i}-F_{t-1}(x_{i}))\)<ul>
<li>绝对值的导数就是1或者-1,当<br>$$y_{i}-F_{t-1}(x_{i}) &gt; 0$$</li>
<li>对损失函数求导得到<code>-1</code>,负梯度为<code>1</code></li>
<li>同理得到，当<br>$$y_{i}-F_{t-1}(x_{i}) &gt; 0$$</li>
<li>对损失函数求导得到<code>1</code>,负梯度为<code>-1</code></li>
<li>总结得到负梯度为<br>$$\tilde{y}_{i}=sign(y_{i}-F_{t-1}(x_{i}))$$</li>
</ul>
</li>
<li>2.2中正常拟合(使用线性回归和CART回归树均可)</li>
<li>2.3中基函数的权重\(\rho^{\star}\)不再是常数，推导如下</li>
<li>待更新*<br>$$<br>\begin{align}<br>待更新<br>\end{align}<br>$$</li>
</ul>
<h5 id="回归树-基函数"><a href="#回归树-基函数" class="headerlink" title="回归树(基函数)"></a>回归树(基函数)</h5><p><em>Regression Trees</em></p>
<ul>
<li><strong>传统GBDT</strong>中原始论文使用<strong>树回归</strong>,论文见<a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">Firedman 1999</a>,后来作者提出可以使用<strong>逻辑回归</strong>,论文见<a href="https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf" target="_blank" rel="noopener">Friedman 2000</a></li>
<li>回归树和逻辑回归的优缺点比较<ul>
<li><strong>树模型优点</strong>:<ul>
<li>可解释性强</li>
<li>可处理混合类型特征(混合类型特征指的是数值型和类别型均可处理)</li>
<li>具有伸缩不变性(无需特征归一化: 神经网络和逻辑回归都需要,逻辑回归中是为了保证随机梯度下降的方向正确,速度快)</li>
<li>可自然的处理缺失值, C4.5树处理缺失值默认使用的方法是先用未缺失样本计算信息增益确定分裂结点,然后将缺失值的每个样本按照权重(当前叶子节点未缺失样本数量 / 未缺失样本总数)分配到各个结点</li>
<li>对异常点鲁棒(不用去除异常点,LR不具有这个优点)</li>
<li>有特征选择的作用</li>
<li>可扩展性强,容易并行(并行是最好的,用来解释为什么XGBoost等都用树模型)</li>
</ul>
</li>
<li><strong>树模型缺点</strong>:<ul>
<li>缺乏平滑性(回归预测时输出值只能输出若干种数值,而不是连续的数值,所以不平滑[不平滑即离散])</li>
<li>不适合处理高维稀疏数据(当数据量太少时,非常容易过拟合,树的深度太深,从而模型变得太复杂)</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="传统GDBT与XGBoost的比较"><a href="#传统GDBT与XGBoost的比较" class="headerlink" title="传统GDBT与XGBoost的比较"></a>传统GDBT与XGBoost的比较</h3><ul>
<li>参考博客: <a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94XGBoost-vs-%E4%BC%A0%E7%BB%9FGBDT.html">ML——XGBoost-vs-传统GBDT</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——GBDT-梯度提升树-概念性总结</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94GBDT-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-%E6%A6%82%E5%BF%B5%E6%80%A7%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>GBDT(GradientBoostingDecisionTree), 梯度提升树</em><br><em>GBDT泛指所有梯度提升树，包括XGBoost(XGBoost是GBDT的变种),平时为了进行区分,GBDT特指“Greedy Function Approximation:A Gradient Boosting Machine”(GBDT论文原文)提出的算法,只利用了一阶导数信息(XGBoost利用了二阶导数信息)<br>*梯度的数学定义:函数上升最快的方向</em></p>
<p>参考论文:<a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" title="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">Greedy Function Approximation: A Gradient Boosting Machine</a><br>一篇很详细的论文阅读笔记:<a href="http://xtf615.com/paper/GBM.html" title="http://xtf615.com/paper/GBM.html" target="_blank" rel="noopener">GBM Paper Reading</a></p>
<p>引用一个常见的通俗例子:<strong>GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了（残差作为下一轮拟合的数据的理解）。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小,最终预测时使用他们的结果</strong></p>
<hr>
<h3 id="五种简称"><a href="#五种简称" class="headerlink" title="五种简称"></a>五种简称</h3><ul>
<li>各种简称,都是同一种算法:<ul>
<li>GBDT(Gradient Boosting Decision Tree)</li>
<li>MART(Multiple Additive Regression Tree)</li>
<li>GBT(Gradient Boosting Tree)</li>
<li>GTB(Gradient Tree Boosting)</li>
<li>GBRT(Gradient Boosting Regression Tree)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="模型原理"><a href="#模型原理" class="headerlink" title="模型原理"></a>模型原理</h3><h4 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h4><ul>
<li>使用CART作为基础模型(GDBT只能使用CART,不能使用其他树C4.5和ID3等), 后来作者提出也可以使用逻辑回归模型</li>
<li>每棵树学习的是前一棵树的残差(预测值与真实值的差)[这里是当损失函数是均方误差(平方损失, square loss)时可直接使用残差,其他类似的学习算法中若]</li>
<li>残差 = 真实值 - 预测值</li>
</ul>
<h4 id="如何理解GBDT中的梯度G？"><a href="#如何理解GBDT中的梯度G？" class="headerlink" title="如何理解GBDT中的梯度G？"></a>如何理解GBDT中的梯度G？</h4><ul>
<li>G表示Gradient，表示梯度，在GBDT中梯度是指损失函数关于每一个迭代中模型的梯度</li>
</ul>
<h4 id="与AdaBoost不同"><a href="#与AdaBoost不同" class="headerlink" title="与AdaBoost不同"></a>与AdaBoost不同</h4><ul>
<li><p>AdaBoost是通过利用前一轮弱学习器的误差率来更新训练集的权重</p>
<ul>
<li>增大分类错误的样本权重,从而使得样本更加关注上一步中分类错误的样本</li>
</ul>
</li>
<li><p>GBDT是通过学习上一轮学习器的残差来实现对真实结果的不断逼近</p>
<ul>
<li>上一步中预测越接近真实结果的样本,残差越接近0,下一轮中对该样本的关注度越低</li>
<li>上一步中预测越不接近真实结果的样本,残差越大,下一轮中对该样本的关注度越高</li>
</ul>
</li>
<li><p>GBDT的弱学习器只能使用CART回归树,只能用回归树这一点与AdaBoost和RF均不同</p>
<ul>
<li>因为我们的目标是拟合上一若学习器的残差</li>
<li>而残差往往是数值,不是类别,所以只能使用回归树CART</li>
</ul>
</li>
</ul>
<h4 id="GBDT分类"><a href="#GBDT分类" class="headerlink" title="GBDT分类"></a>GBDT分类</h4><h5 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h5><ul>
<li>GBDT实现二分类时可以每轮迭代直接用一个模型去学残差</li>
<li>此时类别可编码为一个一维向量,取值0或1</li>
</ul>
<h5 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h5><ul>
<li>GBDT实现多分类时每轮使用一个模型不够了,因为三个模型时使用1,2,3编码显然是不科学的,类别之间不应该有这种数值大小关系</li>
<li>此时三分类模型的类别可编码为一个三维向量,每一维的取值为0或1</li>
<li>在每一轮迭代时,为每个类训练一个CART树,每棵CART树是相互独立的</li>
<li>然后每个模型每轮分别学习当前特征的残差</li>
<li>每个模型都会用到所有的样本<ul>
<li>比如一个标记为标记为类别2的样本(x, y=2)</li>
<li>编码为(x, [0,1,0])</li>
<li>对于CART1和CART3(类别1和类别3的CART树)来说,该样本对应输入为(x, y=0)</li>
<li>对于CART2(类别2CART树)来说,该样本对应输入为(x, y=1)</li>
</ul>
</li>
<li>可以理解为把一个三分类问题转化成了3个二分类问题解决了</li>
<li>最后预测时<ul>
<li>给定一个未标记样本</li>
<li>每个类对应的模型(每个类的模型个数是该类上模型的迭代次数)都对应给出该类的打分</li>
<li>最后选择分数最高的一个类为最终分类即可</li>
</ul>
</li>
</ul>
<hr>
<h3 id="GDBT-LR"><a href="#GDBT-LR" class="headerlink" title="GDBT + LR"></a>GDBT + LR</h3><ul>
<li>为什么在广告CTR预估中, GDBT+LR能够提升效果?<ul>
<li>和LR对比: 线性模型</li>
<li>和GBDT对比: </li>
</ul>
</li>
</ul>
<hr>
<h3 id="GBDT和神经网络的优劣"><a href="#GBDT和神经网络的优劣" class="headerlink" title="GBDT和神经网络的优劣"></a>GBDT和神经网络的优劣</h3><h4 id="深度神经网络"><a href="#深度神经网络" class="headerlink" title="深度神经网络"></a>深度神经网络</h4><ul>
<li>通过不同层级和类型的网络可以对时空信息建模</li>
<li>适合图像, 声音, 文字等带有时序特质的数据</li>
</ul>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><ul>
<li>基于树模型的GBDT则能很好地处理表格数据</li>
<li>模型的可解释性</li>
<li>输入数据的不变性(几乎不用格式化数据)</li>
<li>更易于调参等特质更适合数值型数据</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——RF</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94RF-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.html</url>
    <content><![CDATA[<p><em>随机森林是一种集成学习(Ensemble)方法,属于集成学习中的Bagging族,是一种典型的Bagging方法</em></p>
<hr>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><ul>
<li>假设训练集总大小为N,特征总数为K</li>
<li>有放回抽样N个训练样本<ul>
<li>说明:这种有放回的抽样技术又称为自助法(Bootstrap)重采样技术 </li>
</ul>
</li>
<li><strong>随机</strong>从所有特征中<strong>选取k个特征</strong>(k一般远小于总特征数K)</li>
<li>用bootstrap采样的结果(N个样本)和k个特征训练一棵决策树(训练时不剪枝,每棵树都尽可能生长)</li>
<li>重复上述三个步骤:bootstrap采样和训练新的决策树,直到生成m棵决策树</li>
<li>将m棵决策树组合成随机森林</li>
</ul>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><ul>
<li>对于一个确定的未标记样本</li>
<li>经过每一棵决策树预测</li>
<li>投票产生结果,确定最终分类</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>避免过拟合(bootstrap采样[随机]和随机选择部分特征)</li>
<li>便于处理高维度的数据</li>
<li>对高维度的数据无需做特征选择,全部保留也无所谓</li>
<li>可并行计算 </li>
<li>模型实现简单</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>模型训练多棵树,所以训练时间长</li>
<li>预测时经过多棵树,预测时间长</li>
</ul>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li>决策树类型: 三种(ID3,C4.5,CART)均可选,默认均分的三种算法混合到一起<ul>
<li>决策树详细信息可参考<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94DT.html" title="/Notes/ML/Models/ML——DT.html">ML——DT</a></li>
</ul>
</li>
<li>树的数量m:<ul>
<li>n_estimators</li>
<li>int i: n_estimators = i</li>
<li>默认为10</li>
</ul>
</li>
<li>特征个数k(以Sklearn库为例):<ul>
<li>max_features = k</li>
<li>int i: k = i</li>
<li>float f: k = f * K</li>
<li>sqrt: k = sqrt(K)</li>
<li>log2: k = log2(K)</li>
<li>None: k = K </li>
<li>Sklearn.ensemble.RandomRorestClassifier默认是”auto”,也就是sqrt(K)</li>
</ul>
</li>
<li>树的深度: <ul>
<li>max_depth</li>
<li>单棵树的深度</li>
<li>默认是None,不做限制</li>
<li>int i: 指定树的深度为i</li>
</ul>
</li>
<li>叶子节点最小样本数:<ul>
<li>int i: min_samples_leaf = i</li>
<li>float f: min_samples_leaf = f * N</li>
<li>默认为1</li>
</ul>
</li>
</ul>
<h4 id="问题-随机森林的随机体现在哪里"><a href="#问题-随机森林的随机体现在哪里" class="headerlink" title="问题: 随机森林的随机体现在哪里?"></a>问题: 随机森林的随机体现在哪里?</h4><ul>
<li>个人理解<ul>
<li>体现在<strong>每棵树的训练样本随机</strong>,有放回采样(自助法(Bootstrap)重采样技术)</li>
<li>体现在<strong>每棵树的训练样本的特征随机</strong>, 对每棵树而言,随机从K个特征中选取k个特征,当前树的每个叶节点分裂时只能从这k个特征中选择</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>RF</tag>
      </tags>
  </entry>
  <entry>
    <title>RS——CTR-CVR的预估模型</title>
    <url>/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94CTR-CVR%E7%9A%84%E9%A2%84%E4%BC%B0%E6%A8%A1%E5%9E%8B.html</url>
    <content><![CDATA[<p><em>本文主要广告计算领域用于CTR,CVR预估的模型</em></p>
<hr>
<h3 id="计算广告领域的术语"><a href="#计算广告领域的术语" class="headerlink" title="计算广告领域的术语"></a>计算广告领域的术语</h3><h4 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h4><ul>
<li>CPA(Cost Per Action): <ul>
<li>一种广告计费模式</li>
<li>按照行为(Action)来计费</li>
<li>这里的行为可以是注册,咨询,放入购入车等</li>
</ul>
</li>
<li>CPC(Cost Per Click)<ul>
<li>一种广告计费模式</li>
<li>按照点击(Click)次数</li>
</ul>
</li>
</ul>
<h4 id="衡量指标"><a href="#衡量指标" class="headerlink" title="衡量指标"></a>衡量指标</h4><ul>
<li>点击率CTR(click-through rate), 又名点击通过率:<br>$$CTR = \frac{Count_{click}}{Count_{show}} $$<ul>
<li>分母是广告的实际展示次数</li>
<li>分子是广告的实际点击次数</li>
</ul>
</li>
<li>转化率CVR(conversion rate), 一种CPA衡量指标<br>$$CVR= \frac{Count_{conversion}}{Count_{click}}$$<ul>
<li>分母是广告的实际点击次数</li>
<li>分子是广告的转化次数,不同场景对转化成功的定义不同,比如手机号码注册用户为一次有效转化,那么这里CVR统计的就是所有点击了广告的人中有多少进行了实际的手机号注册</li>
</ul>
</li>
</ul>
<h3 id="预估CTR-CVR的模型"><a href="#预估CTR-CVR的模型" class="headerlink" title="预估CTR,CVR的模型"></a>预估CTR,CVR的模型</h3><h4 id="人工特征工程-LR"><a href="#人工特征工程-LR" class="headerlink" title="人工特征工程+LR"></a>人工特征工程+LR</h4><ul>
<li>人工提取当前广告的特征</li>
<li>LR模型预估用户是否会点击该广告或者注册该网站(二分类)</li>
</ul>
<h4 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h4><ul>
<li>参考博客: <a href="https://www.jianshu.com/p/96173f2c2fb4" target="_blank" rel="noopener">https://www.jianshu.com/p/96173f2c2fb4</a></li>
<li>Facebook paper中的一个例子<img src="/Notes/RecommenderSystem/RS——CTR-CVR的预估模型/GBDT+LR.png">
<ul>
<li>图中的GBDT只包含两棵树,实际上使用时可包含更多</li>
<li>LR的特征数量(样本维度)就是所有树的叶节点树,样本落到当前叶节点则,当前叶节点对应的特征值为1,否则为0</li>
<li>用于LR训练的特征维度共num_trees * num_leaves(也就是所有树叶节点的总数)</li>
<li>由于有多棵树,每个原始样本在每棵树都会落到一个叶节点上,所以得到的新样本中可能有很多个特征值为1</li>
<li>实践中原始输出时可能维度是树的棵数,每个数表示当前树中样本落到第几个叶节点上,对每棵树分别使用OneHot编码即可的到上面的 (num_trees * num_leaves) 维数据</li>
</ul>
</li>
</ul>
<h4 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h4><ul>
<li>参考博客<a href="/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94FMM%E6%A8%A1%E5%9E%8B.html">RS——FM-因子分解机</a></li>
</ul>
<h4 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h4><ul>
<li>参考博客<a href="/Notes/RecommenderSystem/RS%E2%80%94%E2%80%94FMM%E6%A8%A1%E5%9E%8B.html">RS——FMM模型</a></li>
</ul>
]]></content>
      <tags>
        <tag>RS</tag>
        <tag>ML</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——XGBoost-推导过程</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94XGBoost-%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B.html</url>
    <content><![CDATA[<p><em>XGBoost,全称: Extreme Gradient Boosting</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li>CART回归树是XGBoost的基分类器</li>
</ul>
<hr>
<h3 id="XGBoost模型推导-假设回归树的结构确定"><a href="#XGBoost模型推导-假设回归树的结构确定" class="headerlink" title="XGBoost模型推导(假设回归树的结构确定)"></a>XGBoost模型推导(假设回归树的结构确定)</h3><h4 id="模型-F-的加法定义："><a href="#模型-F-的加法定义：" class="headerlink" title="模型\(F\)的加法定义："></a>模型\(F\)的加法定义：</h4><p>$$<br>\begin{align}<br>F(x;w) = \sum_{k=0}^{K} f_{k}(x;w_{k})<br>\end{align}<br>$$</p>
<ul>
<li>其中, \(x\)为输入样本, \(f_{k}\)为分类回归树,可以是分类回归树空间中的任意一棵树, \(w_{k}\)是树\(f_{k}\)的参数</li>
</ul>
<h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><p>$$<br>\begin{align}<br>L^{t} &amp;= \sum_{i=1}^{n}l(y_{i},\hat{y}_{i}^{t}) + \Omega(f_{t}) \\<br>&amp;=  \sum_{i=1}^{n}l(y_{i},\hat{y}_{i}^{t-1} + f_{t}(x_{i})) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2<br>\end{align}<br>$$</p>
<ul>
<li>\(L^{t}\): 第 \(t\) 轮迭代的损失函数</li>
<li>\(l(y_{i},\hat{y}_{i}^{t})\) 表示单个样本的损失函数定义</li>
<li>\(\hat{y}_{i}^{t} = \hat{y}_{i}^{t-1} + f_{t}(x_{i})\) 表示第 \(t\) 轮 \(x_{i}\) 样本的预测值<ul>
<li>加法模型: 第 \(t\) 轮预测值 = 第 \(t-1\) 轮预测值 + 第 \(t\) 棵(轮)决策树的预测值</li>
</ul>
</li>
<li>\(\Omega(f_{t})\) 表示正则项<ul>
<li>\(n\) 表示样本的个数</li>
<li>\(T\) 表示<strong>叶子结点的个数</strong></li>
<li>\(w_{j}\) 表示<strong>叶节点分数</strong>, 即任意样本落到第 \(t\) 棵(轮)决策树的第 \(j\) 叶子结点时的预测值(可称为第 \(t\) 棵(轮)决策树的第 \(j\) 叶子结点的预测值)</li>
<li>\(\gamma\)和\(\lambda\)都是正则项参数</li>
</ul>
</li>
</ul>
<h4 id="第-t-轮训练的目标"><a href="#第-t-轮训练的目标" class="headerlink" title="第 \(t\) 轮训练的目标"></a>第 \(t\) 轮训练的目标</h4><ul>
<li>找到一个最优的分类器 \(f_t^{\star}(x)\), 满足<br>$$<br>\begin{align}<br>f_t^{\star}(x) &amp;= \arg\max_{f_t(x)}L^{t} \\<br>&amp;= \arg\max_{f_t(x)}\left(\sum_{i=1}^{n}l(y_{i},\hat{y}_{i}^{t}) + \Omega(f_{t})\right) \\<br>&amp;= \arg\max_{f_t(x)}\left(\sum_{i=1}^{n}l(y_{i},\hat{y}_{i}^{t-1} + f_{t}(x_{i})) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2 \right)<br>\end{align}<br>$$</li>
</ul>
<h4 id="最小化损失函数推导"><a href="#最小化损失函数推导" class="headerlink" title="最小化损失函数推导"></a>最小化损失函数推导</h4><p><em>损失函数<strong>二阶泰勒展开</strong></em></p>
<ul>
<li><p>回忆传统泰勒展开:<br>$$<br>\begin{aligned}<br>f(x) &amp;= f(x_0) + f’(x_0)(x-x_0) + \frac{f’’(x_0)}{2!}(x-x_0)^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n \\<br>&amp; = \sum\limits_{n=0}^{\infty}\frac{f^{(n)}x_0}{n!}(x - x_0)^n<br>\end{aligned}<br>$$</p>
</li>
<li><p>二阶泰勒展开公式<br>$$<br>\begin{align}<br>f(x+\Delta x) \approx f(x) + f’(x)\Delta x + \frac{1}{2}f’’(x)\Delta x^2<br>\end{align}<br>$$</p>
</li>
<li><p>在我们的场景中,令</p>
<ul>
<li>\(x = \hat{y}_{i}^{t-1}\)</li>
<li>\(\Delta x = f_{t}(x_{i})\)</li>
</ul>
</li>
<li><p>则有对单个样本能得到<br>$$<br>\begin{align}<br>l(y_{i},\hat{y}_{i}^{t-1} + f_{t}(x_{i})) \approx l(y_i,\hat{y}_i^{t-1}) + l’(y_i,\hat{y}_i^{t-1})f_t(x_i) + \frac{1}{2}l’’(y_i,\hat{y}_i^{t-1})f_t^2(x_i)<br>\end{align}<br>$$</p>
<ul>
<li>\(l’(y_i,\hat{y}_i^{t-1})\) 是 \(l(y_i,\hat{y}_i)\) 对 \(\hat{y}_i\) 的一阶导数在 \(\hat{y}_i = \hat{y}_i^{t-1}\) 处的值 </li>
<li>\(l’’(y_i,\hat{y}_i^{t-1})\) 是 \(l(y_i,\hat{y}_i)\) 对 \(\hat{y}_i\) 的二阶导数在 \(\hat{y}_i = \hat{y}_i^{t-1}\) 处的值 </li>
</ul>
</li>
<li><p>我们第 \(t\) 轮的目标是找到一个最优的分类器 \(f_t^{\star}(x)\)最小化损失函数 \(L^{t}\)<br>$$<br>\begin{align}<br>L^{t} \approx \sum_{i=1}^{n}\left(l(y_i,\hat{y}_i^{t-1}) + l’(y_i,\hat{y}_i^{t-1})f_t(x_i) + \frac{1}{2}l’’(y_i,\hat{y}_i^{t-1})f_t^2(x_i)\right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2<br>\end{align}<br>$$</p>
</li>
<li><p>显然上面式子中的 \(l(y_i,\hat{y}_i^{t-1})\) 与 \(f_t(x_i)\) 无关,可以移除,于是有<br>$$<br>\begin{align}<br>L^{t} \approx \sum_{i=1}^{n}\left(l’(y_i,\hat{y}_i^{t-1})f_t(x_i) + \frac{1}{2}l’’(y_i,\hat{y}_i^{t-1})f_t^2(x_i)\right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2<br>\end{align}<br>$$</p>
</li>
<li><p>令:</p>
<ul>
<li>\(g_i = l’(y_i,\hat{y}_i^{t-1})\) 为 \(l(y_i,\hat{y}_i)\) 对 \(\hat{y}_i\) 的一阶导数在 \(\hat{y}_i = \hat{y}_i^{t-1}\) 处的值 </li>
<li>\(h_i = l’’(y_i,\hat{y}_i^{t-1})\) 是 \(l(y_i,\hat{y}_i)\) 对 \(\hat{y}_i\) 的二阶导数在 \(\hat{y}_i = \hat{y}_i^{t-1}\) 处的值 </li>
</ul>
</li>
<li><p>则有:<br>$$<br>\begin{align}<br>L^{t} \approx \sum_{i=1}^{n}\left(g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2<br>\end{align}<br>$$</p>
</li>
<li><p>做一个重要的<strong>转换</strong>:<br>$$f_t(x_i) = w_j, \quad s.t. \ x_i \in I_j$$</p>
<ul>
<li>上面的式子前半部分成立的条件是 \(x_i\) 落在叶子结点 \(j\) 上</li>
<li>我们将条件\(x_i\) 落在叶子结点 \(j\) 上表示为 \(\ x_i \in I_j\)</li>
</ul>
</li>
<li><p>于是: 我们可以<strong>将前面对样本的累加变成对叶子结点的累加</strong><br>$$<br>\begin{align}<br>L^{t} &amp;\approx \sum_{j=1}^{T}\left((\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i)w_j^2\right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2 \\<br>&amp;\approx  \sum_{j=1}^{T}\left((\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w_j^2\right) + \gamma T<br>\end{align}<br>$$</p>
</li>
<li><p>令:</p>
<ul>
<li>\(G_j = \sum_{i \in I_j}g_i\)</li>
<li>\(H_j = \sum_{i \in I_j}h_i\)</li>
</ul>
</li>
<li><p>则有<br>$$<br>\begin{align}<br>L^{t} \approx  \sum_{j=1}^{T}\left(G_jw_j + \frac{1}{2}(H_j + \lambda)w_j^2\right) + \gamma T<br>\end{align}<br>$$</p>
</li>
<li><p>\(L^t\) 对 \(w_j\) 求偏导有<br>$$<br>\begin{align}<br>\frac{\partial L^{t}}{\partial w_j} = G_j + (H_j + \lambda)w_j<br>\end{align}<br>$$</p>
</li>
<li><p>令导数 \(\frac{\partial L^{t}}{\partial w_j} = 0\) 可得<br>$$<br>\begin{align}<br>w_j^{\star} = -\frac{G_j}{H_j+\lambda}<br>\end{align}<br>$$</p>
</li>
<li><p>同时<br>$$<br>\begin{align}<br>L^{\star^t} &amp;\approx min(L^t) \\<br>&amp;\approx \sum_{j=1}^{T}\left(G_j\cdot (-\frac{G_j}{H_j+\lambda}) + \frac{1}{2}(H_j + \lambda)\cdot(-\frac{G_j}{H_j+\lambda})^2\right) + \gamma T \\<br>&amp;\approx -\frac{1}{2}\sum_{j=1}^{T}\left(\frac{G_j^2}{H_j+\lambda}\right) + \gamma T<br>\end{align}<br>$$</p>
</li>
<li><p>目标函数的计算示例: 目标分数越小越好</p>
<img src="/Notes/ML/Models/ML——XGBoost-推导过程/example_of_xgboost.png">

</li>
</ul>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul>
<li>在<strong>第 \(t\) 轮回归树的结构确定后</strong>,我们得到的<strong>最优叶节点分数</strong>与结构无关 \(w_j^{\star} = -\frac{G_j}{H_j+\lambda}\)</li>
<li>最小损失与第 \(t\) 轮回归树的结构的复杂度(叶节点数量)相关</li>
<li>我们<strong>还需要</strong>确定第 \(t\) 轮回归树的结构</li>
</ul>
<hr>
<h3 id="回归树的结构确定"><a href="#回归树的结构确定" class="headerlink" title="回归树的结构确定"></a>回归树的结构确定</h3><h4 id="普通决策树树结构的确定"><a href="#普通决策树树结构的确定" class="headerlink" title="普通决策树树结构的确定"></a>普通决策树树结构的确定</h4><ul>
<li>详细情况可参考: <a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94DT-%E5%86%B3%E7%AD%96%E6%A0%91.html">ML——DT-决策树</a></li>
</ul>
<h5 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h5><ul>
<li>最大化信息增益来选择分裂特征</li>
</ul>
<h5 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h5><ul>
<li>最大化信息增益比来选择分裂特征</li>
</ul>
<h5 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h5><h6 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h6><ul>
<li>最小化基尼指数来选择分裂特征</li>
</ul>
<h6 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h6><ul>
<li>最小化平方误差来选择分裂特征</li>
</ul>
<h4 id="XGBoost结点分裂前后的信息增益"><a href="#XGBoost结点分裂前后的信息增益" class="headerlink" title="XGBoost结点分裂前后的信息增益"></a>XGBoost结点分裂前后的信息增益</h4><ul>
<li>信息增益应该与前面的损失函数相关, 损失越小越好</li>
<li>原始损失函数为<br>$$<br>\begin{align}<br>L^{\star^t} \approx -\frac{1}{2}\sum_{j=1}^{T}\left(\frac{G_j^2}{H_j+\lambda}\right) + \gamma T<br>\end{align}<br>$$</li>
<li>显然,要使得损失函数最小,我们需要使得下面的式子最大:<br>$$<br>\begin{align}<br>\frac{G_j^2}{H_j+\lambda}<br>\end{align}<br>$$</li>
<li>一个结点分裂后的信息为<br>$$<br>\begin{align}<br>\frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda}<br>\end{align}<br>$$<ul>
<li>左子树的信息 + 右子树的信息</li>
</ul>
</li>
<li>一个结点分裂前信息为<br>$$<br>\begin{align}<br>\frac{(G_L + G_R)^2}{H_L+ H_R + \lambda}<br>\end{align}<br>$$</li>
<li>从而我们定义一个结点分列前后的信息增益为:<br>$$<br>\begin{align}<br>Gain = \frac{G_L^2}{H_L+ \lambda} + \frac{G_R^2}{H_R+ \lambda} - \frac{(G_L + G_R)^2}{H_L+ H_R + \lambda} - \gamma<br>\end{align}<br>$$<ul>
<li>Gain 越大说明当前结点的当前分裂方式越好</li>
<li>其中 \(\gamma\) 是一个超参数用于防止<strong>过拟合</strong>,可以理解为有两层含义:<ul>
<li>用于对叶节点数目进行控制</li>
<li>用于增大分裂前后Gain的阈值</li>
<li>事实上 \(\gamma\) 不是凭空来的,开始的定义中 \(\gamma\) 就是L1正则化(叶节点数量)的系数, 这里分裂后叶节点数量多了1个, 我们希望树的叶节点越少越好(模型越简单越好),而这个的信息增益越大越好, 所以减去 \(\gamma\) 值,防止结点分裂得太多</li>
<li>对于每个特征的每个分裂点, \(\gamma\) 值都相同,所以 \(\gamma\) 值对特征和分裂点的选择没有影响,只是影响了某个结点是否能够被分裂(信息增益太小或者为负时我们可以选择不分裂)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="XGBoost树节点分裂"><a href="#XGBoost树节点分裂" class="headerlink" title="XGBoost树节点分裂"></a>XGBoost树节点分裂</h4><h5 id="精确分裂算法"><a href="#精确分裂算法" class="headerlink" title="精确分裂算法"></a>精确分裂算法</h5><ul>
<li><strong>单个</strong>叶节点<strong>精确</strong>计算信息增益<strong>分裂流程</strong>(特征和特征分裂取值的选择, 假设一共m个特征)<ul>
<li>注意: 每次只对叶节点分裂,已经分裂了的就不是叶节点了,不能二次分裂</li>
<li>原图来自: <a href="https://www.cnblogs.com/massquantity/p/9794480.html" target="_blank" rel="noopener">https://www.cnblogs.com/massquantity/p/9794480.html</a><img src="/Notes/ML/Models/ML——XGBoost-推导过程/exact_greedy_algorithm_for_split_finding.png">

</li>
</ul>
</li>
</ul>
<h5 id="近似分裂算法"><a href="#近似分裂算法" class="headerlink" title="近似分裂算法"></a>近似分裂算法</h5><ul>
<li>将每个特征的取值划分为多个分位点</li>
<li>每次考察特征时值考察分位点,减少计算复杂度</li>
<li>其他的步骤与前面的精确分裂算法相同,包括分数计算和选择最大分数等<img src="/Notes/ML/Models/ML——XGBoost-推导过程/approximate_algorithm_for_split_finding.png">

</li>
</ul>
<h5 id="分桶策略与GBDT的不同"><a href="#分桶策略与GBDT的不同" class="headerlink" title="分桶策略与GBDT的不同"></a>分桶策略与GBDT的不同</h5><ul>
<li>传统的GBDT分桶时每个样本的权重都是相同的</li>
<li>XGBoost中每个样本的权重为损失函数在该样本点的二阶导数(对不同的样本,计算得到的损失函数的二阶导数是不同的), 这里优点AdaBoost的思想,重点关注某些样本的感觉</li>
<li>这里影响的是划分点的位置(我们划分划分点[桶]时都是均匀划分样本到桶里面,当不同样本的权重不同时,每个桶里面的样本数量可能会不同)</li>
<li>下图是一个示例<img src="/Notes/ML/Models/ML——XGBoost-推导过程/split_point_method_weighted_in_xgboost.png"></li>
<li>详细推导解释,为什么选择损失函数在当前样本的二阶导数作为权重?<br>$$<br>\begin{align}<br>L^{t} &amp;\approx \sum_{i=1}^{n}\left(g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2 \\<br>&amp;\approx \sum_{i=1}^{n}\frac{1}{2}h_i\left(2\frac{g_i}{h_i}f_t(x_i) + f_t^2(x_i)\right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2 \\<br>&amp;\approx \sum_{i=1}^{n}\frac{1}{2}h_i\left(f_t(x_i) - \frac{g_i}{h_i}\right)^2 + \sum_{i=1}^{n}\left ( \frac{g_i^2}{2h_i} - 2g_if_t(x_i) \right) + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^2 \\<br>\end{align}<br>$$<ul>
<li>上面的式子中第二项在原文中用constant来表示,可以被看成某种正则项</li>
<li>第一项是一个平方误差表达式,样本 \(x_i\) 对应的输出值为 \(\frac{g_i}{h_i}\), 而样本权重就是 \(h_i\)</li>
<li>权重代表概率,概率越大说明该点出现的次数或者该点附近的值出现的次数就越多.</li>
</ul>
</li>
<li>XGBoost分桶流程<ul>
<li>[待更新]</li>
</ul>
</li>
</ul>
<h4 id="XGBoost整棵树分裂流程"><a href="#XGBoost整棵树分裂流程" class="headerlink" title="XGBoost整棵树分裂流程"></a>XGBoost整棵树分裂流程</h4><ul>
<li>初始化\(f^0(x)\)</li>
<li>for t=1 to M:<ul>
<li>计算损失函数对每个训练样本点的一阶导数 \(g_i\) ,二阶导数 \(h_i\)</li>
<li>递归对叶子节点运用树分裂算法生成一颗决策树 \(f^t(x)\),    <ul>
<li>(这一步包括叶子节点的分数 \(w_j\) 也都确定下来)</li>
<li>(这一步可以使用近似分裂算法加快训练速度)</li>
</ul>
</li>
<li>把新生成的决策树加入到模型中, \(\hat{y}^t = \hat{y}^{t-1} + f_m(x)\)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="传统GDBT与XGBoost的比较"><a href="#传统GDBT与XGBoost的比较" class="headerlink" title="传统GDBT与XGBoost的比较"></a>传统GDBT与XGBoost的比较</h3><ul>
<li>参考博客: <a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94XGBoost-vs-%E4%BC%A0%E7%BB%9FGBDT.html">ML——XGBoost-vs-传统GBDT</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——BN-LN-IN-GN-LRN-WN</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94BN-LN-IN-GN-LRN-WN.html</url>
    <content><![CDATA[<p><em>本文介绍各种不同的Normalization方法</em></p>
<ul>
<li>BN: Batch Normalization</li>
<li>LN: Layer Normalization</li>
<li>IN: Instance Normalization</li>
<li>GN: Group Normalization</li>
<li>LRN: Local Response Normalization</li>
<li>WN: Weight Normalization<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

</li>
</ul>
<hr>
<h3 id="Normalization总体介绍"><a href="#Normalization总体介绍" class="headerlink" title="Normalization总体介绍"></a>Normalization总体介绍</h3><ul>
<li>BN,LN,IN的归一化的步骤都是使用下面的公式:<br>$$<br>\begin{align}<br>u &amp;= \frac{1}{m}\sum_{k\in S}x_k \\<br>\sigma &amp;= \sqrt{\frac{1}{m}\sum_{k\in S}(x_k-u) + \epsilon} \\<br>\hat{x} &amp;= \frac{1}{\sigma}(x-u) \\<br>y &amp;= \gamma \hat{x} + \beta<br>\end{align}<br>$$<ul>
<li>\(u\) 为均值</li>
<li>\(\sigma\) 为标准差</li>
<li>\(\gamma\) 和 \(beta\) 是可以训练的参数</li>
<li>\(\epsilon\) 是平滑因子, 防止分母为0</li>
<li>BN,LN,IN三种不同的归一化方法, 对应的数据集 \(S\)不同<ul>
<li>BN对同一批数据进行归一化, 不管其他神经元, 只针对某个神经元的Mini Batch个样本输出值做归一化</li>
<li>LN对同一个样本的同一层输出进行归一化, 不依赖其他样本, 每次只依赖当前样本本身</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h3><p><em>Batch Normalization</em></p>
<ul>
<li><p>对一批数据实行归一化</p>
</li>
<li><p>对某个具体的神经元的Mini Batch个样本输出做归一化, 与其他神经元的输出无关</p>
<img src="/Notes/DL/DL——BN-LN-IN-GN-LRN-WN/Batch_Normalization.jpg"></li>
<li><p>代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mu = np.mean(x,axis=0)</span><br><span class="line">sigma2 = np.var(x,axis=0)</span><br><span class="line">x_hat = (x-mu)/np.sqrt(sigma2+eps)</span><br><span class="line">out = gamma*x_hat + beta</span><br></pre></td></tr></table></figure>
</li>
<li><p>特别说明：TensorFlow在BN训练过程中（trainable=True）使用的是当前批次的均值和方差归一化，同时将均值和方法以滑动平均的方式更新并存储下来。最终，在预估/推断(trainable=False)阶段，则直接使用滑动平均的结果。</p>
<ul>
<li>隐藏问题：当使用BN时，如果更新的轮次不够（训练global step太少），会导致均值和方差滑动平均的结果并未贴近真实的均值和方差，会导致训练时模型输出正常，预测时模型输出异常的情况，且这种问题比较隐晦，难以排查</li>
<li>解决方案：<ul>
<li>当训练的轮次较少时，要注意动量不要设置太大，否则更新不足，此时设置小的动量可以缓解BN均值方差更新不足的问题（不建议使用这种方式，原因是：一般来说，动量太小会导致最终的均值方差仅被最近的Batch决定，模型效果波动大）</li>
<li>建议在使用BN时，设置较大的动量，且注意保证足够的训练轮次，充分更新动量和方法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h3><p><em>Layer Normalization</em></p>
<ul>
<li>对单个训练样本的同一层所有神经元的输入做归一化</li>
<li>与其他样本无关<img src="/Notes/DL/DL——BN-LN-IN-GN-LRN-WN/Layer_Normalization.jpg">


</li>
</ul>
<h4 id="BN的作用和说明"><a href="#BN的作用和说明" class="headerlink" title="BN的作用和说明"></a>BN的作用和说明</h4><ul>
<li>Batch Normalization把网络每一层的输出Y固定在一个变化范围的作用</li>
<li>BN都能显著提高训练速度</li>
<li>BN可以解决梯度消失问题<ul>
<li>归一化操作将每一层的输出从饱和区拉到了非饱和区(导数),从而解决了梯度消失问题</li>
</ul>
</li>
<li>普通的优化器加上BN后效果堪比Adam<br>  $$ ReLU + Adam \approx ReLU + SGD + BN$$</li>
<li>如果对于具有<strong>分布极不平衡</strong>的<strong>二分类</strong>测试任务, <strong>不要使用BN</strong></li>
<li>BN一定程度上有归一化作用<ul>
<li>BN本身就能提高网络模型的泛化能力</li>
<li>使用BN后,不用太依赖Dropout, L2正则化等,可以将L2正则化的参数变小一点</li>
</ul>
</li>
</ul>
<h3 id="WN"><a href="#WN" class="headerlink" title="WN"></a>WN</h3><p><em>Weight Normalization</em></p>
<ul>
<li>对参数做归一化</li>
<li>与数据无关<img src="/Notes/DL/DL——BN-LN-IN-GN-LRN-WN/Weight_Normalization.jpg">

</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="BN和WN对比"><a href="#BN和WN对比" class="headerlink" title="BN和WN对比"></a>BN和WN对比</h4><ul>
<li>BN是对对一个mini batch的数据在同一个神经元计算均值和方差</li>
<li>WN对网络的网络权值 W 进行归一化(L2归一化)</li>
</ul>
<h4 id="BN和LN对比"><a href="#BN和LN对比" class="headerlink" title="BN和LN对比"></a>BN和LN对比</h4><ul>
<li>BN高度依赖于mini-batch的大小，实际使用中会对mini-Batch大小进行约束，不适合类似在线学习（mini-batch为1）情况；</li>
<li>BN不适用于RNN网络中normalize操作：<ul>
<li>BN实际使用时需要计算并且保存某一层神经网络mini-batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；</li>
<li>但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其的sequence长很多，这样training时，计算很麻烦</li>
</ul>
</li>
<li>但LN可以有效解决上面这两个问题</li>
<li>LN适用于LSTM的加速，但用于CNN加速时并没有取得比BN更好的效果</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch——关于Variable类和Tensor类的类型判断</title>
    <url>/Notes/PyTorch/PyTorch%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8EVariable%E7%B1%BB%E5%92%8CTensor%E7%B1%BB%E7%9A%84%E7%B1%BB%E5%9E%8B%E5%88%A4%E6%96%AD.html</url>
    <content><![CDATA[<hr>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><h4 id="requires-grad-True"><a href="#requires-grad-True" class="headerlink" title="requires_grad=True"></a><code>requires_grad=True</code></h4><p><em>等价于<code>requires_grad=a</code>, <code>a</code>为任意非0整数,不能为浮点数</em><br><em>浮点数会报错: TypeError: integer argument expected, got float</em></p>
<ul>
<li><p>测试代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">tensor = torch.ones(1)</span><br><span class="line">variable = Variable(tensor, requires_grad=True)</span><br><span class="line">print(tensor)</span><br><span class="line">print(variable)</span><br><span class="line">print(&quot;type1: &quot;, type(tensor), type(variable))</span><br><span class="line">print(tensor.data)</span><br><span class="line">print(variable.data)</span><br><span class="line">print(&quot;type2: &quot;, type(tensor.data), type(variable.data))</span><br><span class="line">print(tensor.data.numpy())</span><br><span class="line">print(variable.data.numpy())</span><br><span class="line">print(&quot;type3: &quot;, type(tensor.data.numpy()), type(variable.data.numpy()))</span><br><span class="line">print(tensor.numpy())</span><br><span class="line">print(variable.numpy())</span><br><span class="line">print(&quot;type4: &quot;, type(tensor.numpy()), type(variable.numpy()))</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">tensor([1.])</span><br><span class="line">tensor([1.], requires_grad=True)</span><br><span class="line">(&apos;type1: &apos;, &lt;class &apos;torch.Tensor&apos;&gt;, &lt;class &apos;torch.Tensor&apos;&gt;)</span><br><span class="line">tensor([1.])</span><br><span class="line">tensor([1.])</span><br><span class="line">(&apos;type2: &apos;, &lt;class &apos;torch.Tensor&apos;&gt;, &lt;class &apos;torch.Tensor&apos;&gt;)</span><br><span class="line">[1.]</span><br><span class="line">[1.]</span><br><span class="line">(&apos;type3: &apos;, &lt;type &apos;numpy.ndarray&apos;&gt;, &lt;type &apos;numpy.ndarray&apos;&gt;)</span><br><span class="line">[1.]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/home/jiahong/JupyterWorkspace/test.py&quot;, line 16, in &lt;module&gt;</span><br><span class="line">    print(variable.numpy())</span><br><span class="line">RuntimeError: Can&apos;t call numpy() on Variable that requires grad. Use var.detach().numpy() instead.</span><br></pre></td></tr></table></figure>
</li>
<li><p>从上面的测试用例可以看出:</p>
<ul>
<li><code>Variable</code>和<code>Tensor</code>在判断类型时都是<code>torch.Tensor</code><ul>
<li><code>type(tensor) == type(variable) == torch.Tensor</code> </li>
</ul>
</li>
<li>几乎所有操作都相同<ul>
<li><code>tensor.data == variable.data</code></li>
<li><code>tensor.data.numpy() == varible.data.numpy()</code></li>
</ul>
</li>
<li>直接输出变量结果不相同<ul>
<li><code>tensor</code>输出时没有<code>requires_grad=True</code></li>
<li><code>variable</code>输出时有<code>requires_grad=True</code></li>
</ul>
</li>
<li><code>variable</code>不能直接调用函数<code>variable.numpy()</code>,会报异常<ul>
<li>异常描述为: 当前<code>Variable</code>变量要求<code>requires grad</code>,也就是<code>requires_grad</code>属性为真时,变量不能直接使用</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="requires-grad-False"><a href="#requires-grad-False" class="headerlink" title="requires_grad=False"></a><code>requires_grad=False</code></h4><p><em>等价于<code>requires_grad=0</code></em><br><em>不等价于<code>requires_grad=None</code>, <code>None</code>会报错: TypeError: an integer is required</em></p>
<ul>
<li><p>测试代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">tensor = torch.ones(1)</span><br><span class="line">variable = Variable(tensor, requires_grad=False)</span><br><span class="line">print(tensor)</span><br><span class="line">print(variable)</span><br><span class="line">print(&quot;type1: &quot;, type(tensor), type(variable))</span><br><span class="line">print(tensor.data)</span><br><span class="line">print(variable.data)</span><br><span class="line">print(&quot;type2: &quot;, type(tensor.data), type(variable.data))</span><br><span class="line">print(tensor.data.numpy())</span><br><span class="line">print(variable.data.numpy())</span><br><span class="line">print(&quot;type3: &quot;, type(tensor.data.numpy()), type(variable.data.numpy()))</span><br><span class="line">print(tensor.numpy())</span><br><span class="line">print(variable.numpy())</span><br><span class="line">print(&quot;type4: &quot;, type(tensor.numpy()), type(variable.numpy()))</span><br><span class="line"></span><br><span class="line"># Output:</span><br><span class="line">tensor([1.])</span><br><span class="line">tensor([1.])</span><br><span class="line">(&apos;type1: &apos;, &lt;class &apos;torch.Tensor&apos;&gt;, &lt;class &apos;torch.Tensor&apos;&gt;)</span><br><span class="line">tensor([1.])</span><br><span class="line">tensor([1.])</span><br><span class="line">(&apos;type2: &apos;, &lt;class &apos;torch.Tensor&apos;&gt;, &lt;class &apos;torch.Tensor&apos;&gt;)</span><br><span class="line">[1.]</span><br><span class="line">[1.]</span><br><span class="line">(&apos;type3: &apos;, &lt;type &apos;numpy.ndarray&apos;&gt;, &lt;type &apos;numpy.ndarray&apos;&gt;)</span><br><span class="line">[1.]</span><br><span class="line">[1.]</span><br><span class="line">(&apos;type4: &apos;, &lt;type &apos;numpy.ndarray&apos;&gt;, &lt;type &apos;numpy.ndarray&apos;&gt;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>从上面的测试用例可以看出:</p>
<ul>
<li>当<code>variable</code>变量的<code>requires_grad=False</code>时,<code>variable</code>完全退化为<code>tensor</code><ul>
<li>直接输出变量时没有<code>requires_grad=False</code>属性</li>
<li>可以直接使用<code>variable.numpy()</code>函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Variable的三种等价定义"><a href="#Variable的三种等价定义" class="headerlink" title="Variable的三种等价定义"></a><code>Variable</code>的三种等价定义</h4><p><em>下面三种定义的<code>Variable</code>类型变量<code>varible</code>等价</em></p>
<ul>
<li><p><code>requires_grad=False</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">variable = Variable(tensor, requires_grad=False)</span><br></pre></td></tr></table></figure>
</li>
<li><p>没有<code>requires_grad</code>参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">variable = Variable(tensor)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>requires_grad=True</code>,然后<code>variable = variable.detach()</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">variable = Variable(tensor, requires_grad=True)</span><br><span class="line">variable = variable.detach()</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面三种定义都等价于原始的<code>tensor</code></p>
<ul>
<li>这里的等价并未经过详细测试,但是至少以下方面等价:<ul>
<li>自身类型相同<code>type</code>, 类型为<code>torch.Tensor</code></li>
<li>可以调用属性<code>.data</code>,类型为<code>torch.Tensor</code></li>
<li>可以调用<code>.grad</code>,只不过都为<code>None</code></li>
<li>直接输出对象完全相同,都不包含<code>requires_grad=True</code>属性</li>
<li>可以调用相同的函数<code>.numpy()</code>, 类型为<code>numpy.ndarray</code></li>
<li>可以调用相同的函数<code>.data.numpy()</code>, 类型为<code>numpy.ndarray</code></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——Attention</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94Attention.html</url>
    <content><![CDATA[<p><em>本文主要介绍Attention的原理和变种</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>参考博客(其中有些错误,本文已经修正): <a href="https://zhuanlan.zhihu.com/p/47063917" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47063917</a></li>
<li>参考论文: <a href="https://arxiv.org/pdf/1811.05544.pdf" target="_blank" rel="noopener">An Introductory Survey on Attention Mechanisms in NLP Problems</a></li>
<li>强烈推荐一篇写得非常好的动画讲解: <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">基于Attention的Seq2Seq可视化神经机器翻译机</a></li>
<li>另一篇不错的Attention和Transformer讲解<a href="https://www.sohu.com/a/226596189_500659" target="_blank" rel="noopener">自然语言处理中的自注意力机制(Self-Attention Mechanism)</a></li>
<li>这个博客中有李宏毅老师的讲解：<a href="https://zhuanlan.zhihu.com/p/576380058" target="_blank" rel="noopener">Self Attention详解——知乎</a></li>
</ul>
<hr>
<h3 id="RNN的局限-Encoder-Decoder模型"><a href="#RNN的局限-Encoder-Decoder模型" class="headerlink" title="RNN的局限: Encoder-Decoder模型"></a>RNN的局限: Encoder-Decoder模型</h3><ul>
<li>RNN 结构<img src="/Notes/DL/DL——Attention/rnn_overview.jpg"></li>
<li>Encoder-Decoder结构<img src="/Notes/DL/DL——Attention/encoder_decoder_overview.jpg">

</li>
</ul>
<hr>
<h3 id="Attention机制的引入"><a href="#Attention机制的引入" class="headerlink" title="Attention机制的引入"></a>Attention机制的引入</h3><ul>
<li>Attention机制的根本优势在于对不同的</li>
<li>引入Attention前后的Encoder和Decoder对比图<img src="/Notes/DL/DL——Attention/traditional_encoder_decoder_vs_attention.png">
<ul>
<li>使用 Attention 前: \(\vec{h_{t}^{out}} = f(\vec{h_{t-1}^{out}},\vec{y_{t-1}})\)</li>
<li>使用 Attention 后: \(\vec{h_{t}^{out}} = f(\vec{h_{t-1}^{out}},\vec{y_{t-1}}, \vec{c_{t}})\)<ul>
<li>\(\vec{c_{t}} = q(\vec{h_{1}^{in}}, \dots, \vec{h_{T}^{in}})\)</li>
<li>\(q\) 是个多层的运算,有多重不同实现,详情参考后面的讲解</li>
</ul>
</li>
</ul>
</li>
<li>动态图理解 Attention 机制<ul>
<li>图中线条越清晰说明对当前结点的影响越大,不清晰说明影响较小<img src="/Notes/DL/DL——Attention/attention_overview.gif">


</li>
</ul>
</li>
</ul>
<ul>
<li>进一步看结构图<img src="/Notes/DL/DL——Attention/attention_details.jpg"></li>
<li>上图中Encoder使用的是双层双向的RNN<ul>
<li>第一层倒序从后\(X_T\)到前\(X_1\)生成, 反方向编码器</li>
<li>第二层正序从前\(X_1\)到后\(X_T\)生成, 正方向编码器</li>
<li>二者combine为一个更高维度的向量, 这个更高维度的向量整个作为Encoder的隐藏层</li>
</ul>
</li>
<li>流程说明:<ul>
<li>利用 RNN 结构得到 Encoder中的 Hidden State (\(\vec{h_1}, \vec{h_2},\dots, \vec{h_T}\))</li>
<li>假设当前 Decoder 的Hidden State 是 \(\vec{s_{t-1}}\), 计算每一个 \(\vec{h_j}\) 与当前输入位置的关联性 \(e_{ij} = a(\vec{s_{t-1}}, \vec{h_j})\), 得到向量 \(\vec{e_t} = (a(\vec{s_{t-1}}, \vec{h_1}), \dots, a(\vec{s_{t-1}}, \vec{h_T})) \)<ul>
<li>这里的 \(a\) 是相关性的(函数)运算符, 常用的可以用向量内积(点成),加权点乘等<ul>
<li>内积点乘: \(e_{tj} = \vec{s_{t-1}}^T\cdot\vec{h_j}\)</li>
<li>加权点乘: \(e_{tj} = \vec{s_{t-1}}^TW\vec{h_j}\) (一般使用这个)</li>
<li>更复杂的: \(e_{tj} = \vec{v}^Ttanh(W_1\vec{s_{t-1}}^T + W_2\vec{h_j})\)</li>
</ul>
</li>
</ul>
</li>
<li>对 \(\vec{e_t}\) 进行 softmax 操作,将其归一化得到 Attention 的分布, \(\vec{\alpha_t} = softmax(\vec{e_t})\)</li>
<li>利用 \(\vec{\alpha_t}\), 我们可以进行加权求和得到相应的上下文向量(context verctor) \(\vec{c_t} = \sum_{j=1}^T\alpha_{tj}\vec{h_j}\)</li>
<li>计算 Decoder 的下一个 Hidden State \(\vec{s_t} = f_h(\vec{s_{t-1}}, \vec{y_{j-1}}, \vec{c_t})\)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Attention的变种"><a href="#Attention的变种" class="headerlink" title="Attention的变种"></a>Attention的变种</h3><p><em>这里的总结参考博客<a href="http://xtf615.com/2019/01/06/attention/" target="_blank" rel="noopener">Attention</a></em></p>
<ul>
<li>基于强化学习的注意力机制：选择性的Attend输入的某个部分</li>
<li>全局&amp;局部注意力机制：其中，局部注意力机制可以选择性的Attend输入的某些部分</li>
<li>多维度注意力机制：捕获不同特征空间中的Attention特征。</li>
<li>多源注意力机制：Attend到多种源语言语句</li>
<li>层次化注意力机制：word-&gt;sentence-&gt;document</li>
<li>注意力之上嵌一个注意力：和层次化Attention有点像。</li>
<li>多跳注意力机制：和前面两种有点像，但是做法不太一样。且借助残差连接等机制，可以使用更深的网络构造多跳Attention。使得模型在得到下一个注意力时，能够考虑到之前的已经注意过的词。</li>
<li>使用拷贝机制的注意力机制：在生成式Attention基础上，添加具备拷贝输入源语句某部分子序列的能力。</li>
<li>基于记忆的注意力机制：把Attention抽象成Query，Key，Value三者之间的交互；引入先验构造记忆库。</li>
<li>自注意力机制：自己和自己做attention(这里的自己只每个文档自身)，使得每个位置的词都有全局的语义信息，有利于建立长依赖关系。</li>
</ul>
<hr>
<h3 id="广义的Attention机制"><a href="#广义的Attention机制" class="headerlink" title="广义的Attention机制"></a>广义的Attention机制</h3><p><em>参考博客: <a href="https://www.sohu.com/a/226596189_500659" target="_blank" rel="noopener">https://www.sohu.com/a/226596189_500659</a></em></p>
<ul>
<li>Attention的本质: <ul>
<li>一个Attention函数可以被描述为<strong>一个把查询(Query)和键-值(Key-Value)对集合变换成输出(Attention Value)的映射</strong> </li>
<li>简单的讲就是一个把 (Query,[Key-Value]s) 映射成一个 Attention Value (输出)</li>
<li>An attention function can be described as Mapping aquery and a set of key-value pairs to an output<img src="/Notes/DL/DL——Attention/essence_of_attention.jpeg"></li>
</ul>
</li>
<li>表示成数学公式如下<img src="/Notes/DL/DL——Attention/math_of_attention.jpeg"></li>
<li>如上图所示,在计算 Attention 时主要分为三步<ul>
<li>第一步是将 Query 和每个 Key 进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等</li>
<li>第二步一般是使用一个 Softmax 函数对这些权重进行归一化</li>
<li>第三步将权重和相应的键值 Value 进行加权求和得到最后的 Attention</li>
</ul>
</li>
<li>Attention过程还可以大致分为两步理解:<ul>
<li><ol>
<li><strong>将Query和Key经过相似度计算(某种数学运算)的结果通过 Softmax 激活函数激活得到上文所说的权重得分布 \(\vec{\alpha} = (\alpha_1\dots \alpha_n)\)</strong><ul>
<li>变换一般包括 <ul>
<li>点乘(Dot): \(f(Q,K_i) = Q^TK_i\)</li>
<li>加权点乘(General): \(f(Q,K_i) = Q^TW_{\alpha}K_i\), \(W_{\alpha}\) 对不同的 \(\alpha_i\)</li>
<li>拼接(Concat): \(f(Q,K_i) = W[Q^T;K_i]\)</li>
<li>感知机(Perceptorn): \(f(Q,K) = \boldsymbol{v}^T tanh(W_Q, UK_i)\)</li>
</ul>
</li>
<li>Query和Key在不同任务中是不同的东西<ul>
<li>在阅读理解中: Query指的是问题,Key指的是文档</li>
<li>在简单的文本分类中: Query和Key可以是同一个句子(这也就是Self Attention), 也就是句子自己和自己做两个词之间的相似度计算的到权重分布</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><ol start="2">
<li><strong>将权重分布 \(\vec{\alpha} = (\alpha_1\dots \alpha_n)\) 对Value做加权求和得到最终的特征表示</strong><ul>
<li>在当前<strong>NLP</strong>任务中, 基本上 <strong>Key == Value</strong></li>
<li>阅读理解任务中, Value指的就是前面的Key, 是文档</li>
<li>简单文本分类中, Value指句子</li>
<li>在 <strong>Self Attention</strong> 机制中, 由于之前提到过, <strong>Query == Key</strong>, 所以有<strong>Key == Value == Query</strong><ul>
<li>输入一个句子，那么里面的每个词都要和该句子中的所有词进行 Attention 计算, 然后Softmax得到当前句子中每个词的权重,进而对句子中的词求和, 输出当前句子在当前模型中的Attention表示(Attention Value), 即$$\boldsymbol{Y_{AttentionOutput}} = Self Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Attention(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})$$</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="对Attention的直观解释是"><a href="#对Attention的直观解释是" class="headerlink" title="对Attention的直观解释是"></a>对Attention的直观解释是</h3><h4 id="请求为向量时"><a href="#请求为向量时" class="headerlink" title="请求为向量时"></a>请求为向量时</h4><ul>
<li>现有查询<strong>向量</strong> q </li>
<li>想从 Value <strong>矩阵</strong>(每列对应一个样本) 中按照比例选择样本进行加权求和得到与 q 相关的查询结果<ul>
<li>要求是样本与 q 越相关，权重越大</li>
</ul>
</li>
<li>Value 中的每个样本都有 Key <strong>矩阵</strong> 中的一个样本与之对应(NLP中 Key 往往是 Value 自己)</li>
<li>将 q 与 Key 的每个样本做相关性计算，得到其与 Key 中每个样本的相关性</li>
<li>对 q 与 Key 的所有相关性做归一化，得到权重比例</li>
<li>按照这个比例将 Value 中的样本加权输出结果</li>
<li>该结果就是 Value 经过 \(F(q, Key)\)加权求和后的结果</li>
<li>也就是 q 对应的结果</li>
</ul>
<h4 id="请求为矩阵时"><a href="#请求为矩阵时" class="headerlink" title="请求为矩阵时"></a>请求为矩阵时</h4><ul>
<li>现有查询<strong>矩阵</strong> Query， 包含 m 个查询向量</li>
<li>相当于重复 m 次做单个请求为向量的运算</li>
<li>每个 q 都能得到一个结果</li>
<li>在实际计算时，可以将整个矩阵一起计算，主要注意归一化是对单个 q 向量与 Key 矩阵生成的结果即可</li>
</ul>
<h4 id="更多分析"><a href="#更多分析" class="headerlink" title="更多分析"></a>更多分析</h4><ul>
<li>当 Key 与 Value 相同时<ul>
<li>其实是说计算 Value 每个样本的权重就用自己去与 q 计算即可</li>
<li>NLP中一般都是这样的</li>
</ul>
</li>
<li>当 Key 与 Query 相同时，<ul>
<li>其实是找自身不同样本间的相关性</li>
<li>然后根据不同样本的对应其他样本的相关性对其他样本进行加权求和得到自己对应的结果</li>
<li>NLP中Self-Attention是这样的</li>
</ul>
</li>
<li>Self-Attention是 Key == Value == Query 的情况</li>
</ul>
<hr>
<h3 id="Attention研究发展趋势"><a href="#Attention研究发展趋势" class="headerlink" title="Attention研究发展趋势"></a>Attention研究发展趋势</h3><img src="/Notes/DL/DL——Attention/NLP_trend.png">
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——NNLM</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94NNLM.html</url>
    <content><![CDATA[<p><em>神经网络语言模型(Nerual Network Language Model, NNLM)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>参考论文: <a href="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" title="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a><br>参考博客: <a href="https://blog.csdn.net/lilong117194/article/details/82018008" title="https://blog.csdn.net/lilong117194/article/details/82018008" target="_blank" rel="noopener">神经网路语言模型(NNLM)的理解</a></p>
<hr>
<h3 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h3><ul>
<li>传统的n元语言模型: $$(p(w_{t}|w_{t-(n-1)},…,w_{t-1}))$$</li>
<li>一般来说可以通过前n-1个词将预测第n个词的概率分布</li>
</ul>
<hr>
<h3 id="NNLM模型原理"><a href="#NNLM模型原理" class="headerlink" title="NNLM模型原理"></a>NNLM模型原理</h3><ul>
<li>NNLM模型直接通过一个神经网络结构对n元条件概率进行评估</li>
</ul>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><ul>
<li>NNLM网络结构图如下：</li>
</ul>
<img src="/Notes/DL/DL——NNLM/NNLM_Overview.png" title="NNLM_Overview">

<h4 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h4><ul>
<li>对于给定的预料库，我们需要生成训练数据集(大量训练样本的集合)，单个样本如下是长度为n的序列\((w_{1},…,w_{n})\)，其中\((w_{1},…,w_{n-1})\)对应训练样本特征值，训练样本标记为\(w_{n}\)，通常可以用One-Hot编码(一个维度为|V|的向量)</li>
</ul>
<h4 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h4><p><em>模型分析主要介绍前向传播过程</em></p>
<h5 id="输入与输出"><a href="#输入与输出" class="headerlink" title="输入与输出"></a>输入与输出</h5><ul>
<li>将构造的数据集作为训练样本集</li>
<li>其中每个样本输入为\((w_{1},…,w_{n-1})\)，输出为一个向量(维度为|V|)，向量代表词的分布，该分布应该与词\(w_{n}\)的One-Hot编码(也是一个|V|维向量)尽量匹配，输出误差就是这两个向量的差异大小(不同损失函数均通过将上述两个向量作为输入，输出一个标量等(也可能n为向量，此时按照不同维度分别计算，或者是其他的值)从而实现当前样本损失的计算</li>
</ul>
<h5 id="模型结构分析"><a href="#模型结构分析" class="headerlink" title="模型结构分析"></a>模型结构分析</h5><ul>
<li><p>\(x=(C_{j|w_{j}=w_{1}};…;C_{j|w_{j}=w_{n-1}})\)</p>
<ul>
<li>x为输入向量，\(x\in R^{(n-1)m}\)，x是词序列\((w_{1},…,w_{n-1})\)对应的拼接向量，其中每个词都会先被矩阵C映射成一个m维的向量，将(n-1)维的向量拼接起来就得到了x</li>
</ul>
</li>
<li><p>\(y=b+Wx+Utanh(d+Hx)\)</p>
<ul>
<li>y为输出向量，\(y\in R^{|V|}\)，\(y_{i}\)表示\(w_{i}\)是第n个单词的概率</li>
</ul>
</li>
</ul>
<h5 id="模型参数分析"><a href="#模型参数分析" class="headerlink" title="模型参数分析"></a>模型参数分析</h5><ul>
<li><p>C:映射矩阵\(C\in R^{|V|\times m}\),其中矩阵的第j行\(C_{j}\)是词\(w_{j}\)对应的特征向量，m为特征向量的维度</p>
</li>
<li><p>H:输入层到隐含层的权重矩阵\(H\in R^{(n-1)m\times h}\),其中h为隐含层神经元的数量</p>
</li>
<li><p>W:输入层到输出层的权重矩阵\(W\in R^{(n-1)m\times |V|}\),W是可选参数，对应模型结构图中的绿色虚线，如果输入层输出层不直接相连，则直接令\(W=0\)即可</p>
</li>
<li><p>U:隐含层到输出层的权重矩阵\(U\in R^{h\times |V|}\)</p>
</li>
<li><p>b:输出层的偏执参数</p>
</li>
<li><p>d:隐含层的偏执参数</p>
</li>
</ul>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><ul>
<li>似然函数:\(L(\theta)=\prod_{t=1}^{T} p(w_{t-(n-1)},…,w_{t-1},w_{t}|\theta)+R(\theta)\)<ul>
<li>T为训练集D中的样本总数，即\(T=|D|\)</li>
<li>\(R(\theta)\)是正则项</li>
</ul>
</li>
<li>对数似然函数:\(L(\theta)=\sum_{t=1}^{T} log(p(w_{t-(n-1)},…,w_{t-1},w_{t})|\theta) + R(\theta)\),其中T为训练集D中的样本总数，即\(T=|D|\)</li>
<li>我们选择优化对数似然函数，原因如下：<ul>
<li>似然函数连乘操作造成浮点数溢出，乘积越来越小(概率值都在[0,1]之间)</li>
<li>似然函数连乘操作耗时大于对数似然函数的连加操作</li>
<li>取对数的操作可以同时把函数中其他的指数项(比如出现在正则项\(R(\theta)\)中)中的处理成连加，减少运算量</li>
<li>对数似然函数的单调性与似然函数相同，最大化对数似然函数等价于最大化似然函数(最重要的一点)</li>
</ul>
</li>
</ul>
<h5 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h5><ul>
<li>最大化对数似然函数(等价于最大化似然函数)</li>
</ul>
<h5 id="参数迭代"><a href="#参数迭代" class="headerlink" title="参数迭代"></a>参数迭代</h5><ul>
<li>使用梯度上升法，每轮迭代时朝着正梯度方向移动<ul>
<li>\(\theta=\theta+\lambda\frac{\partial L(\theta)}{\partial\theta}\)</li>
<li>\(\lambda\)为步长</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>NNLM模型使用了低维连续的词向量对上文进行表示，这解决了词袋模型带来的数据稀疏、语义鸿沟等问题</li>
<li>相比传统模型，NNLM是一种更好的n元语言模型(NNLM的n元不是由神经元决定，而是在根据语料库生成训练数据时单个训练样本中包含的词数，也就是窗口大小)<ul>
<li>n元模型指的是跟军前n-1个词预测第n个词的语言模型，而不是根据前n个词生成第n+1个词的模型</li>
</ul>
</li>
<li>根据相似的上下文语境，NNLM模型可以预测出相似的目标词，而传统模型无法做到这一点</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——DeepFM</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94DeepFM.html</url>
    <content><![CDATA[<p><em>文本介绍DeepFM的理论和实现</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>原始论文: <a href="https://www.ijcai.org/proceedings/2017/0239.pdf" target="_blank" rel="noopener">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, IJCAI 2017</a></li>
<li>参考博客: <a href="https://www.jianshu.com/p/6f1c2643d31b" target="_blank" rel="noopener">https://www.jianshu.com/p/6f1c2643d31b</a></li>
</ul>
<h3 id="回顾特征组合的问题"><a href="#回顾特征组合的问题" class="headerlink" title="回顾特征组合的问题"></a>回顾特征组合的问题</h3><h4 id="传统解决方案"><a href="#传统解决方案" class="headerlink" title="传统解决方案"></a>传统解决方案</h4><ul>
<li>FM: (Factorization Machines, FM)因子分解机</li>
<li>FMM: (Field Factorization Machines, FFM)</li>
</ul>
<h5 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h5><ul>
<li>只能二阶特征组合,无法做到高阶特征组合<ul>
<li>理论上来讲FM经过简单的拓展后可以组合高阶特征,但是那样的话参数会爆炸增加,所以实际上使用时一般只是二阶特征.</li>
</ul>
</li>
</ul>
<h4 id="DNN建模高阶组合特征"><a href="#DNN建模高阶组合特征" class="headerlink" title="DNN建模高阶组合特征"></a>DNN建模高阶组合特征</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>理论上DNN建模高阶组合特征是可行的</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>由于离散特征中我们使用One-Hot编码,会导致输入维度增加,网络参数很多<img src="/Notes/DL/DL——DeepFM/too_many_parameters_in_DNN.png">

</li>
</ul>
<h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>利用<strong>FFM</strong>中的思想,<strong>特征</strong>分为不同的<strong>Field</strong></li>
<li>基本思想是从One-Hot编码换成Dense Vector<img src="/Notes/DL/DL——DeepFM/one_hot2dense_vector.png"></li>
<li>进一步加上两个全连接层(隐藏层),让刚刚学到的Dense Vector进行组合,于是得到高阶组合特征<img src="/Notes/DL/DL——DeepFM/add2hiden_layers.png"></li>
<li>此时,高阶和低阶的特征体现在隐藏层中,我们希望把低阶特征组合单独建模,然后融合高阶特征组合<img src="/Notes/DL/DL——DeepFM/split_low_gram_features.png"></li>
<li>将DNN与FM进行一个合理的融合<img src="/Notes/DL/DL——DeepFM/merge_dnn_and_fm.png"></li>
<li>二者的融合分两种方式: 串行结构和并行结构<img src="/Notes/DL/DL——DeepFM/two_merge_methods.png">

</li>
</ul>
<h3 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h3><ul>
<li><p>是一种并行化的解决方案</p>
<img src="/Notes/DL/DL——DeepFM/deepfm_overview.png">
</li>
<li><p>包含 <strong>FM</strong> 和 <strong>DNN</strong> 两个部分, <strong>FM</strong> 负责<strong>低阶组合特征</strong>的提取,<strong>DNN</strong> 负责<strong>高阶组合特征</strong>的提取,两部分<strong>共享同样的输入</strong></p>
</li>
<li><p>DeepFM的预测结果可以表示为如下的形式<br>$$\hat{y} = sigmoid(y_{FM} + y_{DNN})$$</p>
</li>
</ul>
<h4 id="FM部分"><a href="#FM部分" class="headerlink" title="FM部分"></a>FM部分</h4><img src="/Notes/DL/DL——DeepFM/fm_component.png">
<ul>
<li>输出如下<br>$$ y(x) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n w_{ij} x_i x_j $$</li>
</ul>
<h4 id="DNN部分"><a href="#DNN部分" class="headerlink" title="DNN部分"></a>DNN部分</h4><img src="/Notes/DL/DL——DeepFM/dnn_component.png">
<ul>
<li>DNN部分是一个前馈神经网络</li>
<li>与图像语音的区别:<ul>
<li>图像语音输入为连续且密集的</li>
<li>CTR中使用的一般是稀疏的</li>
</ul>
</li>
<li>在进入隐藏层之前,使用一个嵌入层(DenseEmbeddings): 将<strong>高维稀疏输入向量</strong>压缩为<strong>低维稠密向量</strong><img src="/Notes/DL/DL——DeepFM/dnn_component_embedding_part.png"></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——MLP及其BP算法</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94MLP%E5%8F%8A%E5%85%B6BP%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<p><em>多层感知机(Multi-Layer Perception， MLP)及其BP(Back Propagation)算法</em></p>
<hr>
<h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><ul>
<li>图示如下</li>
</ul>
<img src="/Notes/DL/DL——MLP及其BP算法/MLP-overview.gif" title="MLP-overview.gif">

<hr>
<h3 id="BP算法"><a href="#BP算法" class="headerlink" title="BP算法"></a>BP算法</h3><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p><em>以一维输出(二分类)为例</em></p>
<img src="/Notes/DL/DL——MLP及其BP算法/MLP-BP-overview.png" title="MLP-BP-overview.png">

<h4 id="详细流程"><a href="#详细流程" class="headerlink" title="详细流程"></a>详细流程</h4><ul>
<li>动图</li>
</ul>
<img src="/Notes/DL/DL——MLP及其BP算法/MLP-BP-process.gif" title="MLP-BP-process.gif">

<hr>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>References:</p>
<ul>
<li><a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a></li>
<li><a href="https://www.cnblogs.com/ooon/p/5577241.html" target="_blank" rel="noopener">https://www.cnblogs.com/ooon/p/5577241.html</a></li>
<li><a href="https://blog.csdn.net/guotong1988/article/details/52096724" target="_blank" rel="noopener">https://blog.csdn.net/guotong1988/article/details/52096724</a></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——softmax遇上交叉熵损失时的梯度计算</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94softmax%E9%81%87%E4%B8%8A%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E6%97%B6%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97.html</url>
    <content><![CDATA[<hr>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><h4 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h4><img src="/Notes/DL/DL——softmax遇上交叉熵损失时的梯度计算/softmax_define.png">
<h4 id="展开定义"><a href="#展开定义" class="headerlink" title="展开定义"></a>展开定义</h4><img src="/Notes/DL/DL——softmax遇上交叉熵损失时的梯度计算/softmax.png">

<hr>
<h3 id="softmax求导"><a href="#softmax求导" class="headerlink" title="softmax求导"></a>softmax求导</h3><img src="/Notes/DL/DL——softmax遇上交叉熵损失时的梯度计算/partial_softmax.png">

<hr>
<h3 id="交叉熵定义"><a href="#交叉熵定义" class="headerlink" title="交叉熵定义"></a>交叉熵定义</h3><p><em>Cross Entropy Loss</em></p>
<img src="/Notes/DL/DL——softmax遇上交叉熵损失时的梯度计算/cross_entropy_loss.png">

<hr>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><h4 id="偏置量的梯度"><a href="#偏置量的梯度" class="headerlink" title="偏置量的梯度"></a>偏置量的梯度</h4><img src="/Notes/DL/DL——softmax遇上交叉熵损失时的梯度计算/gradient_b.png">

<h4 id="权重参数的梯度"><a href="#权重参数的梯度" class="headerlink" title="权重参数的梯度"></a>权重参数的梯度</h4><p><em>同偏置量的梯度计算相似步骤可得</em></p>
<img src="/Notes/DL/DL——softmax遇上交叉熵损失时的梯度计算/gradient_w.png">]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——关于参数的初始化</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E5%8F%82%E6%95%B0%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96.html</url>
    <content><![CDATA[<hr>
<h3 id="为什么参数不能初始化为全0？"><a href="#为什么参数不能初始化为全0？" class="headerlink" title="为什么参数不能初始化为全0？"></a>为什么参数不能初始化为全0？</h3><ul>
<li>因为此时会导致同一隐藏层的神经元互相对称，可以通过递推法证明，不管迭代多少次，此时所有的神经元都将计算完全相同的函数</li>
<li>并不会因为参数都为0就导致所有神经元死亡！</li>
</ul>
<hr>
<h3 id="为什么参数不能初始化为太大的数值？"><a href="#为什么参数不能初始化为太大的数值？" class="headerlink" title="为什么参数不能初始化为太大的数值？"></a>为什么参数不能初始化为太大的数值？</h3><ul>
<li><p>因为参数太大会导致sigmoid(z)或tanh(z)中的z太大，从而导致梯度太小而更新太慢</p>
</li>
<li><p>如果网络中完全没有sigmoid和tanh等激活函数，那就还好，但是要注意，二分类中使用sigmoid函数于输出层时也不应该将参数初始化太大</p>
</li>
<li><p>单层隐藏层的神经网络一般这样初始化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">W = np.random.randn((n1, n2)) * 0.01</span><br></pre></td></tr></table></figure>

<ul>
<li>适用于单层隐藏层神经网络的参数</li>
<li>如果是深层网络则要考虑使用其他常数而不是<code>0.01</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="神经网络的层数也可当做参数"><a href="#神经网络的层数也可当做参数" class="headerlink" title="神经网络的层数也可当做参数"></a>神经网络的层数也可当做参数</h3><ul>
<li>不是越深越好</li>
<li>一个问题的开始一般从单层网络开始，即Logistic回归开始</li>
<li>逐步加深网络层数，不断测试效果，寻找合适的网络层数即可</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——为什么Dropout能防止过拟合</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E4%B8%BA%E4%BB%80%E4%B9%88Dropout%E8%83%BD%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88.html</url>
    <content><![CDATA[<ul>
<li>参考博客: <a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/81976571" target="_blank" rel="noopener">https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/81976571</a></li>
</ul>
<hr>
<h3 id="关于Dropout"><a href="#关于Dropout" class="headerlink" title="关于Dropout"></a>关于Dropout</h3><ul>
<li>用途是防止过拟合,关于过拟合的讲解可参考<ul>
<li><a href="/DL/DL%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%99%8D%E4%BD%8E%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95.html">DL——深度学习中降低过拟合的方法</a></li>
<li><a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html">ML——模型的方差与偏差(机器学习中的正则化与过拟合)</a></li>
</ul>
</li>
</ul>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是<strong>暂时</strong>，对于随机梯度下降来说，由于是随机丢弃，故而<strong>每一个mini-batch都在训练不同的网络</strong>。(因为每一轮被丢弃的神经元不同)</li>
</ul>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><ul>
<li>在CNN中防止过拟合的效果明显</li>
<li>一般选择0.5比较好,因为0.5的时候Dropout随机生成的网络结果最多,但是实际使用中一般需要调节甚至变化<ul>
<li>亲测: 在使用VGG16模型迁移学习来分类Dogs2Cats数据集时,先使用0.5,然后再使用0.2略优于一直使用0.5的情况</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Why能防止过拟合"><a href="#Why能防止过拟合" class="headerlink" title="Why能防止过拟合?"></a>Why能防止过拟合?</h3><ul>
<li>虽然Dropout在实际应用中的确能防止过拟合,但是关于Dropout防止过拟合的原理,大家众说纷纭</li>
<li>下面介绍两个主流的观点</li>
</ul>
<h4 id="组合派观点"><a href="#组合派观点" class="headerlink" title="组合派观点"></a>组合派观点</h4><h5 id="集成学习方法论"><a href="#集成学习方法论" class="headerlink" title="集成学习方法论"></a>集成学习方法论</h5><ul>
<li>传统神经网络的缺点: <strong>费时</strong>, <strong>容易过拟合</strong> </li>
<li>过拟合是很多机器学习的通病</li>
<li>一种修改模型的过拟合解决思路是: 采用Ensemble方法的Bagging方法(平均多个模型的结果,从而能够减少模型的方差,同时减轻过拟合)或者Boosting方法(减小模型的偏差,同时能减轻过拟合?[待更新]),即训练多个模型做组合</li>
<li>但是解决了过拟合后, <strong>费时</strong>就成为一个大问题，不仅训练起来费时，测试起来多个模型也很费时</li>
<li><strong>Dropout</strong>能同时解决以上问题:<ul>
<li>Dropout的示意图如下: 左图是原图结构,右图是加入Dropout层的<img src="/Notes/DL/DL——为什么Dropout能防止过拟合/dropout_overview.png"></li>
<li>从图上可以看出,有了Dropout,训练的模型就可以看成是多个模型的组合,最终预测时丢弃Dropout即可的到所有模型的组合,从而实现类似于Ensemble方法的Bagging方法,实现了多个模型的组合</li>
</ul>
</li>
</ul>
<h5 id="动机论"><a href="#动机论" class="headerlink" title="动机论"></a>动机论</h5><ul>
<li><p>虽然直观上看dropout是ensemble在分类性能上的一个近似，然而实际中，dropout毕竟还是在一个神经网络上进行的，只训练出了一套模型参数。那么他到底是因何而有效呢？</p>
</li>
<li><p>首先分析一个小故事</p>
</li>
</ul>
<blockquote>
<p>在自然界中，在中大型动物中，一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。但是自然选择中毕竟没有选择无性繁殖，而选择了有性繁殖，须知物竞天择，适者生存。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。</p>
</blockquote>
<ul>
<li><p>基本思想: 有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。</p>
</li>
<li><p>dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。</p>
</li>
</ul>
<h4 id="噪声派观点"><a href="#噪声派观点" class="headerlink" title="噪声派观点"></a>噪声派观点</h4><ul>
<li>对于每一个dropout后的网络，进行训练时，相当于做了<strong>Data Augmentation</strong>，因为，总可以找到一个样本，使得在原始的网络上也能达到dropout单元后的效果。 比如，对于某一层，dropout一些单元后，形成的结果是(1.5,0,2.5,0,1,2,0)，其中0是被drop的单元，那么总能找到一个样本(<strong>新样本</strong>)，使得结果也是如此。这样，每一次dropout其实都相当于增加了样本。</li>
</ul>
<h5 id="噪声派观点总结"><a href="#噪声派观点总结" class="headerlink" title="噪声派观点总结"></a>噪声派观点总结</h5><ul>
<li>将dropout映射回得样本训练一个完整的网络，可以达到dropout的效果。</li>
<li>dropout由固定值变为一个区间，可以提高效果</li>
<li>将dropout后的表示映射回输入空间时，并不能找到一个样本 x 使得所有层都能满足dropout的结果，但可以为每一层都找到一个样本，这样，对于每一个dropout，都可以找到一组样本可以模拟结果。</li>
</ul>
<hr>
<h4 id="其他需要注意的点"><a href="#其他需要注意的点" class="headerlink" title="其他需要注意的点"></a>其他需要注意的点</h4><ul>
<li>数据量小的时候，dropout效果不好，数据量大了，dropout效果好。</li>
<li>dropout的缺点就在于训练时间是没有dropout网络的2-3倍。</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——深度学习中降低过拟合的方法</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%99%8D%E4%BD%8E%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<h4 id="添加Dropout"><a href="#添加Dropout" class="headerlink" title="添加Dropout"></a>添加Dropout</h4><ul>
<li>详情可参考: <a href="/DL/DL%E2%80%94%E2%80%94%E4%B8%BA%E4%BB%80%E4%B9%88Dropout%E8%83%BD%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88.html">DL——为什么Dropout能防止过拟合</a></li>
</ul>
<h4 id="参数范书惩罚"><a href="#参数范书惩罚" class="headerlink" title="参数范书惩罚"></a>参数范书惩罚</h4><p><em>相关参数: Weight decay(权重衰减)</em><br><em>添加L2或L1正则化, 详情可参考: <a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE.html">ML——模型的方差与偏差</a></em></p>
<ul>
<li><p>参考文档：</p>
<ul>
<li>L1正则化与L2正则化的详细讲解（L1具有稀疏性，L2让参数更小）：<a href="https://www.cnblogs.com/nxf-rabbit75/p/9954394.html" target="_blank" rel="noopener">L1正则化和L2正则化</a></li>
<li>L1具有稀疏性的证明：<a href="https://blog.csdn.net/b876144622/article/details/81276818" target="_blank" rel="noopener">L1正则为什么更容易获得稀疏解</a><ul>
<li>求导后可知，在0点附近，权重大于0和小于0会产生正负不同的梯度值（当原始损失函数关于当前权重在0点的偏导绝对值小于正则化权重时，整体梯度基本由正则化梯度主导），从而使得参数倾向于走到0点</li>
</ul>
</li>
</ul>
</li>
<li><p>L1正则化: </p>
<ul>
<li>L1又称为: <strong>Lasso Regularization(稀疏规则算子)</strong></li>
<li>计算公式为: <strong>参数绝对值求和</strong> </li>
<li>意义: 趋向于让一些参数为0, 可以起到特征选择的作用</li>
</ul>
</li>
<li><p>L2正则化:</p>
<ul>
<li>L2又称为: <strong>Ridge Regression(岭回归)</strong></li>
<li>Weight decay 是放在正则项(Regularization)前面的一个系数,正则项一般指模型的复杂度</li>
<li>Weight decay 控制模型复杂度对损失函数的影响, 若Weight Decay很大,则模型的损失函数值也就大</li>
<li>pytorch中实现了L2正则化，也叫做权重衰减，具体实现是在优化器中，参数是 <code>weight_decay</code>, 默认为0</li>
</ul>
</li>
<li><p>PyTorch中的<code>weight_decay</code>参数说明</p>
</li>
</ul>
<blockquote>
<p>weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</p>
</blockquote>
<ul>
<li><p>我之前的实现代码:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># zero the parameter gradients</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"># forward</span><br><span class="line">outputs = model(inputs)</span><br><span class="line"># _, preds = torch.max(outputs.data, 1)</span><br><span class="line">loss = loss_criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line"># L1 regularization</span><br><span class="line">l1_loss = 0</span><br><span class="line">for w in model.parameters():</span><br><span class="line">    l1_loss += torch.sum(torch.abs(w))</span><br><span class="line">loss += l1_rate * l1_loss</span><br><span class="line"></span><br><span class="line"># backward + optimize only if in training phase</span><br><span class="line">if phase == &apos;train&apos;:</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<ul>
<li>其中 <code># L1 regularization</code>后面是添加的L1 正则化</li>
</ul>
</li>
<li><p>就整体而言，对比加入正则化和未加入正则化的模型，训练输出的loss和Accuracy信息，我们可以发现，加入正则化后，loss下降的速度会变慢，准确率Accuracy的上升速度会变慢，并且未加入正则化模型的loss和Accuracy的浮动比较大（或者方差比较大），而加入正则化的模型训练loss和Accuracy，表现的比较平滑。并且随着正则化的权重lambda越大，表现的更加平滑。这其实就是正则化的对模型的惩罚作用，通过正则化可以使得模型表现的更加平滑，即通过正则化可以有效解决模型过拟合的问题。</p>
</li>
</ul>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><ul>
<li>提高模型的泛化能力最好的办法是, <strong>使用更多的训练数据进行训练</strong></li>
<li>创造一些假数据添加到训练集中</li>
<li>实例: <ul>
<li>AlexNet中使用对图片旋转等方式生成新的图片作为样本加入训练, 误差能降低1%</li>
</ul>
</li>
</ul>
<h4 id="提前终止训练"><a href="#提前终止训练" class="headerlink" title="提前终止训练"></a>提前终止训练</h4><ul>
<li>当发现数据在验证集上的损失趋于收敛甚至开始增加时,停止训练</li>
<li>即使模型在验证集上的损失还在减小</li>
</ul>
<h4 id="参数绑定与参数共享"><a href="#参数绑定与参数共享" class="headerlink" title="参数绑定与参数共享"></a>参数绑定与参数共享</h4><p><em>Soft Weight Sharing</em></p>
<ul>
<li>类似于CNN中卷积层的权重共享方法</li>
<li>RNN中也有权重共享, 整条时间链上的参数共享</li>
</ul>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><ul>
<li>其实bagging的方法是可以起到正则化的作用,因为正则化就是要减少泛化误差,而bagging的方法可以组合多个模型起到减少泛化误差的作用</li>
<li>在深度学习中同样可以使用此方法,但是其会增加计算和存储的成本<ul>
<li>这一点在Kaggle比赛中有用过,的确有很大提高</li>
</ul>
</li>
</ul>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><ul>
<li>在Google Inception V2中所采用,是一种非常有用的正则化方法,可以让大型的卷积网络训练速度加快很多倍,同事收敛后分类的准确率也可以大幅度的提高.</li>
<li>N在训练某层时,会对每一个mini-batch数据进行标准化(normalization)处理,使输出规范到N(0,1)的正太分布,减少了Internal convariate shift(内部神经元分布的改变),传统的深度神经网络在训练是,每一层的输入的分布都在改变,因此训练困难,只能选择用一个很小的学习速率,但是每一层用了BN后,可以有效的解决这个问题,学习速率可以增大很多倍</li>
<li>更多信息参考: <a href="/Notes/DL/DL%E2%80%94%E2%80%94BN-LN-IN-GN-LRN-WN.html">DL——BN-LN-IN-GN-LRN-WN</a></li>
</ul>
<h4 id="辅助分类节点"><a href="#辅助分类节点" class="headerlink" title="辅助分类节点"></a>辅助分类节点</h4><p><em>(auxiliary classifiers)</em></p>
<ul>
<li>在Google Inception V1中,采用了辅助分类节点的策略,即将<strong>中间某一层的输出用作分类,并按一个较小的权重加到最终的分类结果中</strong>,这样相当于做了模型的融合,同时给网络增加了反向传播的梯度信号,提供了额外的正则化的思想.</li>
</ul>
<h4 id="尝试不同神经网络架构"><a href="#尝试不同神经网络架构" class="headerlink" title="尝试不同神经网络架构"></a>尝试不同神经网络架构</h4><ul>
<li>尝试替换以下方面:<ul>
<li>激活函数</li>
<li>层数</li>
<li>权重?</li>
<li>层的参数?</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——各种梯度下降相关的优化算法</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<p><em>本文从梯度下降(Gradient Descent, GD)开始,讲述深度学习中的各种优化算法（优化器，Optimizer）</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>参考文章:<a href="https://m.sohu.com/a/131923387_473283/?pvid=000115_3w_a" title="https://m.sohu.com/a/131923387_473283/?pvid=000115_3w_a" target="_blank" rel="noopener">【干货】深度学习必备：随机梯度下降（SGD）优化算法及可视化</a> </p>
<hr>
<h3 id="三种梯度下降框架"><a href="#三种梯度下降框架" class="headerlink" title="三种梯度下降框架"></a>三种梯度下降框架</h3><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><h5 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>每次从随机从训练集中选择一个训练样本来计算误差,进而更新模型参数</li>
<li>单次迭代时参数移动方向可能不太精确甚至相反,但是最终会收敛</li>
<li>单次迭代的波动也带来了一个好处,可以到达一个更好的局部最优点,甚至到达全局最优点</li>
</ul>
<h5 id="参数更新公式"><a href="#参数更新公式" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><p><em>Stochastic Gradient Descent, SGD</em></p>
<ul>
<li>公式: \(\theta=\theta-\lambda\frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta}\)</li>
<li>其中:\(L(\theta;x_{i};y_{i})=L(f(\theta;x_{i}),y_{i})\)</li>
</ul>
<h4 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h4><p><em>Batch Gradient Descent, BGD</em></p>
<h5 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>每次使用全量的训练集样本(假设共m个)来计算误差,进而更新模型参数</li>
<li>每次参数能够朝着正确的方向移动</li>
<li>每次遍历所有数据,耗费时间较长</li>
</ul>
<h5 id="参数更新公式-1"><a href="#参数更新公式-1" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>公式: \(\theta=\theta-\lambda\frac{\partial L(\theta;x_{1:m};y_{1:m})}{\partial \theta}\)</li>
<li>一般来说: \(L(\theta;x_{1:m};y_{1:m}) = \frac{1}{m}\sum_{i=1}^{m} L(\theta;x_{i};y_{i})\)</li>
</ul>
<h4 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h4><h5 id="核心思想-2"><a href="#核心思想-2" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>每次从随机从训练集中选择k(k &lt; m)个训练样本来计算误差,进而更新模型参数</li>
<li>介于SGD和BGD之间<ul>
<li>波动小</li>
<li>内存占用也相对较小</li>
</ul>
</li>
</ul>
<h5 id="参数更新公式-2"><a href="#参数更新公式-2" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><p><em>Mini-Batch Gradient Descent, MBGD</em></p>
<ul>
<li>公式: \(\theta=\theta-\lambda\frac{\partial L(\theta;x_{i:i+k};y_{i:i+k})}{\partial \theta}\)</li>
<li>一般来说: \(L(\theta;x_{1:k};y_{1:k}) = \frac{1}{k}\sum_{i=1}^{k} L(\theta;x_{i};y_{i})\)</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ul>
<li>梯度下降算法应用广泛,算法效果很好</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><h6 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h6><ul>
<li>大小很难确定,太大容易震荡,太小则收敛太慢</li>
<li>学习速率一般为定值,有时候会实现为逐步衰减</li>
<li>但是无论如何,都需要事前固定一个值,因此无法自适应不同的数据集特点</li>
</ul>
<h6 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h6><ul>
<li>对于非凸的目标函数,容易陷入局部极值点中</li>
<li>比局部极值点更严重的问题:有时候会嵌入鞍点?</li>
</ul>
<hr>
<h3 id="SD算法的优化"><a href="#SD算法的优化" class="headerlink" title="SD算法的优化"></a>SD算法的优化</h3><h4 id="Momentum法-动量法"><a href="#Momentum法-动量法" class="headerlink" title="Momentum法(动量法)"></a>Momentum法(动量法)</h4><h5 id="核心思想-3"><a href="#核心思想-3" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>考虑一种情况,在峡谷地区(某些方向比另一些方向陡峭很多)<ul>
<li>SGD(或者MBGD,实际上,SGD是特殊的MBGD,平时可以认为这两者是相同的东西)会在这些放附近振荡,从而导致收敛速度变慢</li>
<li>这里最好的例子是鞍点,鞍点出的形状像一个马鞍,一个方向两头上翘,一个方向两头下垂,当上翘的方向比下垂的方向陡峭很多时,SDG和MDG等方法容易在上翘方向上震荡</li>
</ul>
</li>
<li>此时动量可以使得<ul>
<li>当前梯度方向与上一次梯度方向相同的地方进行加强,从而加快收敛速度</li>
<li>当前梯度方向与上一次梯度方向不同的地方进行削减,从而减少振荡</li>
</ul>
</li>
<li>动量可以理解为一个从山顶滚下的小球,遇到新的力(当前梯度)时,会结合之前的梯度方向决定接下来的运动方向</li>
</ul>
<h5 id="参数更新公式-3"><a href="#参数更新公式-3" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>公式: \(\theta=\theta-m_{t}\)<ul>
<li>\(m_{t}\)表示当前下降方向, \(m_{t-1}\)表示上一次的下降方向</li>
<li>\(m_{t}=\gamma m_{t-1}+\lambda\frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta}\)</li>
<li>\(\gamma&lt;1\),值一般取0.9</li>
<li>\(\gamma m_{t-1}\)是动量项</li>
<li>\(\gamma\)是衰减量</li>
<li>\(\lambda\)是学习率</li>
</ul>
</li>
</ul>
<h5 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h5><img src="/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_sgd.png" title="momentum_sgd.png">
<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/momentum_description.png" title="momentum_description.png">
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><ul>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新v和参数\(\theta\)</li>
</ul>
</li>
</ul>
<h4 id="NAG-涅斯捷罗夫梯度加速法"><a href="#NAG-涅斯捷罗夫梯度加速法" class="headerlink" title="NAG,涅斯捷罗夫梯度加速法"></a>NAG,涅斯捷罗夫梯度加速法</h4><p><em>Nesterov Accelerated Gradient,NAG</em></p>
<h5 id="核心思想-4"><a href="#核心思想-4" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>继续考虑普通的SDG算法,添加了Momentum,此时从山顶滚下的球会盲目的选择斜坡</li>
<li>更好的方式是在遇到向上的斜坡时减慢速度</li>
<li>NAG在计算梯度时首先获取(近似获得)未来的参数而不是当前参数,然后计算未来参数对应的损失函数的梯度</li>
<li>NAG在预测了未来的梯度后,根据<strong>未来</strong>(\(\theta - \gamma m_{t-1}\))梯度方向和之前梯度的方向决定当前的方向, 这样可以保证在遇到下一点为上升斜坡时适当减慢当前点的速度(否则可能由于惯性走上斜坡, 提前知道\(\theta - \gamma m_{t-1}\)处的梯度, 从而保证不要走上去), 从而找到了比Momentum超前的更新方向</li>
<li>对比: Momentum是根据<strong>当前</strong>梯度方向和之前梯度方向决定当前的方向</li>
</ul>
<h5 id="参数更新公式-4"><a href="#参数更新公式-4" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>公式: \(\theta=\theta-m_{t}\)<ul>
<li>\(m_{t}=\gamma m_{t-1}+\lambda\frac{\partial L(\theta - \gamma v_{t-1};x_{i};y_{i})}{\partial \theta}\)</li>
<li><strong>NAG</strong>使用的是<strong>未来</strong>的梯度方向(<strong>Momentum</strong>使用的是<strong>当前</strong>梯度方向)和之前的梯度方向</li>
</ul>
</li>
</ul>
<h5 id="图示-1"><a href="#图示-1" class="headerlink" title="图示"></a>图示</h5><ul>
<li>Momentum(动量)法首先计算当前的梯度值(小蓝色向量)，然后在更新的积累向量（大蓝色向量）方向前进一大步</li>
<li>NAG 法则首先(试探性地)在之前积累的梯度方向(棕色向量)前进一大步，再根据当前地情况修正，以得到最终的前进方向(绿色向量)</li>
<li>这种基于预测的更新方法，使我们避免过快地前进，并提高了算法地响应能力(responsiveness)，大大改进了 RNN 在一些任务上的表现<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/nag.png" title="nag.png">
<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/nag_description.png" title="nag_description.png">
<ul>
<li>公式中\(-\gamma m_{t-1}\)对应BC向量</li>
<li>\(\theta-\gamma m_{t-1}\)就对应C点(参数)</li>
</ul>
</li>
</ul>
<h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><ul>
<li>Momentum和NAG法可以使得参数更新过程中根据随时函数的斜率自适应的学习,从而加速SGD的收敛</li>
<li>实际应用中,NAG将比Momentum收敛快很多</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新v和参数\(\theta\)</li>
</ul>
</li>
</ul>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><h5 id="核心思想-5"><a href="#核心思想-5" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>对于较少出现的特征,使用较大的学习率更新,即对低频的参数给予更大的更新</li>
<li>对于较多出现的特征,使用较小的学习率更新,即对高频的参数给予更小的更新</li>
<li>很适合处理稀疏数据</li>
</ul>
<h5 id="参数更新公式-5"><a href="#参数更新公式-5" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>计算梯度<ul>
<li>分量形式: \(g_{t,k} = \frac{\partial L(\theta;x_{i};y_{i})}{\theta}|_{\theta = \theta_{t-1,k}}\)<ul>
<li>\(g_{t,k}\)是指第t次迭代时第k个参数\(\theta_{t-1, k}\)的梯度</li>
<li>有些地方会这样表达: \(g_{t,k} = \frac{\partial L(\theta_{t-1,k};x_{i};y_{i})}{\theta_{t-1,k}}\)<ul>
<li>式子中使用\(\theta_{t-1, k}\)在梯度中,事实上不够严谨, 容易让人误解分子分母都不是函数,而是一个确定的值, 事实上我们是先求了导数然后再带入 \(\theta = \theta_{t-1}\) 的</li>
</ul>
</li>
</ul>
</li>
<li>向量形式: \(g_{t} = \frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta}|_{\theta=\theta_{t-1}}\)</li>
</ul>
</li>
<li>此时普通的SGD如下更新参数<ul>
<li>分量形式:\(\theta_{t,k} = \theta_{t-1,k} - \lambda g_{t,k}\)</li>
<li>向量形式:\(\theta_{t} = \theta_{t-1} - \lambda g_{t}\)</li>
</ul>
</li>
<li>而Adagrad对学习率\(\lambda\)根据不同参数进行了修正<ul>
<li>分量形式:\(\theta_{t,k} = \theta_{t-1,k} - \frac{\lambda}{\sqrt{G_{t,kk}+\epsilon}} g_{t,k}\)<ul>
<li>\(G_{t,kk}=\sum_{r=1}^{t}(g_{r,k})^{2}\)</li>
</ul>
</li>
<li>向量形式:\(\theta_{t} = \theta_{t-1} - \frac{\lambda}{\sqrt{G_{t}+\epsilon}}\bigodot g_{t}\)<ul>
<li>\(G_{t}=\sum_{r=1}^{t}g_{r}\bigodot g_{r}\)</li>
<li>\(\bigodot\)表示按照对角线上的值与对应梯度相乘</li>
<li>进一步可以简化写为: \(G_t = G_{t-1} + g_t^2\)<ul>
<li>注意: 这里\(g_t^2\)是指向量按照维度分别相乘, 计算后还是原始向量维度</li>
</ul>
</li>
</ul>
</li>
<li>G是一个对角矩阵,对角线上的元素(\(G_{k,k}\))是从一开始到k次迭代目标函数对于参数(\(\theta_{k}\))的梯度的平方和<ul>
<li>G的累计效果保证了出现次数多的参数(\(\theta_{k}\))对应的对角线上的元素(\(G_{k,k}\))大,从而得到更小的更新</li>
</ul>
</li>
<li>\(\epsilon\)是一个平滑项,用于防止分母为0</li>
</ul>
</li>
<li>总结参数更新公式:<ul>
<li>\(\theta_{t} = \theta_{t-1} - \frac{\lambda}{\sqrt{G_{t}+\epsilon}} g_{t}\)</li>
<li>\(g_{t} = \frac{\partial L(\theta;x_{i};y_{i})}{\partial \theta }|_{\theta = \theta_{t-1}}\)</li>
<li>\(G_t = G_{t-1} + g_t^2\)</li>
</ul>
</li>
</ul>
<h5 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h5><ul>
<li>在分母上<strong>累计了平方梯度和</strong>,造成训练过程中<strong>G的对角线元素越来越大</strong>,最终导致<strong>学习率非常小</strong>,甚至是无限小的值,从而<strong>学不到东西</strong></li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新G的每个元素,再根据G以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><h5 id="核心思想-6"><a href="#核心思想-6" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>是Adagrad的一个扩展,目标是解决Adagrad学习率单调下降的问题</li>
<li>解决方案:只累计一段时间内的平方梯度值?</li>
<li>实际上实现是累加时给前面的平方梯度和一个衰减值</li>
<li>方法名delta的来源是选取部分</li>
</ul>
<h5 id="参数更新公式-6"><a href="#参数更新公式-6" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>将矩阵G的每一项变成当前梯度平方加上过去梯度平方的衰减值(指数衰减)即可<ul>
<li>指数衰减:前n-1项的系数是衰减率的n-1次方</li>
<li>实现指数衰减</li>
<li>在Adagrad的基础上修改为: \(G_t = \gamma G_{t-1} + (1-\gamma)g_t^2\)<ul>
<li>注意: 这里\(g_t^2\)是指向量按照维度分别相乘, 计算后还是原始向量维度</li>
</ul>
</li>
<li>我们通常也把 \(G_t\) 表达为 \(E[g^2]_t\)<ul>
<li>因为修改后的 \(G_t\)可以视为于对 \(g_t^2\) 求期望(不同的\(t\)概率权重不一样的分布的期望)</li>
<li>进一步表达为: \(E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h5><ul>
<li>经过衰减后,G的每一项(忽略掉平滑项\(\epsilon\))相当于有权重的梯度均方差(Root Mean Square, RMS),后面RMSprop算法就用了这个RMS来命名<ul>
<li>均方根的定义是:对所有数求平方和,取平均值(每一项的权重根据概率分布可以不同),再开方</li>
</ul>
</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新G的每个元素,再根据G以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p><em>Root Mean Square prop</em></p>
<h5 id="核心思想-7"><a href="#核心思想-7" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>一种适应性学习率方法,至今未公开发表</li>
<li>是Adagrad的一个扩展,目标也是解决Adagrad学习率单调下降的问题</li>
<li>RMS的来源是由于分母相当于(忽略掉平滑项\(\epsilon\))是梯度的均方根(Root Mean Squared, RMS)</li>
</ul>
<h5 id="参数更新公式-7"><a href="#参数更新公式-7" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>参见Adadelta</li>
<li>RMSprop的本质是对Adadelta简单的取之前值和当前值的权重为0.9和0.1实现指数加权平均, 即 \(\gamma = 0.9\)</li>
<li>有些地方也说RMSprop权重取的是0.5和0.5实现指数加权平均即 \(\gamma = 0.5\)</li>
<li>学习率\(\lambda\)一般取值为0.001</li>
</ul>
<h5 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h5><ul>
<li><strong>RMSprop是Adadelta的一种特殊形式</strong></li>
<li>Adagrad的分母不能算是均方差(即使忽略平滑项\(\epsilon\)),因为这里没有取平均值的操作</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新G的每个元素,再根据G以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p><em>Adaptive Moment Estimation</em></p>
<h5 id="核心思想-8"><a href="#核心思想-8" class="headerlink" title="核心思想"></a>核心思想</h5><ul>
<li>一种适应性学习率方法,相当于 <strong>RMSprop + Momentum + Bias Correction</strong></li>
<li>像Adadelta和RMSprop一样存储了梯度的平方的指数衰减平均值</li>
<li>像Momentum一样保持了过去梯度的指数衰减平均值</li>
<li>Bias Correction是为了得到期望的<strong>无偏估计</strong></li>
</ul>
<h5 id="参数更新公式-8"><a href="#参数更新公式-8" class="headerlink" title="参数更新公式"></a>参数更新公式</h5><ul>
<li>\(\theta_{t} = \theta_{t-1} - \frac{\lambda}{\sqrt{\tilde{v}_t+\epsilon}} \tilde{m}_t\)</li>
<li>\(\tilde{v}_t=\frac{v_{t}}{1-\beta_{1}^{t}}\)</li>
<li>\(\tilde{m}_t=\frac{m_{t}}{1-\beta_{2}^{t}}\)</li>
<li>梯度的指数衰减:\(m_{t} = \beta_{2}m_{t-1}+(1-\beta_{2})g_{t}\)</li>
<li>梯度平方的指数衰减:\(v_{t} = \beta_{1}v_{t-1}+(1-\beta_{1})g_{t}^{2}\)<ul>
<li>\(m_t\) 和 \(v_t\) 也叫作一阶动量和二阶动量，是对梯度一阶矩估计和二阶矩估计<ul>
<li>数学定义：随机变量的一阶矩是随机变量的期望\(E[X]\)，二阶矩是随机变量的方差\(E[X-E[X]]\)</li>
<li>其实梯度平方的期望不是梯度的方差，这只是一种近似，数学上，随机变量\(X\)二阶矩等价于方差，是\(E[(X-E[X])^2] = E[X^2]-E[X]^2\)，当\(E[X]=0\)时，\(E[X^2]\)就是方差</li>
<li>这种滑动平均之所以能代表期望，是因为滑动平均的思想是一种折扣平均，确实可以用来作为期望和方差的估计</li>
</ul>
</li>
<li>\(m_t\) 和 \(v_t\) 可以看做是对 \(E[g]_t\) 和 \(E[g^2]_t\) 的估计</li>
<li>\(\tilde{m}_t\) 和 \(\tilde{v}_t\) 是对 \(m_t\) 和 \(v_t\) 的 <strong>Bias Correction</strong>, 这样可以近似为对对期望 \(E[g]_t\) 和 \(E[g^2]_t\) 的<strong>无偏估计</strong><ul>
<li>注意：修正项\(\tilde{v}_t=\frac{v_{t}}{1-\beta_{1}^{t}}\)中的\(\beta_{1}^{t}\)是\(\beta_{1}\)的\(t\)次方的意思，基本思路可以理解为在每一步都尽量将梯度修正到\(t=0\)大小</li>
<li>进行修正的原因是当\(t\)较小时，\(v_t\)也较小，而\(\beta\)一般较大（0.9或者0.999），此时加权平均的结果也会很小，当\(t\)很大时，实际上可以不用修正了，个人理解：应该可以不用修正，只是前期训练时更新速度比较慢而已</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h5><ul>
<li>超参数设定推荐<ul>
<li>梯度平方衰减率:\(\beta_{1}=0.999\)</li>
<li>梯度动量衰减率:\(\beta_{2}=0.9\)</li>
<li>平滑项:\(\epsilon=10e^-8=1*10^{-8}\)</li>
<li>一阶动量\(v\),初始化为0</li>
<li>二阶动量\(m\),初始化为0</li>
</ul>
</li>
<li>学习过程<ul>
<li>从训练集中的随机抽取一批容量为m的样本\({x_{1},…,x_{m}}\),以及相关的输出\({y_{1},…,y_{m}}\)</li>
<li>计算梯度和误差,更新\(v\)和\(m\),再根据\(v\)和\(m\)以及梯度计算参数更新量 </li>
</ul>
</li>
</ul>
<h4 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h4><p><em>Adam with Weight decay是Adam的一种优化</em></p>
<h5 id="Adam中的L2正则"><a href="#Adam中的L2正则" class="headerlink" title="Adam中的L2正则"></a>Adam中的L2正则</h5><ul>
<li><p>一般的L2正则<br>$$<br>Loss(w) = f(w) + \frac{1}{2}\eta||w||^2<br>$$</p>
</li>
<li><p>权重衰减后的参数更新如下<br>$$<br>\begin{align}<br>w &amp;= w - \alpha\nabla Loss(w) \\<br>&amp;= w - \alpha (\nabla f(w) + \eta w) \\<br>&amp;= w - \alpha \nabla f(w) - \alpha \eta w \\<br>\end{align}<br>$$</p>
</li>
<li><p>由于L2正则化项的存在，每次权重更新时都会减去一定比例的权重，即 \(\alpha \eta w \) ，这种现象叫做权重衰减（L2正则的目标就是让权重往小的方向更新，所以L2正则也叫作权重衰减）</p>
</li>
<li><p>L2正则也称为权重衰减，所以Adam优化的损失函数中添加L2正则的目标本应该也是为了权重衰减</p>
</li>
<li><p>Adam中的L2正则</p>
<ul>
<li>在每次求损失函数梯度前都计算\(\nabla Loss(w) = \nabla f(w) + \eta w\)</li>
<li>由于L2正则项的梯度\(\eta w\)也会被累加到一阶动量和二阶动量中，带有L2的Adam不再是简单的权重衰减，L2正则项还会影响到其他值的更新</li>
<li>Adam中的L2正则会产生我们不期望的结果，因为此时L2正则项影响了Adam参数的正常更新（我们想要L2做的仅仅是权重衰减，但在Adam中，L2产生了别的影响，这个不是我们想要的）</li>
</ul>
</li>
</ul>
<h5 id="AdamW——Adam-权重衰减"><a href="#AdamW——Adam-权重衰减" class="headerlink" title="AdamW——Adam+权重衰减"></a>AdamW——Adam+权重衰减</h5><ul>
<li>AdamW则不直接将L2添加到损失函数中，而是显示的把权重衰减提出来，主要修改是下面两步<ul>
<li>在计算梯度时，将L2正则从损失函数中去除</li>
<li>在更新参数时，显示增加权重衰减项</li>
</ul>
</li>
<li>相当于在更新参数时增加了L2正则，但是计算梯度时没有L2正则</li>
<li>原始论文：<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="noopener">Decoupled Weight Decay Regularization</a><img src="/Notes/DL/DL——各种梯度下降相关的优化算法/SGDW-AdamW.png">
<ul>
<li>图中紫色是原始Adam+L2实现部分，在AdamW中会被去除；</li>
<li>绿色是AdamW中新增的权重衰减部分（相当于更新参数时增加了L2正则项）</li>
</ul>
</li>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/643452086" target="_blank" rel="noopener">Adam和AdamW</a>，<a href="https://zhuanlan.zhihu.com/p/653605711" target="_blank" rel="noopener">从梯度下降到AdamW一文读懂机器学习优化算法</a></li>
<li>目前大模型常用的就是AdamW</li>
</ul>
<hr>
<h3 id="优化器与内存-显存"><a href="#优化器与内存-显存" class="headerlink" title="优化器与内存/显存"></a>优化器与内存/显存</h3><ul>
<li>训练的过程中，需要的内存/显存大小与优化器（Optimizer）有关<ul>
<li>需要存储到内存的变量包括以下几个方面<ul>
<li>梯度</li>
<li>参数</li>
<li>优化器状态（Optimizer States)，普通SGD没有这一项，而Adam和AdamW则需要存储一阶动量和二阶动量</li>
</ul>
</li>
</ul>
</li>
<li>优化器、参数量、内存/显存消耗、混合精度训练相关概念可参考<a href="https://arxiv.org/pdf/1910.02054.pdf" target="_blank" rel="noopener">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><ul>
<li>有些论文中也会直接将二阶动量叫做方差(Variance)或者二阶矩，因为二阶动量可以近似方差（当期望为0时）</li>
</ul>
</li>
<li>ZeRO论文中指出，在混合精度训练 + Adam/AdamW时，需要存储的变量包括<ul>
<li>FP16的参数</li>
<li>FP16的梯度</li>
<li>FP32的参数</li>
<li>FP32的一阶动量</li>
<li>FP32的二阶动量</li>
<li>注意：动量不能使用FP16吗？是的，不能，因为为了精度考虑使用时还是要被转换到FP32</li>
</ul>
</li>
</ul>
<hr>
<h3 id="各种优化方法的比较"><a href="#各种优化方法的比较" class="headerlink" title="各种优化方法的比较"></a>各种优化方法的比较</h3><h4 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h4><ul>
<li>SGD optimization on saddle point<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/gd_comparations_a.gif" title="gd_comparations_a.gif">

</li>
</ul>
<h4 id="等高线表面"><a href="#等高线表面" class="headerlink" title="等高线表面"></a>等高线表面</h4><ul>
<li><p>SGD optimization on loss surface contours</p>
<img src="/Notes/DL/DL——各种梯度下降相关的优化算法/gd_comparations_b.gif" title="gd_comparations_b.gif">
</li>
<li><p>上面两种情况都可以看出，Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到</p>
</li>
<li><p>由图可知自适应学习率方法即 Adagrad, Adadelta, RMSprop, Adam 在这种情景下会更合适而且收敛性更好</p>
</li>
</ul>
<h4 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h4><ul>
<li>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam<ul>
<li>因为他们能够为出现更新次数少(确切的说是梯度累计结果小)的特征分配更高的权重</li>
</ul>
</li>
<li>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的</li>
<li>Adam 可解释为 <strong>RMSprop + Momentum + Bias Correction</strong></li>
<li>随着梯度变的稀疏，Adam 比 RMSprop 效果会好</li>
<li><strong>整体来讲，Adam 是最好的选择</strong></li>
<li>很多论文里都会用 SGD，没有 momentum 等, SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点, 在不正确的方向上来回震荡</li>
<li>如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——特殊函数的反向传播</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E7%89%B9%E6%AE%8A%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.html</url>
    <content><![CDATA[<hr>
<h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><ul>
<li>参考文献：<a href="https://blog.csdn.net/xunan003/article/details/86597954" target="_blank" rel="noopener">普通max pooling反向传播与RoI max pooling反向传播解读</a></li>
</ul>
<h3 id="argmax"><a href="#argmax" class="headerlink" title="argmax"></a>argmax</h3><ul>
<li>参考文献：<a href="https://blog.csdn.net/weixin_39326879/article/details/105968540?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_aa&utm_relevant_index=1" target="_blank" rel="noopener">pytorch使用argmax argsoftmax</a></li>
<li>参考文献：Cross DQN</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>DL——迁移学习-元学习-联邦学习</title>
    <url>/Notes/DL/DL%E2%80%94%E2%80%94%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-%E5%85%83%E5%AD%A6%E4%B9%A0-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<h3 id="迁移学习-元学习-联邦学习对比"><a href="#迁移学习-元学习-联邦学习对比" class="headerlink" title="迁移学习-元学习-联邦学习对比"></a>迁移学习-元学习-联邦学习对比</h3><ul>
<li>迁移学习侧重于知识从源任务到目标任务的迁移</li>
<li>元学习侧重于快速适应新任务的能力</li>
<li>联邦学习则侧重于在数据隐私保护的前提下进行分布式学习。</li>
</ul>
<h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><ul>
<li>迁移学习（Transfer Learning）允许模型在一个任务上学习得到的知识应用到另一个不同但相关的任务上。这种方法特别适用于目标任务的数据量不足时。在迁移学习中，通常有一个源域（source domain）和一个目标域（target domain），模型首先在源域上进行训练，然后将学到的特征或参数迁移到目标域以提高学习效率和性能。</li>
<li>参考博客: <a href="https://blog.csdn.net/dakenz/article/details/85954548" target="_blank" rel="noopener">https://blog.csdn.net/dakenz/article/details/85954548</a></li>
</ul>
<h4 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h4><ul>
<li>元学习（Meta-Learning），又称为“学会学习”，是指模型不仅学习如何处理具体的任务，而且学习如何从经验中快速适应和学习新任务的过程。元学习特别关注于当面对新任务时，如何利用已有的知识来加速学习过程。元学习的一个典型应用是通过少量的样本（例如，少样本学习）快速适应新任务。</li>
<li>参考链接：<a href="https://www.bilibili.com/video/BV1UN4y1A7hr/?vd_source=b4442974569859635a5e307b2d4e3b56" target="_blank" rel="noopener">【李宏毅-元学习】少样本&amp;元学习Meta Learning_MAML最新机器学习课程！！！</a></li>
</ul>
<h4 id="联邦学习"><a href="#联邦学习" class="headerlink" title="联邦学习"></a>联邦学习</h4><ul>
<li>联邦学习（Federated Learning）是一种分布式机器学习范式，它允许多个参与者在保持数据隐私和数据本地化的前提下共同构建机器学习模型。在联邦学习中，数据不需要集中存储或处理，而是在各个参与者的本地进行训练，只有模型的更新（如参数）在参与者之间共享。这种方式可以解决数据孤岛问题，同时保护用户隐私。</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch——backward函数详细解析</title>
    <url>/Notes/PyTorch/PyTorch%E2%80%94%E2%80%94backward%E5%87%BD%E6%95%B0%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90.html</url>
    <content><![CDATA[<p><em>本文主要介绍PyTorch中backward函数和grad的各种用法</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="梯度的定义"><a href="#梯度的定义" class="headerlink" title="梯度的定义"></a>梯度的定义</h3><ul>
<li>\(y\)对\(x\)的梯度可以理解为: <strong>当 \(x\) 增加1的时候, \(y\) 值的增加量</strong></li>
<li>如果\(x\)是矢量(矩阵或者向量等),那么计算时也需要看成是多个标量的组合来计算,算出来的值表示的也是 \(x\) 当前维度的值增加1的时候, \(y\) 值的增加量</li>
</ul>
<hr>
<h3 id="backward基础用法"><a href="#backward基础用法" class="headerlink" title="backward基础用法"></a>backward基础用法</h3><ul>
<li>tensorflow是先建立好图，在前向过程中可以选择执行图的某个部分(每次前向可以执行图的不同部分，前提是，图里必须包含了所有可能情况)</li>
<li>pytorch是每次前向过程都会重新建立一个图，反向(backward)的时候会释放，每次的图可以不一样, 所以在Pytorch中可以随时使用<code>if</code>, <code>while</code>等语句 <ul>
<li>tensorflow中使用<code>if</code>, <code>while</code>就得在传入数据前(构建图时)告诉图需要构建哪些逻辑,然后才能传入数据运行</li>
<li>PyTorch中由于不用在传入数据前先定义图(图和数据一起到达,图构建的同时开始计算数据?)</li>
</ul>
</li>
</ul>
<h4 id="计算标量对标量的梯度"><a href="#计算标量对标量的梯度" class="headerlink" title="计算标量对标量的梯度"></a>计算标量对标量的梯度</h4><ul>
<li><p>结构图如下所示</p>
<img src="/Notes/PyTorch/PyTorch——backward函数详细解析/backward_tree_scalar2scalar.jpg"></li>
<li><p>上面图的代码构建如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">w1 = Variable(torch.Tensor([2]),requires_grad=True)</span><br><span class="line">w2 = Variable(torch.Tensor([3]),requires_grad=True)</span><br><span class="line">w3 = Variable(torch.Tensor([5]),requires_grad=True)</span><br><span class="line">x = w1 + w2</span><br><span class="line">y = w2*w3</span><br><span class="line">z = x+y</span><br><span class="line">z.backward()</span><br><span class="line">print(w1.grad)</span><br><span class="line">print(w2.grad)</span><br><span class="line">print(w3.grad)</span><br><span class="line">print(x.grad)</span><br><span class="line">print(y.grad)</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">tensor([1.])</span><br><span class="line">tensor([6.])</span><br><span class="line">tensor([3.])</span><br><span class="line">None</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<ul>
<li>从图中的推导可知,梯度符合预期</li>
<li>\(x, y\)不是叶节点,没有梯度存储下来,注意可以理解为梯度计算了,只是没有存储下来,PyTorch中梯度是一层层计算的</li>
</ul>
</li>
</ul>
<h4 id="计算标量对矢量的梯度"><a href="#计算标量对矢量的梯度" class="headerlink" title="计算标量对矢量的梯度"></a>计算标量对矢量的梯度</h4><ul>
<li><p>修改上面的构建为</p>
<ul>
<li>增加变量 \(s = z.mean\),然后直接求取\(s\)的梯度</li>
</ul>
</li>
<li><p>结构图如下:</p>
<img src="/Notes/PyTorch/PyTorch——backward函数详细解析/backward_tree_scalar2vector.jpg"></li>
<li><p>代码如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">w1 = Variable(torch.ones(2,2)*2,requires_grad=True)</span><br><span class="line">w2 = Variable(torch.ones(2,2)*3,requires_grad=True)</span><br><span class="line">w3 = Variable(torch.ones(2,2)*5,requires_grad=True)</span><br><span class="line">x = w1 + w2</span><br><span class="line">y = w2*w3</span><br><span class="line">z = x+y</span><br><span class="line"># z.backward()</span><br><span class="line">s = z.mean()</span><br><span class="line">s.backward()</span><br><span class="line">print(w1.grad)</span><br><span class="line">print(w2.grad)</span><br><span class="line">print(w3.grad)</span><br><span class="line">print(x.grad)</span><br><span class="line">print(y.grad)</span><br><span class="line"># output:</span><br><span class="line">tensor([[0.2500, 0.2500],</span><br><span class="line">        [0.2500, 0.2500]])</span><br><span class="line">tensor([[1.5000, 1.5000],</span><br><span class="line">        [1.5000, 1.5000]])</span><br><span class="line">tensor([[0.7500, 0.7500],</span><br><span class="line">        [0.7500, 0.7500]])</span><br><span class="line">None</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<ul>
<li>显然推导结果符合代码输出预期</li>
<li>梯度的维度与原始自变量的维度相同,每个元素都有自己对应的梯度,表示<strong>当当前元素增加1的时候, 因变量值的增加量</strong></li>
</ul>
</li>
</ul>
<h4 id="计算矢量对矢量的梯度"><a href="#计算矢量对矢量的梯度" class="headerlink" title="计算矢量对矢量的梯度"></a>计算矢量对矢量的梯度</h4><ul>
<li><p>还以上面的结构图为例</p>
</li>
<li><p>直接求中间节点 \(z\) 关于自变量的梯度</p>
</li>
<li><p>代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">w1 = Variable(torch.ones(2,2)*2, requires_grad=True)</span><br><span class="line">w2 = Variable(torch.ones(2,2)*3, requires_grad=True)</span><br><span class="line">w3 = Variable(torch.ones(2,2)*5, requires_grad=True)</span><br><span class="line">x = w1 + w2</span><br><span class="line">y = w2*w3</span><br><span class="line">z = x+y</span><br><span class="line">z_w1_grad = torch.autograd.grad(outputs=z, inputs=w1, grad_outputs=torch.ones_like(z))</span><br><span class="line">print(z_w1_grad)</span><br></pre></td></tr></table></figure>

<ul>
<li>在因变量是矢量时,<code>grad_outputs</code>参数不能为空,标量时可以为空(<code>grad_outputs</code>为空时和<code>grad_outputs</code>维度为1时等价)</li>
<li><code>grad_outputs</code>的维度必须和<code>outputs</code>参数的维度兼容</li>
</ul>
</li>
</ul>
<h4 id="关于autograd-grad函数"><a href="#关于autograd-grad函数" class="headerlink" title="关于autograd.grad函数"></a>关于autograd.grad函数</h4><ul>
<li>参考博客: <a href="https://blog.csdn.net/qq_36556893/article/details/91982925" target="_blank" rel="noopener">https://blog.csdn.net/qq_36556893/article/details/91982925</a></li>
</ul>
<h5 id="grad-outputs参数详解"><a href="#grad-outputs参数详解" class="headerlink" title="grad_outputs参数详解"></a><code>grad_outputs</code>参数详解</h5><ul>
<li>在因变量是矢量时,<code>grad_outputs</code>参数不能为空,标量时可以为空(<code>grad_outputs</code>为空时和<code>grad_outputs</code>维度为1时等价)</li>
<li><code>grad_outputs</code>的维度必须和<code>outputs</code>参数的维度兼容<br>[待更新]</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch——使用问题记录</title>
    <url>/Notes/PyTorch/PyTorch%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95.html</url>
    <content><![CDATA[<h3 id="PyTorch和torchvision版本不兼容"><a href="#PyTorch和torchvision版本不兼容" class="headerlink" title="PyTorch和torchvision版本不兼容"></a>PyTorch和torchvision版本不兼容</h3><ul>
<li><p>问题描述：<br><code>RuntimeError: Couldn&#39;t load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.version and your torchvision version with torchvision.version and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install. </code></p>
</li>
<li><p>解决方案：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install torch torchvision --upgrade</span><br></pre></td></tr></table></figure>

</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch——CrossEntopyLoss和NLLLoss的区别</title>
    <url>/Notes/PyTorch/PyTorch%E2%80%94%E2%80%94CrossEntopyLoss%E5%92%8CNLLLoss%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><ul>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.nn.NLLLoss()</span><br></pre></td></tr></table></figure>
</li>
<li><p>具体操作</p>
<ul>
<li>返回负对数似然损失(The negative log likelihood loss)</li>
<li>虽然命名是负对数自然损失, 但是实际上本函数不含有<code>log</code>操作,本函数假设<code>log</code>操作在输入前已经完成了</li>
</ul>
</li>
<li><p>常用于分类问题的损失函数</p>
</li>
<li><p>一般适用于网络最后一层为<code>log_softmax</code>的时候</p>
</li>
</ul>
<h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><ul>
<li>单个样本的计算公式:<ul>
<li>普通样本计算公式:<br>$$loss(x, class) = -x[class]$$</li>
<li>带有权重的单个样本计算公式:<br>$$loss(x, class) = -weights[class] * x[class]$$</li>
<li>因为多类别分类中,类别中只有一个维度是1, 其他维度都是0, 所以在计算时只考虑为1的维度就行, 为0的维度与当前类别值相乘为0<ul>
<li>(在这里我们存储的不是向量,而是该为1的维度的索引,所以使用-x[class]即可直接取出该样本对应的对数似然损失,其中,取对数的操作在输入前已经完成了) </li>
</ul>
</li>
</ul>
</li>
<li>批量样本的计算公式:<ul>
<li><code>size_average=True</code>(default):<br>$$all\_loss = \frac{1}{mini\_batch\_size}\sum_i loss(x_i, class_i)$$</li>
<li><code>size_average=False</code>:<br>$$all\_loss = \sum_i loss(x_i, class_i)$$</li>
</ul>
</li>
</ul>
<hr>
<h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><ul>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
</li>
<li><p>具体操作</p>
<ul>
<li>等价于 <code>log_softmax</code> + <code>torch.nn.NLLLoss()</code></li>
<li>先对网络输出做对数似然, 然后再</li>
</ul>
</li>
<li><p>softmax的定义<br>$$softmax(x_{i}) = \frac{e^{x_{i}}}{\sum_{j=1}x_{j}}$$</p>
</li>
<li><p>log_softmax的定义<br>$$log(softmax(x_{i}))$$</p>
<ul>
<li>注意: 这里的<code>log</code>是以<code>e</code>为底的对数</li>
</ul>
</li>
</ul>
<h4 id="为什么是这种实现方式"><a href="#为什么是这种实现方式" class="headerlink" title="为什么是这种实现方式?"></a>为什么是这种实现方式?</h4><ul>
<li>为什么是<code>log_softmax</code> + <code>torch.nn.NLLLoss()</code>的方式而不是普通的计算方式<ul>
<li>普通的计算方式是直接对每个概率求出log值, 然后相加, 本质上是一样的</li>
<li><code>CrossEntropyLoss</code>中多了个softmax是为了保证输入都是概率值</li>
</ul>
</li>
<li><code>log(softmax(x))</code>的优化<ul>
<li>实际上使用的是<code>log_softmax(x)</code></li>
<li><code>log_softmax(x)</code>的运算速度比单独计算<code>softmax</code> + <code>log</code>的速度快</li>
<li>同时二者的运算结果相同</li>
<li>文档中没有提到, 但是一种可能的优化方法是<br>$$<br>\begin{align}<br>log_sofmax(x) &amp;= log \frac{e^{x_{i}}}{\sum_{j=1}x_{j}} \\<br>&amp;= log e^{x_i} - log \sum_{j=1}x_{j} \\<br>&amp;= x_i - log \sum_{j=1}x_{j}<br>\end{align}<br>$$</li>
<li>上面的式子中,只需要计算一次 \(log \sum_{j=1}x_{j}\)即可(且不同维度可重用该值), 其他的都是加减法运算</li>
</ul>
</li>
</ul>
<h3 id="相关损失函数-BCELoss"><a href="#相关损失函数-BCELoss" class="headerlink" title="相关损失函数 BCELoss"></a>相关损失函数 BCELoss</h3><ul>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.nn.BCELoss()</span><br></pre></td></tr></table></figure>
</li>
<li><p>具体操作</p>
<ul>
<li>就是实现了书上定义的二分类交叉熵定义</li>
</ul>
</li>
</ul>
<h4 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式:"></a>计算公式:</h4><ul>
<li>单个样本的计算公式:<ul>
<li>普通样本计算公式:<br>$$ loss(o,t)=-\frac{1}{n}\sum_i(t[i] log(o[i])+(1-t[i]) log(1-o[i])) $$</li>
<li>带有权重的单个样本计算公式:<br>$$ loss(o,t)=-\frac{1}{n}\sum_iweights[i]\ (t[i]log(o[i])+(1-t[i])* log(1-o[i])) $$</li>
<li>因为多类别分类中,类别中只有一个维度是1, 其他维度都是0, 所以在计算时只考虑为1的维度就行, 为0的维度与当前类别值相乘为0<ul>
<li>(在这里我们存储的不是向量,而是该为1的维度的索引,所以使用-x[class]即可直接取出该样本对应的对数似然损失,其中,取对数的操作在输入前已经完成了) </li>
</ul>
</li>
</ul>
</li>
<li>批量样本的计算公式:<ul>
<li><code>size_average=True</code>(default):<br>$$all\_loss = \frac{1}{mini\_batch\_size}\sum_i loss(o_i, t_i)$$</li>
<li><code>size_average=False</code>:<br>$$all\_loss = \sum_i loss(o_i, t_i)$$</li>
</ul>
</li>
</ul>
<h4 id="BCELoss-vs-CrossEntropyLoss"><a href="#BCELoss-vs-CrossEntropyLoss" class="headerlink" title="BCELoss vs CrossEntropyLoss"></a>BCELoss vs CrossEntropyLoss</h4><ul>
<li><code>BCELoss</code>对应的网络只有一个输出值</li>
<li><code>CrossEntropyLoss</code>对应的网络有两个输出值</li>
<li>可以证明, 二分类时<code>BCELoss</code> 与 <code>CrossEntropyLoss</code>等价<ul>
<li>证明时, 将每个<code>CrossEntropyLoss</code>的计算公式中的 <code>softmax</code> 函数分子分母同时除以<code>shift</code>, 即可得到为下面的定义, 进一步可得到<code>BCELoss</code>的计算公式<br>$$f_i(x) = \frac{e^{(x_i - shift)}} { \sum^j e^{(x_j - shift)}},shift = max (x_i)$$</li>
</ul>
</li>
</ul>
<h3 id="相关损失函数-MultiLabelMarginLoss"><a href="#相关损失函数-MultiLabelMarginLoss" class="headerlink" title="相关损失函数 MultiLabelMarginLoss"></a>相关损失函数 MultiLabelMarginLoss</h3><ul>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.nn.MultiLabelMarginLoss()</span><br></pre></td></tr></table></figure>
</li>
<li><p>用于多标签分类的损失函数</p>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>一般来说直接使用<code>CrossEntropyLoss</code>即可<ul>
<li>二分类时还可以使用<code>nn.BCELoss</code></li>
<li>二分类时使用<code>nn.BCELoss</code>的话,输入的<code>input</code>和<code>target</code>维度都为<code>n * 1</code>的维度</li>
<li>二分类时使用<code>CrossEntropyLoss</code>则输入的<code>input</code>为<code>n * 2</code>的维度</li>
</ul>
</li>
<li>如果使用<code>NLLLoss</code>则一定记得在输出层最后加一层<code>log_softmax</code>层</li>
<li>注意,<code>log</code>指的是以<code>e</code>为底的对数函数,而不是以<code>10</code>为底的<ul>
<li>Mac自带的计算器中<code>log</code>是以<code>10</code>为底的,<code>ln</code>才是以<code>e</code>为底的</li>
</ul>
</li>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch——计算机视觉torchvision</title>
    <url>/Notes/PyTorch/PyTorch%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89torchvision.html</url>
    <content><![CDATA[<p><em>PyTorch中有个torchvision包,里面包含着很多计算机视觉相关的数据集(datasets),模型(models)和图像处理的库(transforms)等</em><br><em>本文主要介绍数据集中(ImageFolder)类和图像处理库(transforms)的用法</em></p>
<hr>
<h3 id="PyTorch预先实现的Dataset"><a href="#PyTorch预先实现的Dataset" class="headerlink" title="PyTorch预先实现的Dataset"></a>PyTorch预先实现的Dataset</h3><ul>
<li><p>ImageFolder</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.datasets import ImageFolder</span><br></pre></td></tr></table></figure>
</li>
<li><p>COCO</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.datasets import coco</span><br></pre></td></tr></table></figure>
</li>
<li><p>MNIST</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.datasets import mnist</span><br></pre></td></tr></table></figure>
</li>
<li><p>LSUN</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.datasets import lsun</span><br></pre></td></tr></table></figure>
</li>
<li><p>CIFAR10</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.datasets import CIFAR10</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h3><ul>
<li><p><code>ImageFolder</code>假设所有的文件按照文件夹保存,每个文件夹下面存储统一类别的文件,文件夹名字为类名</p>
</li>
<li><p>构造函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImageFolder(root, transform=None, target_transform=None, loader=default_loader)</span><br></pre></td></tr></table></figure>

<ul>
<li>root：在root指定的路径下寻找图片,root下面的每个子文件夹就是一个类别,每个子文件夹下面的所有文件作为当前类别的数据</li>
<li>transform：对PIL Image进行的转换操作，transform的输入是使用loader读取图片的返回对象    <ul>
<li>PIL是 Python Imaging Library 的简称,是Python平台上图像处理的标准库</li>
</ul>
</li>
<li>target_transform：对label的转换, 默认会自动编码<ul>
<li>默认编码为从0开始的数字,如果我们自己将文件夹命名为从0开头的数字,那么将按照我们的意愿命名,否则命名顺序不确定</li>
<li>测试证明,如果文件夹下面是<code>root/cat/</code>, <code>root/dog/</code>两个文件夹,则自动编码为{‘cat’: 0, ‘dog’: 1}</li>
<li><code>class_to_idx</code>属性存储着文件夹名字和类别编码的映射关系,<code>dict</code></li>
<li><code>classes</code>属性存储着所有类别,<code>list</code></li>
</ul>
</li>
<li>loader：从硬盘读取图片的函数<ul>
<li>不同的图像读取应该用不同的loader</li>
<li>默认读取为RGB格式的PIL Image对象</li>
<li>下面是默认的<code>loader</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def default_loader(path):</span><br><span class="line">    from torchvision import get_image_backend</span><br><span class="line">    if get_image_backend() == &apos;accimage&apos;:</span><br><span class="line">        return accimage_loader(path)</span><br><span class="line">    else:</span><br><span class="line">        return pil_loader(path)</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="transfroms详解"><a href="#transfroms详解" class="headerlink" title="transfroms详解"></a><code>transfroms</code>详解</h4><ul>
<li><p>包导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.transforms import transforms</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>transforms</code>包中包含着很多封装好的<code>transform</code>操作</p>
<ul>
<li><code>transforms.Scale(size)</code>:将数据变成制定的维度</li>
<li><code>transforms.ToTensor()</code>:将数据封装成PyTorch的<code>Tensor</code>类</li>
<li><code>transforms.Normalize(mean, std)</code>: 将数据标准话,具体标准化的参数可指定</li>
</ul>
</li>
<li><p>可将多个操作组合到一起,同时传入 <code>ImageFolder</code> 等对数据进行同时操作,每个操作被封装成一个类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">simple_transform = transforms.Compose([transforms.Resize((224,224))</span><br><span class="line">                                       ,transforms.ToTensor()</span><br><span class="line">                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span><br><span class="line">train = ImageFolder(&apos;dogsandcats/train/&apos;,simple_transform)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torchvision.transforms.transforms</code>包下的操作类都是基于<code>torchvision.transforms.functional</code>下的函数实现的</p>
<ul>
<li>导入<code>torchvision.transforms.functional</code>的方式<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision.transforms import functional</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch——各种常用函数总结</title>
    <url>/Notes/PyTorch/PyTorch%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>PyTorch封装了很多有用的函数,本文主要介绍介绍其中常用的函数</em></p>
<hr>
<h3 id="torch-max"><a href="#torch-max" class="headerlink" title="torch.max"></a>torch.max</h3><p><em><code>torch.min</code>与<code>torch.max</code>完全类似</em></p>
<h4 id="单参数"><a href="#单参数" class="headerlink" title="单参数"></a>单参数</h4><ul>
<li><p>用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.max(input) -&gt; Tensor</span><br></pre></td></tr></table></figure>

<ul>
<li><code>input</code>: 一个<code>Tensor</code>的对象</li>
<li>return: 返回<code>input</code>变量中的最大值</li>
</ul>
</li>
</ul>
<h4 id="多参数"><a href="#多参数" class="headerlink" title="多参数"></a>多参数</h4><ul>
<li><p>用法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.max(input, dim, keepdim=False, out=None) -&gt; tuple[Tensor, Tensor]</span><br></pre></td></tr></table></figure>

<ul>
<li><code>input</code>: 一个<code>Tensor</code>的对象</li>
<li><code>dim</code>: 指明维度<ul>
<li><code>dim=0</code>: 生成的结果是第一维的数据为1, 对每个元素, 当前数据是遍历第一维的数据后的最大值<ul>
<li>如果数据为2维, 则搜索每一列中最大的那个元素, 且返回最大元素的行索引(实际上相当于对每个列我们要求出来一个数,这个数是遍历第一维(行)得到的), 每列返回一个行索引(该索引就是当前列中数字最大的行)</li>
<li><code>input</code> 为 <code>(2,3)</code>, 则返回 <code>(1,3)</code></li>
</ul>
</li>
<li><code>dim=1</code>: <ul>
<li>如果数据为2维, 则搜索每一行中最大的那个元素, 且返回最大元素的列索引(实际上相当于对每个行我们要求出来一个数,这个数是遍历第2维(列)得到的), 每列返回一个列索引(该索引就是当前行中数字最大的列)</li>
<li><code>input</code> 为 <code>(2,3)</code>, 则返回 <code>(2,1)</code></li>
</ul>
</li>
</ul>
</li>
<li><code>keepdim</code>: 指明是否</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark——DataFrame读取Array类型</title>
    <url>/Notes/Spark/Spark%E2%80%94%E2%80%94DataFrame%E8%AF%BB%E5%8F%96Array%E7%B1%BB%E5%9E%8B.html</url>
    <content><![CDATA[<hr>
<h3 id="spark从DataFrame中读取Array类型的列"><a href="#spark从DataFrame中读取Array类型的列" class="headerlink" title="spark从DataFrame中读取Array类型的列"></a>spark从DataFrame中读取Array类型的列</h3><ul>
<li>代码<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataFrame.rdd.map(row =&gt; &#123;</span><br><span class="line">    val vectorCol = row.getAs[Seq[Double]](&quot;VectorCol&quot;)</span><br><span class="line">    vectorCol.toArray</span><br><span class="line">&#125;).collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell——进程查找</title>
    <url>/Notes/Shell/Shell%E2%80%94%E2%80%94%E8%BF%9B%E7%A8%8B%E6%9F%A5%E6%89%BE.html</url>
    <content><![CDATA[<hr>
<h3 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h3><ul>
<li>应用场景：当使用命令<code>sh run.sh</code>启动一个进程后，想要删除，却不知道进程号</li>
<li>查找步骤：<ul>
<li>首先使用<code>ps aux | grep run.sh</code>列出进程</li>
<li>杀死进程<code>kill -9 [PID]</code></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark——SQL-Join详解</title>
    <url>/Notes/Spark/Spark%E2%80%94%E2%80%94SQL-Join%E8%AF%A6%E8%A7%A3.html</url>
    <content><![CDATA[<hr>
<p>*参考链接：<a href="https://www.jianshu.com/p/608872377546" target="_blank" rel="noopener">Spark调优 | 一文搞定 Join 优化</a></p>
]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow——使用笔记</title>
    <url>/Notes/TensorFlow/TensorFlow%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>TensorFlow封装了很多有用的函数,本文主要介绍介绍其中常用的函数</em></p>
<ul>
<li>优秀参考链接：<ul>
<li><a href="https://www.cnblogs.com/ying-chease/p/9723309.html" target="_blank" rel="noopener">https://www.cnblogs.com/ying-chease/p/9723309.html</a></li>
<li><a href="https://www.cnblogs.com/wuzhitj/p/6298004.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuzhitj/p/6298004.html</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="tf-identify"><a href="#tf-identify" class="headerlink" title="tf.identify()"></a>tf.identify()</h3><ul>
<li>参考链接： <a href="https://blog.csdn.net/qq_23981335/article/details/81361748" target="_blank" rel="noopener">https://blog.csdn.net/qq_23981335/article/details/81361748</a></li>
<li><code>y=x</code> 图上是看不到y的具体节点的，因为这仅仅是个普通的Python变量引用复制，不是一个TF操作<ul>
<li>由于不是一个TF操作，所以执行<code>y</code>与对<code>x</code>取值操作一样，没有多余的操作</li>
</ul>
</li>
<li><code>tf.identify()</code>会复制一个变量，此时图中会增加一个操作</li>
</ul>
<hr>
<h3 id="string与数字类型的转换"><a href="#string与数字类型的转换" class="headerlink" title="string与数字类型的转换"></a>string与数字类型的转换</h3><ul>
<li>tf.as_string()</li>
<li>tf.string_to_number()</li>
</ul>
<hr>
<h3 id="tf-pad"><a href="#tf-pad" class="headerlink" title="tf.pad()"></a>tf.pad()</h3><hr>
<h3 id="tf-slice"><a href="#tf-slice" class="headerlink" title="tf.slice()"></a>tf.slice()</h3><ul>
<li><code>tf.slice(x, [0, 2], [-1, 1])</code>等价于<code>x[:, 2:3]</code><ul>
<li>两者都会在graph中增加一个节点</li>
<li>需要注意，在graph里面两者的节点不一样，但是获得的结果是相同的<ul>
<li>图中<code>x[:, 2:3]</code>会多几个输出属性，所以猜测<code>tf.slice</code>更好些【不精确】</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="tf-map-fn"><a href="#tf-map-fn" class="headerlink" title="tf.map_fn()"></a>tf.map_fn()</h3><hr>
<h3 id="字符串转固定二维数组"><a href="#字符串转固定二维数组" class="headerlink" title="字符串转固定二维数组"></a>字符串转固定二维数组</h3><ul>
<li><p>场景：每行包含一个字符串，每个字符串中包含多个值，按照’;’分隔开</p>
<h4 id="转换方式1"><a href="#转换方式1" class="headerlink" title="转换方式1"></a>转换方式1</h4></li>
<li><p>方案：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nums = tf.string_split(tf.reshape(nums, [-1]), delimiter=&quot;;&quot;)</span><br><span class="line"></span><br><span class="line"># SparseTensor不能使用string_to_number，下面的句子不能用</span><br><span class="line"># nums = tf.string_to_number(nums, out_type=tf.float32)</span><br><span class="line"></span><br><span class="line"># 如果知道batch_size的话可以用下面的句子来减少内存使用，但不是必要的</span><br><span class="line"># nums = tf.sparse_slice(nums, [0, 0], [batch_size, target_len]) </span><br><span class="line"></span><br><span class="line">nums = tf.sparse_tensor_to_dense(nums, default_value=&apos;0.0&apos;)</span><br><span class="line">nums = tf.string_to_number(nums, out_type=tf.float32)</span><br><span class="line"># 目前无法保障行的长度，通过pad 0实现最大长度，然后再截断到目标长度（有点浪费内存了）</span><br><span class="line">nums = tf.pad(nums, paddings=[[0, 0], [0, target_len]], mode=&quot;CONSTANT&quot;)</span><br><span class="line"># nums = nums[:, :target_len] == </span><br><span class="line">nums = tf.slice(nums, [0, 0], [-1, target_len])</span><br></pre></td></tr></table></figure>
</li>
<li><p>缺点</p>
<ul>
<li>浪费内存和时间</li>
</ul>
</li>
</ul>
<h4 id="转换方式2"><a href="#转换方式2" class="headerlink" title="转换方式2"></a>转换方式2</h4><ul>
<li><p>方案：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nums = tf.string_split(tf.reshape(nums, [-1]), delimiter=&quot;;&quot;)</span><br><span class="line"></span><br><span class="line"># 确保每行大小不超过target_len，否则sparse_to_dense在稀疏长度大于目标长度会报错</span><br><span class="line"># target_len可以大于稀疏长度的，第一个维度不能为-1，可用nums.dense_shape[0]</span><br><span class="line">nums = tf.sparse_slice(nums, [0, 0], [nums.dense_shape[0], target_len]) </span><br><span class="line"># sparse_to_dense的batch_size是必要的，不能为-1，处理到最后一个batch时，`nums = tf.sparse_to_dense(nums.indices, [batch_size, target_len], nums.values, default_value=&apos;0.0&apos;)`会出现问题，因为最后一层真实的数据量不是batch_size</span><br><span class="line">nums = tf.sparse_to_dense(nums.indices, [nums.dense_shape[0], target_len], nums.values, default_value=&apos;0.0&apos;)</span><br><span class="line">nums = tf.string_to_number(nums, out_type=tf.float32)</span><br></pre></td></tr></table></figure>
</li>
<li><p>优点</p>
<ul>
<li>简单快捷，节省内存</li>
</ul>
</li>
</ul>
<hr>
<h3 id="数据mask处理"><a href="#数据mask处理" class="headerlink" title="数据mask处理"></a>数据mask处理</h3><ul>
<li>应用场景：屏蔽部分数据，比如包含所有粗排队列及输入ctr长度信息，需要屏蔽掉没有进入CTR的数据</li>
<li>屏蔽方案：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 屏蔽被移除的广告对应的数据，max_len为最大长度，fix_len为截断长度，默认值为0</span><br><span class="line">def mask_fix_len_nums(nums, max_len, real_len, name):</span><br><span class="line">    with tf.name_scope(&quot;mask_%s&quot; % name):</span><br><span class="line">        mask = tf.sequence_mask(real_len, max_len)</span><br><span class="line">        mask_nums = tf.where(mask, nums, tf.zeros_like(nums))</span><br><span class="line">    return mask_nums</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="tf-layers-max-pooling1d"><a href="#tf-layers-max-pooling1d" class="headerlink" title="tf.layers.max_pooling1d"></a>tf.layers.max_pooling1d</h3><ul>
<li><p>需要先将数据处理为rank=3才可以，如果原始数据为rank=1或rank=2，可通过reshape函数修改shape，然后pooling处理完后再恢复到原来的shape</p>
</li>
<li><p>举例</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pool_raw_bids = tf.reshape(raw_bids, [-1, 1, 100])</span><br><span class="line">    print pool_raw_bids.shape</span><br><span class="line">bid_pool_result = tf.layers.max_pooling1d(pool_raw_bids, 10, strides=10, padding=&quot;valid&quot;, data_format=&quot;channels_first&quot;)</span><br><span class="line"># 10个一组，pooling后长度为10</span><br><span class="line">final_result = tf.reshape(bid_pool_result, [-1, 10])</span><br></pre></td></tr></table></figure>
</li>
<li><p>关于参数<code>valid</code>:<a href="https://vimsky.com/article/3881.html" target="_blank" rel="noopener">https://vimsky.com/article/3881.html</a></p>
</li>
<li><p>参数<code>data_format</code>: </p>
<ul>
<li>“channels_first”: 指明channel维度在前数据维度前  </li>
<li>“channels_last”: 指明channel维度在前数据维度后</li>
<li>详情看函数源码文档即可</li>
</ul>
</li>
</ul>
<hr>
<h3 id="人工转换数值型特征为类别型"><a href="#人工转换数值型特征为类别型" class="headerlink" title="人工转换数值型特征为类别型"></a>人工转换数值型特征为类别型</h3><ul>
<li>代码  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">queue_len = tf.convert_to_tensor([81, 72, 63, 144, 33, 10, 209])</span><br><span class="line">ori_len = tf.convert_to_tensor([84, 109, 80, 203, 68, 300, 245])</span><br><span class="line">queue_len = tf.reshape(queue_len, [-1])</span><br><span class="line">ori_len = tf.reshape(ori_len, [-1])</span><br><span class="line">b = [4,10,19,33,52,78,114,164,240]</span><br><span class="line">def bucket(x):</span><br><span class="line">    # 下面个的代码无法保证顺序，会出现编码混乱的局面，返回值全是[8,8,...]</span><br><span class="line">    # bucket_dict = &#123;</span><br><span class="line">    #     tf.less(x, b[0]): lambda: 0,</span><br><span class="line">    #     tf.less(x, b[1]): lambda: 1,</span><br><span class="line">    #     tf.less(x, b[2]): lambda: 2,</span><br><span class="line">    #     tf.less(x, b[3]): lambda: 3,</span><br><span class="line">    #     tf.less(x, b[4]): lambda: 4,</span><br><span class="line">    #     tf.less(x, b[5]): lambda: 5,</span><br><span class="line">    #     tf.less(x, b[6]): lambda: 6,</span><br><span class="line">    #     tf.less(x, b[7]): lambda: 7,</span><br><span class="line">    #     tf.less(x, b[8]): lambda: 8</span><br><span class="line">    # &#125;</span><br><span class="line">    # 下面的代码也无法保证顺序</span><br><span class="line">    # bucket_dict = list()</span><br><span class="line">    # for i in range(len(b)):</span><br><span class="line">    #     bucket_dict.append((tf.less(x, b[i]), lambda: b[i]))</span><br><span class="line">    bucket_dict = [</span><br><span class="line">        (tf.less(x, b[0]), lambda: 0),</span><br><span class="line">        (tf.less(x, b[1]), lambda: 1),</span><br><span class="line">        (tf.less(x, b[2]), lambda: 2),</span><br><span class="line">        (tf.less(x, b[3]), lambda: 3),</span><br><span class="line">        (tf.less(x, b[4]), lambda: 4),</span><br><span class="line">        (tf.less(x, b[5]), lambda: 5),</span><br><span class="line">        (tf.less(x, b[6]), lambda: 6),</span><br><span class="line">        (tf.less(x, b[7]), lambda: 7),</span><br><span class="line">        (tf.less(x, b[8]), lambda: 8)</span><br><span class="line">    ]</span><br><span class="line">    return tf.case(bucket_dict, default=lambda:len(b), exclusive=False)</span><br><span class="line">ori_len_bucket = tf.map_fn(bucket, elems=ori_len)</span><br><span class="line">queue_len_bucket = tf.map_fn(lambda x: x/10, elems=queue_len)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="慎用随机算子"><a href="#慎用随机算子" class="headerlink" title="慎用随机算子"></a>慎用随机算子</h3><ul>
<li><p>使用随机算子后，每次调用都会重新初始化随机算子，获取到的值可能不同</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data = tf.random_uniform([1], 0.0, 1.0)</span><br><span class="line"># data = tf.constant([2], tf.float32)</span><br><span class="line">data = tf.Print(data, [data])</span><br><span class="line">pow_data_2 = tf.pow(data, [2])</span><br><span class="line">data_3 = tf.multiply(pow_data_2, 3)</span><br><span class="line"># print data</span><br><span class="line">with tf.Session() as s:</span><br><span class="line">    s.run(tf.global_variables_initializer())</span><br><span class="line">    # print s.run([data])</span><br><span class="line">    print s.run([data])</span><br><span class="line">    print s.run([pow_data_2])</span><br><span class="line">    print s.run([data_3])</span><br><span class="line"># 以上代码会答应3次tf.Print,3次print的结果不同，因为调用了两次初始化操作</span><br><span class="line"># TensorFlow是惰性的，不执行s.run操作就不会走一遍算子</span><br></pre></td></tr></table></figure>
</li>
<li><p>进一步的</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data = tf.random_uniform([1], 0.0, 1.0)</span><br><span class="line">data_list = [data, data, data]</span><br><span class="line"># data = tf.constant([2], tf.float32)</span><br><span class="line">data = tf.Print(data, [data])</span><br><span class="line">pow_data_2 = tf.pow(data, 2)</span><br><span class="line">data_3 = tf.multiply(pow_data_2, 3)</span><br><span class="line">data_list = tf.Print(data_list, [data_list])</span><br><span class="line"># print data</span><br><span class="line">with tf.Session() as s:</span><br><span class="line">    s.run(tf.global_variables_initializer())</span><br><span class="line">    # print s.run([data])</span><br><span class="line">    print s.run([data])</span><br><span class="line">    print s.run([pow_data_2])</span><br><span class="line">    print s.run([data_3])</span><br><span class="line">    print s.run([data_list])</span><br><span class="line">&quot;&quot;&quot; 输出如下, 上面四行为Print结果，后面的是run的结果</span><br><span class="line">[0.974354625]</span><br><span class="line">[0.14039886]</span><br><span class="line">[0.219203591]</span><br><span class="line">[[0.507081509][0.507081509][0.507081509]]</span><br><span class="line">[array([0.9743546], dtype=float32)]</span><br><span class="line">[array([0.01971184], dtype=float32)]</span><br><span class="line">[array([0.14415064], dtype=float32)]</span><br><span class="line">[array([[0.5070815],</span><br><span class="line">    [0.5070815],</span><br><span class="line">    [0.5070815]], dtype=float32)]</span><br><span class="line">&quot;&quot;&quot; </span><br><span class="line"># 由于data_list是一个对象，只会进行一次打印，同理，使用`s.run([data, pow_data_2])`时将打印出相同data对应的data和data^2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="tf-stop-gradient"><a href="#tf-stop-gradient" class="headerlink" title="tf.stop_gradient"></a>tf.stop_gradient</h3><ul>
<li>参考链接：<a href="https://blog.csdn.net/u013745804/article/details/79589514" target="_blank" rel="noopener">关于tf.stop_gradient的使用及理解</a></li>
</ul>
<hr>
<h3 id="tf-AUTO-REUSE"><a href="#tf-AUTO-REUSE" class="headerlink" title="tf.AUTO_REUSE"></a>tf.AUTO_REUSE</h3><ul>
<li>tf.layers.dense, 通过name复用<ul>
<li>被复用时，以dense对应的整个层为单位，这个层的参数数量应该相同<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># encoding=utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">input_layer = tf.Variable([[1,2,3,4]], trainable=False, dtype=tf.float32)</span><br><span class="line">input_layer10 = tf.Variable([[10,20,30,40]], trainable=False, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">def get_net1():</span><br><span class="line">    kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)</span><br><span class="line"></span><br><span class="line">    net = tf.layers.dense(</span><br><span class="line">        inputs=input_layer[:, :1],</span><br><span class="line">        units=1,</span><br><span class="line">        kernel_initializer=kernel_initializer,</span><br><span class="line">        trainable=True,</span><br><span class="line">        # reuse=False, # 这个值可以随便设置，似乎不影响程序正确性？</span><br><span class="line">        name=&apos;m&apos;)</span><br><span class="line">    return net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_net2():</span><br><span class="line">    kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)</span><br><span class="line"></span><br><span class="line">    net = tf.layers.dense(</span><br><span class="line">        inputs=input_layer10[:, :1],</span><br><span class="line">        units=1,</span><br><span class="line">        kernel_initializer=kernel_initializer,</span><br><span class="line">        trainable=True,</span><br><span class="line">        # # 当外层有reuse=tf.AUTO_REUSE的variable_scope时，这个reuse为True或False，只要name相同，都会复用；</span><br><span class="line">        # # 否则，当没有外层没有reuse=tf.AUTO_REUSE的variable_scope时，则True代表复用，False代表不复用（不复用时name不能相同）</span><br><span class="line">        # reuse=True,</span><br><span class="line">        name=&apos;m&apos;)</span><br><span class="line">    return net</span><br><span class="line">with tf.variable_scope(&apos;main_qnet&apos;, reuse=tf.AUTO_REUSE):</span><br><span class="line">    net1 = get_net1()</span><br><span class="line">    net2 = get_net2()</span><br><span class="line">    net_list = [net1, net2]</span><br><span class="line">with tf.Session() as s:</span><br><span class="line">    s.run(tf.global_variables_initializer())</span><br><span class="line">    print s.run([net_list])</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="minimize分解为compute-gradients和apply-gradients两步骤"><a href="#minimize分解为compute-gradients和apply-gradients两步骤" class="headerlink" title="minimize分解为compute_gradients和apply_gradients两步骤"></a>minimize分解为compute_gradients和apply_gradients两步骤</h3><ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/343628982" target="_blank" rel="noopener">以终为始：compute_gradients 和 apply_gradients</a></li>
</ul>
<hr>
<h3 id="tf-GraphKeys-UPDATE-OPS"><a href="#tf-GraphKeys-UPDATE-OPS" class="headerlink" title="tf.GraphKeys.UPDATE_OPS"></a>tf.GraphKeys.UPDATE_OPS</h3><ul>
<li>参考链接：<a href="https://blog.csdn.net/huitailangyz/article/details/85015611" target="_blank" rel="noopener">tensorflow中的batch_norm以及tf.control_dependencies和tf.GraphKeys.UPDATE_OPS的探究</a></li>
</ul>
<hr>
<h3 id="tf-train-get-or-create-global-step"><a href="#tf-train-get-or-create-global-step" class="headerlink" title="tf.train.get_or_create_global_step()"></a>tf.train.get_or_create_global_step()</h3><ul>
<li>创建或获取当前</li>
</ul>
<hr>
<h3 id="Estimator-export-savedmodel"><a href="#Estimator-export-savedmodel" class="headerlink" title="Estimator.export_savedmodel()"></a>Estimator.export_savedmodel()</h3><ul>
<li>重点参数：serving_input_receiver_fn<ul>
<li>返回<code>tf.estimator.export.ServingInputReceiver</code>或<code>tf.estimator.export.TensorServingInputReceiver</code>对象的函数</li>
<li>对象的构造包含了输入参数的信息，相当于在定义<code>model_fn</code>的features字段</li>
<li>该对象中的输入字段均是占位符的形式</li>
</ul>
</li>
<li>函数动作：<ul>
<li>通过调用<code>serving_input_receiver_fn</code>生成相关输入tensors，并按照字典结构存储到一个对象，暂命名为<code>features</code>中</li>
<li>将该<code>features</code>作为参数传入<code>model_fn</code>并构造网络</li>
<li>从cpkt中恢复相关参数并将模型必要信息存储到指定目录下</li>
<li>说明：线上使用文件时，可直接加载网络进行推断，其中占位符可以被线上实时指定的向量替代</li>
</ul>
</li>
<li>需要关注的点：<ul>
<li>如果使用estimator框架训练模型，则该函数存储时默认仅存储<code>model_fn</code>部分，在<code>input_fn</code>部分进行的一些无参数的特征工程等操作是不会被继承下来的，也就是说，线上线下一致性保障仅仅从<code>model_fn</code>处开始<ul>
<li>一种良好的编程习惯是：将线上特征处理操作在<code>model_fn</code>前完成，及tfrecord生成或<code>input_fn</code>中完成，后续的操作都放到<code>model_fn</code>中，从而保证线上线下的一致性</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="一般TensorFlow模型存储为PB文件"><a href="#一般TensorFlow模型存储为PB文件" class="headerlink" title="一般TensorFlow模型存储为PB文件"></a>一般TensorFlow模型存储为PB文件</h3><ul>
<li>参考链接：<a href="https://zhuanlan.zhihu.com/p/32887066" target="_blank" rel="noopener">TensorFlow 保存模型为 PB 文件</a></li>
</ul>
<hr>
<h3 id="variable-scope-vs-name-scope"><a href="#variable-scope-vs-name-scope" class="headerlink" title="variable_scope vs name_scope"></a>variable_scope vs name_scope</h3><ul>
<li>测试代码1：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def test():</span><br><span class="line">    data = tf.ones(shape=[3,5], dtype=tf.float32)</span><br><span class="line">    with tf.variable_scope(&quot;vs_test&quot;):</span><br><span class="line">        x = tf.get_variable(&quot;x&quot;, initializer=[10])</span><br><span class="line">        y = tf.constant(20)</span><br><span class="line">        z = tf.layers.dense(inputs=data, units=1, name=&quot;output&quot;)</span><br><span class="line">        a = tf.Variable(&quot;a&quot;)</span><br><span class="line">    with tf.name_scope(&quot;ns_test&quot;):</span><br><span class="line">        x1 = tf.get_variable(&quot;x&quot;, initializer=[10])</span><br><span class="line">        y1 = tf.constant(20)</span><br><span class="line">        z1 = tf.layers.dense(inputs=data, units=1, name=&quot;output&quot;)</span><br><span class="line">        a1 = tf.Variable(&quot;a&quot;)</span><br><span class="line">test()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>&lt;tf.Variable ‘vs_test/x:0’ shape=(1,) dtype=int32_ref&gt;<br>Tensor(“vs_test/Const:0”, shape=(), dtype=int32)<br>Tensor(“vs_test/output/BiasAdd:0”, shape=(3, 1), dtype=float32)<br>&lt;tf.Variable ‘vs_test/Variable:0’ shape=() dtype=string_ref&gt;<br>==========<br>&lt;tf.Variable ‘x:0’ shape=(1,) dtype=int32_ref&gt;<br>Tensor(“ns_test/Const:0”, shape=(), dtype=int32)<br>Tensor(“ns_test/output/BiasAdd:0”, shape=(3, 1), dtype=float32)<br>&lt;tf.Variable ‘ns_test/Variable:0’ shape=() dtype=string_ref&gt;</p>
</blockquote>
<ul>
<li><p>结论1：</p>
<ul>
<li>对于<code>variable_scope()</code>来说，所有方式获取的变量或layer等调用都会被加上前缀<ul>
<li><code>variable_scope()</code>包含<code>reuse</code>参数，对这个scope下的所有变量生效（包括通过<code>layer</code>调用或<code>get_variable</code>获取的变量）<ul>
<li><code>reuse = True</code>: 复用之前的同名变量，没有同名变量则抛出异常</li>
<li><code>reuse = False</code>: 创建新变量，有同名变量则抛出异常</li>
<li><code>reuse = tf.AUTO_REUSE</code>: 如果有同名变量，则复用之前的同名变量，否则创建新变量</li>
</ul>
</li>
</ul>
</li>
<li>对于<code>name_scope()</code>来说，通过<code>tf.get_variable</code>和<code>layer</code>获取到的变量不会被加上前缀，上面示例中打印出来的不是变量，而是网络输出值，可以被<code>name_scope</code>来管理<ul>
<li><code>name_scope()</code>没有<code>reuse</code>参数</li>
</ul>
</li>
</ul>
</li>
<li><p>参考链接：<a href="https://blog.csdn.net/shenxiaoming77/article/details/79141078" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaoming77/article/details/79141078</a></p>
<blockquote>
<p>name_scope: 为了更好地管理变量的命名空间而提出的。比如在 tensorboard 中，因为引入了 name_scope， 我们的 Graph 看起来才井然有序。<br>variable_scope: 大部分情况下，跟 tf.get_variable() 配合使用，实现变量共享的功能。</p>
</blockquote>
</li>
<li><p>测试代码2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def test():</span><br><span class="line">    data = tf.ones(shape=[3,5], dtype=tf.float32)</span><br><span class="line">    with tf.variable_scope(&quot;vs_test&quot;):</span><br><span class="line">        x = tf.get_variable(&quot;x&quot;, initializer=[10])</span><br><span class="line">        y = tf.constant(20)</span><br><span class="line">        z = tf.layers.dense(inputs=data, units=1, name=&quot;output&quot;)</span><br><span class="line">        a = tf.Variable(1, name=&quot;a&quot;)</span><br><span class="line">    with tf.variable_scope(&quot;vs_test&quot;):</span><br><span class="line">        x = tf.get_variable(&quot;y&quot;, initializer=[10])</span><br><span class="line">        # 下面这行会报错ValueError: Variable vs_test/output/kernel already exists</span><br><span class="line">        # z = tf.layers.dense(inputs=data, units=1, name=&quot;output&quot;)</span><br><span class="line">        a = tf.Variable(1, name=&quot;b&quot;)</span><br><span class="line">    with tf.name_scope(&quot;ns_test&quot;):</span><br><span class="line">        x1 = tf.get_variable(&quot;x&quot;, initializer=[10])</span><br><span class="line">        y1 = tf.constant(20)</span><br><span class="line">        z1 = tf.layers.dense(inputs=data, units=1, name=&quot;output&quot;)</span><br><span class="line">        a1 = tf.Variable(1, name=&quot;a&quot;)</span><br><span class="line">test()</span><br><span class="line"></span><br><span class="line">print &quot;=====trainable====&quot;</span><br><span class="line">trainable_var = tf.trainable_variables()</span><br><span class="line">for v in trainable_var: print v</span><br><span class="line"></span><br><span class="line">print &quot;=====vs_test====&quot;</span><br><span class="line">main_qnet_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=&apos;vs_test&apos;)</span><br><span class="line">for v in main_qnet_var: print v</span><br><span class="line"></span><br><span class="line">print &quot;=====ns_test====&quot;</span><br><span class="line">main_qnet_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=&apos;ns_test&apos;)</span><br><span class="line">for v in main_qnet_var: print v</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>=====trainable====<br>&lt;tf.Variable ‘vs_test/x:0’ shape=(1,) dtype=int32_ref&gt;<br>&lt;tf.Variable ‘vs_test/output/kernel:0’ shape=(5, 1) dtype=float32_ref&gt;<br>&lt;tf.Variable ‘vs_test/output/bias:0’ shape=(1,) dtype=float32_ref&gt;<br>&lt;tf.Variable ‘vs_test/a:0’ shape=() dtype=int32_ref&gt;<br>&lt;tf.Variable ‘vs_test/y:0’ shape=(1,) dtype=int32_ref&gt;<br>&lt;tf.Variable ‘vs_test_1/b:0’ shape=() dtype=int32_ref&gt;<br>&lt;tf.Variable ‘x:0’ shape=(1,) dtype=int32_ref&gt;<br>&lt;tf.Variable ‘output/kernel:0’ shape=(5, 1) dtype=float32_ref&gt;<br>&lt;tf.Variable ‘output/bias:0’ shape=(1,) dtype=float32_ref&gt;<br>&lt;tf.Variable ‘ns_test/a:0’ shape=() dtype=int32_ref&gt;<br>=====vs_test====<br>&lt;tf.Variable ‘vs_test/x:0’ shape=(1,) dtype=int32_ref&gt;<br>&lt;tf.Variable ‘vs_test/output/kernel:0’ shape=(5, 1) dtype=float32_ref&gt;<br>&lt;tf.Variable ‘vs_test/output/bias:0’ shape=(1,) dtype=float32_ref&gt;<br>&lt;tf.Variable ‘vs_test/a:0’ shape=() dtype=int32_ref&gt;<br>&lt;tf.Variable ‘vs_test/y:0’ shape=(1,) dtype=int32_ref&gt;<br>&lt;tf.Variable ‘vs_test_1/b:0’ shape=() dtype=int32_ref&gt;<br>=====ns_test====<br>&lt;tf.Variable ‘ns_test/a:0’ shape=() dtype=int32_ref&gt;</p>
</blockquote>
<ul>
<li>结论2：<ul>
<li>在多次定义<code>vs_test</code>后，<ul>
<li><code>tf.get_variable</code>获得的变量命名是<code>vs_test</code>开头的</li>
<li><code>tf.Variable</code>获得的变量命名是<code>vs_test_1</code>开头的</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="变量的reuse和trainable"><a href="#变量的reuse和trainable" class="headerlink" title="变量的reuse和trainable"></a>变量的reuse和trainable</h3><ul>
<li>reuse和trainable互不影响<ul>
<li>两次resue变量时使用不同的trainable是不会影响变量复用的，只要name相同，符合reuse条件即可</li>
<li>变量的trainable属性(是否被加入<code>tf.GraphKeys.TRAINABLE_VARIABLES</code>)由其第一次被定义时决定，后续对该变量的复用(<code>reuse=true</code>)将不会影响变量的trainable属性(不管后续变量申明时使用<code>trainable=True</code> or <code>trainable=False</code>)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="tf-layers-batch-normalization-方法"><a href="#tf-layers-batch-normalization-方法" class="headerlink" title="tf.layers.batch_normalization()方法"></a>tf.layers.batch_normalization()方法</h3><ul>
<li>需要注意两个参数<ul>
<li><code>training</code>: 是否处于train模式？默认为False<ul>
<li>True表示会根据当前的batch滑动平均更新均值和方差参数</li>
<li>False则表示不会更新该值，如果训练阶段设置为False，则滑动平均的均值和方差不会被更新</li>
</ul>
</li>
<li><code>trainable</code>: 是否将参数加入<code>GraphKeys.TRAINABLE_VARIABLES</code>中<ul>
<li>这里的参数不包括均值和方差（均值和方差由滑动平均根据Batch数据更新，不是训练参数）</li>
<li>参数指beta和gamma</li>
<li>只有参数被加入<code>GraphKeys.TRAINABLE_VARIABLES</code>中时才会被更新</li>
</ul>
</li>
</ul>
</li>
<li>总结，参数<code>training</code>和<code>trainable</code>参数在训练阶段一般设置为True，其他阶段设置为False<ul>
<li>其中<code>training</code>负责滑动平均的均值和方差更新，设置为False，相当于均值和方差都不会自动更新</li>
</ul>
</li>
</ul>
<hr>
<h3 id="tf-identity、tf-Print与tf-gradients"><a href="#tf-identity、tf-Print与tf-gradients" class="headerlink" title="tf.identity、tf.Print与tf.gradients"></a>tf.identity、tf.Print与tf.gradients</h3><ul>
<li><code>tf.Print</code>与<code>tf.identity</code>类似，可以认为是增加打印其他变量的<code>tf.identity</code></li>
<li><code>tf.Print</code>与<code>tf.identity</code>操作可以认为是复制操作，梯度为1</li>
<li>注意:<code>tf.Print</code>与<code>tf.identity</code>都是图里面的一个操作，会分支出来一个节点，在求梯度时，必须保证目标自变量和因变量在同一个路径上(注意判断<code>identity</code>节点是否在梯度路径上)</li>
</ul>
<hr>
<h3 id="梯度裁剪相关函数"><a href="#梯度裁剪相关函数" class="headerlink" title="梯度裁剪相关函数"></a>梯度裁剪相关函数</h3><ul>
<li>参考链接：<a href="https://www.cnblogs.com/marsggbo/p/10055760.html" target="_blank" rel="noopener">https://www.cnblogs.com/marsggbo/p/10055760.html</a></li>
</ul>
<hr>
<h3 id="不可导梯度处理"><a href="#不可导梯度处理" class="headerlink" title="不可导梯度处理"></a>不可导梯度处理</h3><ul>
<li>在神经网络中，有许多包含不可导点的函数，常用的比如<ul>
<li>MAE损失函数在x=0(或pred和label相等)时不可导</li>
<li>ReLU在x=0处不可导</li>
</ul>
</li>
<li>在TensorFlow中，定义这些函数时都可以直接正常定义，在计算梯度tf.gradients时，会自动处理不可导点，处理逻辑如下：<ul>
<li>MAE中，x&lt;0梯度为-1，x&gt;0梯度为1，则x=0取两者之间的值均可，比如TensorFlow中默认取0</li>
<li>ReLU类似的，在x&lt;0时梯度为0，x&gt;0时梯度为1，则x=0时取两者之间的值均可，比如TensorFlow中默认取0</li>
</ul>
</li>
<li>注意，整数可以做MAE运算，但是不能求梯度，会报错<code>TypeError: Fetch argument None has invalid type &lt;type &#39;NoneType&#39;&gt;</code><ul>
<li>这一点上，所有运算都一样，整数不能求梯度</li>
</ul>
</li>
</ul>
<hr>
<h3 id="tf-squeeze慎用"><a href="#tf-squeeze慎用" class="headerlink" title="tf.squeeze慎用"></a><code>tf.squeeze</code>慎用</h3><ul>
<li>在预估服务中，往往是单个请求输入的，输入会导致<code>tf.squeeze</code>将该维度丢弃，从而产生一些意想不到的问题</li>
</ul>
<hr>
<h3 id="tf-shape和a-get-shape"><a href="#tf-shape和a-get-shape" class="headerlink" title="tf.shape和a.get_shape"></a><code>tf.shape</code>和<code>a.get_shape</code></h3><ul>
<li><code>tf.shape</code>: 返回一个Tensor</li>
<li><code>a.get_shape</code>: 返回一个元组<ul>
<li>对于batch等未知值返回<code>?</code>, 例如<code>(?, 1)</code></li>
</ul>
</li>
<li>在某些特殊情况下，使用<code>tf.shape</code>作为reshape的shape参数，会导致输出shape为None，但是使用<code>a.get_shape</code>则不会，建议任何地方均优先使用<code>a.get_shape</code></li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala——Scala对象转Java对象</title>
    <url>/Notes/Scala/Scala%E2%80%94%E2%80%94Scala%E5%AF%B9%E8%B1%A1%E8%BD%ACJava%E5%AF%B9%E8%B1%A1.html</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>使用Scala语言调用Java的接口时，经常出现参数传递需要将Scala对象转换成Java对象的情况</li>
</ul>
<h3 id="转换方式"><a href="#转换方式" class="headerlink" title="转换方式"></a>转换方式</h3><ul>
<li><p>增加一行接口引用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scala.collection.JavaConverters._</span><br></pre></td></tr></table></figure>
</li>
<li><p>将对象转成Java对象</p>
<ul>
<li><p>原生的一些对象转换</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val a: Double = 1.0</span><br><span class="line">calDouble(java.lang.Double.valueOf(a)) // calDouble的参数要求是Java的对象</span><br></pre></td></tr></table></figure>
</li>
<li><p>List对象转换</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val a: List[Double] = List(1.0D,2.0D,3.0D,4.0D)</span><br><span class="line">calList(list.map(java.lang.Double.valueOf).asJava) // calDouble的参数要求是Java的对象</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow——LossNan问题</title>
    <url>/Notes/TensorFlow/TensorFlow%E2%80%94%E2%80%94LossNan%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>训练时梯度出现nan或-nan</li>
<li>问题详细描述：在训练时，出现异常：NanLossDuringTrainingError: NaN loss during training. </li>
</ul>
<h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><ul>
<li>进一步分析，batch_size=1且worker=1时，发现首先是线上梯度出现nan或-nan，然后下一轮跌待中所有数据都是nan,进一步导致loss为nan</li>
<li>在问题确认过程中，需要一步步排除：<ul>
<li>排除分母除0的情况，可以简单的用<code>tf.div_no_nan</code>代替<code>\</code></li>
<li>排除log参数为负数或0的情况</li>
<li>排除抽取batch中选择符合条件的行后出现行数为[]的情况，使用 <code>tf.shape(x)[0]</code> 是否为0来判断是否为空</li>
<li>重点：排除整数除法等导致的值为0的情况，这种情况比较隐晦，难以感知</li>
<li>有时候会是TensorFlow版本导致，需要注意</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow——LossNan问题</title>
    <url>/Notes/TensorFlow/TensorFlow%E2%80%94%E2%80%94%E5%8F%98%E9%87%8F%E5%AE%9A%E4%B9%89%E5%8F%8A%E5%A4%8D%E7%94%A8.html</url>
    <content><![CDATA[<h3 id="定量定义及复用"><a href="#定量定义及复用" class="headerlink" title="定量定义及复用"></a>定量定义及复用</h3><ul>
<li>示例代码<pre><code></code></pre>
</li>
<li>运行结果</li>
<li>结果分析</li>
<li>结论<ul>
<li><code>trainable = None</code> 与 <code>trainable = True</code>等价，也是<code>trainable</code>参数的默认值，会被添加到训练变量中<ul>
<li>只有当<code>trainable = False</code>时不会被添加到训练变量中</li>
</ul>
</li>
<li><code>reuse</code>参数取值为<code>True</code>,<code>None</code>,<code>tf.AUTO_REUSE</code>,<code>False</code>(说明文档没明确列出<code>False</code>,但尝试可以用)<ul>
<li><code>True</code>: 表示读取一个旧的同名变量，如没有则抛出异常</li>
<li><code>None</code>: 表示继承父scope的情况，若没有父scope，则默认为创建新变量，此时有同名变量则会抛出异常</li>
<li><code>tf.AUTO_REUSE</code>: 表示使用自动模式，没有同名变量，则创建一个，否则返回同名变量</li>
<li><code>False</code>特殊:<ul>
<li>经测试，当reuse参数在<code>variable_scope</code>或者<code>layer</code>中使用时，<code>False</code>等价于<code>None</code>，会继承父scope的情况</li>
</ul>
</li>
<li>注意，reuse参数在<code>variable_scope</code>或者<code>layer</code>中使用时<ul>
<li>父：<code>tf.AUTO_REUSE</code>，子<code>None</code>,<code>False</code> =&gt; 最终<code>tf.AUTO_REUSE</code></li>
<li>父：<code>tf.AUTO_REUSE</code>，子<code>True</code> =&gt; 最终<code>True</code></li>
<li>父：<code>True</code>，子<code>None</code>,<code>False</code> =&gt; 最终<code>True</code></li>
<li>父：<code>True</code>，子<code>tf.AUTO_REUSE</code> =&gt; 最终<code>tf.AUTO_REUSE</code></li>
<li>父：<code>None</code>,<code>False</code>, 子 xxx =&gt; 最终 xxx</li>
<li>对于多层的级联架构中，依然满足上面的规律，举例来说，中途任意一层有一个</li>
</ul>
</li>
</ul>
</li>
<li><code>reuse</code>参数只在<code>variable_scope</code>或者<code>layer</code>中可以使用，<code>tf.get_variable</code>,<code>name_scope</code>,<code>tf.Variable</code>中都不可用</li>
<li><code>tf.Variable</code>一定会新建变量，而且会在重名时自动修改变量名称<ul>
<li>注意， <code>tf.Variable</code>不等价于<code>tf.get_variable</code>中<code>reuse=False</code>的情况，后者在有同名变量时会报错</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>DL</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——归一化与标准化</title>
    <url>/Notes/ML/ML%E2%80%94%E2%80%94%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%8E%E6%A0%87%E5%87%86%E5%8C%96.html</url>
    <content><![CDATA[<p><em>本文总结归一化与标准化的理解和使用, 涉及到无量纲,中心化等知识, 但是并不完全</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p><em>Normalization</em></p>
<ul>
<li><p>把数据变成<strong>[0,1]或[-1,1]</strong>的小数</p>
</li>
<li><p>把有量纲表达式变成<strong>无量纲</strong>表达式，便于不同单位或量级的指标能够进行比较和加权</p>
</li>
<li><p>归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量</p>
<ul>
<li>无量纲的理解: <ul>
<li>通过某种数值变换去掉单位,的影响,比如”kg”和”g”都可以表示体重,但是前者的数字比后者小</li>
<li>不管是”kg”还是”g”作为单位,无量纲后他们的数值应该是一样的 </li>
<li><strong>无量纲的本质</strong>是说: <strong>变换后, 单位不再影响数据的数值</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>应用:</p>
<ul>
<li>LR等用梯度下降时, 先使用数据归一化可以使得梯度下降速度加快(否则可能下降方向并不是最好方向)</li>
</ul>
</li>
</ul>
<h4 id="归一化的不同方法"><a href="#归一化的不同方法" class="headerlink" title="归一化的不同方法"></a>归一化的不同方法</h4><h5 id="线性归一化"><a href="#线性归一化" class="headerlink" title="线性归一化"></a>线性归一化</h5><ul>
<li>Min-Max Normalization<br>$$ x_i’ = \frac{x_i-min(X)}{max(X)-min(X)}$$</li>
<li>平均归一化<br>$$ x_i’ = \frac{x_i-mean(X)}{max(X)-min(X)}$$</li>
<li>上面两种归一化在新数据加入时最大最小值会变化,所以不能在线归一化</li>
</ul>
<h5 id="非线性归一化"><a href="#非线性归一化" class="headerlink" title="非线性归一化"></a>非线性归一化</h5><ul>
<li>对数函数转换等</li>
</ul>
<hr>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p><em>Standardization</em></p>
<ul>
<li>使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1</li>
<li>数学描述<br>$$x_i’ = \frac{x-mean(X)}{std(X)}$$</li>
</ul>
<hr>
<h3 id="中心化"><a href="#中心化" class="headerlink" title="中心化"></a>中心化</h3><ul>
<li>平均值为0，对标准差无要求</li>
<li>数学描述<br>$$x_i’ = x-mean(X)$$</li>
</ul>
<hr>
<h3 id="归一化-vs-标准化"><a href="#归一化-vs-标准化" class="headerlink" title="归一化 vs 标准化"></a>归一化 vs 标准化</h3><h4 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h4><ul>
<li>归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种</li>
<li>标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响</li>
</ul>
<h4 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h4><ul>
<li>都能取消由于量纲不同引起的误差</li>
<li>都是一种线性变换(都是对向量X按照比例压缩再进行平移)</li>
<li>个人理解: <strong>标准化可以看作是一种特殊的归一化</strong></li>
</ul>
<h3 id="中心化-vs-标准化"><a href="#中心化-vs-标准化" class="headerlink" title="中心化 vs 标准化"></a>中心化 vs 标准化</h3><ul>
<li>标准化 = 中心化 + 数据除以标准差(使得数据标准差为1)</li>
<li>有些地方也把零均值归一化(Z-Score Normalization)称为标准化,公式与标准化相同</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>一些模型一般需要归一化<ul>
<li>LR: 加快梯度下降过程(因为不同维度使用相同的学习率,所以容易造成走弯路)</li>
<li>KNN: 防止计算距离时大数吃小数的情况发生</li>
<li>PCA: 归一化后 \(X^TX\) 才能表示数据的协方差矩阵</li>
<li><strong>神经网络</strong></li>
</ul>
</li>
</ul>
<h4 id="关于神经网络为什么需要归一化"><a href="#关于神经网络为什么需要归一化" class="headerlink" title="关于神经网络为什么需要归一化?"></a>关于神经网络为什么需要归一化?</h4><h5 id="数值问题"><a href="#数值问题" class="headerlink" title="数值问题"></a>数值问题</h5><ul>
<li>归一化可以避免很多不必要的数值问题<ul>
<li>我理解的一种情况: 输入太大时, 权重太小, 容易造成精度问题, 数值太小时同理</li>
</ul>
</li>
<li>归一化可以避免梯度爆炸等问题<ul>
<li>个人理解: 如果数值很大的话, 梯度一般也会对应很大, 连续乘以后就容易造成梯度爆炸, 数值太小, 同理, 容易造成梯度消失</li>
</ul>
</li>
<li>加快学习<ul>
<li>与LR一样, 使用梯度下降优化神经网络参数时, 如果数值差别太大, 可能造成优化时梯度优化走弯路(因为不同维度使用的是相同的学习率)</li>
</ul>
</li>
<li>避免某些数值小的神经元输出被数值大的输出吞掉的情况<ul>
<li>个人理解: 虽然说我们找到合适权重后,可以使得二者到下一个神经元的差值没那么大, 但是我的理解是初始化的时候不知道数值,所以权重是随机的,之后如果两个神经元数值差距太大的话,是否会大值吞小值很难说</li>
</ul>
</li>
</ul>
<h4 id="关于LR为什么需要归一化"><a href="#关于LR为什么需要归一化" class="headerlink" title="关于LR为什么需要归一化?"></a>关于LR为什么需要归一化?</h4><p><em>不是必要的</em></p>
<ul>
<li>因为LR使用梯度下降法求解参数时, 特征之间差别太大容易影响收敛速度, 归一化可以提升LR的收敛速度, 同时不影响分类结果</li>
</ul>
<h4 id="关于SVM为什么需要归一化"><a href="#关于SVM为什么需要归一化" class="headerlink" title="关于SVM为什么需要归一化?"></a>关于SVM为什么需要归一化?</h4><p><em>是必要的</em></p>
<ul>
<li>因为SVM寻找的是所谓的<strong>间隔</strong>(margin), 就是两个支持向量的间隔</li>
<li>如果<strong>不归一化</strong>的话, 这个间隔会因为不同特征的单位等, 数值被放大或者缩小, 从而造成<strong>无法评估间隔</strong>, 所以归一化对于SVM很重要</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Sklearn——总体介绍</title>
    <url>/Notes/ML/Sklearn/Sklearn%E2%80%94%E2%80%94%E6%80%BB%E4%BD%93%E4%BB%8B%E7%BB%8D.html</url>
    <content><![CDATA[<p><em>Sklearn 总体介绍</em></p>
<hr>
<h3 id="总结介绍图"><a href="#总结介绍图" class="headerlink" title="总结介绍图"></a>总结介绍图</h3><img src="/Notes/ML/Sklearn/Sklearn——总体介绍/sklearn-summary.png" title="sklearn-summary">
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>Python</tag>
        <tag>Sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac——环境变量配置.md</title>
    <url>/Notes/Mac/Mac%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE.html</url>
    <content><![CDATA[<hr>
<h3 id="Mac环境变量配置文件"><a href="#Mac环境变量配置文件" class="headerlink" title="Mac环境变量配置文件"></a>Mac环境变量配置文件</h3><ul>
<li><code>/etc/profile</code><ul>
<li>全局的</li>
</ul>
</li>
<li><code>~/.bash_profile</code><ul>
<li>用户自己的，可能需要自己生成</li>
</ul>
</li>
<li>生效方式<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Terminal自动导入环境变量"><a href="#Terminal自动导入环境变量" class="headerlink" title="Terminal自动导入环境变量"></a>Terminal自动导入环境变量</h3><ul>
<li><p>默认Terminal打开后不会导入用户自定义的环境变量</p>
</li>
<li><p>自动导入可以设置为</p>
<ul>
<li><p>在<code>~/.zshrc</code>（这个文件没有的话需要自己生成）中添加一行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>也可直接在<code>~/.zshrc</code>中直接定义环境变量，使用<code>~/.bash_profile</code>是为了和Linux匹配且方便切换不同的Bash工具</p>
</li>
<li><p>原理：Terminal打开的是zsh工具，这个工具启动时会自动执行<code>~/.zshrc</code></p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><ul>
<li>Idea中Terminal打开后可能不会跟系统一样自动导入环境变量<ul>
<li>如果使用的Terminal是zsh的话，也会自动导入</li>
</ul>
</li>
<li>可以自己手动添加（一次添加，以后都可以自动使用）</li>
<li>或者每次打开Terminal后执行<code>source ~/.bash_profile</code></li>
</ul>
]]></content>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——MLE和MAP的区别举例说明</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94MLE%E5%92%8CMAP%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E.html</url>
    <content><![CDATA[<p><em>本文用一个简单的例子在直观上说明极大似然估计与最大后验估计的区别</em></p>
<hr>
<h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><ul>
<li><p>事件定义</p>
<ul>
<li>A = 好好学习 </li>
<li>B = 作弊</li>
<li>C = 班级第一</li>
</ul>
</li>
<li><p>条件概率</p>
<ul>
<li>P(C|A) = 0.1</li>
<li>P(C|B) = 0.5</li>
</ul>
</li>
<li><p>已知某同学考了年级第一</p>
<ul>
<li>此时可得概率：<ul>
<li>P(C) = 1</li>
</ul>
</li>
<li>ML(极大似然估计，找一个使得结果发生可能性最大的条件<br>  <strong>计算条件概率P(结果|条件)</strong><ul>
<li>参数为e</li>
<li>P(C|e = A) = 0.1</li>
<li>P(C|e = B) = 0.5</li>
<li>该同学更可能是作弊了</li>
</ul>
</li>
<li>若此时添加先验：<ul>
<li>P(A) = 0.99</li>
<li>P(B) = 0.01</li>
</ul>
</li>
<li>MAP(计算后验概率,在结果发生时,最可能是什么产生的?<br>  <strong>计算后验概率:P(条件|结果)</strong>,令P(C)=1<ul>
<li>P(A|C) = P(C|A)P(A)/P(C) = 0.1*0.99/1 = 0.099</li>
<li>P(B|C) = P(C|B)P(B)/P(C) = 0.5*0.01/1 = 0.005</li>
<li>此时可发现该同学更可能是好好学习了</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="深入分析：MLE与MAP的区别"><a href="#深入分析：MLE与MAP的区别" class="headerlink" title="深入分析：MLE与MAP的区别"></a>深入分析：MLE与MAP的区别</h3><ul>
<li>MAP比MLE多考虑了一个先验概率，这个先验概率指明了条件概率的条件(例如 P(C|A)的先验是P(A))</li>
<li>MAP的是比较后验概率，ML的是比较条件概率，但是两者本质上都是找一个使得结果发生的最可能的原因</li>
<li>MLE可以认为是特殊的MAP<ul>
<li>ML是先验概率为均匀分布时的MAP(上例中： 若P(A) = P(B), 则MLE等价于MAP)</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——参数估计MLE-MAP-BEP</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1MLE-MAP-BEP.html</url>
    <content><![CDATA[<p><em>极大似然估计(Maximum Likelihood Estimation,MLE)</em><br><em>最大后验概率估计(Maximum a posteriori Estimation,MAP)</em><br><em>贝叶斯估计(Bayesian Parameter Estimation,BPE)</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="一般理解"><a href="#一般理解" class="headerlink" title="一般理解"></a>一般理解</h3><ul>
<li>极大似然估计(MLE): <strong>频率学派</strong>(参数为固定的某个值)</li>
<li>贝叶斯估计(BPE): <strong>贝叶斯学派</strong>(参数服从某个分布)</li>
<li>最大后验概率估计(MAP): 一般的书籍认为属于<strong>频率学派</strong><ul>
<li>个人理解:MAP<strong>前半部分是贝叶斯学派,后半部分是频率学派</strong><ul>
<li>前半部分认为参数\(\theta\))有先验分布,此时参数服从一个分布,而不是确定的值,是贝叶斯学派的</li>
<li>后半部分认为参数不是服从后验分布,而是一个具体的使得后验概率最大的值,这里是频率学派的做法</li>
</ul>
</li>
</ul>
</li>
<li>三者的终极目标都是估计模型参数\(\theta\),<strong>MAP和MLE估计参数\(\theta\)为具体某个值,BPE估计参数\(\theta\)为一个分布</strong></li>
<li>其他贝叶斯网络模型(有向图概率图模型)参数估计的方法还有吉布斯采样,变分推断,EM算法等</li>
</ul>
<h4 id="极大似然估计与最大后验概率估计"><a href="#极大似然估计与最大后验概率估计" class="headerlink" title="极大似然估计与最大后验概率估计"></a>极大似然估计与最大后验概率估计</h4><ul>
<li>比较MLE与MAP直观理解见我的另一篇博客：<a href="/Notes/ML/Models/ML%E2%80%94%E2%80%94MLE%E5%92%8CMAP%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E.html" title="/Notes/ML/Models/ML——MLE和MAP的区别举例说明.html">MLE-vs-MAP——一个简单的例子说明二者的区别</a></li>
<li>都只关注参数的具体某个最优值</li>
<li>极大似然估计相当于是参数\(\theta\)的先验分布\(P(\theta)\)为均匀分布的最大后验概率估计</li>
</ul>
<h4 id="最大后验概率估计与贝叶斯估计"><a href="#最大后验概率估计与贝叶斯估计" class="headerlink" title="最大后验概率估计与贝叶斯估计"></a>最大后验概率估计与贝叶斯估计</h4><ul>
<li>相似点：<ul>
<li>都考虑了参数的先验分布(贝叶斯学派)</li>
<li>最大后验概率估计是贝叶斯估计的一种简化实现(认为\(\theta\)都是出现在最可能出现的地方，牺牲一点精度，不用求复杂的分母积分)</li>
<li>二者都围绕下面的公式进行估计<br>$$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$$</li>
</ul>
</li>
<li>区别：<ul>
<li><strong>贝叶斯估计</strong>:求得的结果估计参数\(\theta\)服从一个分布，不能忽略归一化因子\(P(X)\),此时\(P(X)=\int_{\theta}P(X|\theta)P(\theta)d_{\theta}\),贝叶斯估计要计算后验分布,所以需要计算归一化因子，虽然对不同参数值,分母积分结果是相同的，但是计算分布时需要这个归一化因子，否则后续得到的不是分布，也无法求取参数的期望等</li>
<li><strong>最大后验概率估计</strong>:求得的结果估计参数\(\theta\)是一个具体的值，把分母\(P(X)\)给忽略了,直接对分子极大化求得最优的参数\(\theta^{\star}\),由于跟贝叶斯估计一样，对不同参数值,分母积分结果是相同的,所以\(P(X)\)的值不影响最优参数值的选取【不求参数\(\theta\)的分布，所以不用求分母，分母是不影响求参数最优值的】,不用计算归一化因子(很复杂,需要积分),速度会快很多</li>
</ul>
</li>
<li>最大后验概率估计相当于是参数后验分布\(P(\theta|X)\)被最可能的参数值\(\theta^{\star}=\arg\max_{\theta}P(\theta|X)\)替代的贝叶斯估计</li>
</ul>
<table>
<thead>
<tr>
<th align="center">估计模型</th>
<th align="center">目标值</th>
<th align="left">关心对象</th>
<th align="left">求值方式</th>
</tr>
</thead>
<tbody><tr>
<td align="center">MLE</td>
<td align="center">$$\theta^{\star}=\arg\max_{\theta}(P(X\mid \theta)$$</td>
<td align="left">\(P(X\mid \theta)\)</td>
<td align="left">最大化似然函数\(P(X\mid \theta)\)</td>
</tr>
<tr>
<td align="center">MAP</td>
<td align="center">$$\theta^{\star}=\arg\max_{\theta}(P(\theta\mid X)$$</td>
<td align="left">\(P(X\mid \theta)P(\theta)\)</td>
<td align="left">最大化后验概率\(P(\theta\mid X)=\frac{P(X\mid \theta)P(\theta)}{P(X)}\)<br>只需最大化分子\(P(X,\theta)=P(X\mid \theta)P(\theta)\)</td>
</tr>
<tr>
<td align="center">BPE</td>
<td align="center">\(P(\theta\mid X)\)<br>(\(\theta\)的后验概率分布)</td>
<td align="left">\(\frac{P(X\mid \theta)P(\theta)}{P(X)}\)</td>
<td align="left">求参数\(\theta\)的后验概率分布\(P(\theta\mid X)\)<br>\(P(\theta\mid X)=\frac{P(X\mid \theta)P(\theta)}{\int_{\theta}P(X\mid\theta)P(\theta)d_{\theta}}\)</td>
</tr>
</tbody></table>
<hr>
<h3 id="一个容易理解的角度"><a href="#一个容易理解的角度" class="headerlink" title="一个容易理解的角度"></a>一个容易理解的角度</h3><ul>
<li>参考博客: <a href="https://blog.csdn.net/liu1194397014/article/details/52766760" target="_blank" rel="noopener">https://blog.csdn.net/liu1194397014/article/details/52766760</a></li>
</ul>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><ul>
<li><p>已知数据集为\(X=(x_{1}, x_{2},,,x_{n})\)</p>
</li>
<li><p>极大似然估计</p>
<ul>
<li>已知数据集\(X\)的情况下,求参数最优值\(\theta^{\star}\),使得似然函数\(P(X|\theta)\)最大</li>
</ul>
</li>
<li><p>最大后验概率估计</p>
<ul>
<li>已知数据集\(X\)的情况下,求参数最优值\(\theta^{\star}\),使得后验概率\(P(\theta|X)\)最大,实际只需使得\(P(X,\theta)=P(X|\theta)P(\theta)\)最大</li>
</ul>
</li>
<li><p>贝叶斯估计</p>
<ul>
<li>已知数据集\(X\)的情况下,求参数\(\theta\)的后验分布\(P(\theta|X)\)</li>
</ul>
</li>
</ul>
<h4 id="极大似然估计-MLE"><a href="#极大似然估计-MLE" class="headerlink" title="极大似然估计(MLE)"></a>极大似然估计(MLE)</h4><ul>
<li>推导<br>$$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$$<ul>
<li>\(P(\theta)\)的取值与参数\(\theta\)无关【认为每个参数\(\theta\)出现的概率相等】<ul>
<li>\(P(\theta)\)与参数\(\theta\)无关可以理解为参数\(\theta\)服从<strong>均匀分布</strong>: 假设参数\(\theta\)有k个离散取值\(\theta_{1}, \theta_{2},,,\theta_{k}\),那么\(P(\theta_{1})=P(\theta_{2})=…=P(\theta_{k})=\frac{1}{k}\)</li>
</ul>
</li>
<li>\(P(X)\)的取值与\(\theta\)无关,设置为\(P(X)=1\)即可</li>
<li>所以得到下面的表达式<br>$$\arg\max_{\theta}P(\theta|X)=\arg\max_{\theta}P(X|\theta)$$<ul>
<li>式子中\(P(X|\theta)\)被称为<strong>似然函数</strong></li>
</ul>
</li>
</ul>
</li>
<li>结论<ul>
<li>极大似然估计: \(\theta^{\star}=\arg\max_{\theta}P(X|\theta)\)</li>
<li>文字解释: 极大似然估计的目标是找一个参数\(\theta\),使得在参数\(\theta\)对应的概率分布模型下,数据集\(X\)出现的概率最大(似然函数最大)</li>
</ul>
</li>
</ul>
<h4 id="最大后验概率估计-MAP"><a href="#最大后验概率估计-MAP" class="headerlink" title="最大后验概率估计(MAP)"></a>最大后验概率估计(MAP)</h4><ul>
<li>推导<br>$$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$$<ul>
<li>\(P(\theta)\)为的取值与参数\(\theta\)相关,是关于\(\theta\)的一个先验概率【不同的参数\(\theta\)出现的概率可能不同】<ul>
<li>\(P(\theta)\)与参数\(\theta\)相关可以理解为: 假设参数\(\theta\)有k个取值\(\theta_{1}, \theta_{2},,,\theta_{k}\),那么\(\sum_{i=1}^{k}P(\theta_{i})=1\)，但\(P(\theta_{1}),P(\theta_{2}),,,P(\theta_{k})\)的取值不能确定，也不一定相等</li>
</ul>
</li>
<li>\(P(X)=\int_{\theta}P(X|\theta)P(\theta)d_{\theta}\)积分结果与\(\theta\)不相关,分母的值不影响后验概率最大值的参数值,直接忽略(设置为\(P(X)=1\))即可</li>
<li>所以得到<br>$$\arg\max_{\theta}P(\theta|X)=\arg\max_{\theta}P(X|\theta)P(\theta)=\arg\max_{\theta}P(X,\theta)$$</li>
</ul>
</li>
<li>结论<ul>
<li>最大后验概率估计: \(\theta^{\star}=\arg\max_{\theta}P(X|\theta)P(\theta)\)</li>
<li>文字解释: 最大后验概率估计的目标是找一个参数\(\theta\),使得在参数\(\theta\)服从已知先验分布\(P(\theta)\)和数据集\(X\)的情况下,对应的后验概率\(P(\theta|X)\)最大(等价于\(P(X,\theta)=P(X|\theta)P(\theta)\)最大)</li>
</ul>
</li>
</ul>
<h4 id="贝叶斯估计-BPE"><a href="#贝叶斯估计-BPE" class="headerlink" title="贝叶斯估计(BPE)"></a>贝叶斯估计(BPE)</h4><ul>
<li>推导<br>$$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$$<ul>
<li>\(P(\theta)\)为的取值与参数\(\theta\)相关,是关于\(\theta\)的一个先验概率<ul>
<li>\(P(\theta)\)与参数\(\theta\)相关可以理解为: 假设参数\(\theta\)有k个取值\(\theta_{1}, \theta_{2},,,\theta_{k}\),那么\(\sum_{i=1}^{k}P(\theta_{i})=1\)，但\(P(\theta_{1}),P(\theta_{2}),,,P(\theta_{k})\)的取值不确定，也不一定相等</li>
</ul>
</li>
<li>\(P(X)\)的取值与\(\theta\)本身不相关,但是为了求出后验分布\(P(\theta|X)\),\(P(X)\)作为归一化因子需要计算<ul>
<li>\(P(X)\)的值与参数的先验分布,模型的定义(高斯分布还是贝塔分布等)和数据集\(X\)有关系</li>
<li>没有分母作为归一化因子的话单独的分子是联合分布\(P(X,\theta)\),这个分布对\(\theta\)积分结果不为1(而是\(P(X)\)),联合分布不能确定\(\theta\)的后验分布</li>
<li>\(P(X)=\int_{\theta}P(X|\theta)P(\theta)d_{\theta}\)</li>
</ul>
</li>
<li>所以得到<br>$$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{\int_{\theta}P(X|\theta)P(\theta)d_{\theta}}$$</li>
</ul>
</li>
<li>结论<ul>
<li>贝叶斯估计: \(P(\theta|X)=\frac{P(X|\theta)P(\theta)}{\int_{\theta}P(X|\theta)P(\theta)d_{\theta}}\)</li>
<li>文字解释: 贝叶斯估计的目标是求参数的后验分布\(P(\theta|X)\),参数\(\theta\)服从先验分布\(P(\theta)\),在已知数据集\(X\)修正后,参数\(\theta\)的后验概率分布为\(P(\theta|X)\)</li>
</ul>
</li>
<li>缺点<ul>
<li>计算\(P(X)=\int_{\theta}P(X|\theta)P(\theta)d_{\theta}\)比较耗时</li>
</ul>
</li>
</ul>
<hr>
<h3 id="一个更深入的理解"><a href="#一个更深入的理解" class="headerlink" title="一个更深入的理解"></a>一个更深入的理解</h3><h4 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h4><ul>
<li>已知数据集为\(X=(x_{1}, x_{2},,,x_{n})\),新数据集为\(D=(d_{1}, d_{2},,,d_{m})\)</li>
<li>求已知数据集\(X\)的情况下,假设数据集\(X\)由某个模型M(参数为\(\theta\))生成,那么数据集\(D\)也由模型M生成的概率\(P(D|X)\)<ul>
<li>模型M的确定因素:<ul>
<li>由三个因素唯一确定<ul>
<li>参数\(\theta\)的先验分布\(P(\theta)\)</li>
<li>模型的类型定义\(f(\theta)\)(高斯分布,贝塔分布,二项分布还是其他什么分布)</li>
<li>模型的已知观察数据集\(X\)</li>
</ul>
</li>
<li>由两个因素唯一确定<ul>
<li>模型的类型定义\(f(\theta)\)(高斯分布,贝塔分布,二项分布还是其他什么分布)</li>
<li>参数\(\theta\)的后验分布\(P(\theta|X)\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>另一种表述: 用模型M(参数为\(\theta\))生成数据,在已观察到模型M生成了数据集\(X\)后,预测模型M接下来生成数据集\(D\)的概率</li>
</ul>
<h4 id="极大似然估计-MLE-1"><a href="#极大似然估计-MLE-1" class="headerlink" title="极大似然估计(MLE)"></a>极大似然估计(MLE)</h4><ul>
<li><p>特点:假设模型M(参数\(\theta\))没有先验分布</p>
</li>
<li><p>模型参数估计</p>
<ul>
<li>模型参数为一个确定的最优值</li>
<li>\(\theta^{\star}=\arg\max_{\theta}P(X|\theta)\)    </li>
</ul>
</li>
<li><p>概率计算</p>
<ul>
<li>\(P(D|X)=P(D|\theta^{\star})\)</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>计算速度快</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>不够精确,特别是当观察到的数据集\(X\)太小时,这种情况不考虑参数的先验分布可能会造成过拟合<ul>
<li>举例: 判断一个学校男女比例,在观察到两个男生,一个女生走出校门后就判断男女比例为2:1显然不合适<ul>
<li>注意:单个同学是男生或者女生的概率服从伯努利分布,多个同学中男生的数量服从二项分布,整个学校的某个学生是男生的概率服从贝塔分布</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="贝叶斯估计-BPE-1"><a href="#贝叶斯估计-BPE-1" class="headerlink" title="贝叶斯估计(BPE)"></a>贝叶斯估计(BPE)</h4><ul>
<li>特点:假设模型M(参数\(\theta\))有先验分布\(P(\theta)\),计算观察到数据集\(X\)后\(\theta\)的后验分布\(P(\theta|X)\),然后求\(P(D|\theta)\)关于后验分布\(P(\theta|X)\)的期望(这个期望也就是模型M生成数据集\(D\)的概率)<ul>
<li>这里可以理解为观察到的数据集是对先验分布的修正,修正后的后验分布会更符合模型实际情况(因为先验分布很可能是瞎猜的)</li>
</ul>
</li>
<li>模型参数估计<ul>
<li>模型参数为一个分布 </li>
<li>\(P(\theta|X)=\frac{P(X|\theta)P(\theta)}{\int_{\theta}P(X|\theta)P(\theta)d_{\theta}}\)</li>
<li>分子为归一化因子\(P(X)\)<ul>
<li>必须计算,该值与参数的先验分布\(P(\theta)\),模型的类型定义(\(f(\theta)\))以及数据集\(X\)有关系,不计算\(P(X)\)的话后面计算得到的\(P(D|X)\)也将是一个不确定的值(不是概率值,是\(P(X)\)的函数)</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>概率计算<ul>
<li>\(P(D|X)=\int_{\theta}P(D,\theta|X)d_{\theta}=\int_{\theta}P(D|\theta,X)P(\theta|X)d_{\theta}=\int_{\theta}P(D|\theta)P(\theta|X)d_{\theta}\)<ul>
<li>式子中\(P(D|\theta,X)=P(D|\theta)\),在通过\(X\)确定\(\theta\)后模型也就确定了,\(X\)不会继续影响\(D\)的生成</li>
</ul>
</li>
<li>这里\(P(D|X)\)相当于\(P(D|\theta)\)关于后验分布\(P(\theta|X)\)的期望</li>
</ul>
</li>
<li>优点<ul>
<li>计算结果最精确,能根据参数的先验分布和数据集\(X\)的知识,准确计算模型生成数据集\(D\)的概率</li>
</ul>
</li>
<li>缺点<ul>
<li>计算\(P(X)=\int_{\theta}P(X|\theta)P(\theta)d_{\theta}\)比较耗时,不常用</li>
</ul>
</li>
</ul>
<h5 id="关于-P-X-是否需要计算"><a href="#关于-P-X-是否需要计算" class="headerlink" title="关于\(P(X)\)是否需要计算?"></a>关于\(P(X)\)是否需要计算?</h5><ul>
<li>无需计算的情况<ul>
<li>判断两个数据集\(D_{1},D_{2}\)由同一个模型M生成的概率谁大,可以不用计算\(P(X)\)<ul>
<li>因为对于同一模型M,\(P(X)\)相同大小,此时只需要比较\(P(D_{1},X)\)和\(P(D_{2},X)\)谁大即可知道\(P(D_{1}|X)\)和\(P(D_{2}|X)\)谁大</li>
</ul>
</li>
</ul>
</li>
<li>必须计算的情况<ul>
<li>判断数据集\(D\)由两个模型\(M_{1},M_{2}\)生成的概率谁大,必须计算\(P(X)\)<ul>
<li>因为对于不同模型\(M_{1},M_{2}\),\(P(X)\)不相同【每个模型生成\(X\)的概率不同】,此时仅仅比较\(P(D_{1},X)\)和\(P(D_{2},X)\)谁大不能确定\(P(D_{1}|X)\)和\(P(D_{2}|X)\)谁大</li>
</ul>
</li>
<li>利用模型M采样生成新数据(预测问题),必须计算\(P(X)\)<ul>
<li>这时候每次采样时需要根据参数\(\theta\)的后验分布采样生成\(\theta_{i}\),然后再根据\(\theta_{i}\)确定的模型\(M_{i}\)采样生成观测数据,两次采样的过程都必须知道准确的分布(积分为1,也就是归一化后的),所以此时必须计算\(P(X)\)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="最大后验概率估计-MAP-1"><a href="#最大后验概率估计-MAP-1" class="headerlink" title="最大后验概率估计(MAP)"></a>最大后验概率估计(MAP)</h4><ul>
<li><p>特点:假设模型M(参数\(\theta\))有先验分布\(P(\theta)\),但不计算模型的后验分布\(P(\theta|X)\),只用最可能的\(\theta\)代替后验分布来确定模型M</p>
</li>
<li><p>模型参数估计</p>
<ul>
<li>模型参数为一个确定的最优值</li>
<li>\(\theta^{\star}=\arg\max_{\theta}P(\theta|X)=\arg\max_{\theta}P(X|\theta)P(\theta)\)<ul>
<li>这里与贝叶斯估计作对比可以发现,MAP相当于把参数后验分布简化为最可能的那个参数值,<strong>用最可能的参数值代替参数后验分布</strong>,这样做对最终预测结果\(P(D|X)\)可能有点误差,但是不用计算复杂的积分\(P(X)=\int_{\theta}P(X|\theta)P(\theta)d_{\theta}\)</li>
<li>这里能够用最优值替代分布的前提是分布是很集中的(也就是要\(P(\theta|X)\)方差小,比如贝塔分布,狄利克雷分布和高斯分布等钟型分布的塔尖要尖),不然误差可能会比较大</li>
<li>注意后验分布\(P(\theta|X)\)最可能值的参数值是概率最大的地方 \(\theta^{\star}=\arg\max_{\theta}P(\theta|X)\),而不是参数\(\theta\)关于后验分布的期望\(E_{P(\theta|X)}[\theta]=\int_{\theta}\theta P(\theta|X)d_{\theta}\) (<strong>期望对应平均值,而不是最可能的值</strong>,特例:对称钟型分布的期望同时也是它最可能的值)</li>
</ul>
</li>
</ul>
</li>
<li><p>概率计算</p>
<ul>
<li>\(P(D|X)=P(D|\theta^{\star})\)</li>
</ul>
</li>
<li><p>计算速度和精确度都介于MLE和BPE之间</p>
</li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>贝叶斯估计可以退化成最大后验概率估计<ul>
<li>最大后验概率估计相当于是参数后验分布\(P(\theta|X)\)被最可能的参数值\(\theta^{\star}=\arg\max_{\theta}P(\theta|X)\)替代的贝叶斯估计</li>
</ul>
</li>
<li>最大后验概率估计可以退化成极大似然估计<ul>
<li>极大似然估计相当于是参数\(\theta\)的先验分布\(P(\theta)\)为均为分布的最大后验概率估计</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Python——环境之版本和库以及搜索路径管理总结</title>
    <url>/Notes/Python/Python%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E4%B9%8B%E7%89%88%E6%9C%AC%E5%92%8C%E5%BA%93%E4%BB%A5%E5%8F%8A%E6%90%9C%E7%B4%A2%E8%B7%AF%E5%BE%84%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>关于Python版本与pip的关系</em></p>
<hr>
<h3 id="pip相关命令"><a href="#pip相关命令" class="headerlink" title="pip相关命令"></a>pip相关命令</h3><p>查看pip版本和安装位置</p>
<pre><code>pip --version</code></pre>
<p>列出pip安装库</p>
<pre><code>pip list</code></pre>
<p>查看安装库详细信息（安装目录和版本等）</p>
<pre><code>pip show &lt;lib name&gt;</code></pre>
<p>pip安装库</p>
<pre><code>pip install &lt;lib name&gt;</code></pre>
<p>pip安装(指定版本version)的库(libname)(到指定路径dir)</p>
<pre><code>pip install libname==version -t dir</code></pre>
<hr>
<h3 id="Mac自带的python"><a href="#Mac自带的python" class="headerlink" title="Mac自带的python:"></a>Mac自带的python:</h3><pre><code>python: /usr/bin/python
pip:     /usr/local/bin/pip
libs:    /Library/Python/2.7/site-packages(可通过pip --verison查看到)</code></pre>
<hr>
<h3 id="anaconda"><a href="#anaconda" class="headerlink" title="anaconda"></a>anaconda</h3><ul>
<li>Python2为主安装Python3版本<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python2</span><br><span class="line">python: /anaconda2/bin/python</span><br><span class="line">pip:	/anaconda2/bin/pip</span><br><span class="line">python3</span><br><span class="line">python: /anaconda2/envs/python3/bin/python</span><br><span class="line">pip 	/anaconda2/envs/python3/bin/pip</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="anaconda版本切换命令"><a href="#anaconda版本切换命令" class="headerlink" title="anaconda版本切换命令"></a>anaconda版本切换命令</h3><p>默认在python2环境下（对于安装时选择以python2.7版本为基础的anaconda而言）</p>
<pre><code>source activate python3
source deactivate python3</code></pre>
<hr>
<h3 id="Python-搜索路径与包（package）"><a href="#Python-搜索路径与包（package）" class="headerlink" title="Python 搜索路径与包（package）"></a>Python 搜索路径与包（package）</h3><p><strong>不同Python版本的搜索路径不一样</strong></p>
<h4 id="查看搜索路径"><a href="#查看搜索路径" class="headerlink" title="查看搜索路径"></a>查看搜索路径</h4><p><em>查看该Python版本的搜索路径</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python 		#在终端打开对应版本python（如果是在程序中则不需要这个命令）</span><br><span class="line">import sys	#导入sys包</span><br><span class="line">sys.path   	#输出该版本Python对应的path</span><br></pre></td></tr></table></figure>

<h4 id="运行时追加路径"><a href="#运行时追加路径" class="headerlink" title="运行时追加路径"></a>运行时追加路径</h4><p><em>运行时我们可以通过向sys.path追加路径来添加自己的库</em></p>
<ul>
<li>通过sys模块来添加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sys.path.append(&apos;./python&apos;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><em>如果项目需要添加自定义的库，比如z3py这样的库，我们可以通过在项目中包含该库的源码，然后获取当前程序文件的绝对地址，然后计算出z3py库的绝对地址，并在程序中添加z3py库的绝对路径到sys.path</em></p>
<p><strong>(为什么是绝对地址而不是相对地址：因为我们的代码(比如z3schedule.py)可能在不同的工作目录下被执行，不同的工作目录自然得到的相对路径也就不同，所以如果我们在其他目录下运行当前python文件，那么我们在这个文件下添加到sys.path的z3py对应的库就访问不到了，所以要读取当前文件的绝对路径而不是使用当前路径)</strong></p>
<ul>
<li>通过<code>__buildin__</code>来添加环境变量<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import __builtin__</span><br><span class="line">__builtin__.Z3_LIB_DIRS = /z3py/bin</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>注意：PYTHONPATH环境变量指定的目录会自动被添加到搜索路径，这个环境变量与sys.path没关系！！！</strong></p>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac——HomeBrew简介</title>
    <url>/Notes/Mac/Mac%E2%80%94%E2%80%94HomeBrew%E7%AE%80%E4%BB%8B.html</url>
    <content><![CDATA[<hr>
<h3 id="HomeBrew是什么？"><a href="#HomeBrew是什么？" class="headerlink" title="HomeBrew是什么？"></a>HomeBrew是什么？</h3><ul>
<li>HomeBrew是一个Mac OS上的软件包管理器，命令是brew，类似于Ubuntu上的DPKG(apt-get)或者Centos上的RPM(yum)等</li>
</ul>
<hr>
<h3 id="HomeBrew的安装"><a href="#HomeBrew的安装" class="headerlink" title="HomeBrew的安装"></a>HomeBrew的安装</h3><ul>
<li><p>安装命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/bin/ruby -e&quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看安装结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew -v</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="HomeBrew管理软件包"><a href="#HomeBrew管理软件包" class="headerlink" title="HomeBrew管理软件包"></a>HomeBrew管理软件包</h3><ul>
<li><em>HomeBrew的安装的软件包默认在<code>/usr/local/Cellar</code>下</em></li>
<li><em>HomeBrew用<code>@</code>指明版本号</em></li>
</ul>
<h4 id="搜索云端软件"><a href="#搜索云端软件" class="headerlink" title="搜索云端软件"></a>搜索云端软件</h4><p><em>返回结果中可以看到版本号，后续安装可指定版本号安装</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew search key_str</span><br></pre></td></tr></table></figure>

<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install pkg_name</span><br></pre></td></tr></table></figure>

<h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew remove pkg_name</span><br></pre></td></tr></table></figure>

<h4 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew upgrade pkg_name</span><br></pre></td></tr></table></figure>

<h4 id="列出所有已安装软件"><a href="#列出所有已安装软件" class="headerlink" title="列出所有已安装软件"></a>列出所有已安装软件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew list</span><br></pre></td></tr></table></figure>

<h4 id="查看某个云端软件包信息"><a href="#查看某个云端软件包信息" class="headerlink" title="查看某个云端软件包信息"></a>查看某个云端软件包信息</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew info pkg_name</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="HomeBrew管理服务"><a href="#HomeBrew管理服务" class="headerlink" title="HomeBrew管理服务"></a>HomeBrew管理服务</h3><h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew services start service_name</span><br></pre></td></tr></table></figure>

<h4 id="停止服务"><a href="#停止服务" class="headerlink" title="停止服务"></a>停止服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew services stop service_name</span><br></pre></td></tr></table></figure>

<h4 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew services restart service_name</span><br></pre></td></tr></table></figure>

<h4 id="列出所有HomeBrew管理的服务"><a href="#列出所有HomeBrew管理的服务" class="headerlink" title="列出所有HomeBrew管理的服务"></a>列出所有HomeBrew管理的服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew services list</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu——添加自定义的软件到启动器</title>
    <url>/Notes/Linux/Ubuntu%E2%80%94%E2%80%94%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%88%B0%E5%90%AF%E5%8A%A8%E5%99%A8.html</url>
    <content><![CDATA[<p><em>日常使用Ubuntu时,某些自己编写的脚本(程序)或者下载的程序不能从启动器搜索到</em></p>
<hr>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><ul>
<li>自己编写的脚本</li>
<li>自己编写的程序</li>
<li>网上自己下载的程序(不是通过apt-get或者APPStore安装)</li>
</ul>
<hr>
<h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><h4 id="编写程序启动脚本"><a href="#编写程序启动脚本" class="headerlink" title="编写程序启动脚本"></a>编写程序启动脚本</h4><p><em>如果需要运行的就是脚本这一步可以不用</em></p>
<ul>
<li>编写shell脚本,脚本内容为启动运行程序<ul>
<li>涉及到程序的名称这里应该用绝对路径</li>
</ul>
</li>
<li>脚本可以放到任意地方,一般和当前程序在一个文件夹下即可</li>
</ul>
<h4 id="添加脚本执行权限"><a href="#添加脚本执行权限" class="headerlink" title="添加脚本执行权限"></a>添加脚本执行权限</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x exmaple.sh</span><br></pre></td></tr></table></figure>

<h4 id="新建启动"><a href="#新建启动" class="headerlink" title="新建启动"></a>新建启动</h4><ul>
<li><p>新建文件到指定文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /usr/share/applications</span><br><span class="line">sudo gedit example.desktop</span><br></pre></td></tr></table></figure>
</li>
<li><p>编辑文件如下格式</p>
<ul>
<li>Name是程序搜索时会出现的名字</li>
<li>Exec是执行脚本路径,必须有执行权限,否则在启动器中仍然搜索不到该程序</li>
<li>Icon是软件的图标,这里可以任意自定义图标,从网上下载或者自己生成均可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Version=1.0</span><br><span class="line">Name=Example</span><br><span class="line">Exec=/home/username/example.sh</span><br><span class="line">Terminal=false</span><br><span class="line">Icon=/home/username/example.png</span><br><span class="line">Type=Application</span><br><span class="line">Categories=Development</span><br></pre></td></tr></table></figure>



</li>
</ul>
</li>
</ul>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ul>
<li>按键: Win键</li>
<li>输入Example</li>
<li>如果出现图标和名字对应的程序,说明成功</li>
<li>如果不成功,复查以下问题<ul>
<li>启动脚本路径是否正确且为绝对路径</li>
<li>执行权限是否已经成功设置</li>
<li>Name是否设置正确</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown——基本语法</title>
    <url>/Notes/Markdown/Markdown%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95.html</url>
    <content><![CDATA[<p><em>Markdown基本语法</em></p>
<hr>
<h3 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h3><pre><code>&lt;sup&gt;上标&lt;/sup&gt;</code></pre>
<blockquote>
<p>文字<sup>上标</sup></p>
</blockquote>
<pre><code>&lt;sub&gt;下标&lt;/sub&gt;</code></pre>
<blockquote>
<p>文字<sub>下标</sub></p>
</blockquote>
<hr>
<h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><pre><code>~~删除线~~</code></pre>
<blockquote>
<p><del>删除线</del></p>
</blockquote>
<hr>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><pre><code>&gt; 一级引用
&gt;&gt; 二级引用
&gt;&gt;&gt; 三级引用</code></pre>
<blockquote>
<p>一级引用</p>
<blockquote>
<p>二级引用</p>
<blockquote>
<p>三级引用</p>
</blockquote>
</blockquote>
</blockquote>
<hr>
<h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><pre><code>* 一级列表
    * 二级列表
    - 二级列表
    + 二级列表
* 一级列表
- 一级列表
+ 一级列表</code></pre>
<blockquote>
<ul>
<li>一级列表<ul>
<li>二级列表</li>
</ul>
<ul>
<li>二级列表</li>
</ul>
<ul>
<li>二级列表</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>一级列表</li>
</ul>
<ul>
<li>一级列表</li>
</ul>
<ul>
<li><p>一级列表</p>
<ol>
<li>有序列表<ul>
<li>无序列表</li>
<li>无序列表</li>
</ul>
</li>
<li>有序列表</li>
</ol>
</li>
</ul>
<blockquote>
<ol>
<li>有序列表<ul>
<li>无序列表</li>
<li>无序列表</li>
</ul>
</li>
<li>有序列表</li>
</ol>
</blockquote>
<hr>
<h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><pre><code>分割线
***
---</code></pre>
<blockquote>
<p>分割线</p>
<hr>
<hr>
</blockquote>
<hr>
<h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><pre><code>[链接](https://joezjh.github.io/ &quot;鼠标放到链接上显示此提示&quot;)</code></pre>
<blockquote>
<p><a href="https://joezjh.github.io/" title="鼠标放到链接上显示此提示">链接</a></p>
</blockquote>
<hr>
<h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><pre><code>![图片标题](图片链接 &quot;鼠标放到图片上显示此提示&quot;)</code></pre>
<blockquote>
<p><img src="%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5" alt="图片标题" title="鼠标放到图片上显示此提示"></p>
</blockquote>
<hr>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><pre><code>| 左对齐标题 | 右对齐标题 | 居中对齐标题 |
| :------| ------: | :------: |
| 短文本 | 中等文本 | 稍微长一点的文本 |
| 稍微长一点的文本 | 短文本 | 中等文本 |</code></pre>
<blockquote>
<table>
<thead>
<tr>
<th align="left">左对齐标题</th>
<th align="right">右对齐标题</th>
<th align="center">居中对齐标题</th>
</tr>
</thead>
<tbody><tr>
<td align="left">短文本</td>
<td align="right">中等文本</td>
<td align="center">稍微长一点的文本</td>
</tr>
<tr>
<td align="left">稍微长一点的文本</td>
<td align="right">短文本</td>
<td align="center">中等文本</td>
</tr>
</tbody></table>
</blockquote>
]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo——不显示post时间</title>
    <url>/Notes/Hexo/Hexo%E2%80%94%E2%80%94%E4%B8%8D%E6%98%BE%E7%A4%BApost%E6%97%B6%E9%97%B4.html</url>
    <content><![CDATA[<hr>
<h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><ul>
<li>不同主题不同</li>
</ul>
<h3 id="next主题"><a href="#next主题" class="headerlink" title="next主题"></a>next主题</h3><ul>
<li><p>去除正文页面的post时间：</p>
<ul>
<li>在<code>./themes/next/layout/_macro/post.swig</code>中找到”post-time”相关的<code>tag</code>，然后注释掉该<code>tag</code>的所有内容（注释方式为``）</li>
</ul>
</li>
<li><p>去除归档中每个文档的时间</p>
<ul>
<li>在<code>./themes/next/layout/_macro/post-collapse.swig</code>中找到”post-time”相关的<code>tag</code>，然后修改该<code>tag</code>中的内容<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;# &#123;&#123; date(post.date, &apos;MM-DD&apos;) &#125;&#125; #&#125;</span><br><span class="line">   &#123;&#123; date(post.date, &apos; &apos;) &#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>去除归档中的年份</p>
<ul>
<li>在<code>./themes/next/layout/archive.swig</code>中找到”archive-year”相关的<code>tag</code>，然后修改该<code>tag</code>中的内容，把关键信息注释掉<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo——相关问题汇总</title>
    <url>/Notes/Hexo/Hexo%E2%80%94%E2%80%94%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB.html</url>
    <content><![CDATA[<p><em>本文记录Hexo使用过程中可能遇到的问题</em></p>
<hr>
<h3 id="Hexo生成的html文件是空的"><a href="#Hexo生成的html文件是空的" class="headerlink" title="Hexo生成的html文件是空的"></a>Hexo生成的html文件是空的</h3><ul>
<li>表现为部署到云端后，显示为空白页面</li>
</ul>
<h4 id="问题起因"><a href="#问题起因" class="headerlink" title="问题起因"></a>问题起因</h4><ul>
<li><p>由于node版本问题导致的，可能是node版本太新了</p>
</li>
<li><p>重新安装node为旧版本即可，本文使用的暂时是node 12版本</p>
</li>
<li><p>安装命令（注意：直接使用<code>brew install node@12</code>可能无法安装成功，会出现找不到指定版本的错误，12版本太老了，过期了）：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install nvm ## 安装nvm后按照提示将相应启动脚本配置到~/.zshrc中</span><br><span class="line">nvm install 12.14.0 </span><br><span class="line">nvm use 12.14.0</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>nvm use</code> 只是临时切换 node 版本，只适用于当前 terminal，如果打开新的 terminal，node 可能还是之前的旧版本。如果想要永久切换的话，可使用如下命令（实际上，本地切换到12.14.0后重启terminal也是该版本，未发现其他问题）：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvm alias default 12.14.0</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Sklearn——LogisticRegression</title>
    <url>/Notes/ML/Sklearn/Sklearn%E2%80%94%E2%80%94LogisticRegression.html</url>
    <content><![CDATA[<p><em>Sklearn解析之LogisticRegression</em></p>
<hr>
<h3 id="parameter"><a href="#parameter" class="headerlink" title="parameter"></a>parameter</h3><ul>
<li>penalty: str, ‘l1’ or ‘l2’, default: ‘l2’</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>Sklearn</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu-Jupyter——Python2与Python3共存</title>
    <url>/Notes/Python/Jupyter/Ubuntu-Jupyter%E2%80%94%E2%80%94Python2%E4%B8%8EPython3%E5%85%B1%E5%AD%98.html</url>
    <content><![CDATA[<p><em>在Jupyter中同时配置Python 2和Python 3环境</em></p>
<hr>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ul>
<li><p>检查对应的虚拟环境(Python2和Python3)是否都已经安装成功，如没有的话请安装相应版本号，下面是管理包时常用的一些命令</p>
<ul>
<li><p>检查版本号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda env list # check all python versions</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建虚拟环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n Python3 Python=3.6 # Python3可以是自己自定义的名字</span><br></pre></td></tr></table></figure>
</li>
<li><p>激活虚拟环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source activate Python3 # Windows中不需要`source`</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止虚拟环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deactivate Python3</span><br></pre></td></tr></table></figure>
</li>
<li><p>从硬盘删除虚拟环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda remove -n Python3 --all</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Jupyter-on-Python-3"><a href="#Jupyter-on-Python-3" class="headerlink" title="Jupyter on Python 3"></a>Jupyter on Python 3</h3><p><em>还需要配置Python2</em></p>
<ul>
<li><p>检查Python2对应的pip版本大于等于9.0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python2 -m pip --version</span><br></pre></td></tr></table></figure>

<ul>
<li>注意，如果<code>python2</code>指的是我们想设置的版本，如果电脑上对应多个版本的Python2，请选择对应的那个，后面的pip会根据Python2版本自动被确定下来，比如可使用<code>/home/jiahong/anaconda2/envs/Python3.6/python</code>绝对路径来指定确定的Python版本</li>
<li>这里的<code>python2 -m</code>是用于指定<code>pip</code>版本的修饰，用于指明<code>pip</code>的版本，所以上一步中的路径指定也可以直接直接pip的绝对路径</li>
</ul>
</li>
<li><p>安装方法1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python2 -m pip install ipykernel</span><br><span class="line">python2 -m ipykernel install --user</span><br></pre></td></tr></table></figure>

<ul>
<li>这种安装方法是可以随意重复的，不会造成重复安装，所以可以直接将Python 2和Python 3两个版本都安装一遍</li>
</ul>
</li>
<li><p>安装方法2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n ipykernel_py2 python=2 ipykernel</span><br><span class="line">source activate ipykernel_py2 # On Windows, remove the word &apos;source&apos;</span><br><span class="line">python -m ipykernel install --user</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="Jupyter-on-Python-2"><a href="#Jupyter-on-Python-2" class="headerlink" title="Jupyter on Python 2"></a>Jupyter on Python 2</h3><ul>
<li>将上面的Python2全都指定为Python3即可<ul>
<li>比如<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m pip install ipykernel</span><br><span class="line">python3 -m ipykernel install --user</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Python</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo——使用总结</title>
    <url>/Notes/Hexo/Hexo%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>Hexo 使用总结</em></p>
<hr>
<h3 id="在博客中引用自己的博客"><a href="#在博客中引用自己的博客" class="headerlink" title="在博客中引用自己的博客"></a>在博客中引用自己的博客</h3><ul>
<li><p>在 _config.yml 中设置属性</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">permalink: :title.html</span><br></pre></td></tr></table></figure>
</li>
<li><p>引用方式</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 引用文档，以引用本文为例</span><br><span class="line"># 引用的应该是生成时使用的文件路径或者title,后面跟上.html,但是还要根据不同设置来修改,总之就是访问时的链接</span><br><span class="line">[引用示例](/Notes/Hexo/Hexo——使用总结.html &quot;/Notes/Hexo/Hexo——使用总结.html&quot;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p><a href="/Notes/Hexo/Hexo%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.html" title="/Notes/Hexo/Hexo——使用总结.html">引用示例</a></p>
</blockquote>
<hr>
<h3 id="导入图片"><a href="#导入图片" class="headerlink" title="导入图片"></a>导入图片</h3><ul>
<li>在同目录下新建与Markdown文档文件名相同的文件夹，将图片存入文件夹中</li>
<li>在Markdown文档中使用特殊语法导入  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% asset_img picture.png descreption %&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<img src="/Notes/Hexo/Hexo——使用总结/picture.png" title="descreption">
</blockquote>
<hr>
<h3 id="插入LaTex公式"><a href="#插入LaTex公式" class="headerlink" title="插入LaTex公式"></a>插入LaTex公式</h3><h4 id="LaTex公式"><a href="#LaTex公式" class="headerlink" title="LaTex公式"></a>LaTex公式</h4><ul>
<li><a href="https://www.codecogs.com/latex/eqneditor.php" title="https://www.codecogs.com/latex/eqneditor.php" target="_blank" rel="noopener">在线公式编辑地址</a></li>
<li>LaTex公式用法参见<a href="/Notes/Others/LaTex%E2%80%94%E2%80%94%E7%AC%94%E8%AE%B0.html">LaTex——笔记</a></li>
</ul>
<h4 id="借助工具插入公式"><a href="#借助工具插入公式" class="headerlink" title="借助工具插入公式"></a>借助工具插入公式</h4><h5 id="MathJax引擎-推荐"><a href="#MathJax引擎-推荐" class="headerlink" title="MathJax引擎(推荐)"></a>MathJax引擎(推荐)</h5><ul>
<li>这种方式只需要在是实现时首先添加引擎代码,后面就可以完全像是Latex一样写公式</li>
</ul>
<h6 id="头部添加"><a href="#头部添加" class="headerlink" title="头部添加"></a>头部添加</h6><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<ul>
<li>添加头部如下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script src=&quot;//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;  type=&quot;text/javascript&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>

<pre><code>* 说明:
    * 第一个头部可用,在各种浏览器均能正确加载
    * 第二个涉及到https的安全性问题,在部署到GitHub时,某些浏览器上不能正确加载</code></pre>
<ul>
<li>测试说明: <ul>
<li>yilla主题下Chrome和Firefox浏览器中不添加上面的头部当前文档也能识别出公式, 其他浏览器未测试</li>
<li>next主题下必须加上这个头部才行</li>
</ul>
</li>
</ul>
<h6 id="写公式"><a href="#写公式" class="headerlink" title="写公式"></a>写公式</h6><ul>
<li>行内公式  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">公式如下:\\(\Large x=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;\\)所示</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>公式如下:\(\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\)所示</p>
</blockquote>
<ul>
<li>行间公式  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">公式如下:$$\Large x=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;$$所示</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>公式如下:$$\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$所示</p>
</blockquote>
<ul>
<li><p>公式等号对齐</p>
<ul>
<li><p>公式如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">Q(\theta, \theta^&#123;i&#125;) &amp;= E_&#123;Z&#125;[logP(Y,Z|\theta)|Y,\theta^&#123;i&#125;] \\\\</span><br><span class="line">	&amp;= E_&#123;Z\sim P(Z|Y,\theta^&#123;i&#125;)&#125;[logP(Y,Z|\theta)] \\\\</span><br><span class="line">	&amp;= \sum_&#123;Z&#125; P(Z|Y,\theta^&#123;i&#125;)logP(Y,Z|\theta) \\\\</span><br><span class="line">	&amp;= \sum_&#123;Z&#125; logP(Y,Z|\theta)P(Z|Y,\theta^&#123;i&#125;)</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<blockquote>
<p>$$<br>\begin{align}<br>Q(\theta, \theta^{i}) &amp;= E_{Z}[logP(Y,Z|\theta)|Y,\theta^{i}] \\<br>    &amp;= E_{Z\sim P(Z|Y,\theta^{i})}[logP(Y,Z|\theta)] \\<br>    &amp;= \sum_{Z} P(Z|Y,\theta^{i})logP(Y,Z|\theta) \\<br>    &amp;= \sum_{Z} logP(Y,Z|\theta)P(Z|Y,\theta^{i})<br>\end{align}<br>$$</p>
</blockquote>
<h5 id="Google-Chart服务器"><a href="#Google-Chart服务器" class="headerlink" title="Google Chart服务器"></a>Google Chart服务器</h5><ul>
<li>引用方式  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># &lt;img src=&quot;http://chart.googleapis.com/chart?cht=tx&amp;chl=Latex公式&quot; style=&quot;border:none;&quot;&gt;</span><br><span class="line">&lt;img src=&quot;http://chart.googleapis.com/chart?cht=tx&amp;chl=\Large x=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;&quot; style=&quot;border:none;&quot;&gt;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" style="border:none;">
</blockquote>
<h5 id="forkosh服务器"><a href="#forkosh服务器" class="headerlink" title="forkosh服务器"></a>forkosh服务器</h5><ul>
<li>引用方式  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;img src=&quot;http://www.forkosh.com/mathtex.cgi? Latex公式&quot;&gt;</span><br><span class="line">&lt;img src=&quot;http://www.forkosh.com/mathtex.cgi?\Large x=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;&quot;&gt;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<img src="http://www.forkosh.com/mathtex.cgi? \Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}">
</blockquote>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTex——笔记</title>
    <url>/Notes/LaTeX/LaTeX%E2%80%94%E2%80%94%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<p><em>LaTex使用笔记</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="LaTex公式相关"><a href="#LaTex公式相关" class="headerlink" title="LaTex公式相关"></a>LaTex公式相关</h3><ul>
<li><p>LaTex公式等号对齐</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">L(x)&amp;=U(x) \\\\</span><br><span class="line">&amp;=N(x) \\\\</span><br><span class="line">&amp;=M(x)</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>

<ul>
<li>其中<code>\\</code>表示换行<code>&amp;</code>表示每一行的对齐点</li>
<li>在Markdown文档中要注意<code>\\\\</code>才对应<code>\\</code>,因为<code>\</code>在Markdown文档中是转义字符</li>
</ul>
</li>
</ul>
<blockquote>
<p>$$<br>\begin{align}<br>L(x)&amp;=U(x) \\<br>&amp;=N(x) \\<br>&amp;=M(x)<br>\end{align}<br>$$</p>
</blockquote>
<ul>
<li>LaTex特殊公式总结<table>
<thead>
<tr>
<th align="center">LaTex</th>
<th align="center">符号</th>
</tr>
</thead>
<tbody><tr>
<td align="center">\partial</td>
<td align="center">\(\partial\)</td>
</tr>
<tr>
<td align="center">\nabla</td>
<td align="center">\(\nabla\)</td>
</tr>
<tr>
<td align="center">\infty</td>
<td align="center">\(\infty\)</td>
</tr>
<tr>
<td align="center">\int</td>
<td align="center">\(\int\)</td>
</tr>
<tr>
<td align="center">\iint</td>
<td align="center">\(\iint\)</td>
</tr>
<tr>
<td align="center">\iiint</td>
<td align="center">\(\iiint\)</td>
</tr>
<tr>
<td align="center">\oint</td>
<td align="center">\(\oint\)</td>
</tr>
<tr>
<td align="center">\triangle</td>
<td align="center">\(\triangle\)</td>
</tr>
<tr>
<td align="center">\bigtriangledown</td>
<td align="center">\(\bigtriangledown\)</td>
</tr>
<tr>
<td align="center">\triangleleft</td>
<td align="center">\(\triangleleft\)</td>
</tr>
<tr>
<td align="center">\triangleright</td>
<td align="center">\(\triangleright\)</td>
</tr>
<tr>
<td align="center">\circ</td>
<td align="center">\(\circ\)</td>
</tr>
<tr>
<td align="center">\odot</td>
<td align="center">\(\odot\)</td>
</tr>
<tr>
<td align="center">\otimes</td>
<td align="center">\(\otimes\)</td>
</tr>
</tbody></table>
</li>
</ul>
<ul>
<li><p>公式空格</p>
<table>
<thead>
<tr>
<th align="center">LaTex</th>
<th align="center">空格效果</th>
</tr>
</thead>
<tbody><tr>
<td align="center">a\qquad b</td>
<td align="center">\(a\qquad b\)</td>
</tr>
<tr>
<td align="center">a\quad b</td>
<td align="center">\(a\quad b\)</td>
</tr>
<tr>
<td align="center">a\ b</td>
<td align="center">\(a\ b\)</td>
</tr>
<tr>
<td align="center">a; b</td>
<td align="center">\(a; b\)</td>
</tr>
<tr>
<td align="center">a, b</td>
<td align="center">\(a, b\)</td>
</tr>
<tr>
<td align="center">ab</td>
<td align="center">\(ab\)</td>
</tr>
<tr>
<td align="center">a!b</td>
<td align="center">\(a!b\)</td>
</tr>
</tbody></table>
</li>
<li><p>公式后面你加上序号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$$y(x) = x^&#123;2&#125; \tag&#123;1&#125;$$</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>$$y(x) = x^{2} \tag{1}$$</p>
</blockquote>
<ul>
<li>大括号的使用<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$$f(x)=</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">0&amp; \text&#123;x=0&#125;\\\\</span><br><span class="line">1&amp; \text&#123;x!=0&#125;</span><br><span class="line">\end&#123;cases&#125;$$</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>$$f(x)=<br>\begin{cases}<br>0&amp; \text{x=0}\\<br>1&amp; \text{x!=0}<br>\end{cases}$$</p>
</blockquote>
<ul>
<li>argmax使用(1)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$$j^&#123;\star&#125;=\mathop&#123;\arg \max&#125;_&#123;j&#125;f(j)$$</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>$$j^{\star}=\mathop{\arg \max}_{j}f(j)$$</p>
</blockquote>
<ul>
<li>argmax使用(2)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$$j^&#123;\star&#125;=\underset&#123;j&#125;&#123;\arg \max&#125;f(j)$$</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>$$j^{\star}=\underset{j}{\arg \max}f(j)$$</p>
</blockquote>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTex——编译问题</title>
    <url>/Notes/LaTeX/LaTeX%E2%80%94%E2%80%94%E7%BC%96%E8%AF%91%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<h3 id="VSCode环境中引用出错问题排查"><a href="#VSCode环境中引用出错问题排查" class="headerlink" title="VSCode环境中引用出错问题排查"></a>VSCode环境中引用出错问题排查</h3><ul>
<li><p>参考链接：<a href="https://blog.csdn.net/qq_38397338/article/details/108193853" target="_blank" rel="noopener">latex报错：Citation ‘××××‘ on page x undefined.与众不同的解决方案</a></p>
</li>
<li><p>除了排查自身代码问题 &amp;&amp; 修改配置文件外，还可以使用命令行重新编译PDF文件，具体流程如下</p>
<ul>
<li><p>切换到xxx.tex文件所在目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xelatex xxx</span><br><span class="line">bibtex xxx</span><br><span class="line">xelatex xxx</span><br><span class="line">xelatex xxx</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行以上语句后PDF引用恢复正常</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda——Python环境和pip包安装方式</title>
    <url>/Notes/Python/Anaconda/Anaconda%E2%80%94%E2%80%94Python%E7%8E%AF%E5%A2%83%E5%92%8Cpip%E5%8C%85%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8F.html</url>
    <content><![CDATA[<p><em>文本适用于anaconda,miniforge等包含conda的所有软件</em></p>
<h3 id="conda对Python虚拟环境的管理方式"><a href="#conda对Python虚拟环境的管理方式" class="headerlink" title="conda对Python虚拟环境的管理方式"></a>conda对Python虚拟环境的管理方式</h3><ul>
<li>虚拟环境均存储在<code>./envs</code>目录下，按照虚拟环境名称命名</li>
<li>在不同虚拟环境下，conda会切换到当前envs目录，从而实现访问不同Python和<code>pip</code></li>
</ul>
<h3 id="pip包的管理方式"><a href="#pip包的管理方式" class="headerlink" title="pip包的管理方式"></a>pip包的管理方式</h3><ul>
<li>pip对包默认存储在当前虚拟环境下的lib目录下与Python版本有关，比如，<code>lib/python3.10/site-package</code></li>
<li>具体包的安装路径可使用<code>pip show [package_name]</code>判断</li>
<li>pip是一个脚本文件，可以使用vim打开，第一行指定了当前pip对应的Python脚本位置，需要设置准确，第一行必须与当前环境下执行<code>which python</code>的结果一致，如果有问题可以修改到指定目录（特别是手动迁移conda管理的虚拟环境时）</li>
</ul>
<h3 id="如何迁移Python虚拟环境？"><a href="#如何迁移Python虚拟环境？" class="headerlink" title="如何迁移Python虚拟环境？"></a>如何迁移Python虚拟环境？</h3><ul>
<li>迁移虚拟环境只需要copy <code>envs</code>文件下下的指定环境目录即可，迁移后记得修改<code>pip/pip3</code>文件对应的第一行路径</li>
</ul>
]]></content>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda——安装和常用命令</title>
    <url>/Notes/Python/Anaconda/Anaconda%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html</url>
    <content><![CDATA[<p><em>Anaconda安装和使用</em></p>
<hr>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ul>
<li><a href="https://www.anaconda.com/" target="_blank" rel="noopener">Anaconda官网</a>下载安装即可</li>
</ul>
<hr>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="管理conda版本"><a href="#管理conda版本" class="headerlink" title="管理conda版本"></a>管理conda版本</h4><ul>
<li><p>检查conda版本</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure>
</li>
<li><p>升级版本</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda update conda</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="管理环境"><a href="#管理环境" class="headerlink" title="管理环境"></a>管理环境</h4><ul>
<li><p>创建环境</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create --name your_env_name python=3.6</span><br><span class="line"># &lt;==&gt;</span><br><span class="line">conda create -n your_env_name python=3.6</span><br></pre></td></tr></table></figure>
</li>
<li><p>激活环境</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source activate your_env_name # On Windows, remove the word &apos;source&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>列出所有环境</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda info --envs </span><br><span class="line"># or </span><br><span class="line">conda info -e</span><br><span class="line"># or </span><br><span class="line">conda env list</span><br></pre></td></tr></table></figure>
</li>
<li><p>退出当前环境</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source deactivate # On Windows, remove the word &apos;source&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>复制一个环境</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n new_env_name --clone old_env_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除环境</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda remove -n your_env_name</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="管理包"><a href="#管理包" class="headerlink" title="管理包"></a>管理包</h4><ul>
<li><p>查看已经安装的包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>
</li>
<li><p>向指定环境中安装包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install --name your_env_name package</span><br></pre></td></tr></table></figure>

<ul>
<li>如果没有<code>--name</code>参数，package默认安装到当前环境中</li>
<li>tips： 一般来说当需要在同一个环境下面安装的包比较多时，我们会先激活想要操作的环境，然后直接不使用    <code>--name</code>参数安装</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda——conda命令不能执行</title>
    <url>/Notes/Python/Anaconda/Anaconda%E2%80%94%E2%80%94conda%E5%91%BD%E4%BB%A4%E4%B8%8D%E8%83%BD%E6%89%A7%E8%A1%8C.html</url>
    <content><![CDATA[<h3 id="出现包导入异常的错误"><a href="#出现包导入异常的错误" class="headerlink" title="出现包导入异常的错误"></a>出现包导入异常的错误</h3><ul>
<li>错误如下</li>
</ul>
<blockquote>
<p>ImportError: No module named conda.cli</p>
</blockquote>
<ul>
<li><p>解决方案:重装anaconda</p>
<ul>
<li>官网上下载anaconda的安装包,注意Python2,和Python3不能错</li>
<li>执行时加上<code>-u</code>参数即可<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash Anaconda2-2018.12-Linux-x86_64.sh -u</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>说明:</p>
<ul>
<li>上面的修复方案不会影响其他第三方包</li>
<li>其他conda环境相关的问题也可以通过重装解决</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda——conda和pip管理包的区别</title>
    <url>/Notes/Python/Anaconda/Anaconda%E2%80%94%E2%80%94conda%E5%92%8Cpip%E7%AE%A1%E7%90%86%E5%8C%85%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content><![CDATA[<p><em>conda和pip对包的管理有什么区别？</em></p>
<ul>
<li>conda 使得我们可以在conda环境中安装任何语言的包（包括C语言和Python）</li>
<li>pip 使得我们可以在任何环境中安装Python包</li>
</ul>
<table>
<thead>
<tr>
<th align="left"><strong>管理工具</strong></th>
<th align="center"><strong>conda</strong></th>
<th align="center"><strong>pip</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>使用环境</strong></td>
<td align="center">conda环境</td>
<td align="center">任意平台</td>
</tr>
<tr>
<td align="left"><strong>包语言</strong></td>
<td align="center">任意语言</td>
<td align="center">Python语言</td>
</tr>
</tbody></table>
<hr>
<h3 id="管理包命令"><a href="#管理包命令" class="headerlink" title="管理包命令"></a>管理包命令</h3><ul>
<li><p>conda</p>
<ul>
<li><p>安装包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 从虚拟环境 your_env_name 中安装 packge_name包</span><br><span class="line">conda install -n your_env_name package_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在虚拟环境 your_env_name 中删除 package_name包</span><br><span class="line">conda remove -n your_env_name package_name</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>pip</p>
<ul>
<li><p>安装包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装包到默认路径</span><br><span class="line">pip install package_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>卸载包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除包</span><br><span class="line">pip uninstall package_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看已经安装的包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 列出所有包</span><br><span class="line">pip list</span><br><span class="line"># 查看指定包的详细信息</span><br><span class="line">pip show --files package_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>升级包</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install --upgrade package_name</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p>关于conda 命令更多详情参考<a href="/Notes/Python/Anaconda/Anaconda%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html" title="/Notes/Python/Anaconda/Anaconda——安装和常用命令.html">Anaconda——安装和常用命令</a></p>
]]></content>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>ML——SVM-支持向量机</title>
    <url>/Notes/ML/Models/ML%E2%80%94%E2%80%94SVM-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html</url>
    <content><![CDATA[<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<hr>
<h3 id="手动推导流程"><a href="#手动推导流程" class="headerlink" title="手动推导流程"></a>手动推导流程</h3><ul>
<li>假设有N样本:\(X = (x_{1}, x_{2}, x_{3},\dots x_{N})\)<ul>
<li>样本点为: \(((x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3}),\dots (x_{N}, y_{N}))\)</li>
</ul>
</li>
<li>假设\(x_{i}\)等所有向量都为列向量, \(w\)为行向量(这里是为了推到方便,与&lt;&lt;统计学习方法&gt;&gt;一致)<ul>
<li>实际代码实现时与行列向量无关,取二者的内积<code>np.inner(w,x)</code>即可</li>
</ul>
</li>
</ul>
<h4 id="确定分类决策函数"><a href="#确定分类决策函数" class="headerlink" title="确定分类决策函数"></a>确定分类决策函数</h4><ul>
<li>分类超平面<br>$$w^{\star}x + b^{\star} = 0$$<ul>
<li>分类超平面由\((w, b)\)唯一确定(注意: LR的分类超平面由\((w, b)\)和阈值唯一确定)</li>
</ul>
</li>
<li>分类决策函数<br>$$f(x) = sign(w^{\star}x + b^{\star})$$</li>
</ul>
<h4 id="确定优化目标"><a href="#确定优化目标" class="headerlink" title="确定优化目标"></a>确定优化目标</h4><h5 id="间隔定义"><a href="#间隔定义" class="headerlink" title="间隔定义"></a>间隔定义</h5><ul>
<li>函数间隔定义<ul>
<li>点\((x_{i}, y_{i})\)到超平面\((w,b)\)的函数间隔为(\(y_{i} \in \{1, -1\})\):<br>$$\hat{\gamma}_{i} = y_{i}(w\cdot x_{i} + b)$$</li>
<li>所有训练样本\(X = (x_{1}, x_{2}, x_{3},\dots x_{m})\)到超平面\((w,b)\)的函数间隔为:<br>$$\hat{\gamma} = \min_{i=1,\dots,N}\hat{\gamma}_{i}$$</li>
</ul>
</li>
<li>几何间隔的定义<ul>
<li>点\((x_{i}, y_{i})\)到超平面\((w,b)\)的几何间隔为:<br>$$\gamma_{i} = y_{i} (\frac{w}{||w||}\cdot x_{i} + \frac{b}{||w||})$$</li>
<li>进一步推导得:<br>$$\gamma_{i} = \frac {y_{i}(w\cdot x_{i} + b)}{||w||} = \frac{\hat{\gamma}}{||w||}$$</li>
<li>所有训练样本\(X = (x_{1}, x_{2}, x_{3},\dots x_{m})\)到超平面\((w,b)\)的几何间隔为:<br>$$\gamma = \min_{i=1,\dots,N}\gamma_{i}$$</li>
</ul>
</li>
</ul>
<h5 id="确定优化目标-1"><a href="#确定优化目标-1" class="headerlink" title="确定优化目标"></a>确定优化目标</h5><ul>
<li><p>求解能够<strong>正确划分训练数据集</strong>并且<strong>几何间隔最大的分类超平面</strong>(分离超平面)</p>
</li>
<li><p>定义约束优化问题:<br>$$<br>\begin{align}<br>&amp;\max_{w,b} \quad \gamma \\<br>&amp;s.t. \quad y_{i} (\frac{w}{||w||}\cdot x_{i} + \frac{b}{||w||}) \geq \gamma, i = 1,2,\dots,N<br>\end{align}<br>$$</p>
</li>
<li><p>将\(\gamma = \frac{\hat{\gamma}}{||w||}\)带入并在条件中消去分母上的\(||w||\),得<br>$$<br>\begin{align}<br>&amp;\max_{w,b} \quad\frac{\hat{\gamma}}{||w||} \\<br>&amp;s.t.\quad y_{i} (w\cdot x_{i} + b) \geq \hat{\gamma}, i = 1,2,\dots,N<br>\end{align}<br>$$</p>
</li>
<li><p>上面的式子中,函数间隔\(\hat{\gamma}\)的值不影响最优化问题的解,于是我们设置\(\hat{\gamma}=1\)</p>
<ul>
<li>因为将参数\((w,b)\)同时扩大或缩小为\((\lambda w,\lambda b)\), 函数间隔对应变成\(\lambda \hat{\gamma}\)</li>
<li>两个等价的优化方法:<ul>
<li>设置\(\hat{\gamma}=1\)再对原始问题进行最优化得到参数\((w^{\star},b^{\star})\)</li>
<li>先对原始问题最优化求得参数\((w,b)\),再将参数同时扩大或缩小为\((\lambda w,\lambda b)\),使得\(\lambda \hat{\gamma}=1\)得到的最终参数\((w^{\star},b^{\star}) = (\lambda w,\lambda b)\)等价<br>$$<br>\begin{align}<br>&amp;\max_{w,b}\quad \frac{1}{||w||} \\<br>&amp;s.t.\quad y_{i} (w\cdot x_{i} + b) \geq 1, i = 1,2,\dots,N<br>\end{align}<br>$$</li>
</ul>
</li>
</ul>
</li>
<li><p>进一步分析,由于最大化\(\frac{1}{||w||}\)与最小化\(\frac{1}{2}||w||^{2}\)等价(这里\(w\)为行向量时,\(||w||^{2} = ww^{T}\)), 最优化问题可等价表示为:<br>$$<br>\begin{align}<br>&amp;\min_{w,b}\quad \frac{1}{2}||w||^{2} \\<br>&amp;s.t.\quad y_{i} (w\cdot x_{i} + b) \geq 1, i = 1,2,\dots,N<br>\end{align}<br>$$</p>
<ul>
<li>这是一个<strong>凸二次规划问题</strong>(convex quadratic programming),凸优化问题参考<a href="/Notes/Others/Math%E2%80%94%E2%80%94%E5%87%B8%E4%BA%8C%E6%AC%A1%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%92%8C%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7.html">Math——凸二次优化问题和拉格朗日对偶性</a></li>
</ul>
</li>
</ul>
<h5 id="优化问题的求解-对偶"><a href="#优化问题的求解-对偶" class="headerlink" title="优化问题的求解(对偶)"></a>优化问题的求解(对偶)</h5><ul>
<li>首先将SVM优化问题转换为一般凸优化形式<br>$$<br>\begin{align}<br>&amp;\min_{w,b}\quad \frac{1}{2}||w||^{2} \\<br>&amp;s.t.\quad 1 - y_{i} (w\cdot x_{i} + b) \leq 0, i = 1,2,\dots,N<br>\end{align}<br>$$</li>
<li>由上述优化问题定义拉格朗日函数<br>$$<br>\begin{align}<br>L(w,b,\alpha) = \frac{1}{2}||w||^{2} + \sum_{i=1}^{N}\alpha_{i}(1-y_{i}(w\cdot x_{i} + b))<br>\end{align}<br>$$</li>
<li>根据拉格朗日对偶性,<br>$$<br>\begin{align}<br>\min_{w,b}\max_{\alpha:\alpha_{i} \geq 0}L(w,b,\alpha) = \max_{\alpha:\alpha_{i} \geq 0}\min_{w,b}L(w,b,\alpha)<br>\end{align}<br>$$<ul>
<li>左边:原始问题为极小极大问题</li>
<li>右边:对偶问题为极大极小问题</li>
</ul>
</li>
<li>解对偶问题:</li>
<li>先求<br>$$<br>\begin{align}<br>\min_{w,b}L(w,b,\alpha)<br>\end{align}<br>$$</li>
<li>求导数为0得<br>$$<br>\begin{align}<br>\nabla_{w}L(w,b,\alpha) &amp;= w - \sum_{i=1}^{N}\alpha_{i}y_{i}x_{i} = 0 \\<br>\nabla_{b}L(w,b,\alpha) &amp;= \sum_{i=1}^{N}\alpha_{i}y_{i} = 0<br>\end{align}<br>$$</li>
<li>解得:<br>$$<br>\begin{align}<br>&amp;w = \sum_{i=1}^{N}\alpha_{i}y_{i}x_{i} \\<br>&amp;\sum_{i=1}^{N}\alpha_{i}y_{i} = 0<br>\end{align}<br>$$</li>
<li>代入原来的拉格朗日函数有:</li>
</ul>
<p>$$<br>\begin{align}<br>\min_{w,b}L(w,b,\alpha) &amp;= \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) + \sum_{i=1}^{N}\alpha_{i}(1-y_{i}(\left [\sum_{j=1}^{N}\alpha_{j}y_{j}x_{j}\right]\cdot x_{i} + b)) \\<br>&amp;= \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) - \sum_{i=1}^{N}\alpha_{i}y_{i}\left [\sum_{j=1}^{N}\alpha_{j}y_{j}x_{j}\right]\cdot x_{i} + \sum_{i=1}^{N}\alpha_{i}y_{i}b + \sum_{i=1}^{N}\alpha_{i}\\<br>&amp;= \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) - \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}y_{i}\alpha_{j}y_{j}(x_{j}\cdot x_{i}) + \sum_{i=1}^{N}\alpha_{i}y_{i}b + \sum_{i=1}^{N}\alpha_{i}\\<br>&amp;= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) + 0 + \sum_{i=1}^{N}\alpha_{i}\\<br>&amp;= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) + \sum_{i=1}^{N}\alpha_{i}<br>\end{align}<br>$$</p>
<ul>
<li><p>上面的\(\min_{w,b}L(w,b,\alpha)\)对\(\alpha\)求极大值有:<br>$$<br>\begin{align}<br>\max_{\alpha} \quad &amp;-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) + \sum_{i=1}^{N}\alpha_{i} \\<br>s.t. \quad &amp;\sum_{i=1}^{N}\alpha_{i}y_{i} = 0 \\<br>\quad &amp;\alpha_{i} \geq 0, i = 1,2,\dots,N<br>\end{align}<br>$$</p>
</li>
<li><p>转换为一般优化问题<br>$$<br>\begin{align}<br>\min_{\alpha} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j}) - \sum_{i=1}^{N}\alpha_{i} \\<br>s.t. \quad &amp;\sum_{i=1}^{N}\alpha_{i}y_{i} = 0 \\<br>\quad &amp;-\alpha_{i} \leq 0, i = 1,2,\dots,N<br>\end{align}<br>$$</p>
</li>
<li><p>假设上面的式子能够求得\(\alpha\)的解为:<br>$$<br>\begin{align}<br>\alpha^{\star} = (\alpha_{1}^{\star}, \alpha_{2}^{\star},\dots, \alpha_{N}^{\star})<br>\end{align}<br>$$</p>
</li>
<li><p>根据拉格朗日对偶问题与原始问题的解对应的条件,<a href="/Notes/Others/Math%E2%80%94%E2%80%94%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%92%8C%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7.html">Math——凸优化问题和拉格朗日对偶性</a>,如果上面的式子中存在下标\(j\),满足\(-\alpha_{j}^{\star} &lt; 0\),即满足:<br>$$\exists j \quad \alpha_{j}^{\star} &gt; 0$$</p>
</li>
<li><p>那么最终可求得:<br>$$<br>\begin{align}<br>&amp;w^{\star} = \sum_{i=1}^{N}\alpha_{i}^{\star}y_{i}x_{i} \\<br>\end{align}<br>$$</p>
</li>
<li><p>\(b^{\star}\)的表达式由\(\alpha_{j}^{\star} &gt; 0\),和KKT条件\(\alpha_{j}^{\star}(y_{j}(w^{\star}\cdot x_{j} + b^{\star})-1) = 0\)可解得:<br>$$<br>\begin{align}<br>y_{j}(w^{\star}\cdot x_{j} + b^{\star})-1 = 0 \\<br>\end{align}<br>$$</p>
</li>
<li><p>注意到\(y_{j}\in \{-1, 1\}\), 所以\(y_{j}^{2}=1\)<br>$$<br>\begin{align}<br>y_{j}^{2}(w^{\star}\cdot x_{j} + b^{\star})-y_{j} = 0 \\<br>w^{\star}\cdot x_{j} + b^{\star} = y_{j} \\<br>\end{align}<br>$$</p>
</li>
<li><p>所以有<br>$$<br>\begin{align}<br>b^{\star} &amp;= y_{j} - w^{\star}\cdot x_{j} \\<br>&amp;= y_{j} - \sum_{i=1}^{N}\alpha_{i}^{\star}y_{i}(x_{i}\cdot x_{j})<br>\end{align}<br>$$</p>
</li>
<li><p>综合\(w^{\star}\)和\(b^{\star}\)的表达式,我们有分类超平面为:<br>$$<br>\begin{align}<br>w^{\star}x + b^{\star} &amp;= 0 \\<br>\sum_{i=1}^{N}\alpha_{i}^{\star}y_{i}(x_{i}\cdot x) + b^{\star} &amp;= 0<br>\end{align}<br>$$</p>
</li>
<li><p>分类决策函数为:<br>$$<br>\begin{align}<br>f(x) &amp;= sign(w^{\star}x + b^{\star}) \\<br>f(x) &amp;= sign\left (\sum_{i=1}^{N}\alpha_{i}^{\star}y_{i}(x_{i}\cdot x) + b^{\star}\right )<br>\end{align}<br>$$</p>
<ul>
<li>这就是分线性可分SVM的对偶形式</li>
<li>分类决策函数只与待预测的样本\(x\)和已有训练数据\(x_{j}\)的输入内积\((x_{j}\cdot x)\)相关<ul>
<li>注意\(b^{\star}\)是通过原始训练样本算出来的定值,\(b^{\star}, w^{\star}\)的值与待预测的未知样本无关,训练样本确定,参数就确定了, 内积的出现是在加入位置样本后\(w^{\star}\cdot x\)产生的</li>
</ul>
</li>
<li>SVM的对偶形式使得我们可以用核函数来扩展SVM(非线性核函数可使得SVM能解决非线性问题)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><ul>
<li>优化问题的推导过程中,又一步,对偶问题与原始问题能够相等的条件是存在下标\(j\),满足\(-\alpha_{j}^{\star} &lt; 0\),即满足:<br>$$\exists j \quad \alpha_{j}^{\star} &gt; 0$$</li>
<li>上面的\(j\)对应的样本点\((x_{j}, y_{j})\)就是<strong>支持向量</strong><ul>
<li>支持向量一定在间隔边界上:<br>$$y_{j}(w^{\star}\cdot x_{j}+b^{\star})-1 = 0$$</li>
<li>或者<br>$$w^{\star}\cdot x_{j}+b^{\star} = \pm 1$$<ul>
<li>\(y_i\) 的取值为 1 或者 -1</li>
</ul>
</li>
<li>实际上,在训练时参数\((w^{\star}, b^{\star})\)只依赖于\(\alpha_{j} &gt; 0\)的点,即只依赖<strong>支持向量</strong><br>$$<br>\begin{align}<br>&amp;w^{\star} = \sum_{i=1}^{N}\alpha_{i}^{\star}y_{i}x_{i} \\<br>&amp;b^{\star} = y_{j} - \sum_{i=1}^{N}\alpha_{i}^{\star}y_{i}(x_{i}\cdot x_{j})<br>\end{align}<br>$$</li>
<li>所有非支持向量对应的\(\alpha_{i} = 0\),所以对最终参数\((w^{\star}, b^{\star})\)只与<strong>支持向量</strong>相关</li>
</ul>
</li>
</ul>
<hr>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><ul>
<li>\(\phi(x)\)定义为从<strong>输入空间</strong>到<strong>希尔伯特空间</strong>的映射<br>$$<br>\begin{align}<br>K(x,z) = \phi(x)\phi(z)<br>\end{align}<br>$$</li>
<li>将SVM对偶形式中的内积\((x_{i}\cdot x_{j})\)用核函数\(K(x_{i},x_{j}) = \phi(x_{i})\phi(x_{j})\)来替代即可在SVM中应用<strong>核技巧</strong></li>
</ul>
<h4 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h4><ul>
<li>多项式核函数<br>$$K(x,z) = (x\cdot z + 1)^{p}$$</li>
<li>高斯核函数<br>$$K(x,z) = e^{\left( -\frac{||x-z||^{2}}{2\sigma^{2}}\right )}$$</li>
<li>字符串核函数<br>[待更新]</li>
</ul>
<hr>
<h3 id="序列最小最优化算法-SMO"><a href="#序列最小最优化算法-SMO" class="headerlink" title="序列最小最优化算法(SMO)"></a>序列最小最优化算法(SMO)</h3><ul>
<li>为什么要用SMO?<ul>
<li>前面我们的推导将 <strong>SVM的参数求解</strong> 转化为了一个求解<strong>凸二次优化</strong>的问题</li>
<li>凸二次优化问题有全局最优解(有很多优化算法可以求解凸二次优化问题)</li>
<li>但是当<strong>样本量太大</strong>时,普通的<strong>凸二次优化算法</strong>会变得<strong>十分低效</strong>,导致<strong>实际操作</strong>中<strong>无法使用</strong></li>
<li>所以我们需要一个能高效实现的算法 SMO</li>
</ul>
</li>
<li>基本思路<ul>
<li>如果所有变量的解都满足最优化问题的KKT条件,那么这个最优化问题的解就得到了<ul>
<li>原因: KKT条件是该最优化问题的<strong>充要条件</strong></li>
</ul>
</li>
<li>否则,存在某些解不满足KKT条件,选择两个变量,固定其他变量,<ul>
<li>两个变量中其中一个是<strong>最不满足KKT条件的变量</strong>,</li>
<li>另一个在第一个确定后,自动由约束条件确定下来</li>
</ul>
</li>
<li>针对这两个变量构建二次规划问题的解,这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解<ul>
<li>原因: 因为二次规划问题的目标函数值变得更小</li>
<li>此时可通过<strong>解析方法</strong>求解(由于只有两个变量,比较简单),整个算法的速度会很快</li>
</ul>
</li>
<li>SMO算法不断的将原问题分解为子问题,迭代更新参数,每次更新两个参数,最终得到近似解</li>
</ul>
</li>
<li>优点:<ul>
<li>训练速度快</li>
</ul>
</li>
</ul>
<hr>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><h4 id="SVM需要特征归一化"><a href="#SVM需要特征归一化" class="headerlink" title="SVM需要特征归一化"></a>SVM需要特征归一化</h4><p><em>是必要的</em></p>
<ul>
<li>因为SVM寻找的是所谓的<strong>间隔</strong>(margin), 就是两个支持向量的间隔</li>
<li>如果<strong>不归一化</strong>的话, 这个间隔会因为不同特征的单位等, 数值被放大或者缩小, 从而造成<strong>无法评估间隔</strong>, 所以归一化对于SVM很重要</li>
<li>线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization<ul>
<li>个人理解: 非线性的SVM其实也应该归一化,不然核函数映射到高位空间后以后还是可能受数值影响?</li>
</ul>
</li>
</ul>
<h4 id="损失函数自带正则"><a href="#损失函数自带正则" class="headerlink" title="损失函数自带正则"></a>损失函数自带正则</h4><ul>
<li>由于优化目标中就自带了参数 \(\frac{1}{2}||w||^2\), 所以SVM实际上已经自己带了L2正则化了</li>
<li>这也是SVM也叫<strong>结构风险最小化</strong>算法的原因<ul>
<li>结构风险最小化就是模型复杂度最小化</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Bash——awk和xargs命令的使用</title>
    <url>/Notes/Linux/Bash%E2%80%94%E2%80%94awk%E5%92%8Cxargs%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
    <content><![CDATA[<p><em>用于在linux bash中设计处理terminal输出自动删除等问题</em></p>
<hr>
<h3 id="使用实例"><a href="#使用实例" class="headerlink" title="使用实例"></a>使用实例</h3><pre><code>sudo docker images | grep &#39;&lt;none&gt;&#39; | awk &#39;{print $3}&#39; | xargs sudo docker rmi </code></pre>
<p>上面的代码是删除所有名字为<code>&lt;none&gt;</code>的docker镜像</p>
<hr>
<h3 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h3><pre><code>awk &#39;{print $3}&#39; //读取每一行的第三列的值</code></pre>
<hr>
<h3 id="xargs"><a href="#xargs" class="headerlink" title="xargs"></a>xargs</h3><pre><code>xargs sudo docker rmi //将管道传递过来的所有行添加到后面的命令后面，等价于sudo docker rmi &lt;rows&gt;</code></pre>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Bash——命令分隔符</title>
    <url>/Notes/Linux/Bash%E2%80%94%E2%80%94%E5%91%BD%E4%BB%A4%E5%88%86%E9%9A%94%E7%AC%A6.html</url>
    <content><![CDATA[<p><em>在Linux Bash命令中，有<code>;</code>,<code>&amp;&amp;</code>,<code>||</code>三种命令链接方式，本文将介绍他们之间的区别</em></p>
<hr>
<h3 id="三种连接方式的区别"><a href="#三种连接方式的区别" class="headerlink" title="三种连接方式的区别"></a>三种连接方式的区别</h3><ul>
<li><code>;</code><ul>
<li>各个命令之间相互独立，不管前面的语句是否成功执行，所有命令都将被执行</li>
</ul>
</li>
<li><code>&amp;&amp;</code><ul>
<li>命令依次被执行，只有当前面的命令执行<strong>成功</strong>后，后面的命令才会被执行</li>
</ul>
</li>
<li><code>||</code><ul>
<li>命令依次被执行，只有当前面的命令执行<strong>失败</strong>后，后面的命令才会被执行</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux——用户管理</title>
    <url>/Notes/Linux/Linux%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86.html</url>
    <content><![CDATA[<hr>
<h3 id="用某个其他用户身份打开可视化软件"><a href="#用某个其他用户身份打开可视化软件" class="headerlink" title="用某个其他用户身份打开可视化软件"></a>用某个其他用户身份打开可视化软件</h3><p><em>在某个用户不能登录桌面时，可能可以通过进入root用户创建一个新的用户，然后用该用户登录桌面</em></p>
<ul>
<li>不同用户之间的软件可能不共享，所以要想打开其他用户装的软件可以考虑从终端切换到其他用户，然后再打开软件</li>
<li>不同用户的git用户信息等都不同，也需要切换到其他用户使用</li>
<li>为了避免用户权限问题，打开软件时都从终端用文件所属用户的身份打开即可</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Gradient——梯度的理解</title>
    <url>/Notes/Others/Gradient%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E7%9A%84%E7%90%86%E8%A7%A3.html</url>
    <content><![CDATA[<p><strong>梯度方向是函数局部上升最快的方向</strong></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<hr>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>对函数\(f(x, y, z)\)而言</li>
<li>方向导数:各个方向的导数,函数,标量<ul>
<li>关于x, y, x的偏导数: \(\frac {\partial f}{\partial x}, \frac {\partial f}{\partial y}, \frac {\partial f}{\partial z}\)</li>
<li>对特定的某一点\((x_{0}, y_{0}, z_{0})\)而言,改点的偏导数是将对应的值带入后得到的数值,是一个确定的数值</li>
</ul>
</li>
<li>梯度:各个方向导数组成的向量,每一维为一个方向导数,矢量<ul>
<li>梯度: \((\frac {\partial f}{\partial x}, \frac {\partial f}{\partial y}, \frac {\partial f}{\partial z}\)),一个向量</li>
<li>梯度方向表示函数局部上升最快的方向</li>
<li>梯度的模\(|\nabla f|\)表示变化率</li>
</ul>
</li>
</ul>
<h4 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h4><ul>
<li>一维函数中,代表参数(自变量)的点在一条直线上运动,所以梯度的方向有正向和负向</li>
<li>二维函数中,代表参数(自变量)的点在一个平面上运动,所以梯度的方向有无数种,但都在二维平面上</li>
<li>三维函数中,代表参数(自变量)的点在一个空间中运动,所以梯度的方向有无数种,但都在三维空间中</li>
<li>无论何时,梯度方向是函数上升最快的方向,指明参数朝着这个方向移动时,函数上升的速度最快</li>
<li>梯度下降法中,求取梯度的意义就在于知道函数上升或者下降最快的方向</li>
</ul>
<h4 id="关于梯度的模-nabla-f"><a href="#关于梯度的模-nabla-f" class="headerlink" title="关于梯度的模\(|\nabla f|\)"></a>关于梯度的模\(|\nabla f|\)</h4><ul>
<li>梯度的方向与参数(自变量)的变化相关,梯度的模\(|\nabla f|\)与函数变化相关</li>
<li>梯度的模可以理解为变化率,也就是在梯度对应的方向上移动单位长度后,函数能上升的数值大小为\(|\nabla f|\)(当然,这是按照当前点周围的极小的曲面拟合出来的平面计算得到的数值,不是真正的函数数值)</li>
<li>梯度下降法中,梯度的方向指明了参数移动的方向,梯度的模暗示着参数移动时函数的增量大小</li>
<li>但为什么梯度下降法中,保留着梯度的模?而不是将梯度向量变成单位向量?梯度的模有什么特殊的意义吗?<ul>
<li>一个猜想:在梯度下降法时,如果超参数步长\(\alpha\)不变的话,每次迭代时参数(自变量)真正移动的长度是\(\alpha\times |\nabla f|\),此时随着迭代次数的增加,\(|\nabla f|\)值会越来越小(可以证明,因为越来越接近最地点,函数变化率越来越小),这会导致\(\alpha\times |\nabla f|\)越来越小,从而减少震荡?</li>
<li>梯度的模的意义可用数学证明,移动梯度的模的\(\alpha\)倍是最优的,详情参考我的博客<a href="/Notes/ML/ML%E2%80%94%E2%80%94%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95-%E6%97%A0%E7%BA%A6%E6%9D%9F%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html">ML——最优化方法-无约束参数优化方法</a>中用一阶泰勒展开推导梯度下降法的过程<ul>
<li>基本思路,将函数在参数的某个值\(\theta_{t} + \delta\)处一阶泰勒展开,然后对\(\delta\)加上正则项.最后在直接求导即可得到梯度下降法的更新表达式</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h3><ul>
<li>链接: <a href="https://choibunbing.com/2019/04/25/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E6%98%AF%E5%87%BD%E6%95%B0%E5%B1%80%E9%83%A8%E4%B8%8A%E5%8D%87%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/" target="_blank" rel="noopener">https://choibunbing.com/2019/04/25/为什么梯度方向是函数局部上升最快的方向/</a></li>
</ul>
<img src="/Notes/Others/Gradient——梯度的理解/Gradient_Blog_Screenshot.png">

]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>General——各种镜像源的管理</title>
    <url>/Notes/Others/General%E2%80%94%E2%80%94%E5%90%84%E7%A7%8D%E9%95%9C%E5%83%8F%E6%BA%90%E7%9A%84%E7%AE%A1%E7%90%86.html</url>
    <content><![CDATA[<p><em>各种镜像源管理</em></p>
<hr>
<h3 id="pip镜像源"><a href="#pip镜像源" class="headerlink" title="pip镜像源"></a>pip镜像源</h3><p><em>一般来说pip默认使用的源可能会比较慢,此时需要修改成国内的源</em></p>
<h4 id="修改方法"><a href="#修改方法" class="headerlink" title="修改方法:"></a>修改方法:</h4><h5 id="临时修改"><a href="#临时修改" class="headerlink" title="临时修改"></a>临时修改</h5><ul>
<li><p>在命令后面添加如下参数即可将安装源换成阿里云</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-i http://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure>
</li>
<li><p>只在当前命令中修改,以后想要使用时需要继续添加参数</p>
</li>
<li><p>特别说明: 在阿里云的服务器上使用这个指令时效果非常明显</p>
</li>
</ul>
<h5 id="永久修改"><a href="#永久修改" class="headerlink" title="永久修改"></a>永久修改</h5><ul>
<li><p>修改文件<code>~/.pip/pip.conf</code>内容, 如果没有该文件则新建一个</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim ~/.pip/pip.conf</span><br><span class="line"># write the following contents and save</span><br><span class="line"># the example is USTC</span><br><span class="line">[global]</span><br><span class="line">index-url = https://pypi.mirrors.ustc.edu.cn/simple/</span><br></pre></td></tr></table></figure>
</li>
<li><p>这个命令修改当前用户的默认<code>pip</code>命令镜像</p>
</li>
</ul>
<h4 id="镜像列表"><a href="#镜像列表" class="headerlink" title="镜像列表"></a>镜像列表</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">清华：https://pypi.tuna.tsinghua.edu.cn/simple </span><br><span class="line">阿里云：http://mirrors.aliyun.com/pypi/simple/ </span><br><span class="line">中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ </span><br><span class="line">华中科技大学：http://pypi.hustunique.com/ </span><br><span class="line">山东理工大学：http://pypi.sdutlinux.org/ </span><br><span class="line">豆瓣：http://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>

<ul>
<li>其中清华的比较常用</li>
</ul>
<hr>
<h3 id="Ubuntu源"><a href="#Ubuntu源" class="headerlink" title="Ubuntu源"></a>Ubuntu源</h3><p><em>Linux一般默认使用自己系统的源,比如Ubuntu使用的就是自己的Ubuntu官网源</em></p>
<h4 id="修改方法-1"><a href="#修改方法-1" class="headerlink" title="修改方法"></a>修改方法</h4><ul>
<li>修改文件<code>/etc/apt/source.list</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo /vim/apt/source.list</span><br><span class="line"></span><br><span class="line"># write new image source</span><br><span class="line"># the example of aliyun</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line"># update and upgrade</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="镜像源列表"><a href="#镜像源列表" class="headerlink" title="镜像源列表"></a>镜像源列表</h4><h5 id="中科大源"><a href="#中科大源" class="headerlink" title="中科大源"></a>中科大源</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure>

<h5 id="163源"><a href="#163源" class="headerlink" title="163源"></a>163源</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure>

<h5 id="清华源"><a href="#清华源" class="headerlink" title="清华源"></a>清华源</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Centos源"><a href="#Centos源" class="headerlink" title="Centos源"></a>Centos源</h3><h4 id="修改方法-2"><a href="#修改方法-2" class="headerlink" title="修改方法"></a>修改方法</h4><ul>
<li>修改文件<code>/etc/yum.repos.d/CentOS-Base.repo</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># backup</span><br><span class="line">sudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak</span><br><span class="line"># create new config file</span><br><span class="line"># the example of aliyun, the config file could be downloaded from remote directly</span><br><span class="line">sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"># update chche</span><br><span class="line">sudo yum makechche</span><br><span class="line"># update</span><br><span class="line">sudo -y update</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="镜像源列表-1"><a href="#镜像源列表-1" class="headerlink" title="镜像源列表"></a>镜像源列表</h4><h5 id="网易源"><a href="#网易源" class="headerlink" title="网易源"></a>网易源</h5><ul>
<li>Centos6: <a href="http://mirrors.163.com/.help/CentOS6-Base-163.repo" target="_blank" rel="noopener">http://mirrors.163.com/.help/CentOS6-Base-163.repo</a></li>
<li>Centos7: <a href="http://mirrors.163.com/.help/CentOS7-Base-163.repo" target="_blank" rel="noopener">http://mirrors.163.com/.help/CentOS7-Base-163.repo</a></li>
</ul>
<h5 id="阿里源"><a href="#阿里源" class="headerlink" title="阿里源"></a>阿里源</h5><ul>
<li>Centos6: <a href="http://mirrors.aliyun.com/repo/Centos-6.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/repo/Centos-6.repo</a></li>
<li>Centos7: <a href="http://mirrors.aliyun.com/repo/Centos-7.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/repo/Centos-7.repo</a></li>
</ul>
<hr>
<h3 id="Mac-brew源"><a href="#Mac-brew源" class="headerlink" title="Mac brew源"></a>Mac brew源</h3><ul>
<li>参考链接：(Mac 下 brew 切换为国内源)[<a href="https://cloud.tencent.com/developer/article/1614039]" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1614039]</a></li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Git——GitHub-Pages的CNAME设置</title>
    <url>/Notes/Git/Git%E2%80%94%E2%80%94GitHub-Pages%E7%9A%84CNAME%E8%AE%BE%E7%BD%AE.html</url>
    <content><![CDATA[<hr>
<h3 id="CNAME是什么"><a href="#CNAME是什么" class="headerlink" title="CNAME是什么"></a>CNAME是什么</h3><ul>
<li>CNAME是DNS解析中的一种别名记录，允许同一个网站有两个不同的记录解析过去</li>
<li>CNAME在GitHub Pages中体现为可以将xxx.github.io的域名添加别名为自定义的域名（这个域名通常是自己买的，在域名那里也要配置解析到当前xxx.github.io）</li>
</ul>
<hr>
<h3 id="GitHub-Pages中使用CNAME时遇到的问题"><a href="#GitHub-Pages中使用CNAME时遇到的问题" class="headerlink" title="GitHub Pages中使用CNAME时遇到的问题"></a>GitHub Pages中使用CNAME时遇到的问题</h3><ul>
<li>我们可以在GitHub账户中设置CNAME</li>
<li>重新提交commit并推送数据到GitHub上后，之前设置的CNAME会被删除，需要重新设置，也就是说每次push操作后都需要重新设置<ul>
<li>这个问题在<code>hexo d</code>后也会出现</li>
</ul>
</li>
</ul>
<hr>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul>
<li>在当前项目的GitHub Pages对应的分支下面新建一个文件，命名为<code>CNAME</code></li>
<li>打开文件并新建一行为目标网站的域名（如果是中文域名则需要编码转换）</li>
<li>在Hexo中，我们可以在<code>source</code>文件夹下面新建该文件<ul>
<li><code>hexo generate</code>时会将<code>source</code>文件夹下面的文件都拷贝到<code>public</code>文件夹下面</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git——使用总结</title>
    <url>/Notes/Git/Git%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p><em>本文总结一些可能误解或者需要注意的指令用法</em></p>
<ul>
<li>值得参考的博客：<a href="https://segmentfault.com/a/1190000008617626" target="_blank" rel="noopener">https://segmentfault.com/a/1190000008617626</a></li>
<li>更详细解释一般直接查看<code>git (command) -h</code>即可</li>
</ul>
<hr>
<h3 id="总体概况总结"><a href="#总体概况总结" class="headerlink" title="总体概况总结"></a>总体概况总结</h3><img src="/Notes/Git/Git——使用总结/git_overview.png">



<hr>
<h3 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h3><ul>
<li><p>查看本地分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看所有分支(包括远程)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch -a</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除本地分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch -d branch_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除远程分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git push origin :branch_name</span><br></pre></td></tr></table></figure>

<ul>
<li>将空的东西推送到远程的branch_name分支，也就是删除远程分支</li>
</ul>
</li>
<li><p>新建本地分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git fetch origin master:branch_name</span><br></pre></td></tr></table></figure>

<ul>
<li><p>fetch将远程分支拉取到本地的Repository中，但不修改本地工作目录，如果本地分支不存在，则新建分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull origin master:branch_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>fetch将远程分支拉取到本地的Repository中，同时修改本地工作目录，如果本地分支不存在，则新建分支</p>
</li>
</ul>
</li>
<li><p>新建远程分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git push origin master:branch_name</span><br></pre></td></tr></table></figure>

<ul>
<li>将本地分支推送到远程，如果branch_name不存在，则新建分支</li>
</ul>
</li>
</ul>
<hr>
<h3 id="冲突管理"><a href="#冲突管理" class="headerlink" title="冲突管理"></a>冲突管理</h3><h4 id="远程分支和本地分支有不同的commit"><a href="#远程分支和本地分支有不同的commit" class="headerlink" title="远程分支和本地分支有不同的commit"></a>远程分支和本地分支有不同的commit</h4><ul>
<li><p><code>git pull</code>和<code>git push</code>均产生<code>reject</code>异常</p>
</li>
<li><p>使用下面语句拉取远程分支到本地old分支并合并</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git fetch origin master:old</span><br><span class="line">git merge old</span><br></pre></td></tr></table></figure>

<ul>
<li>上面两句等价于<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull origin master:old</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>如果能够快速合并，也就是相同文件没有同时被不同提交修改：</p>
<ul>
<li>上面的语句将弹出一个合并窗口提示输入合并这个操作(提交)的Comment，按照提示提交保存即可</li>
<li>保存后自动生成一个以刚才的Comment命名的提交</li>
</ul>
</li>
<li><p>不能快速合并时</p>
<ul>
<li>上面的语句会提示我们哪些文件有合并冲突需要解决的</li>
<li>我们需要根据提示找到并修改文件中冲突</li>
<li>然后重新提交(像正常提交代码一样即可)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;  &quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>合并完成后删除多余分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch -d old</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="fetch和pull"><a href="#fetch和pull" class="headerlink" title="fetch和pull"></a>fetch和pull</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull == git fetch + git merge</span><br><span class="line">git pull origin master:old == git fetch origin master:old + git merge old</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h3><ul>
<li><code>git diff A B</code><ul>
<li>基于A查看B有何变化</li>
<li>显示时会自动识别为基于<code>a/A</code>看<code>b/B</code>有何变化</li>
</ul>
</li>
</ul>
<h3 id="文件恢复"><a href="#文件恢复" class="headerlink" title="文件恢复"></a>文件恢复</h3><h4 id="恢复本地缓冲区文件到disk"><a href="#恢复本地缓冲区文件到disk" class="headerlink" title="恢复本地缓冲区文件到disk"></a>恢复本地缓冲区文件到disk</h4><ul>
<li>恢复某个文件：<code>git checkout -- xx/xx.py</code></li>
<li>恢复某个文件夹下所有文件：<code>git checkout -- xx/*</code></li>
</ul>
<h4 id="恢复HEAD到缓冲区"><a href="#恢复HEAD到缓冲区" class="headerlink" title="恢复HEAD到缓冲区"></a>恢复HEAD到缓冲区</h4><ul>
<li>仅恢复到缓冲区：<code>git reset</code> or <code>git reset HEAD</code></li>
<li>不仅恢复到缓冲区，同时恢复到disk： <code>git reset HEAD --hard</code></li>
</ul>
<h3 id="从远程拉取一个新分支"><a href="#从远程拉取一个新分支" class="headerlink" title="从远程拉取一个新分支"></a>从远程拉取一个新分支</h3><ul>
<li>从远程拉取分支到本地：<code>git fetch origin xxx</code><ul>
<li>此时使用<code>git branch -a</code>可以看到远程分支引用在本地</li>
<li><code>git fetch origin xxx</code>后，<code>FETCH_HEAD</code>会指向远程<code>xxx</code>分支<ul>
<li><code>FETCH_HEAD</code>是个临时的引用，可以对该分支做任意想做的操作，比如此时可用<code>git merge FETCH_HEAD</code>来合并新拉取的分支</li>
</ul>
</li>
</ul>
</li>
<li>创建本地同名新分支：<code>git checkout xxx</code><ul>
<li>必须同名</li>
</ul>
</li>
</ul>
<h3 id="git-clone优化"><a href="#git-clone优化" class="headerlink" title="git clone优化"></a>git clone优化</h3><ul>
<li><code>git clone ssh:xxx</code>默认拉取master分支</li>
<li><code>git clone -b yyy ssh:xxx</code>拉取yyy分支</li>
</ul>
<h3 id="删除本地跟踪远程分支"><a href="#删除本地跟踪远程分支" class="headerlink" title="删除本地跟踪远程分支"></a>删除本地跟踪远程分支</h3><ul>
<li><code>git branch -d --remotes origin/xxx</code><ul>
<li>测试发现：git 1.7.1会报错，git 2.24.3没问题</li>
</ul>
</li>
</ul>
<h3 id="git-pull-和-git-push-默认分支设定"><a href="#git-pull-和-git-push-默认分支设定" class="headerlink" title="git pull 和 git push 默认分支设定"></a>git pull 和 git push 默认分支设定</h3><ul>
<li><code>git branch --set-upstream-to=origin/xxx xxx</code><ul>
<li>经测试：git 1.7.1会报错，git 2.24.3没问题（但git pull不会生效）</li>
</ul>
</li>
</ul>
<h3 id="添加文件到-gitignore"><a href="#添加文件到-gitignore" class="headerlink" title="添加文件到.gitignore"></a>添加文件到.gitignore</h3><ul>
<li>如果文件已经被添加到Git仓库中（常常出现在一些不规范的项目中），则可以考虑使用以下步骤解决：<ul>
<li>假设Git仓库中已经把<code>.DS_Store</code>添加到Git仓库中</li>
<li>首先拉取项目并在<code>.gitignore</code>中添加<code>*.DS_Store</code></li>
<li>执行<code>git rm --cached *.DS_Store</code>, 从缓冲区中删除所有<code>*.DS_Store</code>文件，但保留本地文件</li>
<li>执行<code>git add .</code>并重新提交</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>ACM——刷题时应该注意的一些关键问题</title>
    <url>/Notes/Others/ACM%E2%80%94%E2%80%94%E5%88%B7%E9%A2%98%E6%97%B6%E5%BA%94%E8%AF%A5%E6%B3%A8%E6%84%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%85%B3%E9%94%AE%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<p><em>持续更新</em></p>
<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<hr>
<h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><h4 id="用移位代替乘除法"><a href="#用移位代替乘除法" class="headerlink" title="用移位代替乘除法"></a>用移位代替乘除法</h4><ul>
<li>移位的效率远高于乘除法</li>
<li>除以2时一般直接右移一位即可</li>
<li>乘以2时一般直接左移一位即可</li>
</ul>
<h4 id="用位运算判断奇偶性"><a href="#用位运算判断奇偶性" class="headerlink" title="用位运算判断奇偶性"></a>用位运算判断奇偶性</h4><ul>
<li>将整数与1做与运算,值为1表示为奇数,为0表示为偶数</li>
</ul>
<h4 id="判断二进制数中1的个数"><a href="#判断二进制数中1的个数" class="headerlink" title="判断二进制数中1的个数"></a>判断二进制数中1的个数</h4><h5 id="方法一-n-n-1-amp-n"><a href="#方法一-n-n-1-amp-n" class="headerlink" title="方法一: n=(n-1)&amp;n"></a>方法一: n=(n-1)&amp;n</h5><ul>
<li><code>n = (n-1) &amp; n</code> 可以将二进制数n中的最后一位1变成0</li>
<li>不停执行该语句,知道n为0,执行次数即二进制数n中1的数量</li>
</ul>
<h5 id="方法二-result-n-amp-flag"><a href="#方法二-result-n-amp-flag" class="headerlink" title="方法二: result=n&amp;flag"></a>方法二: result=n&amp;flag</h5><ul>
<li>flag初始化为与n相同的类型(二进制位数一致)</li>
<li>flag从1,2,4….,为2的次方数,即初始化为1,每次向左移动一位,直到flag为0为止(此时说明flag移位溢出了,n的每一位都进行过比较了)</li>
<li>累计result为1的次数就是二进制数n中1的数量</li>
</ul>
<h5 id="错误方法-result-n-amp-1"><a href="#错误方法-result-n-amp-1" class="headerlink" title="错误方法: result=n&amp;1"></a>错误方法: result=n&amp;1</h5><ul>
<li>这种方法不设置flag变量,每次将n左移一位,直到n为0</li>
<li>当n为负数时会引发无线循环,所以此方法是错的</li>
</ul>
<hr>
<h3 id="小数的精度"><a href="#小数的精度" class="headerlink" title="小数的精度"></a>小数的精度</h3><h4 id="比较两个小数是否相等"><a href="#比较两个小数是否相等" class="headerlink" title="比较两个小数是否相等"></a>比较两个小数是否相等</h4><ul>
<li>无论a,b类型是否相等(float, double),都不能直接使用<code>==</code>符号直接比较</li>
<li>定义以下函数实现比较即可|a-b| &lt; \(\epsilon\) (\(\epsilon\)是一个误差允许的极小值)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bool Equal(double num1, double, num2)&#123;</span><br><span class="line">	if(num1-num2 &lt; 0.0000001 &amp;&amp; num1-num2 &gt; -0.0000001)</span><br><span class="line">		return true;</span><br><span class="line">	return flase;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="为什么存在精度问题"><a href="#为什么存在精度问题" class="headerlink" title="为什么存在精度问题"></a>为什么存在精度问题</h4><ul>
<li><p>因为十进制的小数存到内存时是二进制,此时有很多无线循环,但是二进制位数有限,所以有误差</p>
</li>
<li><p>一般来说,<code>double</code>类型足以满足我们日常计算的精度,在无需很高精度时使用<code>float</code>可以减少内存占用</p>
</li>
<li><p>实例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">double a = 0.9;</span><br><span class="line">float b = 0.9;</span><br><span class="line">if(a == (double)b)</span><br><span class="line">	printf(&quot;a==b is true&quot;)</span><br><span class="line">else</span><br><span class="line">	printf(&quot;a==b is flase&quot;)</span><br><span class="line"># output:</span><br><span class="line">a==b is false</span><br></pre></td></tr></table></figure>

<ul>
<li>对于小数<code>0.9</code>,表示为二进制后小数部分是无线循环小数,在<code>float</code>类型时与<code>double</code>类型时由于保留的小数位不同,所以值不同</li>
</ul>
</li>
</ul>
<hr>
<h3 id="最大最小整数"><a href="#最大最小整数" class="headerlink" title="最大最小整数"></a>最大最小整数</h3><h4 id="最小负数"><a href="#最小负数" class="headerlink" title="最小负数"></a>最小负数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># int</span><br><span class="line">int INT_MIN = 0x80000000; //32位</span><br><span class="line"># long</span><br><span class="line">long LONG_MIN = 0x8000000000000000; //64位</span><br></pre></td></tr></table></figure>

<h4 id="最大正数"><a href="#最大正数" class="headerlink" title="最大正数"></a>最大正数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># int</span><br><span class="line">int INT_MAX = 0x7FFFFFFF; //32位</span><br><span class="line"># long </span><br><span class="line">long LONG_MAX = 0x7FFFFFFFFFFFFFFF; //64位</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="第一要务-输入检查"><a href="#第一要务-输入检查" class="headerlink" title="第一要务:输入检查"></a>第一要务:输入检查</h3><ul>
<li>无论何时,函数实现的第一步都应该是检查输入是否合法</li>
</ul>
<h4 id="常见的检查"><a href="#常见的检查" class="headerlink" title="常见的检查"></a>常见的检查</h4><ul>
<li>指针:是否为空</li>
<li>整数:是否满足条件(大于0,小于0等)</li>
<li>多个整数之间的关系:是否满足大小关系等</li>
</ul>
<hr>
<h3 id="访问数组时必须首先确认"><a href="#访问数组时必须首先确认" class="headerlink" title="访问数组时必须首先确认"></a>访问数组时必须首先确认</h3><ul>
<li>数组已初始化</li>
<li>访问的Index没有超过数组</li>
</ul>
<hr>
<h3 id="关于折半搜索和查找"><a href="#关于折半搜索和查找" class="headerlink" title="关于折半搜索和查找"></a>关于折半搜索和查找</h3><ul>
<li>一般来说low从0开始,high从数组长度开始(注意，有时候high如果从len-1开始则可能造成无法访问到最后一个位置？)<ul>
<li>相关题目：LeetCode 35题 Search Insert Position</li>
</ul>
</li>
</ul>
<h4 id="mid定义为-low-high-2"><a href="#mid定义为-low-high-2" class="headerlink" title="mid定义为(low + high)/2"></a>mid定义为(low + high)/2</h4><ul>
<li><p>如果<code>mid</code>的定义如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mid = (low + high) / 2</span><br></pre></td></tr></table></figure>
</li>
<li><p>那么每次查找结束时更新时应该作如下更新</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid + 1</span><br><span class="line">high = mid</span><br></pre></td></tr></table></figure>
</li>
<li><p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid + 1</span><br><span class="line">high = mid - 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果此时使用下面的语句作为更新</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid</span><br></pre></td></tr></table></figure>

<ul>
<li><code>low == high - 1</code>有<code>low == mid</code>,上面的式子将造成死循环</li>
</ul>
</li>
</ul>
<h4 id="mid定义为-low-high-1-2"><a href="#mid定义为-low-high-1-2" class="headerlink" title="mid定义为(low + high + 1)/2"></a>mid定义为(low + high + 1)/2</h4><ul>
<li><p>如果<code>mid</code>的定义如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mid = (low + high + 1) / 2</span><br></pre></td></tr></table></figure>
</li>
<li><p>那么每次查找结束时更新时应该作如下更新</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid</span><br><span class="line">high = mid - 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid + 1</span><br><span class="line">high = mid - 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果使用下面的语句作为更新</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">high = mid</span><br></pre></td></tr></table></figure>

<ul>
<li><code>low == high - 1</code>有<code>high == mid</code>,上面的式子将造成死循环</li>
</ul>
</li>
<li><p>二分查找相等的数时往往可以使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid + 1</span><br><span class="line">high = mid - 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>但是查找的是当前数组不存在的节点的存放位置时要注意查找时</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low = mid + 1</span><br><span class="line">high = mid</span><br></pre></td></tr></table></figure>

<ul>
<li>这样确保i和j退出时相等</li>
<li>相关题目：LeetCode 35 Search Insert Position</li>
</ul>
</li>
</ul>
<h4 id="相关题目"><a href="#相关题目" class="headerlink" title="相关题目"></a>相关题目</h4><ul>
<li>详细情况参考LeetCode 34题 Find First and Last Position of Element in Sorted Array</li>
</ul>
<hr>
<h3 id="深度优先遍历和回溯法的异同"><a href="#深度优先遍历和回溯法的异同" class="headerlink" title="深度优先遍历和回溯法的异同"></a>深度优先遍历和回溯法的异同</h3><ul>
<li><p>深度优先方法(DFS, Depth First Search)使用栈实现,广度优先方法(BFS, Breadth First Search)使用队列实现</p>
</li>
<li><p>一般来说在树中遍历时才有深度优先搜索这种说法，不然比如数组的全部组合遍历都叫做回溯法，而不是DFS</p>
</li>
<li><p>对于树而言，回溯法也是遵循深度优先遍历原则实现的</p>
</li>
<li><p>回溯法可以访问已经访问过的节点，而深度优先一般来说不会再访问已经访问过的节点</p>
</li>
<li><p>DFS是把所有可能的情况考虑到就行了，但回溯法可能会重视访问节点的顺序[LeetCode 79 Word Search]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def solution(nums):</span><br><span class="line">	path, result = [], []</span><br><span class="line">	* 如果nums含有重复元素，需要重新排序</span><br><span class="line">	def backtrace(index):</span><br><span class="line">	    if 满足存储条件:</span><br><span class="line">	    	result.append(path[:])</span><br><span class="line">		if 不需要继续迭代:</span><br><span class="line">			return</span><br><span class="line">	    * 迭代访问内部节点(不同情况访问方式不同，详情看下面)</span><br><span class="line">	    * 一种简单的可能是</span><br><span class="line">	    for i in range(*视具体情况变化):</span><br><span class="line">	    	* 如果nums含有重复元素，需要判断是否跳过(如果nums当前节点与上一个节点重复，则跳过)</span><br><span class="line">	    	path.append(nums[i])</span><br><span class="line">	    	* 不同情况不同回溯方式</span><br><span class="line">	    		* 当path里面不能重复访问同一元素时</span><br><span class="line">	    			* backtrace(i+1)</span><br><span class="line">	    		* 当path里面能重复访问同一元素时</span><br><span class="line">	    			* backtrace(i)</span><br><span class="line">	    	path.pop()</span><br><span class="line">	backtrace(0)</span><br><span class="line">	return result</span><br></pre></td></tr></table></figure>
</li>
<li><p>特殊情况，如果path不能访问同一元素而且不同顺序相同元素算是不同路径的时候，我们可能必须使用rest对象在迭代中记录当前未被访问的对象</p>
<ul>
<li>此时由于不同顺序相同元素算是不同路径，所以我们不能用每次只能向前访问的方法来实现去除重复(及[index:], 这种每次只能向前访问的方法会将先[a, b], [b, a]视为重复而去除，只保留[a, b]当(a &lt; b时))</li>
</ul>
</li>
</ul>
<h4 id="关于递归函数内部的节点访问方式"><a href="#关于递归函数内部的节点访问方式" class="headerlink" title="关于递归函数内部的节点访问方式"></a>关于递归函数内部的节点访问方式</h4><p><strong>其实在使用回溯法访问一个数组时，可以考虑这个我们在访问一棵树，从空节点开始，然后每次迭代所有符合的子节点</strong></p>
<p>| 路径是否可以访问同一节点多次 | 顺序不同节点相同是否算不同路径 | 原始数组中是否包含重复元素 | 预处理 |每一层迭代代码 |<br>| :——: | :——: | :——: | :——: |<br>| 是 | 是 | 是 | data.sort() | for i in [:]: <br> &ensp; if与前一个相等: <br> &ensp; &ensp; continue <br> &ensp; visit(i) <br> &ensp; backtrace() <br> 最后移除重复路径|<br>| 是 | 是 | 否 |      -         | for i in [:]: <br> &ensp; visit(i) <br> &ensp; backtrace() [待更新]|<br>| 是 | 否 | 是 | data.sort() | for i in [index:]: <br> &ensp; if与前一个相等: <br> &ensp; &ensp; continue <br> &ensp; visit(i) <br> &ensp; backtrace(i) <br> 最后移除重复路径|<br>| 是 | 否 | 否 |      -         | for i in [index:]: <br> &ensp; visit(i) <br> &ensp; backtrace(i)|<br>| 否 | 是 | 是 | data.sort() <br> rest存储剩余元素| for i in rest: <br> &ensp; if与前一个相等: <br> &ensp; &ensp; continue <br> &ensp; visit(i) <br> &ensp; backtrace(rest-i)|<br>| 否 | 是 | 否 | rest存储剩余元素 | for i in rest: <br> &ensp; visit(i) <br> &ensp; backtrace(rest-i) |<br>| 否 | 否 | 是 | data.sort() | for i in [index:]: <br> &ensp; if与前一个相等: <br> &ensp; &ensp; continue <br> &ensp; visit(i) <br> &ensp; backtrace(i+1) |<br>| 否 | 否 | 否 |      -         | for i in [index:]: <br> &ensp; visit(i) <br> &ensp; backtrace(i+1)|</p>
<h5 id="相关理解"><a href="#相关理解" class="headerlink" title="相关理解"></a>相关理解</h5><ul>
<li><p>当节点相同顺序不同是相同路径时，使用每次迭代只向后而不向前访问的方式可以避免重复(因为先a后b,与先b后a重复)</p>
<ul>
<li>路径可含有重复节点时，向后迭代从当前节点开始，包括当前节点backtrace(i)</li>
<li>路径不可包含重复节点时，向后迭代从当前节点的下一个开始，不包括当前节点backtrace(i+1)</li>
</ul>
</li>
<li><p>当路径不可包含重复节点时，还可使用rest存储尚未访问的节点或每次迭代时只向后即可解决问题</p>
</li>
<li><p>当数组中包含重复节点时，可以现将其排序，然后在迭代时跳过与前一个节点相同的节点</p>
</li>
<li><p>一个万能的写法是：</p>
<ul>
<li>每次迭代访问所有节点</li>
<li>将所有符合条件的路径加入结果</li>
<li>最后将重复路径删除即可</li>
</ul>
</li>
<li><p>1，3这两种情况实际中很少出现，我们不能提前用某种算法确保加入的数据不重复,这时需要我们最终从result中去除重复的(或者每次都查看result中是否有与当前路径相同的路径)</p>
<ul>
<li>第1，3两种情况中，即使我们保证访问顺序完全相同的路径不会重复出现，但是由于数组中可能有重复值,且每个元素可能被访问多次<ul>
<li>比如[2,1,1]中，容易造成同一个元素a[1]被访问两次的到的[1,1]路径与两个元素a[1],a[2]分别被访问一次得到的[1,1]路径结果完全相同，此时我们只能通过最终手段保证返回结果中路径的唯一性</li>
</ul>
</li>
<li>保证路径的唯一性在Python中可以使用将路径转换为<code>tuple(path)</code>的方式实现路径的比较,且可以放入<code>set</code>中</li>
<li>第1, 3两种情况的不同之处在于第1种情况中不同顺序的路径是不同的,所以无需对路径进行排序,第3种情况中不同顺序相同结点的路径也是相同结点,则需要先对路径进行排序,再转换为tuple才行</li>
</ul>
</li>
<li><p>第4种情况对应LeetCode 39 Combination Sum</p>
</li>
<li><p>第7种情况对应LeetCode 40 Combination Sum II</p>
</li>
<li><p>第8种情况对应LeetCode 216 Combination Sum III</p>
</li>
<li><p>第5种情况对应LeetCode 47 Permutations II</p>
</li>
</ul>
<hr>
<h3 id="对列表中数字列表的排序"><a href="#对列表中数字列表的排序" class="headerlink" title="对列表中数字列表的排序"></a>对列表中数字列表的排序</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">List[List[number]]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>找出所有列表的所有数中最大的数<code>M</code></p>
</li>
<li><p>定义一个函数,用一个<code>M+1</code>进制的数对每个内层列表进行映射表示</p>
<ul>
<li><p>系数:</p>
<ul>
<li>需要递增排序的位前面系数为<code>1</code></li>
<li>需要递减排序的位前面系数为<code>-1</code></li>
<li>无需排序的位前面系数为<code>0</code></li>
</ul>
</li>
<li><p>示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def key(x):</span><br><span class="line">	return x[0]*(M+1)**3 - x[1]*(M+1)**2 - x[2]*(M+1) + x[3]</span><br></pre></td></tr></table></figure>

<ul>
<li>对第一和第四位按照递增排序,第二和第三位按照递减排序</li>
</ul>
</li>
</ul>
</li>
<li><p>映射函数定义后可以使用<code>sort</code>函数排序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l.sort(key=key)</span><br></pre></td></tr></table></figure>
</li>
<li><p>也可以使用<code>lambda</code>函数定义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l.sort(key=lambda x: x[0]*(M+1)**3 - x[1]*(M+1)**2 - x[2]*(M+1) + x[3])</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="使用-memo-存储中间结果"><a href="#使用-memo-存储中间结果" class="headerlink" title="使用 memo 存储中间结果"></a>使用 memo 存储中间结果</h3><p><em>memo可理解为备忘录,但是和传统的备忘录设计模式有一定区别,后者的目标是恢复对象之前的某个状态,前者的目标更应该理解为动态规划的思想dp存储计算中间值</em></p>
<ul>
<li>对某些可能会多次被求解的子空间(子数组,子集等), 且子空间能够独立的情况<ul>
<li>这种题目对应一般可以使用DFS求解,但是由于子空间能够独立,使用DFS会多次求解子空间,从而产生超时的情况</li>
<li>子空间能够独立的就应该使用memo</li>
</ul>
</li>
<li>如果能够唯一标识(能作为字典的Key值)这些空间数据,即可使用memo存储</li>
<li>memo 为一个已子空间唯一标识为 Key, 子空间的对应求解结果为Value</li>
<li>相关题目: LeetCode 140, Word Break II</li>
</ul>
<hr>
<h3 id="在使用特殊字符节省空间"><a href="#在使用特殊字符节省空间" class="headerlink" title="在使用特殊字符节省空间"></a>在使用特殊字符节省空间</h3><ul>
<li>在使用DFS搜索空间中的路径时,往往需要记录访问过的点,不能重复访问</li>
<li>常用的做法是开辟一个新的相同大小的visited数组,记录是否访问过当前结点</li>
<li>一种更好的做法是每次记录当前结点的值, 然后修改为一个特殊字符, DFS 完成后恢复成原始字符, 这种做法无需开辟新的空间</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven——maven包管理问题</title>
    <url>/Notes/Others/Maven%E2%80%94%E2%80%94maven%E5%8C%85%E7%AE%A1%E7%90%86%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<hr>
<h3 id="Maven配置"><a href="#Maven配置" class="headerlink" title="Maven配置"></a>Maven配置</h3><ul>
<li>maven默认配置路径为~/.m2/settings.xml,</li>
<li>默认项目路径为~/.m2/repository</li>
<li>maven包版本号检索:<a href="https://mvnrepository.com/" target="_blank" rel="noopener">https://mvnrepository.com/</a></li>
</ul>
<hr>
<h3 id="Maven不能下载的包"><a href="#Maven不能下载的包" class="headerlink" title="Maven不能下载的包"></a>Maven不能下载的包</h3><p><em>某些包无法通过maven自动下载</em></p>
<ul>
<li>即使他们在<a href="https://mvnrepository.com/" target="_blank" rel="noopener">https://mvnrepository.com/</a> 中能搜到对应版本号</li>
<li>此时一种解决方案是下载包,对应在~/.m2/repository路径下创建包名和版本号相对应的文件夹,并且拷贝jar到对应的路径下<ul>
<li>下面是一个例子<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 包信息</span><br><span class="line">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.1.6.RELEASE&lt;/version&gt;</span><br><span class="line"># 存储到本地</span><br><span class="line">	~/.m2/repository/org/springframework/boot/spring-boot-starter-jdbc/2.1.6.RELEASE/spring-boot-starter-jdbc-2.1.6.RELEASE.jar</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven——maven包管理问题</title>
    <url>/Notes/Others/Maven%E2%80%94%E2%80%94maven%E5%9F%BA%E7%A1%80.html</url>
    <content><![CDATA[<hr>
<h3 id="Maven基础"><a href="#Maven基础" class="headerlink" title="Maven基础"></a>Maven基础</h3><ul>
<li>参考链接：<a href="https://blog.csdn.net/qq_14958051/article/details/124601351" target="_blank" rel="noopener">Maven 最全教程，看了必懂，99% 的人都收藏了！</a></li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>软件安装——opencv安装</title>
    <url>/Notes/Others/Web%E2%80%94%E2%80%94opencv%E5%AE%89%E8%A3%85.html</url>
    <content><![CDATA[<hr>
<h3 id="与Python的关系"><a href="#与Python的关系" class="headerlink" title="与Python的关系"></a>与Python的关系</h3><ul>
<li><p>opencv的兼容性很差，不同Python版本对应不同的opencv需要对应安装，否则会卡在最后出现错误（卡很久）</p>
<ul>
<li><p>比如Python 3.6的opencv-python版本安装为<code>4.5.4.60</code><a href="https://blog.csdn.net/longshaonihaoa/article/details/133301110" target="_blank" rel="noopener">参考链接</a> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install opencv-contrib-python==4.5.4.60</span><br><span class="line">pip install opencv-python==4.5.4.60</span><br></pre></td></tr></table></figure>
</li>
<li><p>即使是一些项目配置好依赖的时候，往往也要自己手动指定版本号安装opencv，否则不能成功 </p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="使用homebrew安装依赖"><a href="#使用homebrew安装依赖" class="headerlink" title="使用homebrew安装依赖"></a>使用homebrew安装依赖</h3><ul>
<li>homebrew安装一些依赖时，一些包可能会特别慢（特别是gcc等），可以选择一个个分别安装，反而能加快速度，否则可能出现下载失败导致需要全部重新安装的尴尬情况<ul>
<li>有时候不用VPN会更快，可尝试</li>
</ul>
</li>
</ul>
<hr>
<h3 id="切换国内镜像"><a href="#切换国内镜像" class="headerlink" title="切换国内镜像"></a>切换国内镜像</h3><ul>
<li>有时候能让homebrew下载更快<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &quot;$(brew --repo)&quot;                                                                          </span><br><span class="line">git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git</span><br><span class="line"></span><br><span class="line">cd &quot;$(brew --repo)/Library/Taps/homebrew/homebrew-core&quot;</span><br><span class="line">git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git</span><br><span class="line"></span><br><span class="line">cd &quot;$(brew --repo)&quot;/Library/Taps/homebrew/homebrew-cask</span><br><span class="line">git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git</span><br><span class="line"></span><br><span class="line">brew update</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="githubusercontent访问失败问题解决"><a href="#githubusercontent访问失败问题解决" class="headerlink" title="githubusercontent访问失败问题解决"></a>githubusercontent访问失败问题解决</h3><h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><ul>
<li><p>在安装文件时出现错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused</span><br><span class="line">Error: opencv: Failed to download resource &quot;cmake.rb&quot;</span><br><span class="line">Download failed: https://raw.githubusercontent.com/Homebrew/homebrew-core/82f2aac1cfd7295db3e59240729e5f9d74b0ec51/Formula/c/cmake.rb</span><br></pre></td></tr></table></figure>
</li>
<li><p>经尝试，手动打开链接<code>https://raw.githubusercontent.com/Homebrew/homebrew-core/82f2aac1cfd7295db3e59240729e5f9d74b0ec51/Formula/c/cmake.rb</code>也不行</p>
</li>
</ul>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li>修改DNS为8.8.8.8即可<ul>
<li>该修改可能造成一些网站不能访问，慎用（用后即使修改）</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>Others</tag>
      </tags>
  </entry>
  <entry>
    <title>Backup——data-backup</title>
    <url>/Backup/Backup.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
